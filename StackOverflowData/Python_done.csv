,QuestionId,AnswerId,ViewCount,Title,AnswerCount,Tags,Score,Body,OwnerUserId,Reputation,QuestionDate,AnswerDate,Interval,User_Reputation,Bronze_badge,Gold_badge,Silver_badge,Accept_rate,Remaining User Info,Processed Body,Code Final,LOC,Question_Length,Count Imgs,Count Urls,Response
0,49908182,49911616,20399,"How to make ""keyword-only"" fields with dataclasses?",2,<python><keyword-argument><python-3.7><python-dataclasses>,51,"<p><a href=""https://www.python.org/dev/peps/pep-3102/"" rel=""noreferrer"">Since 3.0</a> there is support to make an argument keyword only:</p>

<pre><code>class S3Obj:
    def __init__(self, bucket, key, *, storage_class='Standard'):
        self.bucket = bucket
        self.key = key
        self.storage_class = storage_class
</code></pre>

<p>How to get that kind of signature using <a href=""https://www.python.org/dev/peps/pep-0557/"" rel=""noreferrer"">dataclasses</a>?  Something like this, but preferably without the <code>SyntaxError</code>:</p>

<pre><code>@dataclass
class S3Obj:
    bucket: str
    key: str
    *
    storage_class: str = 'Standard'
</code></pre>

<p>Ideally declarative, but using the <code>__post_init__</code> hook and/or a replacement class decorator is fine too - as long as the code is reusable.</p>

<p><em>Edit:</em> maybe something like this syntax, using an ellipsis literal</p>

<pre><code>@mydataclass
class S3Obj:
    bucket: str
    key: str
    ...
    storage_class: str = 'Standard'
</code></pre>
",674039,348162,18-04-2018 20:05,19-04-2018 02:07,1,348698,767,105,631,94,"{'badge_counts': {'bronze': 767, 'silver': 631, 'gold': 105}, 'account_id': 342731, 'is_employee': False, 'last_modified_date': 1710872103, 'last_access_date': 1711143082, 'reputation_change_year': 4383, 'reputation_change_quarter': 4383, 'reputation_change_month': 1127, 'reputation_change_week': 227, 'reputation_change_day': 10, 'reputation': 348698, 'creation_date': 1300923627, 'user_type': 'registered', 'user_id': 674039, 'accept_rate': 94, 'location': 'ℂ&#120153;&#120154;&#120148;&#120146;&#120152;&#120160;, &#120128;&#120131;', 'website_url': 'http://www.wimglenn.com', 'link': 'https://stackoverflow.com/users/674039/wim', 'profile_image': 'https://i.stack.imgur.com/leoFi.gif?s=256&g=1', 'display_name': 'wim'}","Since 3.0 there is support to make an argument keyword only: How to get that kind of signature using dataclasses? Something like this, but preferably without the : Ideally declarative, but using the hook and/or a replacement class decorator is fine too - as long as the code is reusable. Edit: maybe something like this syntax, using an ellipsis literal","class S3Obj:
    def __init__(self, bucket, key, *, storage_class='Standard'):
        self.bucket = bucket
        self.key = key
        self.storage_class = storage_class
 SyntaxError @dataclass
class S3Obj:
    bucket: str
    key: str
    *
    storage_class: str = 'Standard'
 __post_init__ @mydataclass
class S3Obj:
    bucket: str
    key: str
    ...
    storage_class: str = 'Standard'
",12,30,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
1,49506266,49506309,6389,Three lists zipped into list of dicts,4,<python><dictionary><zip><mapping>,20,"<p>Consider the following:</p>

<pre><code>&gt;&gt;&gt; # list of length n
&gt;&gt;&gt; idx = ['a', 'b', 'c', 'd']

&gt;&gt;&gt; # list of length n
&gt;&gt;&gt; l_1 = [1, 2, 3, 4]

&gt;&gt;&gt; # list of length n
&gt;&gt;&gt; l_2 = [5, 6, 7, 8]

&gt;&gt;&gt; # first key
&gt;&gt;&gt; key_1 = 'mkt_o'

&gt;&gt;&gt; # second key
&gt;&gt;&gt; key_2 = 'mkt_c'
</code></pre>

<p>How do I zip this mess to look like this?</p>

<pre><code>{
    'a': {'mkt_o': 1, 'mkt_c': 5},
    'b': {'mkt_o': 2, 'mkt_c': 6},
    'c': {'mkt_o': 3, 'mkt_c': 6},
    'd': {'mkt_o': 4, 'mkt_c': 7},
    ...
}
</code></pre>

<p>The closest I've got is something like this:</p>

<pre><code>&gt;&gt;&gt; dict(zip(idx, zip(l_1, l_2)))
{'a': (1, 5), 'b': (2, 6), 'c': (3, 7), 'd': (4, 8)}
</code></pre>

<p>Which of course has tuples as values instead of dictionaries, and</p>

<pre><code>&gt;&gt;&gt; dict(zip(('mkt_o', 'mkt_c'), (1,2)))
{'mkt_o': 1, 'mkt_c': 2}
</code></pre>

<p>Which seems like it might be promising, but again, fails to meet requirements.</p>
",687739,15088,27-03-2018 07:00,27-03-2018 07:03,0,15128,108,22,77,77,"{'badge_counts': {'bronze': 108, 'silver': 77, 'gold': 22}, 'account_id': 351336, 'is_employee': False, 'last_modified_date': 1708737900, 'last_access_date': 1711061096, 'reputation_change_year': 202, 'reputation_change_quarter': 202, 'reputation_change_month': 50, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 15128, 'creation_date': 1301667354, 'user_type': 'registered', 'user_id': 687739, 'accept_rate': 77, 'location': 'United States', 'website_url': 'http://jasonstrimpel.com', 'link': 'https://stackoverflow.com/users/687739/jason-strimpel', 'profile_image': 'https://i.stack.imgur.com/DBrVG.jpg?s=256&g=1', 'display_name': 'Jason Strimpel'}","Consider the following: How do I zip this mess to look like this? The closest I've got is something like this: Which of course has tuples as values instead of dictionaries, and Which seems like it might be promising, but again, fails to meet requirements.","&gt;&gt;&gt; # list of length n
&gt;&gt;&gt; idx = ['a', 'b', 'c', 'd']

&gt;&gt;&gt; # list of length n
&gt;&gt;&gt; l_1 = [1, 2, 3, 4]

&gt;&gt;&gt; # list of length n
&gt;&gt;&gt; l_2 = [5, 6, 7, 8]

&gt;&gt;&gt; # first key
&gt;&gt;&gt; key_1 = 'mkt_o'

&gt;&gt;&gt; # second key
&gt;&gt;&gt; key_2 = 'mkt_c'
 {
    'a': {'mkt_o': 1, 'mkt_c': 5},
    'b': {'mkt_o': 2, 'mkt_c': 6},
    'c': {'mkt_o': 3, 'mkt_c': 6},
    'd': {'mkt_o': 4, 'mkt_c': 7},
    ...
}
 &gt;&gt;&gt; dict(zip(idx, zip(l_1, l_2)))
{'a': (1, 5), 'b': (2, 6), 'c': (3, 7), 'd': (4, 8)}
 &gt;&gt;&gt; dict(zip(('mkt_o', 'mkt_c'), (1,2)))
{'mkt_o': 1, 'mkt_c': 2}
",21,42,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
2,49781234,49797717,4658,Setting a class __name__ declaratively,1,<python><metaclass><python-datamodel><python-descriptors><python-object>,13,"<p>Why can't you override a class name declaratively, e.g. to use a class name which is not a valid identifier?</p>

<pre><code>&gt;&gt;&gt; class Potato:
...     __name__ = 'not Potato'
...     
&gt;&gt;&gt; Potato.__name__  # doesn't stick
'Potato'
&gt;&gt;&gt; Potato().__name__  # .. but it's in the dict
'not Potato'
</code></pre>

<p>I thought maybe it was simply a case that this was overwritten after the class definition block completes.  But seems that's not true, because the name is writable yet apparently <em>not</em> set in the class dict:</p>

<pre><code>&gt;&gt;&gt; Potato.__name__ = 'no really, not Potato'
&gt;&gt;&gt; Potato.__name__  # works
'no really, not Potato'
&gt;&gt;&gt; Potato().__name__  # but instances resolve it somewhere else
'not Potato'
&gt;&gt;&gt; Potato.__dict__
mappingproxy({'__module__': '__main__',
              '__name__': 'not Potato',  # &lt;--- setattr didn't change that
              '__dict__': &lt;attribute '__dict__' of 'no really, not Potato' objects&gt;,
              '__weakref__': &lt;attribute '__weakref__' of 'no really, not Potato' objects&gt;,
              '__doc__': None})
&gt;&gt;&gt; # the super proxy doesn't find it (unless it's intentionally hiding it..?)
&gt;&gt;&gt; super(Potato).__name__
AttributeError: 'super' object has no attribute '__name__'
</code></pre>

<p>Questions:</p>

<ol>
<li>Where does <code>Potato.__name__</code> resolve?</li>
<li>How is <code>Potato.__name__ = other</code> handled (inside and outside of a class definition block)? </li>
</ol>
",674039,348162,11-04-2018 17:31,12-04-2018 13:21,1,348698,767,105,631,94,"{'badge_counts': {'bronze': 767, 'silver': 631, 'gold': 105}, 'account_id': 342731, 'is_employee': False, 'last_modified_date': 1710872103, 'last_access_date': 1711143082, 'reputation_change_year': 4383, 'reputation_change_quarter': 4383, 'reputation_change_month': 1127, 'reputation_change_week': 227, 'reputation_change_day': 10, 'reputation': 348698, 'creation_date': 1300923627, 'user_type': 'registered', 'user_id': 674039, 'accept_rate': 94, 'location': 'ℂ&#120153;&#120154;&#120148;&#120146;&#120152;&#120160;, &#120128;&#120131;', 'website_url': 'http://www.wimglenn.com', 'link': 'https://stackoverflow.com/users/674039/wim', 'profile_image': 'https://i.stack.imgur.com/leoFi.gif?s=256&g=1', 'display_name': 'wim'}","Why can't you override a class name declaratively, e.g. to use a class name which is not a valid identifier? I thought maybe it was simply a case that this was overwritten after the class definition block completes. But seems that's not true, because the name is writable yet apparently not set in the class dict: Questions: Where does resolve? How is handled (inside and outside of a class definition block)?","&gt;&gt;&gt; class Potato:
...     __name__ = 'not Potato'
...     
&gt;&gt;&gt; Potato.__name__  # doesn't stick
'Potato'
&gt;&gt;&gt; Potato().__name__  # .. but it's in the dict
'not Potato'
 &gt;&gt;&gt; Potato.__name__ = 'no really, not Potato'
&gt;&gt;&gt; Potato.__name__  # works
'no really, not Potato'
&gt;&gt;&gt; Potato().__name__  # but instances resolve it somewhere else
'not Potato'
&gt;&gt;&gt; Potato.__dict__
mappingproxy({'__module__': '__main__',
              '__name__': 'not Potato',  # &lt;--- setattr didn't change that
              '__dict__': &lt;attribute '__dict__' of 'no really, not Potato' objects&gt;,
              '__weakref__': &lt;attribute '__weakref__' of 'no really, not Potato' objects&gt;,
              '__doc__': None})
&gt;&gt;&gt; # the super proxy doesn't find it (unless it's intentionally hiding it..?)
&gt;&gt;&gt; super(Potato).__name__
AttributeError: 'super' object has no attribute '__name__'
 Potato.__name__ Potato.__name__ = other",17,35,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
3,49931096,49931177,17261,How to add a dataclass field without annotating the type?,4,<python><annotations><duck-typing><python-3.7><python-dataclasses>,31,"<p>When there is a field in a dataclass for which the type can be anything, how can you omit the annotation?</p>

<pre><code>@dataclass
class Favs:
    fav_number: int = 80085
    fav_duck = object()
    fav_word: str = 'potato'
</code></pre>

<p>It seems the code above doesn't actually create a field for <code>fav_duck</code>.  It just makes that a plain old class attribute.</p>

<pre><code>&gt;&gt;&gt; Favs()
Favs(fav_number=80085, fav_word='potato')
&gt;&gt;&gt; print(*Favs.__dataclass_fields__)
fav_number fav_word
&gt;&gt;&gt; Favs.fav_duck
&lt;object at 0x7fffea519850&gt;
</code></pre>
",674039,348162,19-04-2018 22:23,19-04-2018 22:31,0,348698,767,105,631,94,"{'badge_counts': {'bronze': 767, 'silver': 631, 'gold': 105}, 'account_id': 342731, 'is_employee': False, 'last_modified_date': 1710872103, 'last_access_date': 1711143082, 'reputation_change_year': 4383, 'reputation_change_quarter': 4383, 'reputation_change_month': 1127, 'reputation_change_week': 227, 'reputation_change_day': 10, 'reputation': 348698, 'creation_date': 1300923627, 'user_type': 'registered', 'user_id': 674039, 'accept_rate': 94, 'location': 'ℂ&#120153;&#120154;&#120148;&#120146;&#120152;&#120160;, &#120128;&#120131;', 'website_url': 'http://www.wimglenn.com', 'link': 'https://stackoverflow.com/users/674039/wim', 'profile_image': 'https://i.stack.imgur.com/leoFi.gif?s=256&g=1', 'display_name': 'wim'}","When there is a field in a dataclass for which the type can be anything, how can you omit the annotation? It seems the code above doesn't actually create a field for . It just makes that a plain old class attribute.","@dataclass
class Favs:
    fav_number: int = 80085
    fav_duck = object()
    fav_word: str = 'potato'
 fav_duck &gt;&gt;&gt; Favs()
Favs(fav_number=80085, fav_word='potato')
&gt;&gt;&gt; print(*Favs.__dataclass_fields__)
fav_number fav_word
&gt;&gt;&gt; Favs.fav_duck
&lt;object at 0x7fffea519850&gt;
",8,18,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
4,48082655,48082967,27012,Custom weighted loss function in Keras for weighing each element,1,<python><tensorflow><keras><loss-function>,20,"<p>I'm trying to create a simple weighted loss function. </p>

<p>Say, I have input dimensions 100 * 5, and output dimensions also 100 * 5. I also have a weight matrix of the same dimension.</p>

<p>Something like the following:</p>

<pre><code>import numpy as np
train_X = np.random.randn(100, 5)
train_Y = np.random.randn(100, 5)*0.01 + train_X

weights = np.random.randn(*train_X.shape)
</code></pre>

<h3>Defining the custom loss function</h3>

<pre><code>def custom_loss_1(y_true, y_pred):
    return K.mean(K.abs(y_true-y_pred)*weights)
</code></pre>

<h3>Defining the model</h3>

<pre><code>from keras.layers import Dense, Input
from keras import Model
import keras.backend as K

input_layer = Input(shape=(5,))
out = Dense(5)(input_layer)
model = Model(input_layer, out)
</code></pre>

<h3>Testing with existing metrics works fine</h3>

<pre><code>model.compile('adam','mean_absolute_error')
model.fit(train_X, train_Y, epochs=1)
</code></pre>

<h3>Testing with our custom loss function doesn't work</h3>

<pre><code>model.compile('adam',custom_loss_1)
model.fit(train_X, train_Y, epochs=10)
</code></pre>

<p>It gives the following stack trace:</p>

<pre><code>InvalidArgumentError (see above for traceback): Incompatible shapes: [32,5] vs. [100,5]
 [[Node: loss_9/dense_8_loss/mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss_9/dense_8_loss/Abs, loss_9/dense_8_loss/mul/y)]]
</code></pre>

<p>Where is the number 32 coming from?</p>

<h3>Testing a loss function with weights as Keras tensors</h3>

<pre><code>def custom_loss_2(y_true, y_pred):
    return K.mean(K.abs(y_true-y_pred)*K.ones_like(y_true))
</code></pre>

<p>This function seems to do the work. So, probably suggests that a Keras tensor as a weight matrix would work. So, I created another version of the loss function.</p>

<h3>Loss function try 3</h3>

<pre><code>from functools import partial

def custom_loss_3(y_true, y_pred, weights):
    return K.mean(K.abs(y_true-y_pred)*K.variable(weights, dtype=y_true.dtype))

cl3 = partial(custom_loss_3, weights=weights)  
</code></pre>

<p>Fitting data using cl3 gives the same error as above.</p>

<pre><code>InvalidArgumentError (see above for traceback): Incompatible shapes: [32,5] vs. [100,5]
     [[Node: loss_11/dense_8_loss/mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss_11/dense_8_loss/Abs, loss_11/dense_8_loss/Variable/read)]]
</code></pre>

<p>I wonder what I'm missing! I could have used the notion of sample_weight in Keras; but then I'd have to reshape my inputs to a 3d vector. </p>

<p>I thought that this custom loss function should really have been trivial.</p>
",743775,11187,03-01-2018 17:54,03-01-2018 18:15,0,11197,77,11,52,91,"{'badge_counts': {'bronze': 77, 'silver': 52, 'gold': 11}, 'account_id': 386159, 'is_employee': False, 'last_modified_date': 1707468600, 'last_access_date': 1711080441, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 11197, 'creation_date': 1304848477, 'user_type': 'registered', 'user_id': 743775, 'accept_rate': 91, 'website_url': 'http://nipunbatra.github.io', 'link': 'https://stackoverflow.com/users/743775/nipun-batra', 'profile_image': 'https://i.stack.imgur.com/66qdU.jpg?s=256&g=1', 'display_name': 'Nipun Batra'}","I'm trying to create a simple weighted loss function. Say, I have input dimensions 100 * 5, and output dimensions also 100 * 5. I also have a weight matrix of the same dimension. Something like the following: Defining the custom loss function Defining the model Testing with existing metrics works fine Testing with our custom loss function doesn't work It gives the following stack trace: Where is the number 32 coming from? Testing a loss function with weights as Keras tensors This function seems to do the work. So, probably suggests that a Keras tensor as a weight matrix would work. So, I created another version of the loss function. Loss function try 3 Fitting data using cl3 gives the same error as above. I wonder what I'm missing! I could have used the notion of sample_weight in Keras; but then I'd have to reshape my inputs to a 3d vector. I thought that this custom loss function should really have been trivial.","import numpy as np
train_X = np.random.randn(100, 5)
train_Y = np.random.randn(100, 5)*0.01 + train_X

weights = np.random.randn(*train_X.shape)
 def custom_loss_1(y_true, y_pred):
    return K.mean(K.abs(y_true-y_pred)*weights)
 from keras.layers import Dense, Input
from keras import Model
import keras.backend as K

input_layer = Input(shape=(5,))
out = Dense(5)(input_layer)
model = Model(input_layer, out)
 model.compile('adam','mean_absolute_error')
model.fit(train_X, train_Y, epochs=1)
 model.compile('adam',custom_loss_1)
model.fit(train_X, train_Y, epochs=10)
 InvalidArgumentError (see above for traceback): Incompatible shapes: [32,5] vs. [100,5]
 [[Node: loss_9/dense_8_loss/mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss_9/dense_8_loss/Abs, loss_9/dense_8_loss/mul/y)]]
 def custom_loss_2(y_true, y_pred):
    return K.mean(K.abs(y_true-y_pred)*K.ones_like(y_true))
 from functools import partial

def custom_loss_3(y_true, y_pred, weights):
    return K.mean(K.abs(y_true-y_pred)*K.variable(weights, dtype=y_true.dtype))

cl3 = partial(custom_loss_3, weights=weights)  
 InvalidArgumentError (see above for traceback): Incompatible shapes: [32,5] vs. [100,5]
     [[Node: loss_11/dense_8_loss/mul = Mul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss_11/dense_8_loss/Abs, loss_11/dense_8_loss/Variable/read)]]
",21,77,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
5,48772631,48772832,971,How/why does set() in {frozenset()} work?,2,<python><hash><set><immutability>,17,"<p>Even though sets are unhashable, membership check in other set works:</p>

<pre><code>&gt;&gt;&gt; set() in {frozenset()}
True
</code></pre>

<p>I expected <code>TypeError: unhashable type: 'set'</code>, consistent with other behaviours in Python:</p>

<pre><code>&gt;&gt;&gt; set() in {}  # doesn't work when checking in dict
TypeError: unhashable type: 'set'
&gt;&gt;&gt; {} in {frozenset()}  # looking up some other unhashable type doesn't work
TypeError: unhashable type: 'dict'
</code></pre>

<p>So, how is set membership in other set implemented?</p>
",674039,348162,13-02-2018 17:43,13-02-2018 17:57,0,348698,767,105,631,94,"{'badge_counts': {'bronze': 767, 'silver': 631, 'gold': 105}, 'account_id': 342731, 'is_employee': False, 'last_modified_date': 1710872103, 'last_access_date': 1711143082, 'reputation_change_year': 4383, 'reputation_change_quarter': 4383, 'reputation_change_month': 1127, 'reputation_change_week': 227, 'reputation_change_day': 10, 'reputation': 348698, 'creation_date': 1300923627, 'user_type': 'registered', 'user_id': 674039, 'accept_rate': 94, 'location': 'ℂ&#120153;&#120154;&#120148;&#120146;&#120152;&#120160;, &#120128;&#120131;', 'website_url': 'http://www.wimglenn.com', 'link': 'https://stackoverflow.com/users/674039/wim', 'profile_image': 'https://i.stack.imgur.com/leoFi.gif?s=256&g=1', 'display_name': 'wim'}","Even though sets are unhashable, membership check in other set works: I expected , consistent with other behaviours in Python: So, how is set membership in other set implemented?","&gt;&gt;&gt; set() in {frozenset()}
True
 TypeError: unhashable type: 'set' &gt;&gt;&gt; set() in {}  # doesn't work when checking in dict
TypeError: unhashable type: 'set'
&gt;&gt;&gt; {} in {frozenset()}  # looking up some other unhashable type doesn't work
TypeError: unhashable type: 'dict'
",3,15,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
6,49907382,49907762,18874,How to remove whitespace from an image in OpenCV?,3,<python><opencv><image-processing><opencv3.0>,24,"<p>I have the following image which has text and a lot of white space underneath the text. I would like to crop the white space such that it looks like the second image. </p>

<p><a href=""https://i.stack.imgur.com/pUq4x.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pUq4x.png"" alt=""enter image description here""></a></p>

<p>Cropped Image</p>

<p><a href=""https://i.stack.imgur.com/iGdb6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/iGdb6.png"" alt=""enter image description here""></a></p>

<p>Here is what I've done</p>

<pre><code>&gt;&gt;&gt; img = cv2.imread(""pg13_gau.jpg.png"")
&gt;&gt;&gt; gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
&gt;&gt;&gt; edged = cv2.Canny(gray, 30,300)
&gt;&gt;&gt; (img,cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
&gt;&gt;&gt; cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:10]
</code></pre>
",654203,34728,18-04-2018 19:13,18-04-2018 19:39,0,34758,281,43,173,71,"{'badge_counts': {'bronze': 281, 'silver': 173, 'gold': 43}, 'account_id': 330219, 'is_employee': False, 'last_modified_date': 1703325032, 'last_access_date': 1636331994, 'reputation_change_year': 380, 'reputation_change_quarter': 380, 'reputation_change_month': 70, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 34758, 'creation_date': 1299786888, 'user_type': 'registered', 'user_id': 654203, 'accept_rate': 71, 'website_url': '', 'link': 'https://stackoverflow.com/users/654203/anthony', 'profile_image': 'https://www.gravatar.com/avatar/f24da0fb92ce004520e876355912f910?s=256&d=identicon&r=PG', 'display_name': 'Anthony'}",I have the following image which has text and a lot of white space underneath the text. I would like to crop the white space such that it looks like the second image. Cropped Image Here is what I've done,"&gt;&gt;&gt; img = cv2.imread(""pg13_gau.jpg.png"")
&gt;&gt;&gt; gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
&gt;&gt;&gt; edged = cv2.Canny(gray, 30,300)
&gt;&gt;&gt; (img,cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
&gt;&gt;&gt; cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:10]
",4,16,2,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
7,49703938,49707027,64280,"How to create a ""dot plot"" in Matplotlib? (not a scatter plot)",6,<python><matplotlib><plot>,15,"<p>I'd like to create what my statistics book calls a ""dot plot"" where the number of dots in the plot equals the number of observations. Here's an example from <a href=""https://www.mathsisfun.com/data/dot-plots.html"" rel=""noreferrer"">mathisfun.com</a>:</p>

<p><a href=""https://i.stack.imgur.com/fOFsW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fOFsW.png"" alt=""example dot plot""></a></p>

<p>In the example, there are six dots above the <code>0</code> value on the X-axis representing the six observations of the value zero.</p>

<p>It seems that a ""dot plot"" can have several variations. In looking up how to create this with Matplotlib, I only came across what I know of as a scatter plot with a data point representing the relationship between the X and Y value.</p>

<p>Is the type of plot I'm trying to create possible with Matplotlib?</p>
",646151,1164,07-04-2018 04:46,07-04-2018 11:45,0,1164,24,2,14,75,"{'badge_counts': {'bronze': 24, 'silver': 14, 'gold': 2}, 'account_id': 325152, 'is_employee': False, 'last_modified_date': 1573684186, 'last_access_date': 1681913881, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1164, 'creation_date': 1299341630, 'user_type': 'registered', 'user_id': 646151, 'accept_rate': 75, 'website_url': '', 'link': 'https://stackoverflow.com/users/646151/scott', 'profile_image': 'https://www.gravatar.com/avatar/3dfb0ae22bada1071424a9299769029e?s=256&d=identicon&r=PG', 'display_name': 'Scott'}","I'd like to create what my statistics book calls a ""dot plot"" where the number of dots in the plot equals the number of observations. Here's an example from mathisfun.com: In the example, there are six dots above the value on the X-axis representing the six observations of the value zero. It seems that a ""dot plot"" can have several variations. In looking up how to create this with Matplotlib, I only came across what I know of as a scatter plot with a data point representing the relationship between the X and Y value. Is the type of plot I'm trying to create possible with Matplotlib?",0,-1,9,1,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
8,48608570,48674021,60380,"Python 3 Boto 3, AWS S3: Get object URL",5,<python><amazon-web-services><amazon-s3><python-3.6><boto3>,30,"<p>I need to retrieve an public object URL directly after uploading a file, this to be able to store it in a database.
This is my upload code:</p>

<pre><code>   s3 = boto3.resource('s3')
   s3bucket.upload_file(filepath, objectname, ExtraArgs={'StorageClass': 'STANDARD_IA'})
</code></pre>

<p>I am not looking for a presigned URL, just the URL that always will be publicly accessable over https.</p>

<p>Any help appreciated.</p>
",611941,2559,04-02-2018 13:29,07-02-2018 22:03,3,2559,52,9,39,73,"{'badge_counts': {'bronze': 52, 'silver': 39, 'gold': 9}, 'account_id': 303157, 'is_employee': False, 'last_modified_date': 1653984000, 'last_access_date': 1581021178, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2559, 'creation_date': 1297365179, 'user_type': 'registered', 'user_id': 611941, 'accept_rate': 73, 'location': 'Levanger, Norway', 'link': 'https://stackoverflow.com/users/611941/hyperdevil', 'profile_image': 'https://www.gravatar.com/avatar/b65f9615495aa1fbfaf0833cfaca5463?s=256&d=identicon&r=PG', 'display_name': 'HyperDevil'}","I need to retrieve an public object URL directly after uploading a file, this to be able to store it in a database. This is my upload code: I am not looking for a presigned URL, just the URL that always will be publicly accessable over https. Any help appreciated.","   s3 = boto3.resource('s3')
   s3bucket.upload_file(filepath, objectname, ExtraArgs={'StorageClass': 'STANDARD_IA'})
",1,10,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
9,49888155,49897523,16375,Class cannot subclass 'QObject' (has type 'Any') using mypy,3,<python><pyqt><subclassing><type-hinting><mypy>,13,"<p>I have a class that subclasses QObject. Everyting works fine but when I run mypy on it I get the error:</p>

<pre><code>""error: Class cannot subclass 'QObject' (has type 'Any')"" 
</code></pre>

<p>At the moment I am totally stuck. I Have been reading the mypy docs but couldn't find where the error was.</p>

<p>Here the code:</p>

<pre><code>from PyQt5.QtCore import QObject

class ServiceLocator(QObject):

    def __init__(self) -&gt; None:
        super().__init__()
        ...
</code></pre>

<p>Cheers.</p>
",574633,6126,17-04-2018 22:03,18-04-2018 10:34,1,6146,103,12,57,76,"{'badge_counts': {'bronze': 103, 'silver': 57, 'gold': 12}, 'account_id': 279463, 'is_employee': False, 'last_modified_date': 1711158600, 'last_access_date': 1710506938, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 6146, 'creation_date': 1294939646, 'user_type': 'registered', 'user_id': 574633, 'accept_rate': 76, 'link': 'https://stackoverflow.com/users/574633/notbad', 'profile_image': 'https://www.gravatar.com/avatar/998cbb418b181eeaccdd47b70f14ed15?s=256&d=identicon&r=PG', 'display_name': 'Notbad'}",I have a class that subclasses QObject. Everyting works fine but when I run mypy on it I get the error: At the moment I am totally stuck. I Have been reading the mypy docs but couldn't find where the error was. Here the code: Cheers.,"""error: Class cannot subclass 'QObject' (has type 'Any')"" 
 from PyQt5.QtCore import QObject

class ServiceLocator(QObject):

    def __init__(self) -&gt; None:
        super().__init__()
        ...
",6,19,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
10,48507460,48507675,6916,How to document a Django project?,1,<python><django>,14,"<p>How should I go about documenting a Django project? I'm not talking about an app that I'm creating that I'll push to github. It's basically internal documentation that will help new developers that we employ to get up to speed with the system. (I guess that's the point of documentation in general)</p>

<p>Should I be documenting each and every view function, model or form as below:</p>

<pre><code>def home(request):
    """"""View that renders the home page.""""""

class MyModel(models.Model):
    ""Documentation regarding my model.""""""
</code></pre>

<p>It seems like a bit of overkill. Are there perhaps some good projects that I can look at for inspiration?</p>
",541729,7167,29-01-2018 18:09,29-01-2018 18:23,0,7197,73,12,47,83,"{'badge_counts': {'bronze': 73, 'silver': 47, 'gold': 12}, 'account_id': 259097, 'is_employee': False, 'last_modified_date': 1703300100, 'last_access_date': 1709560700, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7197, 'creation_date': 1292319416, 'user_type': 'registered', 'user_id': 541729, 'accept_rate': 83, 'website_url': '', 'link': 'https://stackoverflow.com/users/541729/kritz', 'profile_image': 'https://www.gravatar.com/avatar/446361b6e82c2e78966f22ec39d88501?s=256&d=identicon&r=PG', 'display_name': 'Kritz'}","How should I go about documenting a Django project? I'm not talking about an app that I'm creating that I'll push to github. It's basically internal documentation that will help new developers that we employ to get up to speed with the system. (I guess that's the point of documentation in general) Should I be documenting each and every view function, model or form as below: It seems like a bit of overkill. Are there perhaps some good projects that I can look at for inspiration?","def home(request):
    """"""View that renders the home page.""""""

class MyModel(models.Model):
    ""Documentation regarding my model.""""""
",4,12,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
11,48598092,48598168,11964,python 3.5 -> 3.6 Tablib TypeError: cell() missing 1 required positional argument: 'column',1,<python><django><python-3.6><django-import-export><tablib>,11,"<p>Migrating from python 3.5 to 3.6, my unit tests reveal a problem with django-import-export &amp; tablib:</p>

<p>TypeError: cell() missing 1 required positional argument: 'column'</p>

<pre><code>File ""&lt;path&gt;/lib/python3.6/site-packages/tablib/formats/_xlsx.py"", line 122, in dset_sheet
    cell = ws.cell('%s%s' % (col_idx, row_number))
    TypeError: cell() missing 1 required positional argument: 'column'
</code></pre>

<p>The line in tablib:</p>

<pre><code>    cell = ws.cell('%s%s' % (col_idx, row_number))
</code></pre>

<p>So indeed, there is no argument for the column </p>

<p>My view code:</p>

<pre><code>my_resource = MyModelResource(queryset=my_queryset)
dataset = my_resource.export()
response = HttpResponse(dataset.xlsx, content_type='application/vnd.ms-excel')
</code></pre>

<p>This works fine in python3.5 but fails under 3.6</p>

<p>requirements.txt:</p>

<pre><code>...
tablib==0.12.1
django-import-export==0.7.0
Django==1.11.7
...
</code></pre>
",511436,1768,03-02-2018 13:50,03-02-2018 13:59,0,1768,46,1,20,47,"{'badge_counts': {'bronze': 46, 'silver': 20, 'gold': 1}, 'account_id': 240925, 'is_employee': False, 'last_modified_date': 1676024100, 'last_access_date': 1711025189, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1768, 'creation_date': 1290034430, 'user_type': 'registered', 'user_id': 511436, 'accept_rate': 47, 'location': 'Belgium', 'website_url': 'https://swingit.be', 'link': 'https://stackoverflow.com/users/511436/davy', 'profile_image': 'https://i.stack.imgur.com/8a9Zg.jpg?s=256&g=1', 'display_name': 'Davy'}","Migrating from python 3.5 to 3.6, my unit tests reveal a problem with django-import-export &amp; tablib: TypeError: cell() missing 1 required positional argument: 'column' The line in tablib: So indeed, there is no argument for the column My view code: This works fine in python3.5 but fails under 3.6 requirements.txt:","File ""&lt;path&gt;/lib/python3.6/site-packages/tablib/formats/_xlsx.py"", line 122, in dset_sheet
    cell = ws.cell('%s%s' % (col_idx, row_number))
    TypeError: cell() missing 1 required positional argument: 'column'
     cell = ws.cell('%s%s' % (col_idx, row_number))
 my_resource = MyModelResource(queryset=my_queryset)
dataset = my_resource.export()
response = HttpResponse(dataset.xlsx, content_type='application/vnd.ms-excel')
 ...
tablib==0.12.1
django-import-export==0.7.0
Django==1.11.7
...
",8,33,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
12,49780491,49781582,41782,Plotting Histogram for all columns in a Data Frame,2,<python><apache-spark><pyspark><apache-spark-sql>,18,"<p>I am trying to draw histograms for all of the columns in my data frame.
I imported <code>pyspark</code> and <code>matplotlib</code>. 
df is my data frame variable.
plt is <code>matplotlib.pyplot</code> variable</p>

<p>I was able to draw/plot histogram for individual column, like this:</p>

<pre><code>bins, counts = df.select('ColumnName').rdd.flatMap(lambda x: x).histogram(20)
plt.hist(bins[:-1], bins=bins, weights=counts)
</code></pre>

<p>But when I try to plot it for all variables I am having issues. Here is the for loop I have so far:</p>

<pre><code>for x in range(0, len(df.columns)):
    bins, counts = df.select(x).rdd.flatMap(lambda x: x).histogram(20)
    plt.hist(bins[:-1], bins=bins, weights=counts)
</code></pre>

<p>How would I do it? Thanks in advance.</p>
",493829,2712,11-04-2018 16:45,11-04-2018 17:53,0,2712,69,9,44,81,"{'badge_counts': {'bronze': 69, 'silver': 44, 'gold': 9}, 'account_id': 230307, 'is_employee': False, 'last_modified_date': 1697536500, 'last_access_date': 1688737755, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2712, 'creation_date': 1288630515, 'user_type': 'registered', 'user_id': 493829, 'accept_rate': 81, 'location': 'Atlanta, GA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/493829/ozzboy', 'profile_image': 'https://i.stack.imgur.com/hgECH.jpg?s=256&g=1', 'display_name': 'ozzboy'}","I am trying to draw histograms for all of the columns in my data frame. I imported and . df is my data frame variable. plt is variable I was able to draw/plot histogram for individual column, like this: But when I try to plot it for all variables I am having issues. Here is the for loop I have so far: How would I do it? Thanks in advance.","pyspark matplotlib matplotlib.pyplot bins, counts = df.select('ColumnName').rdd.flatMap(lambda x: x).histogram(20)
plt.hist(bins[:-1], bins=bins, weights=counts)
 for x in range(0, len(df.columns)):
    bins, counts = df.select(x).rdd.flatMap(lambda x: x).histogram(20)
    plt.hist(bins[:-1], bins=bins, weights=counts)
",0,19,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
13,48555290,48667606,817,How to make keras in R use the tensorflow installed by Python,2,<python><r><deep-learning><keras>,12,"<p>I have already performed the tensor flow installation with the following command:</p>

<pre><code>pip install --ignore-installed https://github.com/mind/wheels/releases/download/tf1.5-gpu-cuda91-nomkl/tensorflow-1.5.0-cp27-cp27mu-linux_x86_64.whl
</code></pre>

<p>This is the latest tensorflow wheel catered for CUDA 9.1. (3x faster than CUDA 8.0)</p>

<p>And I can call it successfully in my python code. </p>

<p>How can I make the keras in R to call the tensorflow installed by python above? The reason I asked that because I the default installation method</p>

<pre><code>keras::install_keras(method=""conda"", tensorflow = ""gpu"")
</code></pre>

<p>It failed to recognize the cuda-9.1 library.</p>

<pre><code>&gt; conv_base &lt;- keras::application_vgg16(
+   weights = ""imagenet"",
+   include_top = FALSE,
+   input_shape = c(150, 150, 3)
+ )
/home/ubuntu/anaconda2/envs/r-tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Error: ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/anaconda2/envs/r-tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/anaconda2/envs/r-tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/anaconda2/envs/r-tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
</code></pre>

<p>This is because R installation method calling for tensorflow version 1.5.0 that is still not catered for CUDA 9.1.</p>
",67405,62354,01-02-2018 04:20,07-02-2018 15:34,6,62434,485,140,317,79,"{'badge_counts': {'bronze': 485, 'silver': 317, 'gold': 140}, 'account_id': 26018, 'is_employee': False, 'last_modified_date': 1699668900, 'last_access_date': 1698808316, 'reputation_change_year': 620, 'reputation_change_quarter': 620, 'reputation_change_month': 210, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 62434, 'creation_date': 1234881829, 'user_type': 'registered', 'user_id': 67405, 'accept_rate': 79, 'website_url': '', 'link': 'https://stackoverflow.com/users/67405/neversaint', 'profile_image': 'https://www.gravatar.com/avatar/8abbf52b25081326984d4e6717f8bd25?s=256&d=identicon&r=PG', 'display_name': 'neversaint'}",I have already performed the tensor flow installation with the following command: This is the latest tensorflow wheel catered for CUDA 9.1. (3x faster than CUDA 8.0) And I can call it successfully in my python code. How can I make the keras in R to call the tensorflow installed by python above? The reason I asked that because I the default installation method It failed to recognize the cuda-9.1 library. This is because R installation method calling for tensorflow version 1.5.0 that is still not catered for CUDA 9.1.,"pip install --ignore-installed https://github.com/mind/wheels/releases/download/tf1.5-gpu-cuda91-nomkl/tensorflow-1.5.0-cp27-cp27mu-linux_x86_64.whl
 keras::install_keras(method=""conda"", tensorflow = ""gpu"")
 &gt; conv_base &lt;- keras::application_vgg16(
+   weights = ""imagenet"",
+   include_top = FALSE,
+   input_shape = c(150, 150, 3)
+ )
/home/ubuntu/anaconda2/envs/r-tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Error: ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/anaconda2/envs/r-tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/anaconda2/envs/r-tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/anaconda2/envs/r-tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
",15,35,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
14,48925901,48926050,19449,How to automatically save changes before running a Python script in VS Code,6,<python><visual-studio-code>,13,"<p>I'm writing Python scripts in Visual Studio Code, and I execute them with <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>N</kbd>, a shortcut added by the extension <a href=""https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner"" rel=""noreferrer"">Code Runner</a>. Is there a way to force VS Code to save the <code>.py</code> before running, thus eliminating the extra step to save the file? </p>
",705984,815,22-02-2018 11:07,22-02-2018 11:14,0,815,25,3,11,25,"{'badge_counts': {'bronze': 25, 'silver': 11, 'gold': 3}, 'account_id': 362671, 'is_employee': False, 'last_modified_date': 1604821500, 'last_access_date': 1674818008, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 815, 'creation_date': 1302695890, 'user_type': 'registered', 'user_id': 705984, 'accept_rate': 25, 'location': 'Stockholm, Sweden', 'website_url': 'https://hscheidl.com', 'link': 'https://stackoverflow.com/users/705984/h-scheidl', 'profile_image': 'https://i.stack.imgur.com/VyjtJ.jpg?s=256&g=1', 'display_name': 'H.Scheidl'}","I'm writing Python scripts in Visual Studio Code, and I execute them with Ctrl+Alt+N, a shortcut added by the extension Code Runner. Is there a way to force VS Code to save the before running, thus eliminating the extra step to save the file?",.py,-1,1,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
15,49662585,49662626,6072,"How do I compare a Unicode string that has different bytes, but the same value?",3,<python><unicode>,60,"<p>I'm comparing Unicode strings between JSON objects.</p>

<p>They have the same value:</p>

<pre><code>a = '人口じんこうに膾炙かいしゃする'
b = '人口じんこうに膾炙かいしゃする'
</code></pre>

<p>But they have different Unicode representations:</p>

<pre><code>String a : u'\u4eba\u53e3\u3058\u3093\u3053\u3046\u306b\u81be\u7099\u304b\u3044\u3057\u3083\u3059\u308b'
String b : u'\u4eba\u53e3\u3058\u3093\u3053\u3046\u306b\u81be\uf9fb\u304b\u3044\u3057\u3083\u3059\u308b'
</code></pre>

<p>How can I compare between two Unicode strings on their value?</p>
",700043,611,05-04-2018 00:57,05-04-2018 01:02,0,611,16,0,6,75,"{'badge_counts': {'bronze': 16, 'silver': 6, 'gold': 0}, 'account_id': 358938, 'is_employee': False, 'last_modified_date': 1691806800, 'last_access_date': 1698131047, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 611, 'creation_date': 1302360603, 'user_type': 'registered', 'user_id': 700043, 'accept_rate': 75, 'website_url': '', 'link': 'https://stackoverflow.com/users/700043/seunghoon-baek', 'profile_image': 'https://www.gravatar.com/avatar/9fa11feea42aaffe985be6f2b207123e?s=256&d=identicon&r=PG', 'display_name': 'Seunghoon Baek'}",I'm comparing Unicode strings between JSON objects. They have the same value: But they have different Unicode representations: How can I compare between two Unicode strings on their value?,"a = '人口じんこうに膾炙かいしゃする'
b = '人口じんこうに膾炙かいしゃする'
 String a : u'\u4eba\u53e3\u3058\u3093\u3053\u3046\u306b\u81be\u7099\u304b\u3044\u3057\u3083\u3059\u308b'
String b : u'\u4eba\u53e3\u3058\u3093\u3053\u3046\u306b\u81be\uf9fb\u304b\u3044\u3057\u3083\u3059\u308b'
",2,15,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
16,48448473,48448673,152142,Pyspark convert a standard list to data frame,2,<python><apache-spark><pyspark><apache-spark-sql>,51,"<p>The case is really simple, I need to convert a python list into data frame with following code</p>

<pre><code>from pyspark.sql.types import StructType
from pyspark.sql.types import StructField
from pyspark.sql.types import StringType, IntegerType

schema = StructType([StructField(""value"", IntegerType(), True)])
my_list = [1, 2, 3, 4]
rdd = sc.parallelize(my_list)
df = sqlContext.createDataFrame(rdd, schema)

df.show()
</code></pre>

<p>it failed with following error:</p>

<pre><code>    raise TypeError(""StructType can not accept object %r in type %s"" % (obj, type(obj)))
TypeError: StructType can not accept object 1 in type &lt;class 'int'&gt;
</code></pre>
",713896,1505,25-01-2018 17:13,25-01-2018 17:25,0,1505,26,3,18,22,"{'badge_counts': {'bronze': 26, 'silver': 18, 'gold': 3}, 'account_id': 367607, 'is_employee': False, 'last_modified_date': 1653704100, 'last_access_date': 1709835353, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1505, 'creation_date': 1303149309, 'user_type': 'registered', 'user_id': 713896, 'accept_rate': 22, 'location': 'New York, NY', 'website_url': '', 'link': 'https://stackoverflow.com/users/713896/seiya', 'profile_image': 'https://www.gravatar.com/avatar/b1fe1c85774f74a2b4036b009d30665c?s=256&d=identicon&r=PG', 'display_name': 'seiya'}","The case is really simple, I need to convert a python list into data frame with following code it failed with following error:","from pyspark.sql.types import StructType
from pyspark.sql.types import StructField
from pyspark.sql.types import StringType, IntegerType

schema = StructType([StructField(""value"", IntegerType(), True)])
my_list = [1, 2, 3, 4]
rdd = sc.parallelize(my_list)
df = sqlContext.createDataFrame(rdd, schema)

df.show()
     raise TypeError(""StructType can not accept object %r in type %s"" % (obj, type(obj)))
TypeError: StructType can not accept object 1 in type &lt;class 'int'&gt;
",10,19,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
17,48347497,48348006,47886,Pandas groupby multiple fields then diff,2,<python><pandas><dataframe><group-by>,39,"<p>So my dataframe looks like this:</p>
<pre><code>         date    site country  score
0  2018-01-01  google      us    100
1  2018-01-01  google      ch     50
2  2018-01-02  google      us     70
3  2018-01-03  google      us     60
4  2018-01-02  google      ch     10
5  2018-01-01      fb      us     50
6  2018-01-02      fb      us     55
7  2018-01-03      fb      us    100
8  2018-01-01      fb      es    100
9  2018-01-02      fb      gb    100
</code></pre>
<p>Each <code>site</code> has a different score depending on the <code>country</code>. I'm trying to find the 1/3/5-day difference of <code>score</code>s for each <code>site</code>/<code>country</code> combination.</p>
<p>Output should be:</p>
<pre><code>          date    site country  score  diff
8  2018-01-01      fb      es    100   0.0
9  2018-01-02      fb      gb    100   0.0
5  2018-01-01      fb      us     50   0.0
6  2018-01-02      fb      us     55   5.0
7  2018-01-03      fb      us    100  45.0
1  2018-01-01  google      ch     50   0.0
4  2018-01-02  google      ch     10 -40.0
0  2018-01-01  google      us    100   0.0
2  2018-01-02  google      us     70 -30.0
3  2018-01-03  google      us     60 -10.0
</code></pre>
<p>I first tried sorting by <code>site</code>/<code>country</code>/<code>date</code>, then grouping by <code>site</code> and <code>country</code> but I'm not able to wrap my head around getting a difference from a grouped object.</p>
",722950,1949,19-01-2018 18:33,19-01-2018 19:11,0,1949,52,5,31,66,"{'badge_counts': {'bronze': 52, 'silver': 31, 'gold': 5}, 'account_id': 373362, 'is_employee': False, 'last_modified_date': 1673309403, 'last_access_date': 1617978091, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1949, 'creation_date': 1303677754, 'user_type': 'registered', 'user_id': 722950, 'accept_rate': 66, 'link': 'https://stackoverflow.com/users/722950/craig', 'profile_image': 'https://www.gravatar.com/avatar/178d85934b6799e9910c0f25d712a8da?s=256&d=identicon&r=PG', 'display_name': 'Craig'}","So my dataframe looks like this: Each has a different score depending on the . I'm trying to find the 1/3/5-day difference of s for each / combination. Output should be: I first tried sorting by //, then grouping by and but I'm not able to wrap my head around getting a difference from a grouped object.","         date    site country  score
0  2018-01-01  google      us    100
1  2018-01-01  google      ch     50
2  2018-01-02  google      us     70
3  2018-01-03  google      us     60
4  2018-01-02  google      ch     10
5  2018-01-01      fb      us     50
6  2018-01-02      fb      us     55
7  2018-01-03      fb      us    100
8  2018-01-01      fb      es    100
9  2018-01-02      fb      gb    100
 site country score site country           date    site country  score  diff
8  2018-01-01      fb      es    100   0.0
9  2018-01-02      fb      gb    100   0.0
5  2018-01-01      fb      us     50   0.0
6  2018-01-02      fb      us     55   5.0
7  2018-01-03      fb      us    100  45.0
1  2018-01-01  google      ch     50   0.0
4  2018-01-02  google      ch     10 -40.0
0  2018-01-01  google      us    100   0.0
2  2018-01-02  google      us     70 -30.0
3  2018-01-03  google      us     60 -10.0
 site country date site country",10,28,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
18,49356938,49417706,3165,Apply function to grouped data frame in Dask: How do you specify the grouped Dataframe as argument in the function?,2,<python><pandas><dask>,21,"<p>I have a <code>dask dataframe</code> grouped by the index (<code>first_name</code>).</p>

<pre><code>import pandas as pd
import numpy as np

from multiprocessing import cpu_count

from dask import dataframe as dd
from dask.multiprocessing import get 
from dask.distributed import Client


NCORES = cpu_count()
client = Client()

entities = pd.DataFrame({'first_name':['Jake','John','Danae','Beatriz', 'Jacke', 'Jon'],'last_name': ['Del Toro', 'Foster', 'Smith', 'Patterson', 'Toro', 'Froster'], 'ID':['X','U','X','Y', '12','13']})

df = dd.from_pandas(entities, npartitions=NCORES)
df = client.persist(df.set_index('first_name'))
</code></pre>

<p>(Obviously <code>entities</code> in the real life is several thousand rows)</p>

<p>I want to apply a user defined function to each grouped dataframe. I want to compare each row with all the other rows in the group (something similar to <a href=""https://stackoverflow.com/questions/35459316/pandas-compare-each-row-with-all-rows-in-data-frame-and-save-results-in-list-for"">Pandas compare each row with all rows in data frame and save results in list for each row</a>).</p>

<p>The following is the function that I try to apply:</p>

<pre><code>def contraster(x, DF):
    matches = DF.apply(lambda row: fuzz.partial_ratio(row['last_name'], x) &gt;= 50, axis = 1) 
    return [i for i, x in enumerate(matches) if x]
</code></pre>

<p>For the test <code>entities</code> data frame, you could apply the function as usual:</p>

<pre><code>entities.apply(lambda row: contraster(row['last_name'], entities), axis =1)
</code></pre>

<p>And the expected result is:</p>

<pre><code>Out[35]: 
0    [0, 4]
1    [1, 5]
2       [2]
3       [3]
4    [0, 4]
5    [1, 5]
dtype: object
</code></pre>

<p>When <code>entities</code> is huge, the solution is use <code>dask</code>.  Note that <code>DF</code> in the <code>contraster</code> function must be the groupped dataframe.</p>

<p>I am trying to use the following:</p>

<pre><code>df.groupby('first_name').apply(func=contraster, args=????)
</code></pre>

<p>But How should I specify the grouped dataframe (i.e. <code>DF</code> in <code>contraster</code>?)</p>
",754176,8102,19-03-2018 06:24,21-03-2018 22:41,2,8102,73,7,41,78,"{'badge_counts': {'bronze': 73, 'silver': 41, 'gold': 7}, 'account_id': 392732, 'is_employee': False, 'last_modified_date': 1695630900, 'last_access_date': 1692692781, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 8102, 'creation_date': 1305429166, 'user_type': 'registered', 'user_id': 754176, 'accept_rate': 78, 'location': 'Mexico City, Mexico', 'link': 'https://stackoverflow.com/users/754176/nanounanue', 'profile_image': 'https://www.gravatar.com/avatar/66a75feab0b925bfaa31c247d38b1298?s=256&d=identicon&r=PG', 'display_name': 'nanounanue'}","I have a grouped by the index (). (Obviously in the real life is several thousand rows) I want to apply a user defined function to each grouped dataframe. I want to compare each row with all the other rows in the group (something similar to Pandas compare each row with all rows in data frame and save results in list for each row). The following is the function that I try to apply: For the test data frame, you could apply the function as usual: And the expected result is: When is huge, the solution is use . Note that in the function must be the groupped dataframe. I am trying to use the following: But How should I specify the grouped dataframe (i.e. in ?)","dask dataframe first_name import pandas as pd
import numpy as np

from multiprocessing import cpu_count

from dask import dataframe as dd
from dask.multiprocessing import get 
from dask.distributed import Client


NCORES = cpu_count()
client = Client()

entities = pd.DataFrame({'first_name':['Jake','John','Danae','Beatriz', 'Jacke', 'Jon'],'last_name': ['Del Toro', 'Foster', 'Smith', 'Patterson', 'Toro', 'Froster'], 'ID':['X','U','X','Y', '12','13']})

df = dd.from_pandas(entities, npartitions=NCORES)
df = client.persist(df.set_index('first_name'))
 entities def contraster(x, DF):
    matches = DF.apply(lambda row: fuzz.partial_ratio(row['last_name'], x) &gt;= 50, axis = 1) 
    return [i for i, x in enumerate(matches) if x]
 entities entities.apply(lambda row: contraster(row['last_name'], entities), axis =1)
 Out[35]: 
0    [0, 4]
1    [1, 5]
2       [2]
3       [3]
4    [0, 4]
5    [1, 5]
dtype: object
 entities dask DF contraster df.groupby('first_name').apply(func=contraster, args=????)
 DF contraster",15,57,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
19,48063176,48063380,6088,How to reliably check if a domain has been registered or is available?,1,<python><python-3.x><whois>,13,"<h2>Objective</h2>

<p>I need a <strong>reliable</strong> way to check in Python if a domain of <strong>any TLD</strong> has been registered or is available. The bold phrases are the key points that I'm struggling with.</p>

<h2>What I tried?</h2>

<ol>
<li><strong>WHOIS</strong> is the obvious way to do the check and an existing Python library like the popular <strong><a href=""https://pypi.python.org/pypi/python-whois"" rel=""noreferrer"">python-whois</a></strong> was my first try. The problem is that it doesn't seem to be able to retrieve information for some of the TLDs, e.g. <em>.run</em>, while it works mostly fine for older ones, e.g. <em>.com</em>.</li>
<li>So if python-whois is not reliable, maybe just a <strong>wrapper for the Linux's whois</strong> would be better. I tried <strong><a href=""https://pypi.python.org/pypi/whois/0.7"" rel=""noreferrer"">whois library</a></strong> and unfortunately it supports only a limited set of TLDs, apparently to make sure it can always parse the results.</li>
<li><p>As I don't really need to parse the results, I ripped the code out of the whois library and tried to do the query by <strong>calling Linux's whois</strong> myself:</p>

<pre><code>p = subprocess.Popen(['whois', 'example.com'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
r = p.communicate()[0]
print(r.decode())
</code></pre>

<p>That works much better. Except <strong>it's not that reliable either</strong>. I tried one particular domain and got <em>""Your connection limit exceeded. Please slow down and try again later.""</em> Well, it's not me who is exceeding the limit. Being behind a single IP in a huge office means that somebody else might hit the limit before I make a query.</p></li>
<li>Another thought was <strong>not to use WHOIS</strong> and instead do a DNS lookup. However, I need to deal with domains that are registered or in the protected phase after expiry and don't have DNS records so this is apparently not possible.</li>
<li>Last idea was to do the queries via an <strong>API of some 3rd party service</strong>. The problem is trust in those services as they might <a href=""https://en.wikipedia.org/wiki/Domain_name_front_running"" rel=""noreferrer"">snatch</a> an available domain that I check.</li>
</ol>

<h2>Similar questions</h2>

<p>There are already similar questions:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/11584942/a-stable-way-to-check-domain-availability-with-pywhois"">a stable way to check domain availability with pywhois</a></li>
<li><a href=""https://stackoverflow.com/questions/47418061/testing-domain-name-availability-with-pythonwhois"">Testing domain-name availability with pythonwhois</a></li>
</ul>

<p>...but they either deal only with a limited set of TLDs or are not that bothered by reliability.</p>
",772810,8110,02-01-2018 14:58,02-01-2018 15:14,0,8170,46,4,32,,"{'badge_counts': {'bronze': 46, 'silver': 32, 'gold': 4}, 'account_id': 404478, 'is_employee': False, 'last_modified_date': 1708024800, 'last_access_date': 1711124023, 'reputation_change_year': 253, 'reputation_change_quarter': 253, 'reputation_change_month': 144, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 8170, 'creation_date': 1306488599, 'user_type': 'registered', 'user_id': 772810, 'location': 'European Union', 'website_url': 'http://nohup.run', 'link': 'https://stackoverflow.com/users/772810/tmt', 'profile_image': 'https://www.gravatar.com/avatar/73aa32032e732115e004789eac262228?s=256&d=identicon&r=PG', 'display_name': 'tmt'}","Objective I need a reliable way to check in Python if a domain of any TLD has been registered or is available. The bold phrases are the key points that I'm struggling with. What I tried? WHOIS is the obvious way to do the check and an existing Python library like the popular python-whois was my first try. The problem is that it doesn't seem to be able to retrieve information for some of the TLDs, e.g. .run, while it works mostly fine for older ones, e.g. .com. So if python-whois is not reliable, maybe just a wrapper for the Linux's whois would be better. I tried whois library and unfortunately it supports only a limited set of TLDs, apparently to make sure it can always parse the results. As I don't really need to parse the results, I ripped the code out of the whois library and tried to do the query by calling Linux's whois myself: That works much better. Except it's not that reliable either. I tried one particular domain and got ""Your connection limit exceeded. Please slow down and try again later."" Well, it's not me who is exceeding the limit. Being behind a single IP in a huge office means that somebody else might hit the limit before I make a query. Another thought was not to use WHOIS and instead do a DNS lookup. However, I need to deal with domains that are registered or in the protected phase after expiry and don't have DNS records so this is apparently not possible. Last idea was to do the queries via an API of some 3rd party service. The problem is trust in those services as they might snatch an available domain that I check. Similar questions There are already similar questions: a stable way to check domain availability with pywhois Testing domain-name availability with pythonwhois ...but they either deal only with a limited set of TLDs or are not that bothered by reliability.","p = subprocess.Popen(['whois', 'example.com'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
r = p.communicate()[0]
print(r.decode())
",2,31,0,5,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
20,48373419,48574297,17863,Boto 3 DynamoDB batchWriteItem Invalid attribute value type when specifying types,2,<python><amazon-dynamodb><boto3>,13,"<p>I have a strange problem with Python Boto3 when trying to do a batch_write_item to a DynamoDB table. I am following the <a href=""http://boto3.readthedocs.io/en/latest/reference/services/dynamodb.html#DynamoDB.Client.batch_write_item"" rel=""noreferrer"">documentation</a> and trying to write a singe item. The table is setup correctly and I can run batch-write-item via the AWS cli no problem.</p>

<p>Assuming the client and DynamoDB are set up correctly I run:</p>

<pre><code>client.batch_write_item(RequestItems={
    ""myTable"": [
        {
            ""PutRequest"": {
                ""Item"": {
                    ""name"": {
                        ""S"": ""hello""
                    },
                    ""value"": {
                        ""S"": ""world""
                    }
                }
            }
        }
    ]
})
</code></pre>

<p>I get the following error:</p>

<p><strong>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the BatchWriteItem operation: Invalid attribute value type</strong></p>

<p>If I change it, removing the types and run:</p>

<pre><code>client.batch_write_item(RequestItems={
    ""myTable"": [
        {
            ""PutRequest"": {
                ""Item"": {
                    ""name"": ""hello"",
                    ""value"": ""world""
                }
            }
        }
    ]
})
</code></pre>

<p>It works as expected.</p>

<p>I need to use the previous format which follows the documentation and is compatibale with AWS cli.</p>

<p>Is the documentation wrong or I missed a configuration setting, version issue or something else?</p>
",670511,1251,22-01-2018 01:03,02-02-2018 01:14,11,1261,18,1,14,,"{'badge_counts': {'bronze': 18, 'silver': 14, 'gold': 1}, 'account_id': 340468, 'is_employee': False, 'last_modified_date': 1703298900, 'last_access_date': 1710131650, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1261, 'creation_date': 1300764027, 'user_type': 'registered', 'user_id': 670511, 'location': 'Brisbane, QLD, Australia', 'website_url': 'https://shotstack.io', 'link': 'https://stackoverflow.com/users/670511/jeff-s', 'profile_image': 'https://www.gravatar.com/avatar/9e010ed3aa19fcccc1b0b2c4f8e1f192?s=256&d=identicon&r=PG', 'display_name': 'Jeff S.'}","I have a strange problem with Python Boto3 when trying to do a batch_write_item to a DynamoDB table. I am following the documentation and trying to write a singe item. The table is setup correctly and I can run batch-write-item via the AWS cli no problem. Assuming the client and DynamoDB are set up correctly I run: I get the following error: botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the BatchWriteItem operation: Invalid attribute value type If I change it, removing the types and run: It works as expected. I need to use the previous format which follows the documentation and is compatibale with AWS cli. Is the documentation wrong or I missed a configuration setting, version issue or something else?","client.batch_write_item(RequestItems={
    ""myTable"": [
        {
            ""PutRequest"": {
                ""Item"": {
                    ""name"": {
                        ""S"": ""hello""
                    },
                    ""value"": {
                        ""S"": ""world""
                    }
                }
            }
        }
    ]
})
 client.batch_write_item(RequestItems={
    ""myTable"": [
        {
            ""PutRequest"": {
                ""Item"": {
                    ""name"": ""hello"",
                    ""value"": ""world""
                }
            }
        }
    ]
})
",26,47,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
21,48128029,48129025,40000,Installing specific BUILD of an anaconda package,1,<python><anaconda>,88,"<p>Is there any way to install a specific build+version of a package in Anaconda? Stack Overflow post ""<a href=""https://stackoverflow.com/questions/38411942/anaconda-conda-install-a-specific-package-version"">anaconda/conda - install a specific package version</a>"" shows how to install a specific <em>version</em> of the package. But look below--there are several pillow packages version 4.2.1 that has ""py27"" prefix on it.</p>

<p>Background: I am scratching my head to figure the meaning of ""conda search"" output. For example, on my installation, <code>conda search pillow</code> gives:</p>

<pre><code>pillow                       2.1.0                    py26_0  defaults
...
                          *  3.3.1                    py27_0  defaults        
                             3.3.1                    py34_0  defaults        
                             3.3.1                    py35_0  defaults        
....
                             4.2.1                    py27_0  defaults        
                             4.2.1                    py35_0  defaults        
                             4.2.1                    py36_0  defaults        
                             4.2.1            py27h7cd2321_0  defaults        
                             4.2.1            py35h03abc04_0  defaults        
                             4.2.1            py36h9119f52_0  defaults        
                             4.3.0            py35h550890c_1  defaults        
                             4.3.0            py27h353bd0c_1  defaults        
                             4.3.0            py36h6f462bf_1  defaults        
</code></pre>

<p>I understand the meaning of 2.1.0, 3.3.1, and so on--the version numbers. But what do <code>py27_0</code> and <code>defaults</code> mean? More mind boggling was new appearance of the trailing hex numbers like in <code>py27h7cd2321_0</code> . After researching some more:</p>

<p><a href=""https://www.anaconda.com/blog/developer-blog/package-better-conda-build-3/"" rel=""noreferrer"">https://www.anaconda.com/blog/developer-blog/package-better-conda-build-3/</a></p>

<p>tells me that that's a new way to encode the specific build of the package.</p>

<p>So back to my question: given that I'm still on Python 2.7 line of anaconda, how do we choose the <code>py27_0</code> build instead of the other one (<code>py27h7cd2321_0</code>) when we do <code>conda install</code>?</p>
",655885,3773,06-01-2018 13:51,06-01-2018 15:50,0,3793,28,3,29,29,"{'badge_counts': {'bronze': 28, 'silver': 29, 'gold': 3}, 'account_id': 331291, 'is_employee': False, 'last_modified_date': 1578709506, 'last_access_date': 1707944459, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 3793, 'creation_date': 1299871592, 'user_type': 'registered', 'user_id': 655885, 'accept_rate': 29, 'link': 'https://stackoverflow.com/users/655885/wirawan-purwanto', 'profile_image': 'https://www.gravatar.com/avatar/dd946b9538467776f9123ba72c71444f?s=256&d=identicon&r=PG', 'display_name': 'Wirawan Purwanto'}","Is there any way to install a specific build+version of a package in Anaconda? Stack Overflow post ""anaconda/conda - install a specific package version"" shows how to install a specific version of the package. But look below--there are several pillow packages version 4.2.1 that has ""py27"" prefix on it. Background: I am scratching my head to figure the meaning of ""conda search"" output. For example, on my installation, gives: I understand the meaning of 2.1.0, 3.3.1, and so on--the version numbers. But what do and mean? More mind boggling was new appearance of the trailing hex numbers like in . After researching some more: https://www.anaconda.com/blog/developer-blog/package-better-conda-build-3/ tells me that that's a new way to encode the specific build of the package. So back to my question: given that I'm still on Python 2.7 line of anaconda, how do we choose the build instead of the other one () when we do ?","conda search pillow pillow                       2.1.0                    py26_0  defaults
...
                          *  3.3.1                    py27_0  defaults        
                             3.3.1                    py34_0  defaults        
                             3.3.1                    py35_0  defaults        
....
                             4.2.1                    py27_0  defaults        
                             4.2.1                    py35_0  defaults        
                             4.2.1                    py36_0  defaults        
                             4.2.1            py27h7cd2321_0  defaults        
                             4.2.1            py35h03abc04_0  defaults        
                             4.2.1            py36h9119f52_0  defaults        
                             4.3.0            py35h550890c_1  defaults        
                             4.3.0            py27h353bd0c_1  defaults        
                             4.3.0            py36h6f462bf_1  defaults        
 py27_0 defaults py27h7cd2321_0 py27_0 py27h7cd2321_0 conda install",7,28,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
22,50091784,50091816,3551,unittest - compare list irrespective of order,2,<python><unit-testing>,11,"<p>I am doing a unit test on two list of list values:</p>

<pre><code>self.assertEqual(sale, [['1',14], ['2',5], ['3',7], ['4',1]])
</code></pre>

<p>But it gives the below error:</p>

<pre><code>AssertionError: Lists differ: [['1', 14], ['4', 1], ['2', 5], ['3', 7]] != [['1'
, 14], ['2', 5], ['3', 7], ['4', 1]]

First differing element 1:
['4', 1]
['2', 5]

- [['1', 14], ['4', 1], ['2', 5], ['3', 7]]
+ [['1', 14], ['2', 5], ['3', 7], ['4', 1]]
</code></pre>

<p>How can I make this scenario pass, Prevent the assertEqual function to avoid checking the order of the elements in the list.</p>
",848510,3168,29-04-2018 22:10,29-04-2018 22:14,0,3178,66,7,42,87,"{'badge_counts': {'bronze': 66, 'silver': 42, 'gold': 7}, 'collectives': [{'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 452204, 'is_employee': False, 'last_modified_date': 1710552000, 'last_access_date': 1711004154, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3178, 'creation_date': 1310893185, 'user_type': 'registered', 'user_id': 848510, 'accept_rate': 87, 'location': 'Thiruvananthapuram, Kerala, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/848510/tom-j-muthirenthi', 'profile_image': 'https://i.stack.imgur.com/9FcPS.jpg?s=256&g=1', 'display_name': 'Tom J Muthirenthi'}","I am doing a unit test on two list of list values: But it gives the below error: How can I make this scenario pass, Prevent the assertEqual function to avoid checking the order of the elements in the list.","self.assertEqual(sale, [['1',14], ['2',5], ['3',7], ['4',1]])
 AssertionError: Lists differ: [['1', 14], ['4', 1], ['2', 5], ['3', 7]] != [['1'
, 14], ['2', 5], ['3', 7], ['4', 1]]

First differing element 1:
['4', 1]
['2', 5]

- [['1', 14], ['4', 1], ['2', 5], ['3', 7]]
+ [['1', 14], ['2', 5], ['3', 7], ['4', 1]]
",8,19,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
23,48884796,48891252,17855,How to set up entry_points in setup.cfg,2,<python><python-3.x><python-3.6><setuptools>,62,"<p>I am moving my config from setup.py to setup.cfg and having issues setting up the <code>entry_points</code> parameter. At the moment I am using a hybrid approach which works, however, I would like to move the <code>entry_points</code> to setup.cfg.</p>
<p>From</p>
<pre><code>def setup_package():
    setup(version=get_version(),
          entry_points={'console_scripts':['app=my_package.app.run:cli'],})
</code></pre>
<p>to</p>
<pre><code>[metadata]
name = my-package
description = my-package
license = unlicensed
long-description = README.md
platforms = any
classifiers =
  Programming Language :: Python

[options]
zip_safe = False
packages = my_package,  my_package.app
include_package_data = True
package_dir =
  = .
tests_require = pytest; pytest-cov

[entry_points]
console_scripts =
  my-package = my_package.app.run:cli
</code></pre>
",465374,1541,20-02-2018 11:56,20-02-2018 17:43,0,1541,39,4,21,92,"{'badge_counts': {'bronze': 39, 'silver': 21, 'gold': 4}, 'account_id': 212514, 'is_employee': False, 'last_modified_date': 1573684801, 'last_access_date': 1638117397, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1541, 'creation_date': 1286138451, 'user_type': 'registered', 'user_id': 465374, 'accept_rate': 92, 'link': 'https://stackoverflow.com/users/465374/user465374', 'profile_image': 'https://www.gravatar.com/avatar/e0b2e1749c8821fe2ba6d28569fded7c?s=256&d=identicon&r=PG', 'display_name': 'user465374'}","I am moving my config from setup.py to setup.cfg and having issues setting up the parameter. At the moment I am using a hybrid approach which works, however, I would like to move the to setup.cfg. From to","entry_points entry_points def setup_package():
    setup(version=get_version(),
          entry_points={'console_scripts':['app=my_package.app.run:cli'],})
 [metadata]
name = my-package
description = my-package
license = unlicensed
long-description = README.md
platforms = any
classifiers =
  Programming Language :: Python

[options]
zip_safe = False
packages = my_package,  my_package.app
include_package_data = True
package_dir =
  = .
tests_require = pytest; pytest-cov

[entry_points]
console_scripts =
  my-package = my_package.app.run:cli
",19,28,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
24,48979604,48979788,82627,"Pandas, for each unique value in one column, get unique values in another column",3,<python><pandas>,32,"<p>I have a dataframe where each row contains various meta-data pertaining to a single Reddit comment (e.g. author, subreddit, comment text).</p>

<p>I want to do the following: for each author, I want to grab a list of all the subreddits they have comments in, and transform this data into a pandas dataframe where each row corresponds to an author, and a list of all the unique subreddits they comment in.</p>

<p>I am currently trying some combination of the following, but can't get it down:</p>

<p>Attempt 1:</p>

<pre><code>group = df['subreddit'].groupby(df['author']).unique()
list(group) 
</code></pre>

<p>Attempt 2:</p>

<pre><code>from collections import defaultdict
subreddit_dict  = defaultdict(list)

for index, row in df.iterrows():
    author = row['author']
    subreddit = row['subreddit']
    subreddit_dict[author].append(subreddit)

for key, value in subreddit_dict.items():
    subreddit_dict[key] = set(value)

subreddit_df = pd.DataFrame.from_dict(subreddit_dict, 
                            orient = 'index')
</code></pre>
",372526,11337,25-02-2018 23:30,26-02-2018 00:01,1,11347,167,30,98,93,"{'badge_counts': {'bronze': 167, 'silver': 98, 'gold': 30}, 'account_id': 155700, 'is_employee': False, 'last_modified_date': 1708738800, 'last_access_date': 1711051512, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 11347, 'creation_date': 1277000907, 'user_type': 'registered', 'user_id': 372526, 'accept_rate': 93, 'link': 'https://stackoverflow.com/users/372526/parseltongue', 'profile_image': 'https://www.gravatar.com/avatar/4d17467765f5923e9451ad0cbbd89848?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Parseltongue'}","I have a dataframe where each row contains various meta-data pertaining to a single Reddit comment (e.g. author, subreddit, comment text). I want to do the following: for each author, I want to grab a list of all the subreddits they have comments in, and transform this data into a pandas dataframe where each row corresponds to an author, and a list of all the unique subreddits they comment in. I am currently trying some combination of the following, but can't get it down: Attempt 1: Attempt 2:","group = df['subreddit'].groupby(df['author']).unique()
list(group) 
 from collections import defaultdict
subreddit_dict  = defaultdict(list)

for index, row in df.iterrows():
    author = row['author']
    subreddit = row['subreddit']
    subreddit_dict[author].append(subreddit)

for key, value in subreddit_dict.items():
    subreddit_dict[key] = set(value)

subreddit_df = pd.DataFrame.from_dict(subreddit_dict, 
                            orient = 'index')
",13,28,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
25,49711930,49887795,6745,How to use AsciiDoc with Python?,1,<python><asciidoc><asciidoctor>,11,"<p>The original AsciiDoc processor was written in python, but AsciiDoc evolved in the form of Asciidoctor written in Ruby.</p>

<p>On my search for how to process modern AsciiDoc (the Asciidoctor dialect), I only came across a post that there is an older project to port AsciiDoc to Python 3 (<a href=""https://github.com/asciidoc/asciidoc/issues/83"" rel=""noreferrer"">https://github.com/asciidoc/asciidoc/issues/83</a>). </p>

<p>But since the origins of AsciiDoc are in Python, it is hard to find anything useful via google.</p>

<p>So what is currently the best way to process AsciiDoc from within a Python program? Call the Ruby version or commandline processor? </p>
",204769,10852,07-04-2018 20:56,17-04-2018 21:30,10,10852,129,11,70,92,"{'badge_counts': {'bronze': 129, 'silver': 70, 'gold': 11}, 'account_id': 70694, 'is_employee': False, 'last_modified_date': 1707770400, 'last_access_date': 1711044651, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 10852, 'creation_date': 1257508525, 'user_type': 'registered', 'user_id': 204769, 'accept_rate': 92, 'location': 'Frankfurt, Germany', 'website_url': 'https://rdmueller.github.io/', 'link': 'https://stackoverflow.com/users/204769/rdmueller', 'profile_image': 'https://www.gravatar.com/avatar/cc5f3bf8b3cb91c985ed4fd046aa451d?s=256&d=identicon&r=PG', 'display_name': 'rdmueller'}","The original AsciiDoc processor was written in python, but AsciiDoc evolved in the form of Asciidoctor written in Ruby. On my search for how to process modern AsciiDoc (the Asciidoctor dialect), I only came across a post that there is an older project to port AsciiDoc to Python 3 (https://github.com/asciidoc/asciidoc/issues/83). But since the origins of AsciiDoc are in Python, it is hard to find anything useful via google. So what is currently the best way to process AsciiDoc from within a Python program? Call the Ruby version or commandline processor?",,0,7,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
26,48132786,48133124,41612,"Why is this warning ""Expected type 'int' (matched generic type '_T'), got 'Dict[str, None]' instead""?",1,<python><pycharm>,22,"<p>Please watch carefully the question and carefully the answers of <a href=""https://stackoverflow.com/questions/27797011/what-does-this-warning-in-pycharm-mean"">this and you'll see it's not a duplicate</a>, especially because they dont answer my question.</p>

<p>Try to make a new empty project, and add this code. It works fine without warnings:</p>

<pre><code>game_data = {'boats': [], }
game_data['boats'].append({'name': None})
</code></pre>

<p>Now change it to:</p>

<pre><code>game_data = {'boats': [], 'width': None, 'height': None, }
game_data['boats'].append({'name': None})
</code></pre>

<p>Still no warnings.
And change again to:</p>

<pre><code>w = 12
game_data = {'boats': [], 'width': None, 'height': w, }
game_data['boats'].append({'name': None})
</code></pre>

<p>And now you'll get:</p>

<pre><code>Expected type 'int' (matched generic type '_T'), got 'Dict[str, None]' instead
</code></pre>

<p>Am I the only one to have this? And why is this? Is there a solution to make this warning go away?</p>
",106140,15593,06-01-2018 23:08,07-01-2018 00:00,1,15593,219,27,119,99,"{'badge_counts': {'bronze': 219, 'silver': 119, 'gold': 27}, 'account_id': 37145, 'is_employee': False, 'last_modified_date': 1711155900, 'last_access_date': 1711111964, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 15593, 'creation_date': 1242207491, 'user_type': 'registered', 'user_id': 106140, 'accept_rate': 99, 'location': 'France', 'website_url': 'https://olivierpons.fr/', 'link': 'https://stackoverflow.com/users/106140/olivier-pons', 'profile_image': 'https://www.gravatar.com/avatar/f976af5e820922c8ebc645e42f8e17a4?s=256&d=identicon&r=PG', 'display_name': 'Olivier Pons'}","Please watch carefully the question and carefully the answers of this and you'll see it's not a duplicate, especially because they dont answer my question. Try to make a new empty project, and add this code. It works fine without warnings: Now change it to: Still no warnings. And change again to: And now you'll get: Am I the only one to have this? And why is this? Is there a solution to make this warning go away?","game_data = {'boats': [], }
game_data['boats'].append({'name': None})
 game_data = {'boats': [], 'width': None, 'height': None, }
game_data['boats'].append({'name': None})
 w = 12
game_data = {'boats': [], 'width': None, 'height': w, }
game_data['boats'].append({'name': None})
 Expected type 'int' (matched generic type '_T'), got 'Dict[str, None]' instead
",4,28,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
27,49890477,49899524,6684,Log accuracy metric while training a tf.estimator,2,<python><tensorflow><tensorboard><tensorflow-estimator>,12,"<p>What's the simplest way to print accuracy metrics along with the loss when training a pre-canned estimator?</p>

<p>Most tutorials and documentations seem to address the issue of when you're creating a custom estimator -- which seems overkill if the intention is to use one of the available ones.</p>

<p>tf.contrib.learn had a few (now deprecated) Monitor hooks. TF now suggests using the hook API, but it appears that it doesn't actually come with anything that can utilize the labels and predictions to generate an accuracy number. </p>
",98975,7622,18-04-2018 03:01,18-04-2018 12:17,0,7622,54,9,43,90,"{'badge_counts': {'bronze': 54, 'silver': 43, 'gold': 9}, 'account_id': 34994, 'is_employee': False, 'last_modified_date': 1709925600, 'last_access_date': 1711149132, 'reputation_change_year': -20, 'reputation_change_quarter': -20, 'reputation_change_month': -20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7622, 'creation_date': 1241127194, 'user_type': 'registered', 'user_id': 98975, 'accept_rate': 90, 'location': 'San Francisco, CA', 'website_url': 'http://viksit.com', 'link': 'https://stackoverflow.com/users/98975/viksit', 'profile_image': 'https://www.gravatar.com/avatar/fac3974a8e1790ea120c90fe90df1ee4?s=256&d=identicon&r=PG', 'display_name': 'viksit'}","What's the simplest way to print accuracy metrics along with the loss when training a pre-canned estimator? Most tutorials and documentations seem to address the issue of when you're creating a custom estimator -- which seems overkill if the intention is to use one of the available ones. tf.contrib.learn had a few (now deprecated) Monitor hooks. TF now suggests using the hook API, but it appears that it doesn't actually come with anything that can utilize the labels and predictions to generate an accuracy number.",,0,5,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
28,48155294,48175912,27799,What is pycryptodomex and how does it differ from pycryptodome?,1,<python><fork><pycryptodome>,27,"<p>Today I saw PySNMP installing pycryptodomex. The <code>x</code> in that name looked suspicious and surprising.</p>

<p>I tried to track it down, but it looks like both <a href=""https://pypi.org/project/pycryptodome"" rel=""noreferrer"">pycryptodome</a> and <a href=""https://pypi.org/project/pycryptodomex"" rel=""noreferrer"">pycryptodomex</a> are owned by the same account and point back to the <a href=""https://github.com/Legrandin/pycryptodome"" rel=""noreferrer"">same Github repository</a>.</p>

<p>Especially because a cryptography library is a core security feature, I'm suspicious of the duplication.</p>

<p>What's the purpose of this duplication? Could I have discovered this information from open sources?</p>
",70170,41815,08-01-2018 17:36,09-01-2018 20:06,1,41845,93,10,86,95,"{'badge_counts': {'bronze': 93, 'silver': 86, 'gold': 10}, 'account_id': 26832, 'is_employee': False, 'last_modified_date': 1697720700, 'last_access_date': 1711158835, 'reputation_change_year': 316, 'reputation_change_quarter': 316, 'reputation_change_month': 70, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 41845, 'creation_date': 1235440557, 'user_type': 'registered', 'user_id': 70170, 'accept_rate': 95, 'location': 'Pittsburgh, PA, USA', 'website_url': 'http://www.jaraco.com', 'link': 'https://stackoverflow.com/users/70170/jason-r-coombs', 'profile_image': 'https://www.gravatar.com/avatar/163acaf31cfd01645493404a2d379df6?s=256&d=identicon&r=PG', 'display_name': 'Jason R. Coombs'}","Today I saw PySNMP installing pycryptodomex. The in that name looked suspicious and surprising. I tried to track it down, but it looks like both pycryptodome and pycryptodomex are owned by the same account and point back to the same Github repository. Especially because a cryptography library is a core security feature, I'm suspicious of the duplication. What's the purpose of this duplication? Could I have discovered this information from open sources?",x,-1,7,0,3,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
29,49929551,50077729,859,Use full page width with Brother P950NW,2,<python><python-3.x><printing><escp>,14,"<p>I would like to use the full width of the 18mm strips in my Brother P950NW printer for an image. At the moment, I am using ESC/P (not ESC/POS, which this printer does not seem to support), but if it's not possible with that I'm fine with any other protocol this printer supports. (Update: with Brother's Windows software, full-width printing is possible, but it uses the LPR protocol, for which there don't seem to be any Python libraries.)</p>

<p>I'm using the ESC/P command ESC* with density 72 (the highest available according to <a href=""http://download.brother.com/welcome/docp100186/cv_ptp900_eng_escp_101.pdf"" rel=""noreferrer"">the printer's documentation</a>), which only allows filling up the width in steps of 48 dots.</p>

<p>How do I print 200 pixels wide on a strip in ESC/P-speak an image with height 200? That should easily fit onto the strip. However, due to ESC*72 only accepting blocks of 48, everything beyond 192 is output on another strip.</p>

<p>Here's my demo code:</p>

<pre><code>import socket
import struct

def escp(density_code=72):
    stack_size_in_bytes = {72: 6}[density_code]
    height = 200
    width = 130

    yield b'\x1bia\x00'  # ESC/P command mode: ESC/P standard
    yield b'\x1b@'  # Initialize
    yield b'\x1bim\x00\x00'  # margin: 0
    yield b'\x1biXE2\x00\x00\x00'  # barcode margin: 0
    yield b'\x1b3' + struct.pack('!B', 24)  # line feed length: 24 dots (i.e. no space between lines)

    for y_offset in range(0, height, 8 * stack_size_in_bytes):
        yield b'\x1b*' + struct.pack('!B', density_code) + struct.pack('&lt;H', width)
        yield b'\xff' * width * stack_size_in_bytes
        yield b'\x0a'  # linefeed (move position 24 dots down)
    yield b'\x0c' # Print start

c = socket.create_connection(('10.222.2.206', 9100))
c.sendall(b''.join(escp()))
c.close()
</code></pre>

<p>I'm fine with solutions in raw binary; here is <a href=""https://phihag.de/2018/so/escp.hd"" rel=""noreferrer"">the binary file</a> and <a href=""https://phihag.de/2018/so/escp.hd"" rel=""noreferrer"">shortened hexdump</a> generated by the above program.</p>
",35070,282500,19-04-2018 20:18,28-04-2018 14:46,9,282800,478,74,462,78,"{'badge_counts': {'bronze': 478, 'silver': 462, 'gold': 74}, 'account_id': 16209, 'is_employee': False, 'last_modified_date': 1708799400, 'last_access_date': 1711154743, 'reputation_change_year': 1903, 'reputation_change_quarter': 1903, 'reputation_change_month': 558, 'reputation_change_week': 160, 'reputation_change_day': 0, 'reputation': 282800, 'creation_date': 1225976077, 'user_type': 'registered', 'user_id': 35070, 'accept_rate': 78, 'location': 'D&#252;sseldorf, Germany', 'website_url': 'http://phihag.de/', 'link': 'https://stackoverflow.com/users/35070/phihag', 'profile_image': 'https://www.gravatar.com/avatar/6f92354195e8874dbee44d5c8714d506?s=256&d=identicon&r=PG', 'display_name': 'phihag'}","I would like to use the full width of the 18mm strips in my Brother P950NW printer for an image. At the moment, I am using ESC/P (not ESC/POS, which this printer does not seem to support), but if it's not possible with that I'm fine with any other protocol this printer supports. (Update: with Brother's Windows software, full-width printing is possible, but it uses the LPR protocol, for which there don't seem to be any Python libraries.) I'm using the ESC/P command ESC* with density 72 (the highest available according to the printer's documentation), which only allows filling up the width in steps of 48 dots. How do I print 200 pixels wide on a strip in ESC/P-speak an image with height 200? That should easily fit onto the strip. However, due to ESC*72 only accepting blocks of 48, everything beyond 192 is output on another strip. Here's my demo code: I'm fine with solutions in raw binary; here is the binary file and shortened hexdump generated by the above program.","import socket
import struct

def escp(density_code=72):
    stack_size_in_bytes = {72: 6}[density_code]
    height = 200
    width = 130

    yield b'\x1bia\x00'  # ESC/P command mode: ESC/P standard
    yield b'\x1b@'  # Initialize
    yield b'\x1bim\x00\x00'  # margin: 0
    yield b'\x1biXE2\x00\x00\x00'  # barcode margin: 0
    yield b'\x1b3' + struct.pack('!B', 24)  # line feed length: 24 dots (i.e. no space between lines)

    for y_offset in range(0, height, 8 * stack_size_in_bytes):
        yield b'\x1b*' + struct.pack('!B', density_code) + struct.pack('&lt;H', width)
        yield b'\xff' * width * stack_size_in_bytes
        yield b'\x0a'  # linefeed (move position 24 dots down)
    yield b'\x0c' # Print start

c = socket.create_connection(('10.222.2.206', 9100))
c.sendall(b''.join(escp()))
c.close()
",22,34,0,3,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
30,49699244,49797761,21680,Customize module search path (PYTHONPATH) via pipenv,4,<python><python-3.x><pythonpath><pipenv>,40,"<p>I have a Python project consisting of a Jupyter notebook, several scripts in a <code>bin</code> directory and modules in a <code>src</code> directory, with dependencies in a <code>Pipfile</code>:</p>

<pre><code>myproject
├── myproject.ipynb
├── Pipfile
├── Pipfile.lock
├── bin
│   ├── bar.py
│   └── foo.py
└── src
    ├── baz.py
    └── qux.py
</code></pre>

<p>The scripts <code>foo.py</code> and <code>bar.py</code> use the standard shebang</p>

<pre><code>#!/usr/bin/env python
</code></pre>

<p>and can be run with <code>pipenv shell</code>:</p>

<pre><code>mymachine:myproject myname$ pipenv shell
(myproject-U308romt) bash-3.2$ bin/foo.py
foo
</code></pre>

<p>However, I can't easily access the modules in <code>src</code> from the scripts. If I add</p>

<pre><code>import src.baz as baz
</code></pre>

<p>to <code>foo.py</code>, I get:</p>

<pre><code>ModuleNotFoundError: No module named 'src'
</code></pre>

<p>One solution I tried is to add a <code>.env</code> file under <code>myproject</code>:</p>

<pre><code>PYTHONPATH=${PYTHONPATH}:${PWD}
</code></pre>

<p>This works thanks to <code>pipenv</code>'s <a href=""https://github.com/pypa/pipenv/blob/master/docs/advanced.rst#-automatic-loading-of-env"" rel=""noreferrer"">automatic loading of <code>.env</code></a>, but checking the <code>.env</code> file into the git distribution of the project would collide with the traditional use of <code>.env</code> to store secrets such as passwords -- in fact, my default <code>.gitignore</code> for Python projects already excludes <code>.env</code> for just this reason.</p>

<pre><code>$ git add .env
The following paths are ignored by one of your .gitignore files:
.env
Use -f if you really want to add them.
</code></pre>

<p>Alternatively, I could move <code>src</code> under <code>bin</code>, but then the Jupyter notebook would have to reference the modules as <code>bin.src.baz</code> etc., which is also a hassle.</p>

<p>My current workaround is just to add a symlink:</p>

<pre><code>myproject
├── Pipfile
├── Pipfile.lock
├── bin
│   ├── bar.py
│   ├── foo.py
│   └── src -&gt; ../src
└── src
    ├── baz.py
    └── qux.py
</code></pre>

<p>This works, and I suppose has the benefit of being transparent, but it seems like there should be some way to leverage <code>pipenv</code> to solve the same problem.</p>

<p>Is there a portable, distributable way to put these modules on the search path?</p>
",27358,49314,06-04-2018 18:45,12-04-2018 13:23,6,49414,238,27,139,76,"{'badge_counts': {'bronze': 238, 'silver': 139, 'gold': 27}, 'account_id': 13566, 'is_employee': False, 'last_modified_date': 1708740303, 'last_access_date': 1707655135, 'reputation_change_year': 499, 'reputation_change_quarter': 499, 'reputation_change_month': 140, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 49414, 'creation_date': 1223889899, 'user_type': 'registered', 'user_id': 27358, 'accept_rate': 76, 'location': 'San Francisco, CA', 'website_url': 'http://www.dmoles.net/', 'link': 'https://stackoverflow.com/users/27358/david-moles', 'profile_image': 'https://www.gravatar.com/avatar/fed1635f89478c88eda07e87de452bcf?s=256&d=identicon&r=PG', 'display_name': 'David Moles'}","I have a Python project consisting of a Jupyter notebook, several scripts in a directory and modules in a directory, with dependencies in a : The scripts and use the standard shebang and can be run with : However, I can't easily access the modules in from the scripts. If I add to , I get: One solution I tried is to add a file under : This works thanks to 's automatic loading of , but checking the file into the git distribution of the project would collide with the traditional use of to store secrets such as passwords -- in fact, my default for Python projects already excludes for just this reason. Alternatively, I could move under , but then the Jupyter notebook would have to reference the modules as etc., which is also a hassle. My current workaround is just to add a symlink: This works, and I suppose has the benefit of being transparent, but it seems like there should be some way to leverage to solve the same problem. Is there a portable, distributable way to put these modules on the search path?","bin src Pipfile myproject
├── myproject.ipynb
├── Pipfile
├── Pipfile.lock
├── bin
│   ├── bar.py
│   └── foo.py
└── src
    ├── baz.py
    └── qux.py
 foo.py bar.py #!/usr/bin/env python
 pipenv shell mymachine:myproject myname$ pipenv shell
(myproject-U308romt) bash-3.2$ bin/foo.py
foo
 src import src.baz as baz
 foo.py ModuleNotFoundError: No module named 'src'
 .env myproject PYTHONPATH=${PYTHONPATH}:${PWD}
 pipenv .env .env .env .gitignore .env $ git add .env
The following paths are ignored by one of your .gitignore files:
.env
Use -f if you really want to add them.
 src bin bin.src.baz myproject
├── Pipfile
├── Pipfile.lock
├── bin
│   ├── bar.py
│   ├── foo.py
│   └── src -&gt; ../src
└── src
    ├── baz.py
    └── qux.py
 pipenv",3,68,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
31,48186782,48187723,9483,"Combining Django F, Value and a dict to annotate a queryset",2,<python><django>,21,"<p>I have a scenario where I want to annotate a queryset with externally prepared data in a dict.  I want to do something like the following:</p>

<pre><code>value_dict = {""model1"": 123.4, ""model2"": 567.8}
qs = ModelClass.objects.annotate(
    value=Value(value_dict.get(F('model__code'), 0))
)
</code></pre>

<p>The results currently show all as 0 as the F() doesn't seem to be the best way to look up the dict seeing as it doesn't return a string and it is resolved further down the track.</p>

<p>Your help and suggestions would be much appreciated</p>

<p>I'm currently on Python 3.6 and Django 1.11</p>
",1184181,2034,10-01-2018 11:46,10-01-2018 12:41,0,2034,23,1,19,,"{'badge_counts': {'bronze': 23, 'silver': 19, 'gold': 1}, 'account_id': 1216747, 'is_employee': False, 'last_modified_date': 1657831200, 'last_access_date': 1710964241, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2034, 'creation_date': 1328151918, 'user_type': 'registered', 'user_id': 1184181, 'location': 'Amman, Amman Governorate, Jordan', 'website_url': 'http://www.artisan.dev', 'link': 'https://stackoverflow.com/users/1184181/artisan', 'profile_image': 'https://i.stack.imgur.com/D6c8J.jpg?s=256&g=1', 'display_name': 'Artisan'}",I have a scenario where I want to annotate a queryset with externally prepared data in a dict. I want to do something like the following: The results currently show all as 0 as the F() doesn't seem to be the best way to look up the dict seeing as it doesn't return a string and it is resolved further down the track. Your help and suggestions would be much appreciated I'm currently on Python 3.6 and Django 1.11,"value_dict = {""model1"": 123.4, ""model2"": 567.8}
qs = ModelClass.objects.annotate(
    value=Value(value_dict.get(F('model__code'), 0))
)
",3,13,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
32,48978550,48978569,98901,Pandas: Filtering multiple conditions,4,<python><pandas>,43,"<p>I'm trying to do boolean indexing with a couple conditions using Pandas.  My original DataFrame is called <code>df</code>.  If I perform the below, I get the expected result:</p>

<pre><code>temp = df[df[""bin""] == 3]
temp = temp[(~temp[""Def""])]
temp = temp[temp[""days since""] &gt; 7]
temp.head()
</code></pre>

<p>However, if I do this (which I think should be equivalent), I get no rows back:</p>

<pre><code>temp2 = df[df[""bin""] == 3]
temp2 = temp2[~temp2[""Def""] &amp; temp2[""days since""] &gt; 7]
temp2.head()
</code></pre>

<p>Any idea what accounts for the difference? </p>
",1316501,9021,25-02-2018 21:16,25-02-2018 21:18,0,9031,151,26,85,63,"{'badge_counts': {'bronze': 151, 'silver': 85, 'gold': 26}, 'account_id': 1384106, 'is_employee': False, 'last_modified_date': 1700271000, 'last_access_date': 1638487437, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 9031, 'creation_date': 1333669842, 'user_type': 'registered', 'user_id': 1316501, 'accept_rate': 63, 'website_url': '', 'link': 'https://stackoverflow.com/users/1316501/anon-swe', 'profile_image': 'https://www.gravatar.com/avatar/3f0ddaf1d7d2aaeeb19b6ec8a99a70ee?s=256&d=identicon&r=PG', 'display_name': 'anon_swe'}","I'm trying to do boolean indexing with a couple conditions using Pandas. My original DataFrame is called . If I perform the below, I get the expected result: However, if I do this (which I think should be equivalent), I get no rows back: Any idea what accounts for the difference?","df temp = df[df[""bin""] == 3]
temp = temp[(~temp[""Def""])]
temp = temp[temp[""days since""] &gt; 7]
temp.head()
 temp2 = df[df[""bin""] == 3]
temp2 = temp2[~temp2[""Def""] &amp; temp2[""days since""] &gt; 7]
temp2.head()
",4,16,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
33,48087980,48203617,2302,Annotate QuerySet with first value of ordered related model,2,<python><django><postgresql><django-queryset><django-1.10>,15,"<p>I have a <code>QuerySet</code> of some objects. For each one, I wish to annotate with the minimum value of a related model (joined on a few conditions, ordered by date). I can express my desired results neatly in SQL, but am curious how to translate to Django's ORM.</p>

<h3>Background</h3>

<p>Let's say that I have two related models: <code>Book</code>, and <code>BlogPost</code>, each with a foreign key to an <code>Author</code>:</p>

<pre><code>class Book(models.Model):
    title = models.CharField(max_length=255)
    genre = models.CharField(max_length=63)
    author = models.ForeignKey(Author)
    date_published = models.DateField()

class BlogPost(models.Model):
    author = models.ForeignKey(Author)
    date_published = models.DateField()
</code></pre>

<p>I'm trying to find the first mystery book that a given author published after each blog post that they write. In SQL, this can be achieved nicely with windowing.</p>

<h3>Working solution in PostgreSQL 9.6</h3>

<pre class=""lang-sql prettyprint-override""><code>WITH ordered AS (
  SELECT blog_post.id,
         book.title,
         ROW_NUMBER() OVER (
            PARTITION BY blog_post.id ORDER BY book.date_published
         ) AS rn
    FROM blog_post
         LEFT JOIN book ON book.author_id = blog_post.author_id
                       AND book.genre = 'mystery'
                       AND book.date_published &gt;= blog_post.date_published
)
SELECT id,
       title
  FROM ordered
 WHERE rn = 1;
</code></pre>

<h3>Translating to Django's ORM</h3>

<p>While the above SQL suits my needs well (and I could use raw SQL if needed), I'm curious as to how one would do this in QuerySet. I have an existing QuerySet where I'd like to annotate it even further</p>

<pre><code>books = models.Book.objects.filter(...).select_related(...).prefetch_related(...)
annotated_books = books.annotate(
    most_recent_title=...
)
</code></pre>

<p>I'm aware that Django 2.0 supports window functions, but I'm on Django 1.10 for now.</p>

<h3>Attempted solution</h3>

<p>I'd first built a <code>Q</code> object to filter down to mystery books published after the blog post.</p>

<pre><code>published_after = Q(
    author__book__date_published__gte=F('date_published'),
    author__book__genre='mystery'
)
</code></pre>

<p>From here, I attempted to piece together <code>django.db.models.Min</code> and additional <code>F</code> objects to acheive my desired results, but with no success.</p>

<p>Note: Django 2.0 introduces window expressions, but I'm currently on Django 1.10, and curious how one would do this with the QuerySet features available there.</p>
",815632,16885,04-01-2018 02:47,11-01-2018 09:29,7,16895,76,14,66,86,"{'badge_counts': {'bronze': 76, 'silver': 66, 'gold': 14}, 'account_id': 431516, 'is_employee': False, 'last_modified_date': 1704888900, 'last_access_date': 1695093307, 'reputation_change_year': 178, 'reputation_change_quarter': 178, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 16895, 'creation_date': 1309031741, 'user_type': 'registered', 'user_id': 815632, 'accept_rate': 86, 'location': 'Berkeley, CA, USA', 'website_url': 'https://dcain.me', 'link': 'https://stackoverflow.com/users/815632/david-cain', 'profile_image': 'https://www.gravatar.com/avatar/0309bb55fa6f8ddfe21d28d634e44674?s=256&d=identicon&r=PG', 'display_name': 'David Cain'}","I have a of some objects. For each one, I wish to annotate with the minimum value of a related model (joined on a few conditions, ordered by date). I can express my desired results neatly in SQL, but am curious how to translate to Django's ORM. Background Let's say that I have two related models: , and , each with a foreign key to an : I'm trying to find the first mystery book that a given author published after each blog post that they write. In SQL, this can be achieved nicely with windowing. Working solution in PostgreSQL 9.6 Translating to Django's ORM While the above SQL suits my needs well (and I could use raw SQL if needed), I'm curious as to how one would do this in QuerySet. I have an existing QuerySet where I'd like to annotate it even further I'm aware that Django 2.0 supports window functions, but I'm on Django 1.10 for now. Attempted solution I'd first built a object to filter down to mystery books published after the blog post. From here, I attempted to piece together and additional objects to acheive my desired results, but with no success. Note: Django 2.0 introduces window expressions, but I'm currently on Django 1.10, and curious how one would do this with the QuerySet features available there.","QuerySet Book BlogPost Author class Book(models.Model):
    title = models.CharField(max_length=255)
    genre = models.CharField(max_length=63)
    author = models.ForeignKey(Author)
    date_published = models.DateField()

class BlogPost(models.Model):
    author = models.ForeignKey(Author)
    date_published = models.DateField()
 WITH ordered AS (
  SELECT blog_post.id,
         book.title,
         ROW_NUMBER() OVER (
            PARTITION BY blog_post.id ORDER BY book.date_published
         ) AS rn
    FROM blog_post
         LEFT JOIN book ON book.author_id = blog_post.author_id
                       AND book.genre = 'mystery'
                       AND book.date_published &gt;= blog_post.date_published
)
SELECT id,
       title
  FROM ordered
 WHERE rn = 1;
 books = models.Book.objects.filter(...).select_related(...).prefetch_related(...)
annotated_books = books.annotate(
    most_recent_title=...
)
 Q published_after = Q(
    author__book__date_published__gte=F('date_published'),
    author__book__genre='mystery'
)
 django.db.models.Min F",21,63,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
34,50299172,50299238,42423,Range or numpy Arange with end limit include,7,<python><numpy>,25,"<p>I am looking to get :</p>

<p>input:</p>

<pre><code>arange(0.0,0.6,0.2)
</code></pre>

<p>output: </p>

<pre><code>0.,0.4
</code></pre>

<p>I want</p>

<pre><code>0.,0.2,0.4,0.6
</code></pre>

<p>how do i achieve using range or arange. If not what is alternate ?</p>
",841612,1449,11-05-2018 19:40,11-05-2018 19:44,0,1449,43,3,21,54,"{'badge_counts': {'bronze': 43, 'silver': 21, 'gold': 3}, 'account_id': 447780, 'is_employee': False, 'last_modified_date': 1649469300, 'last_access_date': 1687461603, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1449, 'creation_date': 1310506911, 'user_type': 'registered', 'user_id': 841612, 'accept_rate': 54, 'location': 'Texas', 'website_url': '', 'link': 'https://stackoverflow.com/users/841612/wpfkk', 'profile_image': 'https://www.gravatar.com/avatar/7db064a093c3bcb2c8169ff6750146dc?s=256&d=identicon&r=PG', 'display_name': 'WPFKK'}",I am looking to get : input: output: I want how do i achieve using range or arange. If not what is alternate ?,"arange(0.0,0.6,0.2)
 0.,0.4
 0.,0.2,0.4,0.6
",0,18,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
35,49112552,49134333,8300,Vectorized string operations in Numpy: why are they rather slow?,1,<python><numpy><benchmarking>,11,"<p>This is of those ""mostly asked out of pure curiosity (in possibly futile hope I will learn something)"" questions.</p>

<p>I was investigating ways of saving memory on operations on massive numbers of strings, and for <em>some</em> scenarios it seems like <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.char.html"" rel=""noreferrer"">string operations in numpy</a> could be useful. However, I got somewhat surprising results:</p>

<pre><code>import random
import string

milstr = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(1000000)]

npmstr = np.array(milstr, dtype=np.dtype(np.unicode_, 1000000))
</code></pre>

<p>Memory consumption using <code>memory_profiler</code>:</p>

<pre><code>%memit [x.upper() for x in milstr]
peak memory: 420.96 MiB, increment: 61.02 MiB

%memit np.core.defchararray.upper(npmstr)
peak memory: 391.48 MiB, increment: 31.52 MiB
</code></pre>

<p>So far, so good; however, timing results are surprising for me:</p>

<pre><code>%timeit [x.upper() for x in milstr]
129 ms ± 926 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit np.core.defchararray.upper(npmstr)
373 ms ± 2.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>

<p>Why is that? I expected that since Numpy uses contiguous chunks of memory for its arrays AND its operations are vectorized (as the above numpy doc page says) AND numpy string arrays apparently use less memory so operating on them should at least potentially be more on-CPU cache-friendly, performance on arrays of strings would be at least similar to those in pure Python?</p>

<p>Environment: </p>

<p>Python 3.6.3 x64, Linux</p>

<p>numpy==1.14.1</p>
",857741,6573,05-03-2018 14:21,06-03-2018 15:24,1,6574,96,10,55,80,"{'badge_counts': {'bronze': 96, 'silver': 55, 'gold': 10}, 'account_id': 785941, 'is_employee': False, 'last_modified_date': 1710768000, 'last_access_date': 1710866174, 'reputation_change_year': 51, 'reputation_change_quarter': 51, 'reputation_change_month': 31, 'reputation_change_week': 1, 'reputation_change_day': 0, 'reputation': 6574, 'creation_date': 1311330892, 'user_type': 'registered', 'user_id': 857741, 'accept_rate': 80, 'website_url': '', 'link': 'https://stackoverflow.com/users/857741/letmesothat4u', 'profile_image': 'https://www.gravatar.com/avatar/1aebaf4fe7c5b3aa038d52f8c6c3301a?s=256&d=identicon&r=PG', 'display_name': 'LetMeSOThat4U'}","This is of those ""mostly asked out of pure curiosity (in possibly futile hope I will learn something)"" questions. I was investigating ways of saving memory on operations on massive numbers of strings, and for some scenarios it seems like string operations in numpy could be useful. However, I got somewhat surprising results: Memory consumption using : So far, so good; however, timing results are surprising for me: Why is that? I expected that since Numpy uses contiguous chunks of memory for its arrays AND its operations are vectorized (as the above numpy doc page says) AND numpy string arrays apparently use less memory so operating on them should at least potentially be more on-CPU cache-friendly, performance on arrays of strings would be at least similar to those in pure Python? Environment: Python 3.6.3 x64, Linux numpy==1.14.1","import random
import string

milstr = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(1000000)]

npmstr = np.array(milstr, dtype=np.dtype(np.unicode_, 1000000))
 memory_profiler %memit [x.upper() for x in milstr]
peak memory: 420.96 MiB, increment: 61.02 MiB

%memit np.core.defchararray.upper(npmstr)
peak memory: 391.48 MiB, increment: 31.52 MiB
 %timeit [x.upper() for x in milstr]
129 ms ± 926 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit np.core.defchararray.upper(npmstr)
373 ms ± 2.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
",12,37,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
36,48743106,48743337,24041,"What's ""ANSI_X3.4-1968"" encoding?",2,<python><python-3.x><encoding><character-encoding><python-unicode>,25,"<p>See following output on my system:</p>

<pre class=""lang-none prettyprint-override""><code>[STEP 101] # python3 -c 'import sys; print(sys.stdout.encoding)'
ANSI_X3.4-1968
[STEP 102] #
[STEP 103] # locale
LANG=C
LANGUAGE=en_US:en
LC_CTYPE=""C""
LC_NUMERIC=""C""
LC_TIME=""C""
LC_COLLATE=""C""
LC_MONETARY=""C""
LC_MESSAGES=""C""
LC_PAPER=""C""
LC_NAME=""C""
LC_ADDRESS=""C""
LC_TELEPHONE=""C""
LC_MEASUREMENT=""C""
LC_IDENTIFICATION=""C""
LC_ALL=C
[STEP 104] #
</code></pre>

<p>Googled but found very little info about it. Even Python's <em>The Python Library Reference (v3.5.2)</em> does not mention it. Any international standard defines it?</p>

<hr>

<p>(Copied the authoritative ref from the accepted answer's comment: <a href=""https://www.iana.org/assignments/character-sets/character-sets.xhtml"" rel=""noreferrer"">Character Sets</a>)</p>
",900078,20045,12-02-2018 09:27,12-02-2018 09:40,0,20065,60,6,39,73,"{'badge_counts': {'bronze': 60, 'silver': 39, 'gold': 6}, 'account_id': 484418, 'is_employee': False, 'last_modified_date': 1709640300, 'last_access_date': 1711157318, 'reputation_change_year': 440, 'reputation_change_quarter': 440, 'reputation_change_month': 116, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 20065, 'creation_date': 1313651826, 'user_type': 'registered', 'user_id': 900078, 'accept_rate': 73, 'location': 'Beijing, China', 'website_url': '', 'link': 'https://stackoverflow.com/users/900078/pynexj', 'profile_image': 'https://i.stack.imgur.com/m5Mpk.jpg?s=256&g=1', 'display_name': 'pynexj'}",See following output on my system: Googled but found very little info about it. Even Python's The Python Library Reference (v3.5.2) does not mention it. Any international standard defines it? (Copied the authoritative ref from the accepted answer's comment: Character Sets),"[STEP 101] # python3 -c 'import sys; print(sys.stdout.encoding)'
ANSI_X3.4-1968
[STEP 102] #
[STEP 103] # locale
LANG=C
LANGUAGE=en_US:en
LC_CTYPE=""C""
LC_NUMERIC=""C""
LC_TIME=""C""
LC_COLLATE=""C""
LC_MONETARY=""C""
LC_MESSAGES=""C""
LC_PAPER=""C""
LC_NAME=""C""
LC_ADDRESS=""C""
LC_TELEPHONE=""C""
LC_MEASUREMENT=""C""
LC_IDENTIFICATION=""C""
LC_ALL=C
[STEP 104] #
",19,29,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
37,48682147,48682456,24442,aiohttp: rate limiting parallel requests,5,<python><parallel-processing><python-asyncio><aiohttp>,38,"<p>APIs often have rate limits that users have to follow. As an example let's take 50 requests/second. Sequential requests take 0.5-1 second and thus are too slow to come close to that limit. Parallel requests with aiohttp, however, exceed the rate limit.</p>

<p>To poll the API as fast as allowed, one needs to rate limit parallel calls.</p>

<p>Examples that I found so far decorate <code>session.get</code>, approximately like so:</p>

<pre><code>session.get = rate_limited(max_calls_per_second)(session.get)
</code></pre>

<p>This works well for sequential calls. Trying to implement this in parallel calls does not work as intended.</p>

<p>Here's some code as example:</p>

<pre><code>async with aiohttp.ClientSession() as session:
    session.get = rate_limited(max_calls_per_second)(session.get)
    tasks = (asyncio.ensure_future(download_coroutine(  
          timeout, session, url)) for url in urls)
    process_responses_function(await asyncio.gather(*tasks))
</code></pre>

<p>The problem with this is that it will rate-limit the <strong>queueing</strong> of the tasks. The execution with <code>gather</code> will still happen more or less at the same time. Worst of both worlds ;-).</p>

<p>Yes, I found a similar question right here <a href=""https://stackoverflow.com/questions/35196974/aiohttp-set-maximum-number-of-requests-per-second"">aiohttp: set maximum number of requests per second</a>, but neither replies answer the actual question of limiting the rate of requests. Also <a href=""https://quentin.pradet.me/blog/how-do-you-rate-limit-calls-with-aiohttp.html"" rel=""noreferrer"">the blog post from Quentin Pradet</a> works only on rate-limiting the queueing.</p>

<p>To wrap it up: How can one limit the <em>number of requests per second</em> for parallel <code>aiohttp</code> requests?</p>
",996961,1257,08-02-2018 09:39,08-02-2018 09:54,0,1267,18,2,12,40,"{'badge_counts': {'bronze': 18, 'silver': 12, 'gold': 2}, 'account_id': 975398, 'is_employee': False, 'last_modified_date': 1638831300, 'last_access_date': 1683742769, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1267, 'creation_date': 1318690607, 'user_type': 'registered', 'user_id': 996961, 'accept_rate': 40, 'link': 'https://stackoverflow.com/users/996961/boffin', 'profile_image': 'https://www.gravatar.com/avatar/04949eba05c17a974336cb4f3c4cdb4f?s=256&d=identicon&r=PG', 'display_name': 'Boffin'}","APIs often have rate limits that users have to follow. As an example let's take 50 requests/second. Sequential requests take 0.5-1 second and thus are too slow to come close to that limit. Parallel requests with aiohttp, however, exceed the rate limit. To poll the API as fast as allowed, one needs to rate limit parallel calls. Examples that I found so far decorate , approximately like so: This works well for sequential calls. Trying to implement this in parallel calls does not work as intended. Here's some code as example: The problem with this is that it will rate-limit the queueing of the tasks. The execution with will still happen more or less at the same time. Worst of both worlds ;-). Yes, I found a similar question right here aiohttp: set maximum number of requests per second, but neither replies answer the actual question of limiting the rate of requests. Also the blog post from Quentin Pradet works only on rate-limiting the queueing. To wrap it up: How can one limit the number of requests per second for parallel requests?","session.get session.get = rate_limited(max_calls_per_second)(session.get)
 async with aiohttp.ClientSession() as session:
    session.get = rate_limited(max_calls_per_second)(session.get)
    tasks = (asyncio.ensure_future(download_coroutine(  
          timeout, session, url)) for url in urls)
    process_responses_function(await asyncio.gather(*tasks))
 gather aiohttp",1,25,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
38,50089498,50107230,99827,How to set the root directory for Visual Studio Code Python Extension?,6,<python><visual-studio-code>,68,"<p>I have no trouble running and debugging my project with VSCode Python Extension (<code>ms-python.python</code>), but since python sub-project root directory is not the whole project directory, all imports from my sources are underlined with red color and are listed in the <code>problems</code> and so <code>Go to definition</code> and some similar features don't work properly.
How can I tell the IDE where's the start point of my project:</p>

<pre><code>Whole Project path:
  docs
  server
    entities
      user.py
      customer.py
  env
  viewer
  db
</code></pre>

<p>The <code>server</code> directory is where the imports path are started from:</p>

<pre><code>from entities.user import User
</code></pre>
",487460,1825,29-04-2018 17:39,30-04-2018 19:48,1,1835,28,3,23,88,"{'badge_counts': {'bronze': 28, 'silver': 23, 'gold': 3}, 'account_id': 226421, 'is_employee': False, 'last_modified_date': 1693640400, 'last_access_date': 1700393251, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1835, 'creation_date': 1288087123, 'user_type': 'registered', 'user_id': 487460, 'accept_rate': 88, 'location': 'Iran, Mashhad', 'website_url': '', 'link': 'https://stackoverflow.com/users/487460/mtoloo', 'profile_image': 'https://www.gravatar.com/avatar/329f63e22de30447c212302a8ad3a06f?s=256&d=identicon&r=PG', 'display_name': 'mtoloo'}","I have no trouble running and debugging my project with VSCode Python Extension (), but since python sub-project root directory is not the whole project directory, all imports from my sources are underlined with red color and are listed in the and so and some similar features don't work properly. How can I tell the IDE where's the start point of my project: The directory is where the imports path are started from:","ms-python.python problems Go to definition Whole Project path:
  docs
  server
    entities
      user.py
      customer.py
  env
  viewer
  db
 server from entities.user import User
",4,18,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
39,49837474,49837476,134564,How do I install Jupyter notebook on an Android device?,3,<android><python><jupyter><termux>,60,"<p>Is there a way to install a functional instance of Jupyter notebook on an Android device? Specifically, I want to use Jupyter to run a Python notebook.</p>
",425458,13769,15-04-2018 00:37,15-04-2018 00:37,0,13769,65,2,45,61,"{'badge_counts': {'bronze': 65, 'silver': 45, 'gold': 2}, 'account_id': 187417, 'is_employee': False, 'last_modified_date': 1708716000, 'last_access_date': 1711002516, 'reputation_change_year': 624, 'reputation_change_quarter': 624, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 13769, 'creation_date': 1282233373, 'user_type': 'registered', 'user_id': 425458, 'accept_rate': 61, 'location': 'New York, NY', 'website_url': '', 'link': 'https://stackoverflow.com/users/425458/tel', 'profile_image': 'https://i.stack.imgur.com/m6jHW.png?s=256&g=1', 'display_name': 'tel'}","Is there a way to install a functional instance of Jupyter notebook on an Android device? Specifically, I want to use Jupyter to run a Python notebook.",,0,1,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
40,48078636,48078685,5088,"Python's `unittest` lacks an `assertHasAttr` method, what should I use instead?",3,<python><unit-testing><assert><python-unittest><assertion>,17,"<p>Of the many, many assert methods in <a href=""https://docs.python.org/3/library/unittest.html"" rel=""noreferrer"">Python's standard <code>unittest</code> package</a>, <code>.assertHasAttr()</code> is curiously absent. While writing some unit tests I've run into a case in which I'd like to test for the presence of an attribute in an object instance.</p>

<p>What's a safe/correct alternative for the missing <code>.assertHasAttr()</code> method?</p>
",425458,13769,03-01-2018 13:43,03-01-2018 13:47,0,13769,65,2,45,61,"{'badge_counts': {'bronze': 65, 'silver': 45, 'gold': 2}, 'account_id': 187417, 'is_employee': False, 'last_modified_date': 1708716000, 'last_access_date': 1711002516, 'reputation_change_year': 624, 'reputation_change_quarter': 624, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 13769, 'creation_date': 1282233373, 'user_type': 'registered', 'user_id': 425458, 'accept_rate': 61, 'location': 'New York, NY', 'website_url': '', 'link': 'https://stackoverflow.com/users/425458/tel', 'profile_image': 'https://i.stack.imgur.com/m6jHW.png?s=256&g=1', 'display_name': 'tel'}","Of the many, many assert methods in Python's standard package, is curiously absent. While writing some unit tests I've run into a case in which I'd like to test for the presence of an attribute in an object instance. What's a safe/correct alternative for the missing method?",unittest .assertHasAttr() .assertHasAttr(),-3,3,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
41,49546992,49547024,35631,Simplifying an 'if' statement with bool(),3,<python><boolean><refactoring><lint><pylint>,22,"<p>I have some code that causes Pylint to complain:</p>
<blockquote>
<p>The if statement can be replaced with 'var = bool(test)' (simplifiable-if-statement)`</p>
</blockquote>
<p>The code (with obfuscated variable names) is below.</p>
<pre><code>A = True
B = 1
C = [1]
D = False
E = False

if A and B in C:
    D = True
else:
    E = True

print(D, E)
</code></pre>
<p>How can this be simplified so that Pylint does not throw any errors?</p>
<p>I don't quite understand how <code>bool()</code> can be used for this. I know it converts any value to a Boolean value, but I don't know how it can be applied here.</p>
",420217,4051,29-03-2018 02:17,29-03-2018 02:21,0,4051,63,8,38,71,"{'badge_counts': {'bronze': 63, 'silver': 38, 'gold': 8}, 'account_id': 184276, 'is_employee': False, 'last_modified_date': 1649831701, 'last_access_date': 1710861760, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4051, 'creation_date': 1281769321, 'user_type': 'registered', 'user_id': 420217, 'accept_rate': 71, 'website_url': '', 'link': 'https://stackoverflow.com/users/420217/gary', 'profile_image': 'https://www.gravatar.com/avatar/d8263b4bb6e7d385d9b6267b700a954d?s=256&d=identicon&r=PG', 'display_name': 'Gary'}","I have some code that causes Pylint to complain: The if statement can be replaced with 'var = bool(test)' (simplifiable-if-statement)` The code (with obfuscated variable names) is below. How can this be simplified so that Pylint does not throw any errors? I don't quite understand how can be used for this. I know it converts any value to a Boolean value, but I don't know how it can be applied here.","A = True
B = 1
C = [1]
D = False
E = False

if A and B in C:
    D = True
else:
    E = True

print(D, E)
 bool()",10,20,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
42,49734915,49800671,53425,"""Failed to decode response from marionette"" message in Python/Firefox headless scraping script",9,<python><selenium><firefox><web-scraping><geckodriver>,40,"<p>Good Day, I've done a number of searches on here and google and yet to find a solution that address this problem.</p>

<p>The scenario is: </p>

<p>I have a Python script (2.7) that loops through an number of URLs (e.g. think Amazon pages, scraping reviews).  Each page has the same HTML layout, just scraping different information.  I use Selenium with a headless browser as these pages have javascript that needs to execute to grab the information.</p>

<p>I run this script on my local machine (OSX 10.10).  Firefox is the latest v59.  Selenium is version at 3.11.0 and using geckodriver v0.20.  </p>

<p>This script locally has no issues, it can run through all the URLs and scrape the pages with no issue.</p>

<p>Now when I put the script on my server, the only difference is it is Ubuntu 16.04 (32 bit).  I use the appropriate geckodriver (still v0.20) but everything else is the same (Python 2.7, Selenium 3.11).  It appears to randomly crash the headless browser and then all of the <code>browserObjt.get('url...')</code> no longer work.  </p>

<p>The error messages say:</p>

<blockquote>
  <p>Message: failed to decode response from marionette</p>
</blockquote>

<p>Any further selenium requests for pages return the error:</p>

<blockquote>
  <p>Message: tried to run command without establishing a connection</p>
</blockquote>

<hr>

<p>To show some code:</p>

<p>When I create the driver:</p>

<pre><code>    options = Options()
    options.set_headless(headless=True)

    driver = webdriver.Firefox(
        firefox_options=options,
        executable_path=config.GECKODRIVER
    )
</code></pre>

<p><code>driver</code> is passed to the script's function as a parameter <code>browserObj</code> which is then used to call specific pages and then once that loads it is passed to BeautifulSoup for parsing:</p>

<pre><code>browserObj.get(url)

soup = BeautifulSoup(browserObj.page_source, 'lxml')
</code></pre>

<hr>

<p>The error might be pointing to the BeautifulSoup line which is crashing the browser.</p>

<p>What is likely causing this, and what can I do to resolve the issue?</p>

<hr>

<p>Edit: Adding stack trace which points to the same thing:</p>

<pre><code>Traceback (most recent call last):
  File ""main.py"", line 164, in &lt;module&gt;
    getLeague
  File ""/home/ps/dataparsing/XXX/yyy.py"", line 48, in BBB
    soup = BeautifulSoup(browserObj.page_source, 'lxml')
  File ""/home/ps/AAA/projenv/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 670, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File ""/home/ps/AAA/projenv/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 312, in execute
    self.error_handler.check_response(response)
  File ""/home/ps/AAA/projenv/local/lib/python2.7/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
WebDriverException: Message: Failed to decode response from marionette
</code></pre>

<hr>

<p>Note: This script used to work with Chrome.  Because the server is a 32bit server, I can only use chromedriver v0.33, which only supports Chrome v60-62.  Currently Chrome is v65 and on DigitalOcean I don't seem to have an easy way to revert back to an old version - which is why I am stuck with Firefox.</p>
",399523,5197,09-04-2018 14:04,12-04-2018 15:38,3,5197,42,9,31,79,"{'badge_counts': {'bronze': 42, 'silver': 31, 'gold': 9}, 'account_id': 171844, 'is_employee': False, 'last_modified_date': 1703669401, 'last_access_date': 1710216997, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5197, 'creation_date': 1279826646, 'user_type': 'registered', 'user_id': 399523, 'accept_rate': 79, 'location': 'Bydgoszcz, Poland', 'website_url': '', 'link': 'https://stackoverflow.com/users/399523/reily-bourne', 'profile_image': 'https://i.stack.imgur.com/ZfUgF.png?s=256&g=1', 'display_name': 'Reily Bourne'}","Good Day, I've done a number of searches on here and google and yet to find a solution that address this problem. The scenario is: I have a Python script (2.7) that loops through an number of URLs (e.g. think Amazon pages, scraping reviews). Each page has the same HTML layout, just scraping different information. I use Selenium with a headless browser as these pages have javascript that needs to execute to grab the information. I run this script on my local machine (OSX 10.10). Firefox is the latest v59. Selenium is version at 3.11.0 and using geckodriver v0.20. This script locally has no issues, it can run through all the URLs and scrape the pages with no issue. Now when I put the script on my server, the only difference is it is Ubuntu 16.04 (32 bit). I use the appropriate geckodriver (still v0.20) but everything else is the same (Python 2.7, Selenium 3.11). It appears to randomly crash the headless browser and then all of the no longer work. The error messages say: Message: failed to decode response from marionette Any further selenium requests for pages return the error: Message: tried to run command without establishing a connection To show some code: When I create the driver: is passed to the script's function as a parameter which is then used to call specific pages and then once that loads it is passed to BeautifulSoup for parsing: The error might be pointing to the BeautifulSoup line which is crashing the browser. What is likely causing this, and what can I do to resolve the issue? Edit: Adding stack trace which points to the same thing: Note: This script used to work with Chrome. Because the server is a 32bit server, I can only use chromedriver v0.33, which only supports Chrome v60-62. Currently Chrome is v65 and on DigitalOcean I don't seem to have an easy way to revert back to an old version - which is why I am stuck with Firefox.","browserObjt.get('url...')     options = Options()
    options.set_headless(headless=True)

    driver = webdriver.Firefox(
        firefox_options=options,
        executable_path=config.GECKODRIVER
    )
 driver browserObj browserObj.get(url)

soup = BeautifulSoup(browserObj.page_source, 'lxml')
 Traceback (most recent call last):
  File ""main.py"", line 164, in &lt;module&gt;
    getLeague
  File ""/home/ps/dataparsing/XXX/yyy.py"", line 48, in BBB
    soup = BeautifulSoup(browserObj.page_source, 'lxml')
  File ""/home/ps/AAA/projenv/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 670, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File ""/home/ps/AAA/projenv/local/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 312, in execute
    self.error_handler.check_response(response)
  File ""/home/ps/AAA/projenv/local/lib/python2.7/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
WebDriverException: Message: Failed to decode response from marionette
",16,73,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
43,49096749,49097052,12749,When should I raise LookupError in python?,1,<python><exception><lookup><keyerror>,13,"<p><a href=""https://docs.python.org/3/library/exceptions.html"" rel=""noreferrer"">Python's built-in exception</a> documentation defines <code>LookupError</code> as:</p>

<blockquote>
  <p>The base class for the exceptions that are raised when a key or index used on a mapping or sequence is invalid: IndexError, KeyError. This can be raised directly by codecs.lookup().</p>
</blockquote>

<p>Should this base class be used only when catching try sections that access dictionaries using both indices and keys when one wants to shorthand catching both, or is there another case where you would use it?</p>
",374437,956,04-03-2018 15:02,04-03-2018 15:35,0,956,31,2,12,83,"{'badge_counts': {'bronze': 31, 'silver': 12, 'gold': 2}, 'account_id': 156875, 'is_employee': False, 'last_modified_date': 1711164300, 'last_access_date': 1708517574, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 956, 'creation_date': 1277310292, 'user_type': 'registered', 'user_id': 374437, 'accept_rate': 83, 'location': 'Israel', 'website_url': 'http://nikuda.co.il', 'link': 'https://stackoverflow.com/users/374437/veltzer-doron', 'profile_image': 'https://i.stack.imgur.com/5Lakp.jpg?s=256&g=1', 'display_name': 'Veltzer Doron'}","Python's built-in exception documentation defines as: The base class for the exceptions that are raised when a key or index used on a mapping or sequence is invalid: IndexError, KeyError. This can be raised directly by codecs.lookup(). Should this base class be used only when catching try sections that access dictionaries using both indices and keys when one wants to shorthand catching both, or is there another case where you would use it?",LookupError,-1,7,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
44,49173770,49174466,4742,Find occurrences of huge list of phrases in text,8,<python>,13,"<p>I'm building a backend and trying to crunch the following problem.</p>

<ul>
<li>The clients submit text to the backend (around <code>2000</code> characters on average)</li>
<li>Backend endpoint that receives the request has to apply phrase highlighting to the submitted text</li>
<li><p>There is around <code>80k</code> phrases to match. A phrase is a simple object:</p>

<pre><code>{
    'phrase': 'phrase to match'
    'link': 'link_url'
}
</code></pre></li>
<li><p>After finding all matches of phrases that exist in the text, the backend returns to the client what was matched - basically a map:</p>

<pre><code>range in text -&gt; phrase
</code></pre></li>
</ul>

<p>Most is done. I'm about to tackle coding the phrase matching part. Everything else works smoothly. Since I don't want to reinvent the wheel I tried googling to find a Python library that does the job of efficiently finding phrases (from huge list) in text. However, I couldn't find anything.</p>

<p>I checked out the <a href=""https://www.crummy.com/software/BeautifulSoup/"" rel=""noreferrer"">BlueSoup</a> and <a href=""https://www.nltk.org/"" rel=""noreferrer"">Natural Language Toolkit</a>. However they don't seem to be doing what I'm looking for.</p>

<p>Do you guys know if there is a library that would be helpful in such task? Seems like a common thing to implement and I don't want to go custom if there is a well established library for that.</p>
",348796,39800,08-03-2018 13:02,08-03-2018 13:37,0,39850,147,23,116,78,"{'badge_counts': {'bronze': 147, 'silver': 116, 'gold': 23}, 'collectives': [{'collective': {'tags': ['ios', 'android'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who want to share their knowledge and learn more about mobile development practices and platforms', 'link': '/collectives/mobile-dev', 'name': 'Mobile Development', 'slug': 'mobile-dev'}, 'role': 'limited_recognized_member'}], 'account_id': 141617, 'is_employee': False, 'last_modified_date': 1685128500, 'last_access_date': 1711125011, 'reputation_change_year': 160, 'reputation_change_quarter': 160, 'reputation_change_month': 60, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 39850, 'creation_date': 1274693044, 'user_type': 'registered', 'user_id': 348796, 'accept_rate': 78, 'location': 'ZG, Switzerland', 'website_url': 'https://rafalsroka.com/', 'link': 'https://stackoverflow.com/users/348796/rafa%c5%82-sroka', 'profile_image': 'https://i.stack.imgur.com/V0yoz.png?s=256&g=1', 'display_name': 'Rafał Sroka'}","I'm building a backend and trying to crunch the following problem. The clients submit text to the backend (around characters on average) Backend endpoint that receives the request has to apply phrase highlighting to the submitted text There is around phrases to match. A phrase is a simple object: After finding all matches of phrases that exist in the text, the backend returns to the client what was matched - basically a map: Most is done. I'm about to tackle coding the phrase matching part. Everything else works smoothly. Since I don't want to reinvent the wheel I tried googling to find a Python library that does the job of efficiently finding phrases (from huge list) in text. However, I couldn't find anything. I checked out the BlueSoup and Natural Language Toolkit. However they don't seem to be doing what I'm looking for. Do you guys know if there is a library that would be helpful in such task? Seems like a common thing to implement and I don't want to go custom if there is a well established library for that.","2000 80k {
    'phrase': 'phrase to match'
    'link': 'link_url'
}
 range in text -&gt; phrase
",1,23,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
45,49807449,49808218,47478,How to check if a function was called in a unit test using pytest-mock?,2,<python><unit-testing><mocking><pytest>,27,"<p>I have a unit test where I want to check if a function was called. How do I do this with<code>pytest</code> and <code>pytest-mock</code> libraries?</p>

<p>For example, here is a unit test <code>test_hello.py</code>. In this test I call the function <code>my_function</code> and want to verify that it called <code>hello</code> with a given argument.</p>

<pre><code>def hello(name):
    return f'Hello {name}'

def my_function():
    hello('Sam')

def test_hello(mocker):
    mocker.patch('hello')
    my_function()
    hello.assert_called_once_with('Sam')
</code></pre>

<p>The code above returns the following error:</p>

<pre><code>target = 'hello'

    def _get_target(target):
        try:
&gt;           target, attribute = target.rsplit('.', 1)
E           ValueError: not enough values to unpack (expected 2, got 1)

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/mock.py:1393: ValueError

During handling of the above exception, another exception occurred:

mocker = &lt;pytest_mock.MockFixture object at 0x109c5e978&gt;

    def test_hello(mocker):
&gt;       mocker.patch('hello')

test_hello.py:8: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pytest_mock.py:156: in __call__
    return self._start_patch(self.mock_module.patch, *args, **kwargs)
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pytest_mock.py:134: in _start_patch
    p = mock_func(*args, **kwargs)
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/mock.py:1544: in patch
    getter, attribute = _get_target(target)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'hello'

    def _get_target(target):
        try:
            target, attribute = target.rsplit('.', 1)
        except (TypeError, ValueError):
            raise TypeError(""Need a valid target to patch. You supplied: %r"" %
&gt;                           (target,))
E           TypeError: Need a valid target to patch. You supplied: 'hello'

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/mock.py:1396: TypeError
</code></pre>
",297131,36799,13-04-2018 00:01,13-04-2018 02:00,0,36809,174,27,135,93,"{'badge_counts': {'bronze': 174, 'silver': 135, 'gold': 27}, 'account_id': 112690, 'is_employee': False, 'last_modified_date': 1710090600, 'last_access_date': 1710690750, 'reputation_change_year': 140, 'reputation_change_quarter': 140, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 36809, 'creation_date': 1268980352, 'user_type': 'registered', 'user_id': 297131, 'accept_rate': 93, 'location': 'St Kilda, Australia', 'website_url': 'http://evgenii.com', 'link': 'https://stackoverflow.com/users/297131/evgenii', 'profile_image': 'https://www.gravatar.com/avatar/c9b911baaad69b64f3cc6754db12c82e?s=256&d=identicon&r=PG', 'display_name': 'Evgenii'}","I have a unit test where I want to check if a function was called. How do I do this with and libraries? For example, here is a unit test . In this test I call the function and want to verify that it called with a given argument. The code above returns the following error:","pytest pytest-mock test_hello.py my_function hello def hello(name):
    return f'Hello {name}'

def my_function():
    hello('Sam')

def test_hello(mocker):
    mocker.patch('hello')
    my_function()
    hello.assert_called_once_with('Sam')
 target = 'hello'

    def _get_target(target):
        try:
&gt;           target, attribute = target.rsplit('.', 1)
E           ValueError: not enough values to unpack (expected 2, got 1)

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/mock.py:1393: ValueError

During handling of the above exception, another exception occurred:

mocker = &lt;pytest_mock.MockFixture object at 0x109c5e978&gt;

    def test_hello(mocker):
&gt;       mocker.patch('hello')

test_hello.py:8: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pytest_mock.py:156: in __call__
    return self._start_patch(self.mock_module.patch, *args, **kwargs)
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pytest_mock.py:134: in _start_patch
    p = mock_func(*args, **kwargs)
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/mock.py:1544: in patch
    getter, attribute = _get_target(target)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'hello'

    def _get_target(target):
        try:
            target, attribute = target.rsplit('.', 1)
        except (TypeError, ValueError):
            raise TypeError(""Need a valid target to patch. You supplied: %r"" %
&gt;                           (target,))
E           TypeError: Need a valid target to patch. You supplied: 'hello'

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/mock.py:1396: TypeError
",40,56,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
46,49761221,49761398,15869,Make seaborn show a colorbar instead of a legend when using hue in a bar plot?,2,<python><matplotlib><seaborn><bar-chart><colorbar>,19,"<p>Let's say I want to make a bar plot where the hue of the bars represents some continuous quantity. e.g.</p>
<pre><code>import seaborn as sns
titanic = sns.load_dataset(&quot;titanic&quot;)
g = titanic.groupby('pclass')
survival_rates = g['survived'].mean()
n = g.size()
ax = sns.barplot(x=n.index, y=n,
           hue=survival_rates, palette='Reds',
            dodge=False,
          )
ax.set_ylabel('n passengers')
</code></pre>
<p><a href=""https://i.stack.imgur.com/S7FQv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/S7FQv.png"" alt=""bar plot drawn by sns"" /></a></p>
<p>The legend here is kind of silly, and gets even worse the more bars I plot. What would make most sense is a colorbar (such as are used when calling <a href=""https://seaborn.pydata.org/generated/seaborn.heatmap.html#seaborn.heatmap"" rel=""noreferrer""><code>sns.heatmap</code></a>). Is there a way to make seaborn do this?</p>
",262271,8873,10-04-2018 19:07,10-04-2018 19:20,0,8883,39,6,34,77,"{'badge_counts': {'bronze': 39, 'silver': 34, 'gold': 6}, 'account_id': 96501, 'is_employee': False, 'last_modified_date': 1687781100, 'last_access_date': 1662852170, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 8883, 'creation_date': 1264814684, 'user_type': 'registered', 'user_id': 262271, 'accept_rate': 77, 'location': 'Toronto, ON, Canada', 'website_url': 'http://colinmorris.github.io/', 'link': 'https://stackoverflow.com/users/262271/coquelicot', 'profile_image': 'https://www.gravatar.com/avatar/c7e9a25741952f65d158aa3a096d17d0?s=256&d=identicon&r=PG', 'display_name': 'Coquelicot'}","Let's say I want to make a bar plot where the hue of the bars represents some continuous quantity. e.g. The legend here is kind of silly, and gets even worse the more bars I plot. What would make most sense is a colorbar (such as are used when calling ). Is there a way to make seaborn do this?","import seaborn as sns
titanic = sns.load_dataset(&quot;titanic&quot;)
g = titanic.groupby('pclass')
survival_rates = g['survived'].mean()
n = g.size()
ax = sns.barplot(x=n.index, y=n,
           hue=survival_rates, palette='Reds',
            dodge=False,
          )
ax.set_ylabel('n passengers')
 sns.heatmap",8,14,1,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
47,48958387,49046025,1896,google customsearch returns different result?,4,<python><python-3.x><search><google-api><google-custom-search>,13,"<p>I'm trying out google customsearch api to search image and but the weird thing is my search through api returns different result than regular search through browser. for example</p>

<pre><code>from apiclient.discovery import build
import pprint
import sys
api_key='xxxxxxx'
service = build('customsearch', 'v1', developerKey=api_key)
request=service.cse()
query=request.list(cx='xxxx:xxxxx',q='dogs and cats',searchType='image',imgType='photo')
result=query.execute()
pprint.pprint(result)
for i in result.get('items',[]):
    print (i['link'])
</code></pre>

<p>running this code gives totally different result
here is result from running above code</p>

<pre><code>https://s.yimg.com/ny/api/res/1.2/tarWzt2ZXfPOEg8oQVlOWw--/YXBwaWQ9aGlnaGxhbmRlcjtzbT0xO3c9ODAw/http://media.zenfs.com/en-US/homerun/people_218/4d82a5fa19dd37247717704975fdf602
https://www.google.com/about/main/machine-learning-qa/img/cat-dog-flow-horizontal.gif
https://www.google.com/trends/2014/static/images/pets-snapshot-reveal-1920.jpg
https://www.google.com/trends/2014/static/images/pets-share.png
https://www.google.com/about/main/machine-learning-qa/img/cat-dog-flow-vertical.gif
https://s.yimg.com/uu/api/res/1.2/YQWuQgTnzQuwXjYzX.QrWg--~B/aD0xMzMzO3c9MjAwMDtzbT0xO2FwcGlkPXl0YWNoeW9u/http://media.zenfs.com/en-US/homerun/people_218/4d82a5fa19dd37247717704975fdf602
https://www.google.com/trends/2014/static/images/pets-video-1080.jpg
https://www.google.com/trends/2014/static/images/pets-video-320.jpg
https://www.google.com/maps/d/thumbnail?mid=1hO0YkGLATyy-ZI9JxX1lbv-wK1M&amp;hl=en_US
</code></pre>

<p>here is a snapshot of google search from chrome
<a href=""https://i.stack.imgur.com/hicGA.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hicGA.jpg"" alt=""enter image description here""></a>
queries are same, anyone knows why?</p>
",1015633,1860,24-02-2018 01:18,01-03-2018 08:58,5,1870,62,6,32,75,"{'badge_counts': {'bronze': 62, 'silver': 32, 'gold': 6}, 'account_id': 1000731, 'is_employee': False, 'last_modified_date': 1666642200, 'last_access_date': 1680874294, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1870, 'creation_date': 1319675689, 'user_type': 'registered', 'user_id': 1015633, 'accept_rate': 75, 'website_url': '', 'link': 'https://stackoverflow.com/users/1015633/ikel', 'profile_image': 'https://www.gravatar.com/avatar/97d98f3f9a0efccc0869a363f7e406d1?s=256&d=identicon&r=PG', 'display_name': 'ikel'}","I'm trying out google customsearch api to search image and but the weird thing is my search through api returns different result than regular search through browser. for example running this code gives totally different result here is result from running above code here is a snapshot of google search from chrome queries are same, anyone knows why?","from apiclient.discovery import build
import pprint
import sys
api_key='xxxxxxx'
service = build('customsearch', 'v1', developerKey=api_key)
request=service.cse()
query=request.list(cx='xxxx:xxxxx',q='dogs and cats',searchType='image',imgType='photo')
result=query.execute()
pprint.pprint(result)
for i in result.get('items',[]):
    print (i['link'])
 https://s.yimg.com/ny/api/res/1.2/tarWzt2ZXfPOEg8oQVlOWw--/YXBwaWQ9aGlnaGxhbmRlcjtzbT0xO3c9ODAw/http://media.zenfs.com/en-US/homerun/people_218/4d82a5fa19dd37247717704975fdf602
https://www.google.com/about/main/machine-learning-qa/img/cat-dog-flow-horizontal.gif
https://www.google.com/trends/2014/static/images/pets-snapshot-reveal-1920.jpg
https://www.google.com/trends/2014/static/images/pets-share.png
https://www.google.com/about/main/machine-learning-qa/img/cat-dog-flow-vertical.gif
https://s.yimg.com/uu/api/res/1.2/YQWuQgTnzQuwXjYzX.QrWg--~B/aD0xMzMzO3c9MjAwMDtzbT0xO2FwcGlkPXl0YWNoeW9u/http://media.zenfs.com/en-US/homerun/people_218/4d82a5fa19dd37247717704975fdf602
https://www.google.com/trends/2014/static/images/pets-video-1080.jpg
https://www.google.com/trends/2014/static/images/pets-video-320.jpg
https://www.google.com/maps/d/thumbnail?mid=1hO0YkGLATyy-ZI9JxX1lbv-wK1M&amp;hl=en_US
",18,32,1,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
48,49777474,52516563,2239,"Does anyone have the ""Documentation URL""s in PyCharm for the following libraries:",1,<python><pycharm><code-documentation>,13,"<p>I'm a fan of the ""quick documentation"" feature of PyCharm &amp; other Jetbrains IDEs, but it needs to know the specific ""Documentation URL"" for each library, that gets set under <code>Preferences &gt; Tools &gt; Python External Documentation</code> settings.</p>

<p>I was wondering if anybody has worked it out for any of the following libraries:</p>

<ul>
<li>Tensorflow</li>
<li>PyTorch</li>
<li>Matplotlib</li>
<li>Seaborn</li>
<li>Pandas</li>
</ul>
",1059650,1534,11-04-2018 14:17,26-09-2018 11:21,168,1534,18,2,11,,"{'badge_counts': {'bronze': 18, 'silver': 11, 'gold': 2}, 'account_id': 1057215, 'is_employee': False, 'last_modified_date': 1607614993, 'last_access_date': 1710417926, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1534, 'creation_date': 1321960951, 'user_type': 'registered', 'user_id': 1059650, 'website_url': '', 'link': 'https://stackoverflow.com/users/1059650/retnuh', 'profile_image': 'https://www.gravatar.com/avatar/a3f0d762911ed1769b4c9f5260791ddd?s=256&d=identicon&r=PG', 'display_name': 'retnuH'}","I'm a fan of the ""quick documentation"" feature of PyCharm &amp; other Jetbrains IDEs, but it needs to know the specific ""Documentation URL"" for each library, that gets set under settings. I was wondering if anybody has worked it out for any of the following libraries: Tensorflow PyTorch Matplotlib Seaborn Pandas",Preferences &gt; Tools &gt; Python External Documentation,-1,11,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
49,49926843,49926924,2067,Why is a False value (0) smaller in bytes than True (1)?,1,<python><types><size><cpython>,31,"<p>I was playing around with <code>sys</code>'s <code>getsizeof()</code> and found that <code>False</code> (or <code>0</code>) consists of less bytes than <code>True</code> (or <code>1</code>). Why is that?</p>

<pre><code>import sys

print(""Zero: "" + str(sys.getsizeof(0)))
print(""One: "" + str(sys.getsizeof(1)))
print(""False: "" + str(sys.getsizeof(False)))
print(""True: "" + str(sys.getsizeof(True)))

# Prints:
# Zero: 24
# One: 28
# False: 24
# True: 28
</code></pre>

<p>In fact, other numbers (also some that consist of more than one digit) are 28 bytes. </p>

<pre><code>for n in range(0, 12):
  print(str(n) + "": "" + str(sys.getsizeof(n)))

# Prints:
# 0: 24
# 1: 28
# 2: 28
# 3: 28
# 4: 28
# 5: 28
# 6: 28
# 7: 28
# 8: 28
# 9: 28
# 10: 28
# 11: 28
</code></pre>

<p>Even more: <code>sys.getsizeof(999999999)</code> is also 28 bytes! <code>sys.getsizeof(9999999999)</code>, however, is 32.</p>

<p>So what's going on? I assume that the booleans <code>True</code> and <code>False</code> are internally converted to <code>0</code> and <code>1</code> respectively, but why is zero different in size from other lower integers? </p>

<p>Side question: is this specific to how Python (3) represents these items, or is this generally how digits are presented in the OS?</p>
",1150683,27565,19-04-2018 17:24,19-04-2018 17:29,0,27595,254,25,142,96,"{'badge_counts': {'bronze': 254, 'silver': 142, 'gold': 25}, 'collectives': [{'collective': {'tags': ['bert-language-model', 'huggingface-transformers', 'topic-modeling', 'opennlp', 'tf-idf', 'gensim', 'stanford-nlp', 'spacy-3', 'named-entity-recognition', 'word-embedding', 'spacy', 'word2vec', 'nltk', 'sentiment-analysis', 'nlp-question-answering', 'nlp'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective focused on NLP (natural language processing), the transformation or extraction of useful information from natural language data.', 'link': '/collectives/nlp', 'name': 'NLP', 'slug': 'nlp'}, 'role': 'member'}], 'account_id': 1174306, 'is_employee': False, 'last_modified_date': 1698404700, 'last_access_date': 1711119311, 'reputation_change_year': 240, 'reputation_change_quarter': 240, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 27595, 'creation_date': 1326650625, 'user_type': 'registered', 'user_id': 1150683, 'accept_rate': 96, 'location': 'Gent, Belgi&#235;', 'website_url': 'https://bramvanroy.github.io/', 'link': 'https://stackoverflow.com/users/1150683/bram-vanroy', 'profile_image': 'https://i.stack.imgur.com/5Tbtq.jpg?s=256&g=1', 'display_name': 'Bram Vanroy'}","I was playing around with 's and found that (or ) consists of less bytes than (or ). Why is that? In fact, other numbers (also some that consist of more than one digit) are 28 bytes. Even more: is also 28 bytes! , however, is 32. So what's going on? I assume that the booleans and are internally converted to and respectively, but why is zero different in size from other lower integers? Side question: is this specific to how Python (3) represents these items, or is this generally how digits are presented in the OS?","sys getsizeof() False 0 True 1 import sys

print(""Zero: "" + str(sys.getsizeof(0)))
print(""One: "" + str(sys.getsizeof(1)))
print(""False: "" + str(sys.getsizeof(False)))
print(""True: "" + str(sys.getsizeof(True)))

# Prints:
# Zero: 24
# One: 28
# False: 24
# True: 28
 for n in range(0, 12):
  print(str(n) + "": "" + str(sys.getsizeof(n)))

# Prints:
# 0: 24
# 1: 28
# 2: 28
# 3: 28
# 4: 28
# 5: 28
# 6: 28
# 7: 28
# 8: 28
# 9: 28
# 10: 28
# 11: 28
 sys.getsizeof(999999999) sys.getsizeof(9999999999) True False 0 1",14,41,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
50,48941116,48941150,20370,Does python pip have the equivalent of node's package.json?,5,<python><node.js><npm><pip>,42,"<p>In NodeJS's npm you can create a package.json file to track your project dependencies. When you want to install them you just run <code>npm install</code> and it looks at your package file and installs them all with that single command.</p>

<p>When distributing my code, does python have an equivalent concept or do I need to tell people in my README to install each dependency like so:</p>

<pre><code>pip install package1
pip install package2
</code></pre>

<p>Before they can use my code?</p>
",1205871,54414,23-02-2018 04:27,23-02-2018 04:30,0,54504,297,50,250,98,"{'badge_counts': {'bronze': 297, 'silver': 250, 'gold': 50}, 'account_id': 1244024, 'is_employee': False, 'last_modified_date': 1711020000, 'last_access_date': 1711180111, 'reputation_change_year': 756, 'reputation_change_quarter': 756, 'reputation_change_month': 140, 'reputation_change_week': 60, 'reputation_change_day': 0, 'reputation': 54504, 'creation_date': 1329095849, 'user_type': 'registered', 'user_id': 1205871, 'accept_rate': 98, 'location': 'London UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/1205871/danday74', 'profile_image': 'https://i.stack.imgur.com/eDE58.gif?s=256&g=1', 'display_name': 'danday74'}","In NodeJS's npm you can create a package.json file to track your project dependencies. When you want to install them you just run and it looks at your package file and installs them all with that single command. When distributing my code, does python have an equivalent concept or do I need to tell people in my README to install each dependency like so: Before they can use my code?","npm install pip install package1
pip install package2
",0,9,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
51,49171189,50080269,32790,What's the correct way to check if an object is a typing.Generic?,7,<python><generics><type-hinting>,62,"<p>I'm trying to write code that validates type hints, and in order to do so I have to find out what kind of object the annotation is. For example, consider this snippet that's supposed to tell the user what kind of value is expected:</p>

<pre><code>import typing

typ = typing.Union[int, str]

if issubclass(typ, typing.Union):
    print('value type should be one of', typ.__args__)
elif issubclass(typ, typing.Generic):
    print('value type should be a structure of', typ.__args__[0])
else:
    print('value type should be', typ)
</code></pre>

<p>This should print ""value type should be one of (int, str)"", but instead it throws an exception:</p>

<pre><code>Traceback (most recent call last):
  File ""untitled.py"", line 6, in &lt;module&gt;
    if issubclass(typ, typing.Union):
  File ""C:\Python34\lib\site-packages\typing.py"", line 829, in __subclasscheck__
    raise TypeError(""Unions cannot be used with issubclass()."")
TypeError: Unions cannot be used with issubclass().
</code></pre>

<p><code>isinstance</code> doesn't work either:</p>

<pre><code>&gt;&gt;&gt; isinstance(typ, typing.Union)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python34\lib\site-packages\typing.py"", line 826, in __instancecheck__
    raise TypeError(""Unions cannot be used with isinstance()."")
TypeError: Unions cannot be used with isinstance().
</code></pre>

<hr>

<p>What's the correct way to check if <code>typ</code> is a <code>typing.Generic</code>?</p>

<p><strong>If possible, I would like to see a solution that's backed by documentation or a PEP or some other resource.</strong> A ""solution"" that ""works"" by accessing undocumented, internal attributes is easy to find. But more likely than not, it'll turn out to be an implementation detail and will change in future versions. I'm looking for <em>""the right way""</em> to do it.</p>
",1222951,41267,08-03-2018 10:45,28-04-2018 19:24,51,41387,156,13,105,88,"{'badge_counts': {'bronze': 156, 'silver': 105, 'gold': 13}, 'account_id': 1265583, 'is_employee': False, 'last_modified_date': 1703300400, 'last_access_date': 1707255249, 'reputation_change_year': 660, 'reputation_change_quarter': 660, 'reputation_change_month': 190, 'reputation_change_week': 70, 'reputation_change_day': 0, 'reputation': 41387, 'creation_date': 1329815496, 'user_type': 'registered', 'user_id': 1222951, 'accept_rate': 88, 'location': 'Austria', 'website_url': '', 'link': 'https://stackoverflow.com/users/1222951/aran-fey', 'profile_image': 'https://i.stack.imgur.com/s64JD.png?s=256&g=1', 'display_name': 'Aran-Fey'}","I'm trying to write code that validates type hints, and in order to do so I have to find out what kind of object the annotation is. For example, consider this snippet that's supposed to tell the user what kind of value is expected: This should print ""value type should be one of (int, str)"", but instead it throws an exception: doesn't work either: What's the correct way to check if is a ? If possible, I would like to see a solution that's backed by documentation or a PEP or some other resource. A ""solution"" that ""works"" by accessing undocumented, internal attributes is easy to find. But more likely than not, it'll turn out to be an implementation detail and will change in future versions. I'm looking for ""the right way"" to do it.","import typing

typ = typing.Union[int, str]

if issubclass(typ, typing.Union):
    print('value type should be one of', typ.__args__)
elif issubclass(typ, typing.Generic):
    print('value type should be a structure of', typ.__args__[0])
else:
    print('value type should be', typ)
 Traceback (most recent call last):
  File ""untitled.py"", line 6, in &lt;module&gt;
    if issubclass(typ, typing.Union):
  File ""C:\Python34\lib\site-packages\typing.py"", line 829, in __subclasscheck__
    raise TypeError(""Unions cannot be used with issubclass()."")
TypeError: Unions cannot be used with issubclass().
 isinstance &gt;&gt;&gt; isinstance(typ, typing.Union)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python34\lib\site-packages\typing.py"", line 826, in __instancecheck__
    raise TypeError(""Unions cannot be used with isinstance()."")
TypeError: Unions cannot be used with isinstance().
 typ typing.Generic",16,39,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
52,48572831,48573907,48768,How to access the type arguments of typing.Generic?,5,<python><generics>,72,"<p>The <a href=""https://docs.python.org/3/library/typing.html"" rel=""noreferrer""><code>typing</code></a> module provides a base class for generic type hints: The <a href=""https://docs.python.org/3/library/typing.html#typing.Generic"" rel=""noreferrer""><code>typing.Generic</code></a> class.</p>

<p>Subclasses of <code>Generic</code> accept type arguments in square brackets, for example:</p>

<pre><code>list_of_ints = typing.List[int]
str_to_bool_dict = typing.Dict[str, bool]
</code></pre>

<hr>

<p>My question is, how can I access these type arguments?</p>

<p>That is, given <code>str_to_bool_dict</code> as input, how can I get <code>str</code> and <code>bool</code> as output?</p>

<p>Basically I'm looking for a function such that</p>

<pre><code>&gt;&gt;&gt; magic_function(str_to_bool_dict)
(&lt;class 'str'&gt;, &lt;class 'bool'&gt;)
</code></pre>
",1222951,41267,01-02-2018 22:31,02-02-2018 00:21,1,41387,156,13,105,88,"{'badge_counts': {'bronze': 156, 'silver': 105, 'gold': 13}, 'account_id': 1265583, 'is_employee': False, 'last_modified_date': 1703300400, 'last_access_date': 1707255249, 'reputation_change_year': 660, 'reputation_change_quarter': 660, 'reputation_change_month': 190, 'reputation_change_week': 70, 'reputation_change_day': 0, 'reputation': 41387, 'creation_date': 1329815496, 'user_type': 'registered', 'user_id': 1222951, 'accept_rate': 88, 'location': 'Austria', 'website_url': '', 'link': 'https://stackoverflow.com/users/1222951/aran-fey', 'profile_image': 'https://i.stack.imgur.com/s64JD.png?s=256&g=1', 'display_name': 'Aran-Fey'}","The module provides a base class for generic type hints: The class. Subclasses of accept type arguments in square brackets, for example: My question is, how can I access these type arguments? That is, given as input, how can I get and as output? Basically I'm looking for a function such that","typing typing.Generic Generic list_of_ints = typing.List[int]
str_to_bool_dict = typing.Dict[str, bool]
 str_to_bool_dict str bool &gt;&gt;&gt; magic_function(str_to_bool_dict)
(&lt;class 'str'&gt;, &lt;class 'bool'&gt;)
",-4,19,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
53,50157543,50159934,11470,"Unittest Django: Mock external API, what is proper way?",1,<python><django><unit-testing><mocking>,21,"<p>I am having a problem understanding how mock works and how to write unittests with mock objects. I wanted to mock an external api call every time when my model calls save() method. 
My code:
<code>models.py</code></p>

<pre><code>from . import utils

class Book(Titleable, Isactiveable, Timestampable, IsVoidable, models.Model):
   title
   orig_author
   orig_title
   isbn 

    def save(self, *args, **kwargs):
        if self.isbn:
            google_data = utils.get_original_title_and_name(self.isbn)
            if google_data:
                self.original_author = google_data['author']
                self.original_title = google_data['title']
        super().save(*args, **kwargs)
</code></pre>

<p><code>utils.py</code></p>

<pre><code>def get_original_title_and_name(isbn, **kawargs):
    isbn_search_string = 'isbn:{}'.format(isbn)
    payload = {
        'key': GOOGLE_API_KEY,
        'q': isbn_search_string,
        'printType': 'books',
    }
    r = requests.get(GOOGLE_API_URL, params=payload)
    response = r.json()
    if 'items' in response.keys():
        title = response['items'][THE_FIRST_INDEX]['volumeInfo']['title']
        author = response['items'][THE_FIRST_INDEX]['volumeInfo']['authors'][THE_FIRST_INDEX]

        return {
            'title': title,
            'author': author
        }
    else:
        return None
</code></pre>

<p>I began read docs and write test:</p>

<p><code>test.py</code>:</p>

<pre><code>from unittest import mock
from django.test import TestCase
from rest_framework import status
from .constants import THE_FIRST_INDEX, GOOGLE_API_URL, GOOGLE_API_KEY

class BookModelTestCase(TestCase):
    @mock.patch('requests.get')
    def test_get_original_title_and_name_from_google_api(self, mock_get):
        # Define new Mock object
        mock_response = mock.Mock()
        # Define response data from Google API
        expected_dict = {
            'kind': 'books#volumes',
            'totalItems': 1,
            'items': [
                {
                    'kind': 'books#volume',
                    'id': 'IHxXBAAAQBAJ',
                    'etag': 'B3N9X8vAMWg',
                    'selfLink': 'https://www.googleapis.com/books/v1/volumes/IHxXBAAAQBAJ',
                    'volumeInfo': {
                        'title': ""Alice's Adventures in Wonderland"",
                        'authors': [
                            'Lewis Carroll'
                        ]
                    }
                }
                    ]
            }

        # Define response data for my Mock object
        mock_response.json.return_value = expected_dict
        mock_response.status_code = 200

        # Define response for the fake API
        mock_get.return_value = mock_response
</code></pre>

<p>The first of all, I can't write <code>target</code> for the <code>@mock.patch</code> correct. If a define <code>target</code> as <code>utuls.get_original_title_and_name.requests.get</code>, I get <code>ModuleNotFoundError</code>. Also I can't understand how to make fake-call to external API and verify recieved data (whether necessarly its, if I've already define <code>mock_response.json.return_value = expected_dict?</code>) and verify that my save() method work well? </p>

<p>How do I write test for this cases? Could anyone explain me this case?</p>
",1298253,455,03-05-2018 14:30,03-05-2018 16:34,0,455,12,2,4,,"{'badge_counts': {'bronze': 12, 'silver': 4, 'gold': 2}, 'account_id': 1360839, 'is_employee': False, 'last_modified_date': 1607614927, 'last_access_date': 1710934117, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 455, 'creation_date': 1332940274, 'user_type': 'registered', 'user_id': 1298253, 'location': 'Moscow, Russia', 'website_url': '', 'link': 'https://stackoverflow.com/users/1298253/kotmsk', 'profile_image': 'https://www.gravatar.com/avatar/d300fab9fd0863721a2e71e34490d2b8?s=256&d=identicon&r=PG', 'display_name': 'kotmsk'}","I am having a problem understanding how mock works and how to write unittests with mock objects. I wanted to mock an external api call every time when my model calls save() method. My code: I began read docs and write test: : The first of all, I can't write for the correct. If a define as , I get . Also I can't understand how to make fake-call to external API and verify recieved data (whether necessarly its, if I've already define ) and verify that my save() method work well? How do I write test for this cases? Could anyone explain me this case?","models.py from . import utils

class Book(Titleable, Isactiveable, Timestampable, IsVoidable, models.Model):
   title
   orig_author
   orig_title
   isbn 

    def save(self, *args, **kwargs):
        if self.isbn:
            google_data = utils.get_original_title_and_name(self.isbn)
            if google_data:
                self.original_author = google_data['author']
                self.original_title = google_data['title']
        super().save(*args, **kwargs)
 utils.py def get_original_title_and_name(isbn, **kawargs):
    isbn_search_string = 'isbn:{}'.format(isbn)
    payload = {
        'key': GOOGLE_API_KEY,
        'q': isbn_search_string,
        'printType': 'books',
    }
    r = requests.get(GOOGLE_API_URL, params=payload)
    response = r.json()
    if 'items' in response.keys():
        title = response['items'][THE_FIRST_INDEX]['volumeInfo']['title']
        author = response['items'][THE_FIRST_INDEX]['volumeInfo']['authors'][THE_FIRST_INDEX]

        return {
            'title': title,
            'author': author
        }
    else:
        return None
 test.py from unittest import mock
from django.test import TestCase
from rest_framework import status
from .constants import THE_FIRST_INDEX, GOOGLE_API_URL, GOOGLE_API_KEY

class BookModelTestCase(TestCase):
    @mock.patch('requests.get')
    def test_get_original_title_and_name_from_google_api(self, mock_get):
        # Define new Mock object
        mock_response = mock.Mock()
        # Define response data from Google API
        expected_dict = {
            'kind': 'books#volumes',
            'totalItems': 1,
            'items': [
                {
                    'kind': 'books#volume',
                    'id': 'IHxXBAAAQBAJ',
                    'etag': 'B3N9X8vAMWg',
                    'selfLink': 'https://www.googleapis.com/books/v1/volumes/IHxXBAAAQBAJ',
                    'volumeInfo': {
                        'title': ""Alice's Adventures in Wonderland"",
                        'authors': [
                            'Lewis Carroll'
                        ]
                    }
                }
                    ]
            }

        # Define response data for my Mock object
        mock_response.json.return_value = expected_dict
        mock_response.status_code = 200

        # Define response for the fake API
        mock_get.return_value = mock_response
 target @mock.patch target utuls.get_original_title_and_name.requests.get ModuleNotFoundError mock_response.json.return_value = expected_dict?",58,89,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
54,49545947,49668081,19170,sklearn DeprecationWarning truth value of an array,2,<python><scikit-learn><rasa-nlu>,16,"<p>Running a rasa_core example from the docs with</p>

<pre><code>› python3 -m rasa_core.run -d models/dialogue -u models/nlu/default/current
</code></pre>

<p>and get this error output after each message in the dialog:</p>

<pre><code>.../sklearn/...: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty.
</code></pre>

<p>It's an issue with numpy that has been fixed but not been published in the latest release: <a href=""https://github.com/scikit-learn/scikit-learn/issues/10449"" rel=""noreferrer"">https://github.com/scikit-learn/scikit-learn/issues/10449</a></p>

<p>The following has <strong>not worked</strong> to temporarily silence the warning:</p>

<ol>
<li>Adding <code>-W ignore</code></li>
</ol>

<p><code>python3 -W ignore -m rasa_core.run -d models/dialogue -u models/nlu/default/current</code></p>

<ol start=""2"">
<li><code>warnings.simplefilter</code></li>
</ol>

<p><code>python3</code></p>

<pre><code>&gt;&gt;&gt; warnings.simplefilter('ignore', DeprecationWarning)
&gt;&gt;&gt; exit()
</code></pre>

<p><code>python3 -m rasa_core.run -d models/dialogue -u models/nlu/default/current</code></p>
",1338065,3143,28-03-2018 23:50,05-04-2018 08:53,8,3143,16,3,15,71,"{'badge_counts': {'bronze': 16, 'silver': 15, 'gold': 3}, 'account_id': 1411592, 'is_employee': False, 'last_modified_date': 1694625804, 'last_access_date': 1694625597, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3143, 'creation_date': 1334646800, 'user_type': 'registered', 'user_id': 1338065, 'accept_rate': 71, 'website_url': 'https://ryanblakeley.net', 'link': 'https://stackoverflow.com/users/1338065/rojobuffalo', 'profile_image': 'https://www.gravatar.com/avatar/bc4b3c5104521ae3756fc31d9af4d66c?s=256&d=identicon&r=PG', 'display_name': 'rojobuffalo'}",Running a rasa_core example from the docs with and get this error output after each message in the dialog: It's an issue with numpy that has been fixed but not been published in the latest release: https://github.com/scikit-learn/scikit-learn/issues/10449 The following has not worked to temporarily silence the warning: Adding,"› python3 -m rasa_core.run -d models/dialogue -u models/nlu/default/current
 .../sklearn/...: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty.
 -W ignore python3 -W ignore -m rasa_core.run -d models/dialogue -u models/nlu/default/current warnings.simplefilter python3 &gt;&gt;&gt; warnings.simplefilter('ignore', DeprecationWarning)
&gt;&gt;&gt; exit()
 python3 -m rasa_core.run -d models/dialogue -u models/nlu/default/current",-4,31,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
55,48999542,48999797,8214,More efficient weighted Gini coefficient in Python,2,<python><numpy><weighted><gini>,13,"<p>Per <a href=""https://stackoverflow.com/a/48981834/1840471"">https://stackoverflow.com/a/48981834/1840471</a>, this is an implementation of the weighted Gini coefficient in Python:</p>

<pre><code>import numpy as np
def gini(x, weights=None):
    if weights is None:
        weights = np.ones_like(x)
    # Calculate mean absolute deviation in two steps, for weights.
    count = np.multiply.outer(weights, weights)
    mad = np.abs(np.subtract.outer(x, x) * count).sum() / count.sum()
    rmad = mad / np.average(x, weights=weights)
    # Gini equals half the relative mean absolute deviation.
    return 0.5 * rmad
</code></pre>

<p>This is clean and works well for medium-sized arrays, but as warned in its initial suggestion (<a href=""https://stackoverflow.com/a/39513799/1840471"">https://stackoverflow.com/a/39513799/1840471</a>) it's O(n<sup>2</sup>). On my computer that means it breaks after ~20k rows:</p>

<pre><code>n = 20000  # Works, 30000 fails.
gini(np.random.rand(n), np.random.rand(n))
</code></pre>

<p>Can this be adjusted to work for larger datasets? Mine is ~150k rows.</p>
",1840471,15344,27-02-2018 01:00,27-02-2018 01:35,0,15324,138,17,87,66,"{'badge_counts': {'bronze': 138, 'silver': 87, 'gold': 17}, 'account_id': 2064712, 'is_employee': False, 'last_modified_date': 1710556419, 'last_access_date': 1710733368, 'reputation_change_year': 241, 'reputation_change_quarter': 241, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 15324, 'creation_date': 1353456537, 'user_type': 'registered', 'user_id': 1840471, 'accept_rate': 66, 'location': 'Washington, DC', 'website_url': 'http://policyengine.org', 'link': 'https://stackoverflow.com/users/1840471/max-ghenis', 'profile_image': 'https://i.stack.imgur.com/axWtd.jpg?s=256&g=1', 'display_name': 'Max Ghenis'}","Per https://stackoverflow.com/a/48981834/1840471, this is an implementation of the weighted Gini coefficient in Python: This is clean and works well for medium-sized arrays, but as warned in its initial suggestion (https://stackoverflow.com/a/39513799/1840471) it's O(n2). On my computer that means it breaks after ~20k rows: Can this be adjusted to work for larger datasets? Mine is ~150k rows.","import numpy as np
def gini(x, weights=None):
    if weights is None:
        weights = np.ones_like(x)
    # Calculate mean absolute deviation in two steps, for weights.
    count = np.multiply.outer(weights, weights)
    mad = np.abs(np.subtract.outer(x, x) * count).sum() / count.sum()
    rmad = mad / np.average(x, weights=weights)
    # Gini equals half the relative mean absolute deviation.
    return 0.5 * rmad
 n = 20000  # Works, 30000 fails.
gini(np.random.rand(n), np.random.rand(n))
",10,21,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
56,49176376,49176460,8898,Pandas DataFrame to lists of lists including headers,2,<python><pandas><numpy>,11,"<p>Say I have a DataFrame that looks like this:</p>

<pre><code>df = pd.DataFrame([[1, 2, 3], 
                   [4, 5, 6], 
                   [7, 8, 9]], 
                   columns=['Col 1', 'Col 2', 'Col 3'])
&gt;&gt;&gt; df
   Col 1  Col 2  Col 3
0      1      2      3
1      4      5      6
2      7      8      9
</code></pre>

<p>Is there a Pandas way of returning the DataFrame as a list of lists with the headers included?</p>

<p>I can return the headers and values as lists as follows</p>

<pre><code>&gt;&gt;&gt; df.columns.values.tolist()
['Col 1', 'Col 2', 'Col 3']
&gt;&gt;&gt; df.values.tolist()
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]
&gt;&gt;&gt; df.tolist()
</code></pre>

<p>But how could I return the following result?</p>

<pre><code>[['Col 1', 'Col 2', 'Col 3'], [1, 2, 3], [4, 5, 6], [7, 8, 9]]
</code></pre>
",1844376,4330,08-03-2018 15:10,08-03-2018 15:14,0,4340,37,1,29,,"{'badge_counts': {'bronze': 37, 'silver': 29, 'gold': 1}, 'collectives': [{'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 2069621, 'is_employee': False, 'last_modified_date': 1694779800, 'last_access_date': 1711011083, 'reputation_change_year': 158, 'reputation_change_quarter': 158, 'reputation_change_month': 60, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4340, 'creation_date': 1353572797, 'user_type': 'registered', 'user_id': 1844376, 'location': 'Perth, Australia', 'website_url': '', 'link': 'https://stackoverflow.com/users/1844376/scottmcc', 'profile_image': 'https://i.stack.imgur.com/XPxhs.jpg?s=256&g=1', 'display_name': 'ScottMcC'}",Say I have a DataFrame that looks like this: Is there a Pandas way of returning the DataFrame as a list of lists with the headers included? I can return the headers and values as lists as follows But how could I return the following result?,"df = pd.DataFrame([[1, 2, 3], 
                   [4, 5, 6], 
                   [7, 8, 9]], 
                   columns=['Col 1', 'Col 2', 'Col 3'])
&gt;&gt;&gt; df
   Col 1  Col 2  Col 3
0      1      2      3
1      4      5      6
2      7      8      9
 &gt;&gt;&gt; df.columns.values.tolist()
['Col 1', 'Col 2', 'Col 3']
&gt;&gt;&gt; df.values.tolist()
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]
&gt;&gt;&gt; df.tolist()
 [['Col 1', 'Col 2', 'Col 3'], [1, 2, 3], [4, 5, 6], [7, 8, 9]]
",12,28,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
57,48948209,48988121,36899,KeyError when loading pickled scikit-learn model using joblib,6,<python><python-3.x><scikit-learn><joblib>,29,"<p>I have an object that contains within it two <code>scikit-learn</code> models, an <code>IsolationForest</code> and a <code>RandomForestClassifier</code>, that I would like to pickle and later unpickle and use to produce predictions. Apart from the two models, the object contains a couple of <code>StandardScaler</code>s and a couple of Python lists.</p>

<p>Pickling this object using <code>joblib</code> is unproblematic, but when I try to unpickle it later I get the following exception:</p>

<pre><code>Traceback (most recent call last):
 File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
 File ""/home/(...)/python3.5/site-packages/joblib/numpy_pickle.py"", line 578, in load
   obj = _unpickle(fobj, filename, mmap_mode)
 File ""/home/(...)/python3.5/site-packages/joblib/numpy_pickle.py"", line 508, in _unpickle
   obj = unpickler.load()
 File ""/usr/lib/python3.5/pickle.py"", line 1039, in load
   dispatch[key[0]](self)
KeyError: 0
</code></pre>

<p>The same application both pickles and unpickles the object, so the versions of <code>scikit-learn</code>, <code>joblib</code> and other libraries are the same. I'm not sure where to start debugging, given the vague error. Any ideas or pointers?</p>
",1004065,2240,23-02-2018 12:43,26-02-2018 12:04,3,2240,37,4,23,76,"{'badge_counts': {'bronze': 37, 'silver': 23, 'gold': 4}, 'account_id': 510686, 'is_employee': False, 'last_modified_date': 1607615012, 'last_access_date': 1697711673, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2240, 'creation_date': 1319059918, 'user_type': 'registered', 'user_id': 1004065, 'accept_rate': 76, 'link': 'https://stackoverflow.com/users/1004065/haroba', 'profile_image': 'https://www.gravatar.com/avatar/ed58bd30918aa1835e652c17021a8715?s=256&d=identicon&r=PG', 'display_name': 'haroba'}","I have an object that contains within it two models, an and a , that I would like to pickle and later unpickle and use to produce predictions. Apart from the two models, the object contains a couple of s and a couple of Python lists. Pickling this object using is unproblematic, but when I try to unpickle it later I get the following exception: The same application both pickles and unpickles the object, so the versions of , and other libraries are the same. I'm not sure where to start debugging, given the vague error. Any ideas or pointers?","scikit-learn IsolationForest RandomForestClassifier StandardScaler joblib Traceback (most recent call last):
 File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
 File ""/home/(...)/python3.5/site-packages/joblib/numpy_pickle.py"", line 578, in load
   obj = _unpickle(fobj, filename, mmap_mode)
 File ""/home/(...)/python3.5/site-packages/joblib/numpy_pickle.py"", line 508, in _unpickle
   obj = unpickler.load()
 File ""/usr/lib/python3.5/pickle.py"", line 1039, in load
   dispatch[key[0]](self)
KeyError: 0
 scikit-learn joblib",1,16,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
58,49435438,49435511,32399,Pandas validate date format,2,<python><pandas><datetime>,15,"<p>Is there any nice way to validate that all items in a dataframe's column have a valid date format?</p>

<p>My date format is <code>11-Aug-2010</code>.</p>

<p>I saw this generic answer, where:</p>

<pre><code>try:
    datetime.datetime.strptime(date_text, '%Y-%m-%d')
except ValueError:
    raise ValueError(""Incorrect data format, should be YYYY-MM-DD"")
</code></pre>

<p>source: <a href=""https://stackoverflow.com/a/16870699/1374488"">https://stackoverflow.com/a/16870699/1374488</a></p>

<p>But I assume that's not good (efficient) in my case.</p>

<p>I assume I have to modify the strings to be pandas dates first as mentioned here:
<a href=""https://stackoverflow.com/questions/41501726/convert-string-date-time-to-pandas-datetime"">Convert string date time to pandas datetime</a></p>

<p>I am new to the Python world, any ideas appreciated.</p>
",1374488,3976,22-03-2018 17:56,22-03-2018 18:01,0,3986,50,4,34,88,"{'badge_counts': {'bronze': 50, 'silver': 34, 'gold': 4}, 'account_id': 1459890, 'is_employee': False, 'last_modified_date': 1694297700, 'last_access_date': 1710284762, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 3986, 'creation_date': 1336120791, 'user_type': 'registered', 'user_id': 1374488, 'accept_rate': 88, 'website_url': '', 'link': 'https://stackoverflow.com/users/1374488/lukas-o', 'profile_image': 'https://www.gravatar.com/avatar/bf69e588679a9d87084047894027ca46?s=256&d=identicon&r=PG', 'display_name': 'lukas_o'}","Is there any nice way to validate that all items in a dataframe's column have a valid date format? My date format is . I saw this generic answer, where: source: https://stackoverflow.com/a/16870699/1374488 But I assume that's not good (efficient) in my case. I assume I have to modify the strings to be pandas dates first as mentioned here: Convert string date time to pandas datetime I am new to the Python world, any ideas appreciated.","11-Aug-2010 try:
    datetime.datetime.strptime(date_text, '%Y-%m-%d')
except ValueError:
    raise ValueError(""Incorrect data format, should be YYYY-MM-DD"")
",2,20,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
59,48409610,48409996,3159,Share a dict with multiple Python scripts,8,<python><sqlite><dictionary><key-value-store>,13,"<p>I'd like a unique <code>dict</code> (key/value) database to be accessible from multiple Python scripts running at the same time.</p>

<p>If <code>script1.py</code> updates <code>d[2839]</code>, then <code>script2.py</code> should see the <em>modified value</em> when querying <code>d[2839]</code> a few seconds after.</p>

<ul>
<li><p>I thought about using SQLite but it seems that concurrent write/read from multiple processes is not SQLite's strength (let's say <code>script1.py</code> has just modified <code>d[2839]</code>, how would <code>script2.py</code>'s SQLite connection know <a href=""https://stackoverflow.com/questions/48459493/sqlite-concurrency-the-2nd-process-doesnt-get-db-updates"">it has to reload this specific part of the database?</a>)</p></li>
<li><p>I also thought about <em>locking</em> the file when I want to flush the modifications (but it's <a href=""https://stackoverflow.com/questions/186202/what-is-the-best-way-to-open-a-file-for-exclusive-access-in-python"">rather tricky to do</a>), and use <code>json.dump</code> to serialize, then trying to detect the modifications, use <code>json.load</code> to reload if any modification, etc. ... oh no I'm reinventing the wheel, and reinventing a particularly inefficient key/value database!</p></li>
<li><p>redis looked like a solution but <a href=""https://redis.io/download#other-versions"" rel=""noreferrer"">it does not officially support Windows</a>, the same applies for <a href=""https://github.com/google/leveldb"" rel=""noreferrer"">leveldb</a>.</p></li>
<li><p>multiple scripts might want to write at exactly the same time (even if this is a very rare event), is there a way to let the DB system handle this (thanks to a locking parameter? it seems that by default SQLite can't do this because <a href=""https://stackoverflow.com/a/48512801/1422096"">""SQLite supports an unlimited number of simultaneous readers, but it will only allow one writer at any instant in time.""</a>)</p></li>
</ul>

<p><strong>What would be a Pythonic solution for this?</strong></p>

<p>Note: I'm on Windows, and the dict should have maximum 1M items (key and value both integers).</p>
",1422096,43457,23-01-2018 19:42,23-01-2018 20:09,0,43759,730,107,415,63,"{'badge_counts': {'bronze': 730, 'silver': 415, 'gold': 107}, 'account_id': 1522906, 'is_employee': False, 'last_modified_date': 1709342100, 'last_access_date': 1711121107, 'reputation_change_year': 1140, 'reputation_change_quarter': 1140, 'reputation_change_month': 270, 'reputation_change_week': 141, 'reputation_change_day': 0, 'reputation': 43759, 'creation_date': 1338222462, 'user_type': 'registered', 'user_id': 1422096, 'accept_rate': 63, 'website_url': '', 'link': 'https://stackoverflow.com/users/1422096/basj', 'profile_image': 'https://www.gravatar.com/avatar/da3871bf5450fa726f20557fa3f18f51?s=256&d=identicon&r=PG', 'display_name': 'Basj'}","I'd like a unique (key/value) database to be accessible from multiple Python scripts running at the same time. If updates , then should see the modified value when querying a few seconds after. I thought about using SQLite but it seems that concurrent write/read from multiple processes is not SQLite's strength (let's say has just modified , how would 's SQLite connection know it has to reload this specific part of the database?) I also thought about locking the file when I want to flush the modifications (but it's rather tricky to do), and use to serialize, then trying to detect the modifications, use to reload if any modification, etc. ... oh no I'm reinventing the wheel, and reinventing a particularly inefficient key/value database! redis looked like a solution but it does not officially support Windows, the same applies for leveldb. multiple scripts might want to write at exactly the same time (even if this is a very rare event), is there a way to let the DB system handle this (thanks to a locking parameter? it seems that by default SQLite can't do this because ""SQLite supports an unlimited number of simultaneous readers, but it will only allow one writer at any instant in time."") What would be a Pythonic solution for this? Note: I'm on Windows, and the dict should have maximum 1M items (key and value both integers).",dict script1.py d[2839] script2.py d[2839] script1.py d[2839] script2.py json.dump json.load,-10,14,0,5,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
60,48117126,48186077,3481,"When using tweepy cursor, what is the best practice for catching over capacity errors?",1,<python><tweepy>,16,"<p>I'm gathering information on a large number of users using Python's Tweepy library. I've the API initialized as follows </p>

<blockquote>
  <p>api = tweepy.API(auth,wait_on_rate_limit=True,
  wait_on_rate_limit_notify=True)</p>
</blockquote>

<p>where auth contains my tokens. This code responds well to rate limit errors, but doesn't work for some other errors. For example, I sometimes see the following exception. </p>

<blockquote>
<pre><code>tweepy.error.TweepError: [{'message': 'Over capacity', 'code': 130}]
</code></pre>
</blockquote>

<p>I can handle this exception using a try except, but I was wondering if there is a way to handle this exception within the cursor like I'm handling rate limit exceptions. I see parameters like retry_count, but I'm not sure if they will work for this case as they seem designed for HTTP errors. </p>
",1507889,473,05-01-2018 16:01,10-01-2018 11:09,5,473,20,0,8,45,"{'badge_counts': {'bronze': 20, 'silver': 8, 'gold': 0}, 'account_id': 133581, 'is_employee': False, 'last_modified_date': 1605376735, 'last_access_date': 1631638015, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 473, 'creation_date': 1341610573, 'user_type': 'registered', 'user_id': 1507889, 'accept_rate': 45, 'link': 'https://stackoverflow.com/users/1507889/user1507889', 'profile_image': 'https://www.gravatar.com/avatar/f3d6850c91ada65b62df8f909e67719d?s=256&d=identicon&r=PG', 'display_name': 'user1507889'}","I'm gathering information on a large number of users using Python's Tweepy library. I've the API initialized as follows api = tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify=True) where auth contains my tokens. This code responds well to rate limit errors, but doesn't work for some other errors. For example, I sometimes see the following exception. I can handle this exception using a try except, but I was wondering if there is a way to handle this exception within the cursor like I'm handling rate limit exceptions. I see parameters like retry_count, but I'm not sure if they will work for this case as they seem designed for HTTP errors.","tweepy.error.TweepError: [{'message': 'Over capacity', 'code': 130}]
",0,15,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
61,49947942,49947995,755,Elegant alternative to long exception chains?,3,<python>,13,"<p>A lot of times I find myself writing something that looks like this:</p>

<pre><code>try:
    procedure_a()
except WrongProcedureError:
    try:
        procedure_b()
    except WrongProcedureError:
        try:
            procedure_c()
        except WrongProcedureError:
            give_up()
</code></pre>

<p>This is hideous. Is there a more elegant way to implement this kind of ""try things until one doesn't exception"" logic? It seems like this is the kind of thing that would come up a lot; I'm hoping there's some language feature I don't know about that's designed for this exact thing.</p>
",1726380,2364,20-04-2018 18:46,20-04-2018 18:50,0,2364,36,1,19,90,"{'badge_counts': {'bronze': 36, 'silver': 19, 'gold': 1}, 'account_id': 1913131, 'is_employee': False, 'last_modified_date': 1633217533, 'last_access_date': 1665328666, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2364, 'creation_date': 1349598497, 'user_type': 'registered', 'user_id': 1726380, 'accept_rate': 90, 'website_url': '', 'link': 'https://stackoverflow.com/users/1726380/schilcote', 'profile_image': 'https://www.gravatar.com/avatar/00a34191d4cd325019288e7ba41c2595?s=256&d=identicon&r=PG', 'display_name': 'Schilcote'}","A lot of times I find myself writing something that looks like this: This is hideous. Is there a more elegant way to implement this kind of ""try things until one doesn't exception"" logic? It seems like this is the kind of thing that would come up a lot; I'm hoping there's some language feature I don't know about that's designed for this exact thing.","try:
    procedure_a()
except WrongProcedureError:
    try:
        procedure_b()
    except WrongProcedureError:
        try:
            procedure_c()
        except WrongProcedureError:
            give_up()
",9,15,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
62,49065313,49066648,1029,OpenCV digits merging into surrounding boxes,3,<python><c++><opencv><computer-vision><imagemagick>,12,"<p>I've got a bunch of dates i'm trying to OCR using tesseract.
However, a lot of the digits in the dates merge with the lines in the date boxes as so:</p>

<hr>

<p><img src=""https://i.stack.imgur.com/7Ac56.jpg"" alt=""Digits intersecting boxes"">
<img src=""https://i.stack.imgur.com/nYuPm.jpg"" alt=""Digits intersecting boxes"">
<img src=""https://i.stack.imgur.com/RyXka.jpg"" alt=""Digits intersecting boxes"">
<img src=""https://i.stack.imgur.com/AZJ7n.jpg"" alt=""Digits intersecting boxes""></p>

<hr>

<p>Also, here's a good image that i can tesseract well with:
<img src=""https://i.stack.imgur.com/IFbGO.jpg"" alt=""Good Date Image""></p>

<hr>

<p>And here's my code:</p>

<pre><code>import os
import cv2
from matplotlib import pyplot as plt
import subprocess
import numpy as np
from PIL import Image

def show(img):
    plt.figure(figsize=(20,20))
    plt.imshow(img,cmap='gray')
    plt.show()

def sort_contours(cnts, method=""left-to-right""):
    # initialize the reverse flag and sort index
    reverse = False
    i = 0

    # handle if we need to sort in reverse
    if method == ""right-to-left"" or method == ""bottom-to-top"":
        reverse = True

    # handle if we are sorting against the y-coordinate rather than
    # the x-coordinate of the bounding box
    if method == ""top-to-bottom"" or method == ""bottom-to-top"":
        i = 1

    # construct the list of bounding boxes and sort them from top to
    # bottom
    boundingBoxes = [cv2.boundingRect(c) for c in cnts]

    cnts, boundingBoxes = zip(*sorted(zip(cnts, boundingBoxes),
        key=lambda b:b[1][i], reverse=reverse))

    # return the list of sorted contours and bounding boxes
    return cnts, boundingBoxes


def tesseract_it(contours,main_img, label,delete_last_contour=False):
    min_limit, max_limit = (1300,1700)
    idx =0 
    roi_list = []
    slist= set()
    for cnt in contours:
        idx += 1
        x,y,w,h = cv2.boundingRect(cnt)
        if label=='boxes':
            roi=main_img[y+2:y+h-2,x+2:x+w-2]
        else:
            roi=main_img[y:y+h,x:x+w]

        if w*h &gt; min_limit and w*h &lt; max_limit and w&gt;10 and w&lt; 50 and h&gt;10 and h&lt;50:
            if (x,y,w,h) not in slist: # Stops from identifying repeted contours

                roi = cv2.resize(roi,dsize=(45,45),fx=0 ,fy=0, interpolation = cv2.INTER_AREA)
                roi_list.append(roi)
                slist.add((x,y,w,h))

    if not delete_last_contour:
        vis = np.concatenate((roi_list),1)
    else:
        roi_list.pop(-1)
        vis = np.concatenate((roi_list),1)

    show(vis)

    # Tesseract the final image here
    # ...


image = 'bad_digit/1.jpg'
# image = 'bad_digit/good.jpg'
specimen_orig = cv2.imread(image,0)


specimen = cv2.fastNlMeansDenoising(specimen_orig)
#     show(specimen)
kernel = np.ones((3,3), np.uint8)

# Now we erode
specimen = cv2.erode(specimen, kernel, iterations = 1)
#     show(specimen)
_, specimen = cv2.threshold(specimen, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#     show(specimen)
specimen_canny = cv2.Canny(specimen, 0, 0)
#     show(specimen_canny)

specimen_blank_image = np.zeros((specimen.shape[0], specimen.shape[1], 3))
_,specimen_contours, retr = cv2.findContours(specimen_canny.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE )
# print(len(specimen_contours))
cv2.drawContours(specimen_blank_image, specimen_contours, -1, 100, 2)
#     show(specimen_blank_image)
specimen_blank_image = np.zeros((specimen.shape[0], specimen.shape[1], 3))

specimen_sorted_contours, specimen_bounding_box = sort_contours(specimen_contours)

output_string = tesseract_it(specimen_sorted_contours,specimen_orig,label='boxes',)
# return output_string
</code></pre>

<hr>

<p>The output from the good image attached is so:
<img src=""https://i.stack.imgur.com/qHPSd.png"" alt=""Good output""></p>

<hr>

<p>Tesseracting this image does give me accurate results.</p>

<p>However, for the ones where the lines are merging into the digits, my output looks like this:
<img src=""https://i.stack.imgur.com/8aY60.png"" alt=""bad1"">
<img src=""https://i.stack.imgur.com/2GE45.png"" alt=""bad2"">
<img src=""https://i.stack.imgur.com/wiAXc.png"" alt=""bad3"">
<img src=""https://i.stack.imgur.com/MEgxY.png"" alt=""bad4""></p>

<p>These do not work well with Tesseract at all.
I was wondering if there was a way to remove the lines and keep only the digits.</p>

<p>I have tried the following as well:
<a href=""https://docs.opencv.org/3.2.0/d1/dee/tutorial_moprh_lines_detection.html"" rel=""noreferrer"">https://docs.opencv.org/3.2.0/d1/dee/tutorial_moprh_lines_detection.html</a></p>

<p>Which doesn't really seem to do great on the images i've attached.</p>

<p>I've also tried to use imagemagick:</p>

<pre><code>convert original.jpg \
\( -clone 0 -threshold 50% -negate -statistic median 200x1 \)  \
-compose lighten -composite                                    \
\( -clone 0 -threshold 50% -negate -statistic median 1x200 \)  \
-composite output.jpg
</code></pre>

<p>Its results are fair, but the line removed somewhat cuts through the digits as following:</p>

<p><img src=""https://i.stack.imgur.com/0fWQQ.jpg"" alt=""imagemagick1"">
<img src=""https://i.stack.imgur.com/WrNK8.jpg"" alt=""imagemagick2"">
<img src=""https://i.stack.imgur.com/TXdKD.jpg"" alt=""imagemagick3"">
<img src=""https://i.stack.imgur.com/YHnWM.jpg"" alt=""imagemagick4""></p>

<p>Is there a better way i can approach this problem? My final goal is to tesseract the digits, so the final image does need to be quite clear.</p>
",1785374,2679,02-03-2018 08:29,02-03-2018 09:52,0,2679,40,4,25,67,"{'badge_counts': {'bronze': 40, 'silver': 25, 'gold': 4}, 'account_id': 1804440, 'is_employee': False, 'last_modified_date': 1691267100, 'last_access_date': 1711180213, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2679, 'creation_date': 1351596188, 'user_type': 'registered', 'user_id': 1785374, 'accept_rate': 67, 'location': 'India', 'website_url': '', 'link': 'https://stackoverflow.com/users/1785374/boreboar', 'profile_image': 'https://www.gravatar.com/avatar/1fb07483646b47655e7f5d272e4e9949?s=256&d=identicon&r=PG', 'display_name': 'BoreBoar'}","I've got a bunch of dates i'm trying to OCR using tesseract. However, a lot of the digits in the dates merge with the lines in the date boxes as so: Also, here's a good image that i can tesseract well with: And here's my code: The output from the good image attached is so: Tesseracting this image does give me accurate results. However, for the ones where the lines are merging into the digits, my output looks like this: These do not work well with Tesseract at all. I was wondering if there was a way to remove the lines and keep only the digits. I have tried the following as well: https://docs.opencv.org/3.2.0/d1/dee/tutorial_moprh_lines_detection.html Which doesn't really seem to do great on the images i've attached. I've also tried to use imagemagick: Its results are fair, but the line removed somewhat cuts through the digits as following: Is there a better way i can approach this problem? My final goal is to tesseract the digits, so the final image does need to be quite clear.","import os
import cv2
from matplotlib import pyplot as plt
import subprocess
import numpy as np
from PIL import Image

def show(img):
    plt.figure(figsize=(20,20))
    plt.imshow(img,cmap='gray')
    plt.show()

def sort_contours(cnts, method=""left-to-right""):
    # initialize the reverse flag and sort index
    reverse = False
    i = 0

    # handle if we need to sort in reverse
    if method == ""right-to-left"" or method == ""bottom-to-top"":
        reverse = True

    # handle if we are sorting against the y-coordinate rather than
    # the x-coordinate of the bounding box
    if method == ""top-to-bottom"" or method == ""bottom-to-top"":
        i = 1

    # construct the list of bounding boxes and sort them from top to
    # bottom
    boundingBoxes = [cv2.boundingRect(c) for c in cnts]

    cnts, boundingBoxes = zip(*sorted(zip(cnts, boundingBoxes),
        key=lambda b:b[1][i], reverse=reverse))

    # return the list of sorted contours and bounding boxes
    return cnts, boundingBoxes


def tesseract_it(contours,main_img, label,delete_last_contour=False):
    min_limit, max_limit = (1300,1700)
    idx =0 
    roi_list = []
    slist= set()
    for cnt in contours:
        idx += 1
        x,y,w,h = cv2.boundingRect(cnt)
        if label=='boxes':
            roi=main_img[y+2:y+h-2,x+2:x+w-2]
        else:
            roi=main_img[y:y+h,x:x+w]

        if w*h &gt; min_limit and w*h &lt; max_limit and w&gt;10 and w&lt; 50 and h&gt;10 and h&lt;50:
            if (x,y,w,h) not in slist: # Stops from identifying repeted contours

                roi = cv2.resize(roi,dsize=(45,45),fx=0 ,fy=0, interpolation = cv2.INTER_AREA)
                roi_list.append(roi)
                slist.add((x,y,w,h))

    if not delete_last_contour:
        vis = np.concatenate((roi_list),1)
    else:
        roi_list.pop(-1)
        vis = np.concatenate((roi_list),1)

    show(vis)

    # Tesseract the final image here
    # ...


image = 'bad_digit/1.jpg'
# image = 'bad_digit/good.jpg'
specimen_orig = cv2.imread(image,0)


specimen = cv2.fastNlMeansDenoising(specimen_orig)
#     show(specimen)
kernel = np.ones((3,3), np.uint8)

# Now we erode
specimen = cv2.erode(specimen, kernel, iterations = 1)
#     show(specimen)
_, specimen = cv2.threshold(specimen, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
#     show(specimen)
specimen_canny = cv2.Canny(specimen, 0, 0)
#     show(specimen_canny)

specimen_blank_image = np.zeros((specimen.shape[0], specimen.shape[1], 3))
_,specimen_contours, retr = cv2.findContours(specimen_canny.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE )
# print(len(specimen_contours))
cv2.drawContours(specimen_blank_image, specimen_contours, -1, 100, 2)
#     show(specimen_blank_image)
specimen_blank_image = np.zeros((specimen.shape[0], specimen.shape[1], 3))

specimen_sorted_contours, specimen_bounding_box = sort_contours(specimen_contours)

output_string = tesseract_it(specimen_sorted_contours,specimen_orig,label='boxes',)
# return output_string
 convert original.jpg \
\( -clone 0 -threshold 50% -negate -statistic median 200x1 \)  \
-compose lighten -composite                                    \
\( -clone 0 -threshold 50% -negate -statistic median 1x200 \)  \
-composite output.jpg
",100,158,14,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
63,48436643,48436696,6915,Django model property with parameter,1,<python><django><properties>,11,"<p>I've the following models in Django.</p>

<pre><code>class User(models.Model):
    name = models.CharField(max_length=50)
    ...
    ...

    @property
    def get_info(self, key=None):
        value = self.name if key else 'Hello World'
        return value
</code></pre>

<p>But when I try to execute the code in Django shell, I'm getting the following error.</p>

<pre><code>n [4]: user = User.objects.get(id=1)
n [5]: user.get_info(key='test_key')
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-5-f7b917070aee&gt; in &lt;module&gt;()
----&gt; 1 user.get_info(key='test_key')

TypeError: _get_info() takes exactly 2 arguments (1 given)
</code></pre>
",1795073,3641,25-01-2018 06:10,25-01-2018 06:14,0,3651,43,8,33,55,"{'badge_counts': {'bronze': 43, 'silver': 33, 'gold': 8}, 'account_id': 2004911, 'is_employee': False, 'last_modified_date': 1699616100, 'last_access_date': 1711025917, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3651, 'creation_date': 1351878685, 'user_type': 'registered', 'user_id': 1795073, 'accept_rate': 55, 'location': 'Gurgaon, Haryana, India', 'website_url': 'http://chinmayakrpatanaik.com', 'link': 'https://stackoverflow.com/users/1795073/pattu', 'profile_image': 'https://www.gravatar.com/avatar/b5a0ac48d1b8836dc6f679a9003d0dcf?s=256&d=identicon&r=PG', 'display_name': 'Pattu'}","I've the following models in Django. But when I try to execute the code in Django shell, I'm getting the following error.","class User(models.Model):
    name = models.CharField(max_length=50)
    ...
    ...

    @property
    def get_info(self, key=None):
        value = self.name if key else 'Hello World'
        return value
 n [4]: user = User.objects.get(id=1)
n [5]: user.get_info(key='test_key')
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-5-f7b917070aee&gt; in &lt;module&gt;()
----&gt; 1 user.get_info(key='test_key')

TypeError: _get_info() takes exactly 2 arguments (1 given)
",15,24,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
64,49113247,49137722,16644,Python and Visual Studio Code - How do I run a specific file in the editor?,2,<python><visual-studio-code>,15,"<p>I am writing a small application with Visual Studio Code and Python. My application has two files, <code>Main.py</code> and <code>MyCustomClass.py</code>. <code>Main.py</code> is the entry point to the application, <code>MyCustomClass.py</code> contains the logic code to solve some problems.</p>
<p>Currently, the debugger is set to run whatever the active file is. Is it possible to configure Visual Studio Code to run a specific file versus the active file in the editor?</p>
",1846762,6665,05-03-2018 14:57,06-03-2018 18:32,1,6685,148,21,83,48,"{'badge_counts': {'bronze': 148, 'silver': 83, 'gold': 21}, 'account_id': 2072666, 'is_employee': False, 'last_modified_date': 1647050100, 'last_access_date': 1710767790, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 40, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 6685, 'creation_date': 1353653719, 'user_type': 'registered', 'user_id': 1846762, 'accept_rate': 48, 'website_url': '', 'link': 'https://stackoverflow.com/users/1846762/no1lives4ever', 'profile_image': 'https://www.gravatar.com/avatar/cf4b36f24300decf8d09bed51403eeae?s=256&d=identicon&r=PG', 'display_name': 'No1Lives4Ever'}","I am writing a small application with Visual Studio Code and Python. My application has two files, and . is the entry point to the application, contains the logic code to solve some problems. Currently, the debugger is set to run whatever the active file is. Is it possible to configure Visual Studio Code to run a specific file versus the active file in the editor?",Main.py MyCustomClass.py Main.py MyCustomClass.py,-4,2,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
65,48302139,48302929,1245,Second parameter of super()?,2,<python><python-3.x><class><super>,14,"<p>A colleague of mine wrote code analogous to the following today, asked me to have a look, and it took me a while to spot the mistake:</p>

<pre><code>class A():                                                                                         
    def __init__(self):                                                         
        print('A')                                                              

class B(A):                                                                     
    def __init__(self):                                                         
        super(B).__init__()                                               

b = B()
</code></pre>

<p>The problem here is that there's no <code>self</code> parameter to <code>super()</code> in <code>B</code>'s constructor. What surprised me is that absolutely nothing happens in this case, i.e. no error, nothing. What does the <code>super</code> object created by <code>super(B)</code> contain? As an object, it clearly has a constructor, so that's what gets called, but how is that object related to <code>B</code>? In particular, why is this valid code and doesn't throw an exception somewhere? Is <code>super(B)</code> an object with some actual use and what would that be?</p>
",1941914,852,17-01-2018 13:18,17-01-2018 14:00,0,852,16,0,5,69,"{'badge_counts': {'bronze': 16, 'silver': 5, 'gold': 0}, 'account_id': 2196395, 'is_employee': False, 'last_modified_date': 1707563402, 'last_access_date': 1686297119, 'reputation_change_year': -10, 'reputation_change_quarter': -10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 852, 'creation_date': 1357099106, 'user_type': 'registered', 'user_id': 1941914, 'accept_rate': 69, 'location': 'Helsinki, Finland', 'website_url': '', 'link': 'https://stackoverflow.com/users/1941914/edvard-fagerholm', 'profile_image': 'https://www.gravatar.com/avatar/051b651759f8a0f9abe838f1bafeb0d3?s=256&d=identicon&r=PG', 'display_name': 'Edvard Fagerholm'}","A colleague of mine wrote code analogous to the following today, asked me to have a look, and it took me a while to spot the mistake: The problem here is that there's no parameter to in 's constructor. What surprised me is that absolutely nothing happens in this case, i.e. no error, nothing. What does the object created by contain? As an object, it clearly has a constructor, so that's what gets called, but how is that object related to ? In particular, why is this valid code and doesn't throw an exception somewhere? Is an object with some actual use and what would that be?","class A():                                                                                         
    def __init__(self):                                                         
        print('A')                                                              

class B(A):                                                                     
    def __init__(self):                                                         
        super(B).__init__()                                               

b = B()
 self super() B super super(B) B super(B)",1,14,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
66,49170296,49171133,49197,scikit learn - feature importance calculation in decision trees,2,<python><scikit-learn><decision-tree><feature-selection>,29,"<p>I'm trying to understand how feature importance is calculated for decision trees in sci-kit learn. This question has been asked before, but I am unable to reproduce the results the algorithm is providing. </p>

<p>For example: </p>

<pre><code>from StringIO import StringIO

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree.export import export_graphviz
from sklearn.feature_selection import mutual_info_classif

X = [[1,0,0], [0,0,0], [0,0,1], [0,1,0]]

y = [1,0,1,1]

clf = DecisionTreeClassifier()
clf.fit(X, y)

feat_importance = clf.tree_.compute_feature_importances(normalize=False)
print(""feat importance = "" + str(feat_importance))

out = StringIO()
out = export_graphviz(clf, out_file='test/tree.dot')
</code></pre>

<p>results in feature importance:</p>

<pre><code>feat importance = [0.25       0.08333333 0.04166667]
</code></pre>

<p>and gives the following decision tree:</p>

<p><a href=""https://i.stack.imgur.com/ORz4y.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ORz4y.png"" alt=""decision tree""></a></p>

<p>Now, this <a href=""https://stats.stackexchange.com/a/92843/194377"">answer</a> to a similar question suggests the importance is calculated as </p>

<p><a href=""https://i.stack.imgur.com/tmIZI.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tmIZI.gif"" alt=""formula_a""></a></p>

<p>Where G is the node impurity, in this case the gini impurity. This is the impurity reduction as far as I understood it. However, for feature 1 this should be:</p>

<p><a href=""https://i.stack.imgur.com/9MAv2.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9MAv2.gif"" alt=""formula_b""></a></p>

<p>This <a href=""https://stackoverflow.com/a/15821880/1618893"">answer</a> suggests the importance is weighted by the probability of reaching the node (which is approximated by the proportion of samples reaching that node). Again, for feature 1 this should be:</p>

<p><a href=""https://i.stack.imgur.com/LvISu.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/LvISu.gif"" alt=""formula_c""></a></p>

<p>Both formulas provide the wrong result. How is the feature importance calculated correctly?</p>
",1618893,910,08-03-2018 10:00,08-03-2018 10:42,0,910,25,3,11,83,"{'badge_counts': {'bronze': 25, 'silver': 11, 'gold': 3}, 'account_id': 1774953, 'is_employee': False, 'last_modified_date': 1683942600, 'last_access_date': 1704553835, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 910, 'creation_date': 1345704497, 'user_type': 'registered', 'user_id': 1618893, 'accept_rate': 83, 'location': 'Graz, Austria', 'website_url': '', 'link': 'https://stackoverflow.com/users/1618893/roman-purgstaller', 'profile_image': 'https://i.stack.imgur.com/kBiUc.jpg?s=256&g=1', 'display_name': 'Roman Purgstaller'}","I'm trying to understand how feature importance is calculated for decision trees in sci-kit learn. This question has been asked before, but I am unable to reproduce the results the algorithm is providing. For example: results in feature importance: and gives the following decision tree: Now, this answer to a similar question suggests the importance is calculated as Where G is the node impurity, in this case the gini impurity. This is the impurity reduction as far as I understood it. However, for feature 1 this should be: This answer suggests the importance is weighted by the probability of reaching the node (which is approximated by the proportion of samples reaching that node). Again, for feature 1 this should be: Both formulas provide the wrong result. How is the feature importance calculated correctly?","from StringIO import StringIO

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree.export import export_graphviz
from sklearn.feature_selection import mutual_info_classif

X = [[1,0,0], [0,0,0], [0,0,1], [0,1,0]]

y = [1,0,1,1]

clf = DecisionTreeClassifier()
clf.fit(X, y)

feat_importance = clf.tree_.compute_feature_importances(normalize=False)
print(""feat importance = "" + str(feat_importance))

out = StringIO()
out = export_graphviz(clf, out_file='test/tree.dot')
 feat importance = [0.25       0.08333333 0.04166667]
",18,47,4,6,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
67,48242761,48242762,9524,How do I use oauth2 and refresh tokens with the google api?,1,<python><google-api><google-oauth><gmail-api><refresh-token>,20,"<p>So I just spent the last few days trying to figure this out and am asking this question so that I can answer it for other people who are having problems.</p>

<p>First, the google documentation is TERRIBLE and uses different oauth2 libraries depending on which of the many google API examples you are looking at. It is often self-contradictory and sometimes straight up has code in it that doesn't work.</p>

<p>Oh well.</p>

<p>so my questions were basically: </p>

<ol>
<li>how do I use the google api libraries to have my users grant me access to their google accounts? </li>
<li>how do I store the oauth2 access tokens that google returns so that I can use them a few days from now?</li>
<li>how do i actually use the refresh_token and refresh it?</li>
</ol>

<p>see answer below for a fully functional authorization flow, from getting the initial token to saving it, loading it later, refreshing it, and using it.</p>

<p>Cheers.</p>
",1626536,803,13-01-2018 18:03,13-01-2018 18:03,0,803,14,1,7,,"{'badge_counts': {'bronze': 14, 'silver': 7, 'gold': 1}, 'account_id': 1784837, 'is_employee': False, 'last_modified_date': 1573682127, 'last_access_date': 1552278044, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 803, 'creation_date': 1346028073, 'user_type': 'registered', 'user_id': 1626536, 'link': 'https://stackoverflow.com/users/1626536/user1626536', 'profile_image': 'https://www.gravatar.com/avatar/3f29feee2faf82180f19235127c7b444?s=256&d=identicon&r=PG', 'display_name': 'user1626536'}","So I just spent the last few days trying to figure this out and am asking this question so that I can answer it for other people who are having problems. First, the google documentation is TERRIBLE and uses different oauth2 libraries depending on which of the many google API examples you are looking at. It is often self-contradictory and sometimes straight up has code in it that doesn't work. Oh well. so my questions were basically: how do I use the google api libraries to have my users grant me access to their google accounts? how do I store the oauth2 access tokens that google returns so that I can use them a few days from now? how do i actually use the refresh_token and refresh it? see answer below for a fully functional authorization flow, from getting the initial token to saving it, loading it later, refreshing it, and using it. Cheers.",,0,17,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
68,49782749,49791106,6325,ProcessPoolExecutor logging fails to log inside function on Windows but not on Unix / Mac,1,<python><multiprocessing><concurrent.futures>,13,"<p>When I run the following script on a Windows computer, I do not see any of the log messages from the <code>log_pid</code> function, however I do when I run on Unix / Mac. I've read before that multiprocessing is different on Windows compared to Mac, but it's not clear to me what changes should I make to get this script to work on Windows. I'm running Python 3.6.</p>

<pre><code>import logging
import sys
from concurrent.futures import ProcessPoolExecutor
import os


def log_pid(x):
    logger.info('Executing on process: %s' % os.getpid())


def do_stuff():
    logger.info('this is the do stuff function.')
    with ProcessPoolExecutor(max_workers=4) as executor:
        executor.map(log_pid, range(0, 10))


def main():
    logger.info('this is the main function.')
    do_stuff()


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
    logger = logging.getLogger(__name__)

    logger.info('Start of script ...')

    main()

    logger.info('End of script ...')
</code></pre>
",2569531,2379,11-04-2018 19:07,12-04-2018 08:04,1,2389,60,8,45,67,"{'badge_counts': {'bronze': 60, 'silver': 45, 'gold': 8}, 'account_id': 3030377, 'is_employee': False, 'last_modified_date': 1607614727, 'last_access_date': 1684345639, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2389, 'creation_date': 1373475615, 'user_type': 'registered', 'user_id': 2569531, 'accept_rate': 67, 'location': 'New York, United States', 'link': 'https://stackoverflow.com/users/2569531/blahblahblah', 'profile_image': 'https://www.gravatar.com/avatar/5b53069921004ce2d573b6c244e16417?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'blahblahblah'}","When I run the following script on a Windows computer, I do not see any of the log messages from the function, however I do when I run on Unix / Mac. I've read before that multiprocessing is different on Windows compared to Mac, but it's not clear to me what changes should I make to get this script to work on Windows. I'm running Python 3.6.","log_pid import logging
import sys
from concurrent.futures import ProcessPoolExecutor
import os


def log_pid(x):
    logger.info('Executing on process: %s' % os.getpid())


def do_stuff():
    logger.info('this is the do stuff function.')
    with ProcessPoolExecutor(max_workers=4) as executor:
        executor.map(log_pid, range(0, 10))


def main():
    logger.info('this is the main function.')
    do_stuff()


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
    logger = logging.getLogger(__name__)

    logger.info('Start of script ...')

    main()

    logger.info('End of script ...')
",28,33,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
69,48503593,48503627,11721,What is the -t flag for pip?,2,<python><pip>,12,"<p>I was advised to use <code>pip install module-name -t /path/to/project-dir</code>. What does the <code>-t</code> flag mean in this command?</p>
",2622900,985,29-01-2018 14:30,29-01-2018 14:32,0,985,13,1,7,100,"{'badge_counts': {'bronze': 13, 'silver': 7, 'gold': 1}, 'account_id': 3098099, 'is_employee': False, 'last_modified_date': 1631300648, 'last_access_date': 1711027208, 'reputation_change_year': 8, 'reputation_change_quarter': 8, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 985, 'creation_date': 1374844377, 'user_type': 'registered', 'user_id': 2622900, 'accept_rate': 100, 'location': 'Juiz de Fora - MG, Brasil', 'website_url': 'https://krempser.com.br/', 'link': 'https://stackoverflow.com/users/2622900/thiago-krempser', 'profile_image': 'https://i.stack.imgur.com/FhfIO.jpg?s=256&g=1', 'display_name': 'Thiago Krempser'}",I was advised to use . What does the flag mean in this command?,pip install module-name -t /path/to/project-dir -t,-2,1,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
70,49631326,49632307,4863,Why is itertools.chain faster than a flattening list comprehension?,1,<python><list-comprehension><python-itertools><flatten>,16,"<p>In the context of a discussion in the comments of <a href=""https://stackoverflow.com/questions/49630581/why-does-python-forbid-the-use-of-sum-with-strings"">this question</a> it was mentioned that while concatenating a sequence of strings simply takes <code>''.join([str1, str2, ...])</code>, concatenating a sequence of lists would be something like <code>list(itertools.chain(lst1, lst2, ...))</code>, although you can also use a list comprehension like <code>[x for y in [lst1, lst2, ...] for x in y]</code>. What surprised me is that the first method is consistently faster than the second:</p>

<pre><code>import random
import itertools

random.seed(100)
lsts = [[1] * random.randint(100, 1000) for i in range(1000)]

%timeit [x for y in lsts for x in y]
# 39.3 ms ± 436 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
%timeit list(itertools.chain.from_iterable(lsts))
# 30.6 ms ± 866 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
%timeit list(x for y in lsts for x in y)  # Proposed in comments
# 62.5 ms ± 504 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
# Loop-based methods proposed in the comments
%%timeit
a = []
for lst in lsts: a += lst
# 26.4 ms ± 634 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
%%timeit
a = []
for lst in lsts: a.extend(lst)
# 26.7 ms ± 728 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>

<p>It is not a difference of orders of magnitude, but it is not negligible either. I was wondering how that might be the case, since list comprehensions are frequently among the fastest methods to solve a given problem. At first I thought that maybe the <code>itertools.chain</code> object would have a <code>len</code> that the <code>list</code> constructor could use to preallocate the necessary memory, but that is not the case (cannot call <code>len</code> on <code>itertools.chain</code> objects). Is some custom <code>itertools.chain</code>-to-<code>list</code> conversion taking place somehow or is <code>itertools.chain</code> taking advantage of some other mechanism?</p>

<p>Tested in Python 3.6.3 on Windows 10 x64, if that is relevant.</p>

<p><em>EDIT:</em></p>

<p>It seems the fastest method after all is calling <code>.extend</code>ing an empty list with each list, as proposed by <a href=""https://stackoverflow.com/users/7553525/zwer"">@zwer</a>, probably because it works on ""chunks"" of data instead of on a per-element basis.</p>
",1782792,59016,03-04-2018 13:27,03-04-2018 14:13,0,59066,125,7,80,100,"{'badge_counts': {'bronze': 125, 'silver': 80, 'gold': 7}, 'account_id': 1988078, 'is_employee': False, 'last_modified_date': 1706540700, 'last_access_date': 1710179275, 'reputation_change_year': 268, 'reputation_change_quarter': 268, 'reputation_change_month': 80, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 59066, 'creation_date': 1351511020, 'user_type': 'registered', 'user_id': 1782792, 'accept_rate': 100, 'location': 'UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/1782792/jdehesa', 'profile_image': 'https://www.gravatar.com/avatar/8901e429005dcf270bea21e1299f47ea?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jdehesa'}","In the context of a discussion in the comments of this question it was mentioned that while concatenating a sequence of strings simply takes , concatenating a sequence of lists would be something like , although you can also use a list comprehension like . What surprised me is that the first method is consistently faster than the second: It is not a difference of orders of magnitude, but it is not negligible either. I was wondering how that might be the case, since list comprehensions are frequently among the fastest methods to solve a given problem. At first I thought that maybe the object would have a that the constructor could use to preallocate the necessary memory, but that is not the case (cannot call on objects). Is some custom -to- conversion taking place somehow or is taking advantage of some other mechanism? Tested in Python 3.6.3 on Windows 10 x64, if that is relevant. EDIT: It seems the fastest method after all is calling ing an empty list with each list, as proposed by @zwer, probably because it works on ""chunks"" of data instead of on a per-element basis.","''.join([str1, str2, ...]) list(itertools.chain(lst1, lst2, ...)) [x for y in [lst1, lst2, ...] for x in y] import random
import itertools

random.seed(100)
lsts = [[1] * random.randint(100, 1000) for i in range(1000)]

%timeit [x for y in lsts for x in y]
# 39.3 ms ± 436 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
%timeit list(itertools.chain.from_iterable(lsts))
# 30.6 ms ± 866 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
%timeit list(x for y in lsts for x in y)  # Proposed in comments
# 62.5 ms ± 504 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
# Loop-based methods proposed in the comments
%%timeit
a = []
for lst in lsts: a += lst
# 26.4 ms ± 634 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
%%timeit
a = []
for lst in lsts: a.extend(lst)
# 26.7 ms ± 728 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
 itertools.chain len list len itertools.chain itertools.chain list itertools.chain .extend",8,32,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
71,48620168,48693576,13508,Django admin add custom filter,3,<python><django><python-2.7><django-admin><django-admin-filters>,19,"<p>i'm using django 1.10 and I need to display data and create a filter based on a value from a different model(which has a foreign key referencing my model that is used on the admin template)
These are my 2 models:
This one is used to generate the template:</p>
<pre><code>class Job(models.Model):
    company = models.ForeignKey(Company)
    title = models.CharField(max_length=100, blank=False)
    description = models.TextField(blank=False, default='')
    store = models.CharField(max_length=100, blank=True, default='')
    phone_number = models.CharField(max_length=60, null=True, blank=True)
</code></pre>
<p>This is the other one that holds a foreign key reference to my first one:</p>
<pre><code>class JobAdDuration(models.Model):
    job = models.ForeignKey(Job)
    ad_activated = models.DateTimeField(auto_now_add=True)
    ad_finished = models.DateTimeField(blank=True, null=True)
</code></pre>
<p>Inside my template, I have been able to display the(latest)start and end times</p>
<pre><code>def start_date(self,obj):
    if JobAdDuration.objects.filter(job=obj.id).exists():
        tempad = JobAdDuration.objects.filter(job=obj).order_by(&quot;-id&quot;)[0]
        return tempad.ad_activated
</code></pre>
<p>And then I just call this inside the list_display and that is working fine.
However, i have trouble setting a filter field using these criteria.</p>
<p>If I just add it to my list_filter then I get an error that there is no such field inside my model which is true (since that one is in another table that has reference to my job table). So I was wondering what is the right approach to solve this? Do I need to create another function for the filter itself but even then I'm not sure how should I call it inside the list_filter.</p>
<p>Here is a snippet of my Django admin page.</p>
<pre><code>class JobAdmin(admin.OSMGeoAdmin, ImportExportModelAdmin):
    inlines = [
    ]

    readonly_fields = ( 'id', &quot;start_date&quot;, )

    raw_id_fields = (&quot;company&quot;,)

    list_filter = (('JobAdDuration__ad_activated', DateRangeFilter), 'recruitment', 'active', 'deleted', 'position', ('created', DateRangeFilter), 'town')
    search_fields = ('title', 'description', 'company__name', 'id', 'phone_number', 'town')
    list_display = ('title', 'id', 'description', 'active', 'transaction_number', 'company', 'get_position', 'town','created', 'expires', 'views', 'recruitment', 'recruits', 'paid', 'deleted', &quot;start_date&quot;, &quot;end_Date&quot;, &quot;ad_consultant&quot;)


    def start_date(self,obj):
        if JobAdDuration.objects.filter(job=obj.id).exists():
            tempad = JobAdDuration.objects.filter(job=obj).order_by(&quot;-id&quot;)[0]
            return tempad.ad_activated
</code></pre>
<p>EDIT:
In the meantime, I tried to solve it with a simple list filter, but I am unable to get it to work. I would like to place 2 input fields with a calendar(like the default DateRangeFilter) that would represent the start and end time, and then return data based on those values. This is my &quot;prototype&quot; functionality for the simple filter, it works but it returns hard-coded data.</p>
<pre><code>class StartTimeFilter(SimpleListFilter):
    title = ('Start date')
    parameter_name = 'ad_finished'

    def lookups(self, request, model_admin):
       #return JobAdDuration.objects.values_list(&quot;ad_finished&quot;)
       return (
       ('startDate', 'stest1'),
       ('startDate1', 'test2')
       )

    def queryset(self, request, queryset):
        if not self.value():
            return queryset

 
        assigned = JobAdDuration.objects.filter(ad_finished__range=(datetime.now() - timedelta(minutes=45000), datetime.now()))
        allJobs = Job.objects.filter(pk__in=[current.job.id for current in assigned])
        return allJobs

 
</code></pre>
",2919498,1138,05-02-2018 10:14,08-02-2018 19:44,3,1138,28,2,10,69,"{'badge_counts': {'bronze': 28, 'silver': 10, 'gold': 2}, 'account_id': 3489445, 'is_employee': False, 'last_modified_date': 1665191400, 'last_access_date': 1710490628, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1138, 'creation_date': 1382696211, 'user_type': 'registered', 'user_id': 2919498, 'accept_rate': 69, 'website_url': '', 'link': 'https://stackoverflow.com/users/2919498/proxy', 'profile_image': 'https://i.stack.imgur.com/DpkPD.jpg?s=256&g=1', 'display_name': 'Proxy'}","i'm using django 1.10 and I need to display data and create a filter based on a value from a different model(which has a foreign key referencing my model that is used on the admin template) These are my 2 models: This one is used to generate the template: This is the other one that holds a foreign key reference to my first one: Inside my template, I have been able to display the(latest)start and end times And then I just call this inside the list_display and that is working fine. However, i have trouble setting a filter field using these criteria. If I just add it to my list_filter then I get an error that there is no such field inside my model which is true (since that one is in another table that has reference to my job table). So I was wondering what is the right approach to solve this? Do I need to create another function for the filter itself but even then I'm not sure how should I call it inside the list_filter. Here is a snippet of my Django admin page. EDIT: In the meantime, I tried to solve it with a simple list filter, but I am unable to get it to work. I would like to place 2 input fields with a calendar(like the default DateRangeFilter) that would represent the start and end time, and then return data based on those values. This is my &quot;prototype&quot; functionality for the simple filter, it works but it returns hard-coded data.","class Job(models.Model):
    company = models.ForeignKey(Company)
    title = models.CharField(max_length=100, blank=False)
    description = models.TextField(blank=False, default='')
    store = models.CharField(max_length=100, blank=True, default='')
    phone_number = models.CharField(max_length=60, null=True, blank=True)
 class JobAdDuration(models.Model):
    job = models.ForeignKey(Job)
    ad_activated = models.DateTimeField(auto_now_add=True)
    ad_finished = models.DateTimeField(blank=True, null=True)
 def start_date(self,obj):
    if JobAdDuration.objects.filter(job=obj.id).exists():
        tempad = JobAdDuration.objects.filter(job=obj).order_by(&quot;-id&quot;)[0]
        return tempad.ad_activated
 class JobAdmin(admin.OSMGeoAdmin, ImportExportModelAdmin):
    inlines = [
    ]

    readonly_fields = ( 'id', &quot;start_date&quot;, )

    raw_id_fields = (&quot;company&quot;,)

    list_filter = (('JobAdDuration__ad_activated', DateRangeFilter), 'recruitment', 'active', 'deleted', 'position', ('created', DateRangeFilter), 'town')
    search_fields = ('title', 'description', 'company__name', 'id', 'phone_number', 'town')
    list_display = ('title', 'id', 'description', 'active', 'transaction_number', 'company', 'get_position', 'town','created', 'expires', 'views', 'recruitment', 'recruits', 'paid', 'deleted', &quot;start_date&quot;, &quot;end_Date&quot;, &quot;ad_consultant&quot;)


    def start_date(self,obj):
        if JobAdDuration.objects.filter(job=obj.id).exists():
            tempad = JobAdDuration.objects.filter(job=obj).order_by(&quot;-id&quot;)[0]
            return tempad.ad_activated
 class StartTimeFilter(SimpleListFilter):
    title = ('Start date')
    parameter_name = 'ad_finished'

    def lookups(self, request, model_admin):
       #return JobAdDuration.objects.values_list(&quot;ad_finished&quot;)
       return (
       ('startDate', 'stest1'),
       ('startDate1', 'test2')
       )

    def queryset(self, request, queryset):
        if not self.value():
            return queryset

 
        assigned = JobAdDuration.objects.filter(ad_finished__range=(datetime.now() - timedelta(minutes=45000), datetime.now()))
        allJobs = Job.objects.filter(pk__in=[current.job.id for current in assigned])
        return allJobs

 
",47,68,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
72,48370150,48370248,11766,How to implement SMOTE in cross validation and GridSearchCV,2,<python><scikit-learn><pipeline><cross-validation><grid-search>,11,"<p>I'm relatively new to Python. Can you help me improve my implementation of SMOTE to a proper pipeline? What I want is to apply the over and under sampling on the training set of every k-fold iteration so that the model is trained on a balanced data set and evaluated on the imbalanced left out piece. The problem is that when I do that I cannot use the familiar <code>sklearn</code> interface for evaluation and grid search.</p>

<p>Is it possible to make something similar to <code>model_selection.RandomizedSearchCV</code>. My take on this:</p>

<pre><code>df = pd.read_csv(""Imbalanced_data.csv"") #Load the data set
X = df.iloc[:,0:64]
X = X.values
y = df.iloc[:,64]
y = y.values
n_splits = 2
n_measures = 2 #Recall and AUC
kf = StratifiedKFold(n_splits=n_splits) #Stratified because we need balanced samples
kf.get_n_splits(X)
clf_rf = RandomForestClassifier(n_estimators=25, random_state=1)
s =(n_splits,n_measures)
scores = np.zeros(s)
for train_index, test_index in kf.split(X,y):
   print(""TRAIN:"", train_index, ""TEST:"", test_index)
   X_train, X_test = X[train_index], X[test_index]
   y_train, y_test = y[train_index], y[test_index]
   sm = SMOTE(ratio = 'auto',k_neighbors = 5, n_jobs = -1)
   smote_enn = SMOTEENN(smote = sm)
   x_train_res, y_train_res = smote_enn.fit_sample(X_train, y_train)
   clf_rf.fit(x_train_res, y_train_res)
   y_pred = clf_rf.predict(X_test,y_test)
   scores[test_index,1] = recall_score(y_test, y_pred)
   scores[test_index,2] = auc(y_test, y_pred)
</code></pre>
",2623797,164,21-01-2018 18:18,21-01-2018 18:30,0,164,10,1,1,,"{'badge_counts': {'bronze': 10, 'silver': 1, 'gold': 1}, 'account_id': 3099239, 'is_employee': False, 'last_modified_date': 1639479146, 'last_access_date': 1688631224, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 164, 'creation_date': 1374860889, 'user_type': 'registered', 'user_id': 2623797, 'location': 'Bulgaria', 'website_url': '', 'link': 'https://stackoverflow.com/users/2623797/mlearner', 'profile_image': 'https://www.gravatar.com/avatar/7fd6d233a7039d25faaa0a22c65328ce?s=256&d=identicon&r=PG', 'display_name': 'MLearner'}",I'm relatively new to Python. Can you help me improve my implementation of SMOTE to a proper pipeline? What I want is to apply the over and under sampling on the training set of every k-fold iteration so that the model is trained on a balanced data set and evaluated on the imbalanced left out piece. The problem is that when I do that I cannot use the familiar interface for evaluation and grid search. Is it possible to make something similar to . My take on this:,"sklearn model_selection.RandomizedSearchCV df = pd.read_csv(""Imbalanced_data.csv"") #Load the data set
X = df.iloc[:,0:64]
X = X.values
y = df.iloc[:,64]
y = y.values
n_splits = 2
n_measures = 2 #Recall and AUC
kf = StratifiedKFold(n_splits=n_splits) #Stratified because we need balanced samples
kf.get_n_splits(X)
clf_rf = RandomForestClassifier(n_estimators=25, random_state=1)
s =(n_splits,n_measures)
scores = np.zeros(s)
for train_index, test_index in kf.split(X,y):
   print(""TRAIN:"", train_index, ""TEST:"", test_index)
   X_train, X_test = X[train_index], X[test_index]
   y_train, y_test = y[train_index], y[test_index]
   sm = SMOTE(ratio = 'auto',k_neighbors = 5, n_jobs = -1)
   smote_enn = SMOTEENN(smote = sm)
   x_train_res, y_train_res = smote_enn.fit_sample(X_train, y_train)
   clf_rf.fit(x_train_res, y_train_res)
   y_pred = clf_rf.predict(X_test,y_test)
   scores[test_index,1] = recall_score(y_test, y_pred)
   scores[test_index,2] = auc(y_test, y_pred)
",20,28,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
73,49105734,49105844,17112,How to get normalized values from groupby and size,1,<python><pandas><group-by>,14,"<p>I know that we can get normalized values from <code>value_counts()</code> of a pandas series but when we do a group by on a dataframe, the only way to get counts is through <code>size()</code>. Is there any way to get normalized values with size()?</p>

<p>Example:</p>

<pre><code>df = pd.DataFrame({'subset_product':['A','A','A','B','B','C','C'],
                   'subset_close':[1,1,0,1,1,1,0]})
df2 = df.groupby(['subset_product', 'subset_close']).size().reset_index(name='prod_count')

df.subset_product.value_counts()
A    3
B    2
C    2
</code></pre>

<p>df2</p>

<p><a href=""https://i.stack.imgur.com/tEvV6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tEvV6.png"" alt=""enter image description here""></a></p>

<p>Looking to get:</p>

<pre><code>subset_product subset_close prod_count norm
A              0            1          1/3
A              1            2          2/3
B              1            2          2/2
C              1            1          1/2
C              0            1          1/2
</code></pre>

<p>subset_product
Besides manually calculating the normalized values as prod_count/total, is there any way to get normalized values?</p>
",2800939,7835,05-03-2018 07:54,05-03-2018 08:01,0,7865,173,29,93,65,"{'badge_counts': {'bronze': 173, 'silver': 93, 'gold': 29}, 'account_id': 3332303, 'is_employee': False, 'last_modified_date': 1648510200, 'last_access_date': 1668330982, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 30, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 7865, 'creation_date': 1379716635, 'user_type': 'registered', 'user_id': 2800939, 'accept_rate': 65, 'location': 'San Francisco Bay Area', 'website_url': '', 'link': 'https://stackoverflow.com/users/2800939/jxn', 'profile_image': 'https://www.gravatar.com/avatar/f6d580143506504c22826b2b3288e456?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jxn'}","I know that we can get normalized values from of a pandas series but when we do a group by on a dataframe, the only way to get counts is through . Is there any way to get normalized values with size()? Example: df2 Looking to get: subset_product Besides manually calculating the normalized values as prod_count/total, is there any way to get normalized values?","value_counts() size() df = pd.DataFrame({'subset_product':['A','A','A','B','B','C','C'],
                   'subset_close':[1,1,0,1,1,1,0]})
df2 = df.groupby(['subset_product', 'subset_close']).size().reset_index(name='prod_count')

df.subset_product.value_counts()
A    3
B    2
C    2
 subset_product subset_close prod_count norm
A              0            1          1/3
A              1            2          2/3
B              1            2          2/2
C              1            1          1/2
C              0            1          1/2
",10,30,1,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
74,50050617,50050618,17395,Assign Unique Numeric Group IDs to Groups in Pandas,2,<python><pandas><pandas-groupby>,12,"<p>I've consistently run into this issue of having to assign a unique ID to each group in a data set. I've used this when zero padding for RNN's, generating graphs, and many other occasions. </p>

<p>This can usually be done by concatenating the values in each <code>pd.groupby</code> column. However, it is often the case the number of columns that define a group, their dtype, or the value sizes make concatenation an impractical solution that needlessly uses up memory. </p>

<p>I was wondering if there was an easy way to assign a unique numeric ID to groups in pandas. </p>
",1920550,3738,26-04-2018 19:30,26-04-2018 19:30,0,3748,38,2,36,33,"{'badge_counts': {'bronze': 38, 'silver': 36, 'gold': 2}, 'account_id': 2167866, 'is_employee': False, 'last_modified_date': 1694113790, 'last_access_date': 1709872096, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3748, 'creation_date': 1356061546, 'user_type': 'registered', 'user_id': 1920550, 'accept_rate': 33, 'location': 'Lima, Peru', 'link': 'https://stackoverflow.com/users/1920550/seeiespi', 'profile_image': 'https://i.stack.imgur.com/MVkL3.jpg?s=256&g=1', 'display_name': 'seeiespi'}","I've consistently run into this issue of having to assign a unique ID to each group in a data set. I've used this when zero padding for RNN's, generating graphs, and many other occasions. This can usually be done by concatenating the values in each column. However, it is often the case the number of columns that define a group, their dtype, or the value sizes make concatenation an impractical solution that needlessly uses up memory. I was wondering if there was an easy way to assign a unique numeric ID to groups in pandas.",pd.groupby,-1,5,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
75,48715330,48716568,15256,x axis gets transformed to floats,3,<python><pandas><matplotlib>,14,"<p>I am trying to plot my data grouped by year, and for each year, i want to count the number of users. Below, i just transformed the date column from float to integer.</p>

<p>This is my plot<a href=""https://i.stack.imgur.com/Md0nn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Md0nn.png"" alt=""enter image description here""></a></p>

<p>If you see the x-axis, my year ticker seems to have become a float and the each ticker is 0.5 tick apart. </p>

<p>How do i make this purely an integer?</p>

<hr>

<p>Changing the groupby has the same result:
<a href=""https://i.stack.imgur.com/mr3B3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mr3B3.png"" alt=""enter image description here""></a></p>

<hr>

<p>ticks are still 2 spaces apart after converting the year column to a string format</p>

<pre><code>df['year'] = df['year'].astype(str)
</code></pre>

<p>:
<a href=""https://i.stack.imgur.com/zjXv1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zjXv1.png"" alt=""enter image description here""></a></p>
",2800939,7835,09-02-2018 22:57,10-02-2018 02:13,1,7865,173,29,93,65,"{'badge_counts': {'bronze': 173, 'silver': 93, 'gold': 29}, 'account_id': 3332303, 'is_employee': False, 'last_modified_date': 1648510200, 'last_access_date': 1668330982, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 30, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 7865, 'creation_date': 1379716635, 'user_type': 'registered', 'user_id': 2800939, 'accept_rate': 65, 'location': 'San Francisco Bay Area', 'website_url': '', 'link': 'https://stackoverflow.com/users/2800939/jxn', 'profile_image': 'https://www.gravatar.com/avatar/f6d580143506504c22826b2b3288e456?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jxn'}","I am trying to plot my data grouped by year, and for each year, i want to count the number of users. Below, i just transformed the date column from float to integer. This is my plot If you see the x-axis, my year ticker seems to have become a float and the each ticker is 0.5 tick apart. How do i make this purely an integer? Changing the groupby has the same result: ticks are still 2 spaces apart after converting the year column to a string format :","df['year'] = df['year'].astype(str)
",0,22,3,3,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
76,48160728,54612520,70092,ResourceWarning unclosed socket in Python 3 Unit Test,2,<python><python-3.x><sockets><python-unittest>,78,"<p>I'm modifying some code to be compatible between <code>Python 2</code> and <code>Python 3</code>, but have observed a warning in unit test output.</p>

<pre><code>/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/case.py:601:
    ResourceWarning: unclosed socket.socket fd=4,
    family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6,
    laddr=('1.1.2.3', 65087), raddr=('5.8.13.21', 8080)
</code></pre>

<p>A little research determined this was also happening from popular libraries like <a href=""https://github.com/requests/requests/issues/3912"" rel=""noreferrer"">requests</a> and <a href=""https://github.com/boto/boto3/issues/454"" rel=""noreferrer"">boto3</a>.</p>

<p>I could ignore the warning or <a href=""https://stackoverflow.com/questions/14938716/socket-resourcewarning-using-urllib-in-python-3"">filter it</a> completely.  If was my service, I could set the <code>connection: close</code> header in my response (<a href=""https://stackoverflow.com/questions/14938716/socket-resourcewarning-using-urllib-in-python-3"">link</a>).</p>

<p>Here's an example that exhibits the warning in <code>Python 3.6.1</code>:</p>

<p><strong>app.py</strong></p>

<pre><code>import requests

class Service(object):
    def __init__(self):
        self.session = requests.Session()

    def get_info(self):
        uri = 'http://api.stackexchange.com/2.2/info?site=stackoverflow'
        response = self.session.get(uri)
        if response.status_code == 200:
            return response.json()
        else:
            response.raise_for_status()

    def __del__(self):
        self.session.close()

if __name__ == '__main__':
    service = Service()
    print(service.get_info())
</code></pre>

<p><strong>test.py</strong></p>

<pre><code>import unittest

class TestService(unittest.TestCase):
    def test_growing(self):
        import app
        service = app.Service()
        res = service.get_info()
        self.assertTrue(res['items'][0]['new_active_users'] &gt; 1)


if __name__ == '__main__':
    unittest.main()
</code></pre>

<p>Is there a better / correct way to manage the session so that it gets explicitly closed and not rely on <code>__del__()</code> to result in this sort of warning.</p>

<p>Thanks for any help.</p>
",2233231,2232,09-01-2018 02:38,10-02-2019 01:26,397,2242,22,3,18,,"{'badge_counts': {'bronze': 22, 'silver': 18, 'gold': 3}, 'account_id': 2575416, 'is_employee': False, 'last_modified_date': 1686567300, 'last_access_date': 1702010817, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2242, 'creation_date': 1364841091, 'user_type': 'registered', 'user_id': 2233231, 'location': 'San Francisco Bay Area, CA, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/2233231/j12y', 'profile_image': 'https://www.gravatar.com/avatar/8618bb8fbdb432cd2195498fd55c3c88?s=256&d=identicon&r=PG', 'display_name': 'j12y'}","I'm modifying some code to be compatible between and , but have observed a warning in unit test output. A little research determined this was also happening from popular libraries like requests and boto3. I could ignore the warning or filter it completely. If was my service, I could set the header in my response (link). Here's an example that exhibits the warning in : app.py test.py Is there a better / correct way to manage the session so that it gets explicitly closed and not rely on to result in this sort of warning. Thanks for any help.","Python 2 Python 3 /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/case.py:601:
    ResourceWarning: unclosed socket.socket fd=4,
    family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6,
    laddr=('1.1.2.3', 65087), raddr=('5.8.13.21', 8080)
 connection: close Python 3.6.1 import requests

class Service(object):
    def __init__(self):
        self.session = requests.Session()

    def get_info(self):
        uri = 'http://api.stackexchange.com/2.2/info?site=stackoverflow'
        response = self.session.get(uri)
        if response.status_code == 200:
            return response.json()
        else:
            response.raise_for_status()

    def __del__(self):
        self.session.close()

if __name__ == '__main__':
    service = Service()
    print(service.get_info())
 import unittest

class TestService(unittest.TestCase):
    def test_growing(self):
        import app
        service = app.Service()
        res = service.get_info()
        self.assertTrue(res['items'][0]['new_active_users'] &gt; 1)


if __name__ == '__main__':
    unittest.main()
 __del__()",28,57,0,4,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
77,49360480,51729798,43861,Python type hinting for async function as function argument,2,<python><pycharm><python-asyncio><type-hinting>,44,"<p>I am trying to make sure a function parameter is an async function.
So I am playing around with the following code:</p>
<pre><code>async def test(*args, **kwargs):
    pass

def consumer(function_: Optional[Coroutine[Any, Any, Any]]=None):
    func = function_

consumer(test)
</code></pre>
<p>But it doesn't work.</p>
<p>I am presented with the following error during type checking in PyCharm:</p>
<pre class=""lang-none prettyprint-override""><code>Expected type 'Optional[Coroutine]', got '(args: Tuple[Any, ...], kwargs: Dict[str, Any]) -&gt; Coroutine[Any, Any, None]' instead
</code></pre>
<p>Can anyone give me some hints how to solve this?</p>
",1629704,981,19-03-2018 10:18,07-08-2018 14:55,141,981,22,1,12,29,"{'badge_counts': {'bronze': 22, 'silver': 12, 'gold': 1}, 'account_id': 1788874, 'is_employee': False, 'last_modified_date': 1711159800, 'last_access_date': 1710585501, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 981, 'creation_date': 1346143420, 'user_type': 'registered', 'user_id': 1629704, 'accept_rate': 29, 'location': 'Nijmegen, Netherlands', 'website_url': '', 'link': 'https://stackoverflow.com/users/1629704/sanders', 'profile_image': 'https://www.gravatar.com/avatar/d45beb10714cee629bdc36ac089d3e6b?s=256&d=identicon&r=PG', 'display_name': 'sanders'}",I am trying to make sure a function parameter is an async function. So I am playing around with the following code: But it doesn't work. I am presented with the following error during type checking in PyCharm: Can anyone give me some hints how to solve this?,"async def test(*args, **kwargs):
    pass

def consumer(function_: Optional[Coroutine[Any, Any, Any]]=None):
    func = function_

consumer(test)
 Expected type 'Optional[Coroutine]', got '(args: Tuple[Any, ...], kwargs: Dict[str, Any]) -&gt; Coroutine[Any, Any, None]' instead
",6,15,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
78,49997987,49998052,102547,Where is the .profile file on mac?,5,<python><macos><homebrew><macos-sierra>,24,"<p>I have a MacBook Pro 13` inch (without touch bar) and it is running mac os sierra. I have installed Xcode, command line tools and homebrew. Now I want to install python 3 and make it a default interpreter when calling it from the terminal. I have found this guide <a href=""http://docs.python-guide.org/en/latest/starting/install3/osx/"" rel=""noreferrer"">http://docs.python-guide.org/en/latest/starting/install3/osx/</a> and I get confused when they say I should add the path to the ~./profile but I don't know where to find the file. Can somebody help with step by step guide on how to create it if it not there?</p>

<p>Thanks</p>
",1651720,394,24-04-2018 09:24,24-04-2018 09:28,0,394,12,2,3,,"{'badge_counts': {'bronze': 12, 'silver': 3, 'gold': 2}, 'collectives': [{'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 1816779, 'is_employee': False, 'last_modified_date': 1699668300, 'last_access_date': 1711110165, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 394, 'creation_date': 1346928401, 'user_type': 'registered', 'user_id': 1651720, 'location': 'South Africa', 'website_url': 'http://tekstak.com', 'link': 'https://stackoverflow.com/users/1651720/andile', 'profile_image': 'https://i.stack.imgur.com/BEWLk.jpg?s=256&g=1', 'display_name': 'Andile'}","I have a MacBook Pro 13` inch (without touch bar) and it is running mac os sierra. I have installed Xcode, command line tools and homebrew. Now I want to install python 3 and make it a default interpreter when calling it from the terminal. I have found this guide http://docs.python-guide.org/en/latest/starting/install3/osx/ and I get confused when they say I should add the path to the ~./profile but I don't know where to find the file. Can somebody help with step by step guide on how to create it if it not there? Thanks",,0,3,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
79,48288076,48300305,20894,Convert django RawQuerySet to Queryset,2,<python><django><postgresql><django-models><django-managers>,12,"<p>I have 2 Django models, <code>ModelA</code> with an <code>ArrayField</code> that is used to store a large list of primary key values (possibly 50k+ list)</p>

<pre><code>class ModelA(models.Model):
    pk_values = ArrayField(models.IntegerField())

class CustomManager(manager.Manager):

    def get_for_index(self, index_id):
        qs = self.get_queryset()
        obj = ModelA.objects.get(pk=index_id)
        return qs.filter(id__in=obj.pk_values)

class ModelB(models.Model):
    # [...] some fields

    objects = CustomManager()
</code></pre>

<p>This works:</p>

<pre><code>qs = ModelB.objects.get_for_index(index_id=1)
</code></pre>

<p>However, this would be super slow where ""pk_values"" is a large list.</p>

<p>So I tried doing raw SQL queries:</p>

<pre><code>class CustomManager(manager.Manager):
    def get_for_index(self, index_id):
        qs = self.get_queryset()
        sql = ""SELECT * FROM myapp_model_b JOIN myapp_model_a ON myapp_model_b.id = ANY(myapp_model_a.pk_values) WHERE myapp_model_a.id = '%s'"" % index_id
        return qs.raw(sql)
</code></pre>

<p>But this returns a <code>django.db.models.query.RawQuerySet</code> instance.</p>

<p>But with this, I cant do things like <code>queryset.values()</code> afterwards.</p>

<p>How can I convert this to a normal Django queryset? </p>

<p>Is there a better way of doing this?</p>

<p>Docs:</p>

<ul>
<li>ArrayField <a href=""https://docs.djangoproject.com/en/2.0/ref/contrib/postgres/fields/#arrayfield"" rel=""noreferrer"">https://docs.djangoproject.com/en/2.0/ref/contrib/postgres/fields/#arrayfield</a></li>
<li>Custom Manager <a href=""https://docs.djangoproject.com/en/2.0/topics/db/managers/#custom-managers-and-model-inheritance"" rel=""noreferrer"">https://docs.djangoproject.com/en/2.0/topics/db/managers/#custom-managers-and-model-inheritance</a></li>
<li>Raw queries <a href=""https://docs.djangoproject.com/en/2.0/topics/db/sql/#performing-raw-sql-queries"" rel=""noreferrer"">https://docs.djangoproject.com/en/2.0/topics/db/sql/#performing-raw-sql-queries</a></li>
</ul>
",1682844,5567,16-01-2018 18:58,17-01-2018 11:41,1,5567,35,5,26,40,"{'badge_counts': {'bronze': 35, 'silver': 26, 'gold': 5}, 'account_id': 1857582, 'is_employee': False, 'last_modified_date': 1706090584, 'last_access_date': 1711037676, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5567, 'creation_date': 1348052434, 'user_type': 'registered', 'user_id': 1682844, 'accept_rate': 40, 'location': 'London, United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/1682844/mishbah', 'profile_image': 'https://i.stack.imgur.com/NMzLK.jpg?s=256&g=1', 'display_name': 'mishbah'}","I have 2 Django models, with an that is used to store a large list of primary key values (possibly 50k+ list) This works: However, this would be super slow where ""pk_values"" is a large list. So I tried doing raw SQL queries: But this returns a instance. But with this, I cant do things like afterwards. How can I convert this to a normal Django queryset? Is there a better way of doing this? Docs: ArrayField https://docs.djangoproject.com/en/2.0/ref/contrib/postgres/fields/#arrayfield Custom Manager https://docs.djangoproject.com/en/2.0/topics/db/managers/#custom-managers-and-model-inheritance Raw queries https://docs.djangoproject.com/en/2.0/topics/db/sql/#performing-raw-sql-queries","ModelA ArrayField class ModelA(models.Model):
    pk_values = ArrayField(models.IntegerField())

class CustomManager(manager.Manager):

    def get_for_index(self, index_id):
        qs = self.get_queryset()
        obj = ModelA.objects.get(pk=index_id)
        return qs.filter(id__in=obj.pk_values)

class ModelB(models.Model):
    # [...] some fields

    objects = CustomManager()
 qs = ModelB.objects.get_for_index(index_id=1)
 class CustomManager(manager.Manager):
    def get_for_index(self, index_id):
        qs = self.get_queryset()
        sql = ""SELECT * FROM myapp_model_b JOIN myapp_model_a ON myapp_model_b.id = ANY(myapp_model_a.pk_values) WHERE myapp_model_a.id = '%s'"" % index_id
        return qs.raw(sql)
 django.db.models.query.RawQuerySet queryset.values()",13,49,0,3,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
80,50146520,50146630,76726,"""No module named 'urlparse'"" but I'm not using urlparse",3,<python><python-3.x><urlparse><flask-oauthlib>,35,"<p>I'm trying to figure out why I am seeing an error</p>
<blockquote>
<p>ModuleNotFoundError: No module named 'urlparse'</p>
</blockquote>
<p>but I never call urlparse in my code.</p>
<p>When I try to install <code>urlparse</code> with pip, I am seeing that this module doesn't exist. When I try to install <code>urllib.parse</code> with pip I see the same message:</p>
<blockquote>
<p>No matching distribution found for urllib.parse</p>
</blockquote>
<p>What am I missing here?</p>
<pre><code>from flask import Flask, request, redirect, url_for, session, g, flash, \
render_template
from flask_oauth import OAuth

from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import scoped_session, sessionmaker
from sqlalchemy.ext.declarative import declarative_base

# configuration
SECRET_KEY = 'development key'
DEBUG = True

# setup flask
app = Flask(__name__)
app.debug = DEBUG
app.secret_key = SECRET_KEY
oauth = OAuth()

# Use Twitter as example remote app
twitter = oauth.remote_app('twitter',
   base_url='https://api.twitter.com/1/',
   request_token_url='https://api.twitter.com/oauth/request_token',
   access_token_url='https://api.twitter.com/oauth/access_token',
   authorize_url='https://api.twitter.com/oauth/authenticate',
   consumer_key='',
   consumer_secret=''
)


@twitter.tokengetter
def get_twitter_token(token=None):
    return session.get('twitter_token')


@app.route('/')
def index():
    access_token = session.get('access_token')
    if access_token is None:
        return redirect(url_for('login'))
    access_token = access_token[0]
    return render_template('templates/index.html')
</code></pre>

<pre><code>if __name__ == '__main__':
    app.run()
</code></pre>
",2480769,767,03-05-2018 03:31,03-05-2018 03:47,0,767,15,1,6,30,"{'badge_counts': {'bronze': 15, 'silver': 6, 'gold': 1}, 'account_id': 2892258, 'is_employee': False, 'last_modified_date': 1573681271, 'last_access_date': 1621538147, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 767, 'creation_date': 1371096631, 'user_type': 'registered', 'user_id': 2480769, 'accept_rate': 30, 'website_url': '', 'link': 'https://stackoverflow.com/users/2480769/mnickey', 'profile_image': 'https://www.gravatar.com/avatar/e18498c1a173797a970ab5cf1770df4b?s=256&d=identicon&r=PG', 'display_name': 'mnickey'}","I'm trying to figure out why I am seeing an error ModuleNotFoundError: No module named 'urlparse' but I never call urlparse in my code. When I try to install with pip, I am seeing that this module doesn't exist. When I try to install with pip I see the same message: No matching distribution found for urllib.parse What am I missing here?","urlparse urllib.parse from flask import Flask, request, redirect, url_for, session, g, flash, \
render_template
from flask_oauth import OAuth

from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import scoped_session, sessionmaker
from sqlalchemy.ext.declarative import declarative_base

# configuration
SECRET_KEY = 'development key'
DEBUG = True

# setup flask
app = Flask(__name__)
app.debug = DEBUG
app.secret_key = SECRET_KEY
oauth = OAuth()

# Use Twitter as example remote app
twitter = oauth.remote_app('twitter',
   base_url='https://api.twitter.com/1/',
   request_token_url='https://api.twitter.com/oauth/request_token',
   access_token_url='https://api.twitter.com/oauth/access_token',
   authorize_url='https://api.twitter.com/oauth/authenticate',
   consumer_key='',
   consumer_secret=''
)


@twitter.tokengetter
def get_twitter_token(token=None):
    return session.get('twitter_token')


@app.route('/')
def index():
    access_token = session.get('access_token')
    if access_token is None:
        return redirect(url_for('login'))
    access_token = access_token[0]
    return render_template('templates/index.html')
 if __name__ == '__main__':
    app.run()
",39,56,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
81,49581104,49598597,6372,sklearn GridSearchCV not using sample_weight in score function,3,<python><machine-learning><scikit-learn>,23,"<p>I have data with differing weights for each sample. In my application, it is important that these weights are accounted for in estimating the model and comparing alternative models.</p>

<p>I'm using <code>sklearn</code> to estimate models and to compare alternative hyperparameter choices. But this unit test shows that <code>GridSearchCV</code> does not apply <code>sample_weights</code> to estimate scores.</p>

<p>Is there a way to have <code>sklearn</code> use <code>sample_weight</code> to score the models?</p>

<p>Unit test:</p>

<pre><code>from __future__ import division

import numpy as np
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import GridSearchCV, RepeatedKFold


def grid_cv(X_in, y_in, w_in, cv, max_features_grid, use_weighting):
  out_results = dict()

  for k in max_features_grid:
    clf = RandomForestClassifier(n_estimators=256,
                                 criterion=""entropy"",
                                 warm_start=False,
                                 n_jobs=-1,
                                 random_state=RANDOM_STATE,
                                 max_features=k)
    for train_ndx, test_ndx in cv.split(X=X_in, y=y_in):
      X_train = X_in[train_ndx, :]
      y_train = y_in[train_ndx]
      w_train = w_in[train_ndx]
      y_test = y[test_ndx]

      clf.fit(X=X_train, y=y_train, sample_weight=w_train)

      y_hat = clf.predict_proba(X=X_in[test_ndx, :])
      if use_weighting:
        w_test = w_in[test_ndx]
        w_i_sum = w_test.sum()
        score = w_i_sum / w_in.sum() * log_loss(y_true=y_test, y_pred=y_hat, sample_weight=w_test)
      else:
        score = log_loss(y_true=y_test, y_pred=y_hat)

      results = out_results.get(k, [])
      results.append(score)
      out_results.update({k: results})

  for k, v in out_results.items():
    if use_weighting:
      mean_score = sum(v)
    else:
      mean_score = np.mean(v)
    out_results.update({k: mean_score})

  best_score = min(out_results.values())
  best_param = min(out_results, key=out_results.get)
  return best_score, best_param


if __name__ == ""__main__"":
  RANDOM_STATE = 1337
  X, y = load_iris(return_X_y=True)
  sample_weight = np.array([1 + 100 * (i % 25) for i in range(len(X))])
  # sample_weight = np.array([1 for _ in range(len(X))])

  inner_cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=RANDOM_STATE)

  outer_cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=RANDOM_STATE)

  rfc = RandomForestClassifier(n_estimators=256,
                               criterion=""entropy"",
                               warm_start=False,
                               n_jobs=-1,
                               random_state=RANDOM_STATE)
  search_params = {""max_features"": [1, 2, 3, 4]}


  fit_params = {""sample_weight"": sample_weight}
  my_scorer = make_scorer(log_loss, 
               greater_is_better=False, 
               needs_proba=True, 
               needs_threshold=False)

  grid_clf = GridSearchCV(estimator=rfc,
                          scoring=my_scorer,
                          cv=inner_cv,
                          param_grid=search_params,
                          refit=True,
                          return_train_score=False,
                          iid=False)  # in this usage, the results are the same for `iid=True` and `iid=False`
  grid_clf.fit(X, y, **fit_params)
  print(""This is the best out-of-sample score using GridSearchCV: %.6f."" % -grid_clf.best_score_)

  msg = """"""This is the best out-of-sample score %s weighting using grid_cv: %.6f.""""""
  score_with_weights, param_with_weights = grid_cv(X_in=X,
                                                   y_in=y,
                                                   w_in=sample_weight,
                                                   cv=inner_cv,
                                                   max_features_grid=search_params.get(
                                                     ""max_features""),
                                                   use_weighting=True)
  print(msg % (""WITH"", score_with_weights))

  score_without_weights, param_without_weights = grid_cv(X_in=X,
                                                         y_in=y,
                                                         w_in=sample_weight,
                                                         cv=inner_cv,
                                                         max_features_grid=search_params.get(
                                                           ""max_features""),
                                                         use_weighting=False)
  print(msg % (""WITHOUT"", score_without_weights))
</code></pre>

<p>Which produces output:</p>

<pre><code>This is the best out-of-sample score using GridSearchCV: 0.135692.
This is the best out-of-sample score WITH weighting using grid_cv: 0.099367.
This is the best out-of-sample score WITHOUT weighting using grid_cv: 0.135692.
</code></pre>

<p>Explanation: Since manually computing the loss without weighting produces the same scoring as <code>GridSearchCV</code>, we know that the sample weights are not being used.</p>
",2482661,1316,30-03-2018 20:35,01-04-2018 13:31,2,1326,26,0,13,60,"{'badge_counts': {'bronze': 26, 'silver': 13, 'gold': 0}, 'account_id': 2523286, 'is_employee': False, 'last_modified_date': 1702128000, 'last_access_date': 1711096513, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1326, 'creation_date': 1371133958, 'user_type': 'registered', 'user_id': 2482661, 'accept_rate': 60, 'location': 'Washington, DC, United States', 'link': 'https://stackoverflow.com/users/2482661/sycorax', 'profile_image': 'https://i.stack.imgur.com/FRsah.jpg?s=256&g=1', 'display_name': 'Sycorax'}","I have data with differing weights for each sample. In my application, it is important that these weights are accounted for in estimating the model and comparing alternative models. I'm using to estimate models and to compare alternative hyperparameter choices. But this unit test shows that does not apply to estimate scores. Is there a way to have use to score the models? Unit test: Which produces output: Explanation: Since manually computing the loss without weighting produces the same scoring as , we know that the sample weights are not being used.","sklearn GridSearchCV sample_weights sklearn sample_weight from __future__ import division

import numpy as np
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import GridSearchCV, RepeatedKFold


def grid_cv(X_in, y_in, w_in, cv, max_features_grid, use_weighting):
  out_results = dict()

  for k in max_features_grid:
    clf = RandomForestClassifier(n_estimators=256,
                                 criterion=""entropy"",
                                 warm_start=False,
                                 n_jobs=-1,
                                 random_state=RANDOM_STATE,
                                 max_features=k)
    for train_ndx, test_ndx in cv.split(X=X_in, y=y_in):
      X_train = X_in[train_ndx, :]
      y_train = y_in[train_ndx]
      w_train = w_in[train_ndx]
      y_test = y[test_ndx]

      clf.fit(X=X_train, y=y_train, sample_weight=w_train)

      y_hat = clf.predict_proba(X=X_in[test_ndx, :])
      if use_weighting:
        w_test = w_in[test_ndx]
        w_i_sum = w_test.sum()
        score = w_i_sum / w_in.sum() * log_loss(y_true=y_test, y_pred=y_hat, sample_weight=w_test)
      else:
        score = log_loss(y_true=y_test, y_pred=y_hat)

      results = out_results.get(k, [])
      results.append(score)
      out_results.update({k: results})

  for k, v in out_results.items():
    if use_weighting:
      mean_score = sum(v)
    else:
      mean_score = np.mean(v)
    out_results.update({k: mean_score})

  best_score = min(out_results.values())
  best_param = min(out_results, key=out_results.get)
  return best_score, best_param


if __name__ == ""__main__"":
  RANDOM_STATE = 1337
  X, y = load_iris(return_X_y=True)
  sample_weight = np.array([1 + 100 * (i % 25) for i in range(len(X))])
  # sample_weight = np.array([1 for _ in range(len(X))])

  inner_cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=RANDOM_STATE)

  outer_cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=RANDOM_STATE)

  rfc = RandomForestClassifier(n_estimators=256,
                               criterion=""entropy"",
                               warm_start=False,
                               n_jobs=-1,
                               random_state=RANDOM_STATE)
  search_params = {""max_features"": [1, 2, 3, 4]}


  fit_params = {""sample_weight"": sample_weight}
  my_scorer = make_scorer(log_loss, 
               greater_is_better=False, 
               needs_proba=True, 
               needs_threshold=False)

  grid_clf = GridSearchCV(estimator=rfc,
                          scoring=my_scorer,
                          cv=inner_cv,
                          param_grid=search_params,
                          refit=True,
                          return_train_score=False,
                          iid=False)  # in this usage, the results are the same for `iid=True` and `iid=False`
  grid_clf.fit(X, y, **fit_params)
  print(""This is the best out-of-sample score using GridSearchCV: %.6f."" % -grid_clf.best_score_)

  msg = """"""This is the best out-of-sample score %s weighting using grid_cv: %.6f.""""""
  score_with_weights, param_with_weights = grid_cv(X_in=X,
                                                   y_in=y,
                                                   w_in=sample_weight,
                                                   cv=inner_cv,
                                                   max_features_grid=search_params.get(
                                                     ""max_features""),
                                                   use_weighting=True)
  print(msg % (""WITH"", score_with_weights))

  score_without_weights, param_without_weights = grid_cv(X_in=X,
                                                         y_in=y,
                                                         w_in=sample_weight,
                                                         cv=inner_cv,
                                                         max_features_grid=search_params.get(
                                                           ""max_features""),
                                                         use_weighting=False)
  print(msg % (""WITHOUT"", score_without_weights))
 This is the best out-of-sample score using GridSearchCV: 0.135692.
This is the best out-of-sample score WITH weighting using grid_cv: 0.099367.
This is the best out-of-sample score WITHOUT weighting using grid_cv: 0.135692.
 GridSearchCV",98,121,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
82,49456385,49456604,32200,Running Flask from IPython raises SystemExit,4,<python><flask><ipython><jupyter>,14,"<p>I am trying to run my Flask application from IPython. However, it fails with a <code>SystemExit</code> error.</p>

<pre><code>from flask import Flask

app = Flask(__name__)

@app.route('/')
def index():
    return 'Hello, World!'

if __name__ == '__main__':
   app.run(debug=True)
</code></pre>

<p>Running this with IPython shows the following error:</p>

<pre><code>SystemExit                                Traceback (most recent call last)
&lt;ipython-input-35-bfd7690b11d8&gt; in &lt;module&gt;()
     17 
     18 if __name__ == '__main__':
---&gt; 19    app.run(debug = True)

/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/flask/app.pyc in run(self, host, port, debug, **options)
    770         options.setdefault('use_debugger', self.debug)
    771         try:
--&gt; 772             run_simple(host, port, self, **options)
    773         finally:
    774             # reset the first request information if the development server

/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/werkzeug/serving.py in run_simple(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)
    687         from ._reloader import run_with_reloader
    688         run_with_reloader(inner, extra_files, reloader_interval,
--&gt; 689                           reloader_type)
    690     else:
    691         inner()

/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/werkzeug/_reloader.py in run_with_reloader(main_func, extra_files, interval, reloader_type)
    248             reloader.run()
    249         else:
--&gt; 250             sys.exit(reloader.restart_with_reloader())
    251     except KeyboardInterrupt:
    252         pass

SystemExit: 1
</code></pre>
",2906657,531,23-03-2018 18:38,23-03-2018 18:52,0,531,18,2,4,19,"{'badge_counts': {'bronze': 18, 'silver': 4, 'gold': 2}, 'account_id': 3472691, 'is_employee': False, 'last_modified_date': 1573680964, 'last_access_date': 1536395420, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 531, 'creation_date': 1382437604, 'user_type': 'registered', 'user_id': 2906657, 'accept_rate': 19, 'link': 'https://stackoverflow.com/users/2906657/user2906657', 'profile_image': 'https://www.gravatar.com/avatar/42440a92fc560898e2d7de87a0a7ea75?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user2906657'}","I am trying to run my Flask application from IPython. However, it fails with a error. Running this with IPython shows the following error:","SystemExit from flask import Flask

app = Flask(__name__)

@app.route('/')
def index():
    return 'Hello, World!'

if __name__ == '__main__':
   app.run(debug=True)
 SystemExit                                Traceback (most recent call last)
&lt;ipython-input-35-bfd7690b11d8&gt; in &lt;module&gt;()
     17 
     18 if __name__ == '__main__':
---&gt; 19    app.run(debug = True)

/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/flask/app.pyc in run(self, host, port, debug, **options)
    770         options.setdefault('use_debugger', self.debug)
    771         try:
--&gt; 772             run_simple(host, port, self, **options)
    773         finally:
    774             # reset the first request information if the development server

/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/werkzeug/serving.py in run_simple(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)
    687         from ._reloader import run_with_reloader
    688         run_with_reloader(inner, extra_files, reloader_interval,
--&gt; 689                           reloader_type)
    690     else:
    691         inner()

/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/werkzeug/_reloader.py in run_with_reloader(main_func, extra_files, interval, reloader_type)
    248             reloader.run()
    249         else:
--&gt; 250             sys.exit(reloader.restart_with_reloader())
    251     except KeyboardInterrupt:
    252         pass

SystemExit: 1
",35,45,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
83,48519337,48519443,25224,How to freeze the top row and the first column using XlsxWriter?,2,<python><excel><pandas><dataframe><xlsxwriter>,20,"<p>I am exporting a pandas <code>DataFrame</code> to Excel, and since it contains a lot of rows and columns, it would be useful to keep the top row and the first column when browsing its contents.</p>

<p>There is a feature present in Excel that allows for freezing the top row and the first column. Is accessible through <code>XlsxWriter</code> when <a href=""http://xlsxwriter.readthedocs.io/working_with_pandas.html"" rel=""noreferrer"">exporting DataFrames to excel</a>?</p>
",2569042,6629,30-01-2018 10:37,30-01-2018 10:42,0,6679,64,8,44,78,"{'badge_counts': {'bronze': 64, 'silver': 44, 'gold': 8}, 'collectives': [{'collective': {'tags': ['amazon-macie', 'amazon-managed-blockchain', 'amazon-kinesis-video-streams', 'aws-sdk-net', 'aws-sdk-cpp', 'aws-sdk-nodejs', 'aws-vpn', 'amazon-workmail', 'aws-codecommit', 'aws-sdk-js-v3', 'amazon-rekognition', 'aws-serverless', 'aws-iot-sitewise', 'amazon-connect', 'amazon-workspaces', 'amazon-efs', 'amazon-elastic-beanstalk', 'aws-glue', 'aws-datasync', 'aws-xray', 'aws-sdk-ios', 'amazon-sumerian', 'amazon-kinesis-firehose', 'aws-acm', 'amazon-qldb', 'aws-iot-analytics', 'aws-codestar', 'amazon-ses', 'amazon-opensearch', 'amazon-kendra', 'aws-iot', 'aws-storage-gateway', 'aws-sdk-rust', 'aws-sdk-comprehend', 'alexa-account-linking', 'aws-elemental', 'aws-ssm', 'aws-sam-cli', 'amazon-ebs', 'amazon-timestream', 'aws-auto-scaling', 'aws-certificate-manager', 'alexa-interaction-model', 'aws-mediastore', 'aws-cli', 'amazon-location-service', 'aws-amplify', 'aws-copilot-cli', 'alexa-presentation-language', 'amazon-sagemaker', 'amazon-kinesis', 'aws-mobilehub', 'amazon-ecs', 'amazon-swf', 'aws-media-convert', 'aws-codeguru', 'aws-mediaconnect', 'aws-media-live', 'amazon-forecast', 'aws-codepipeline', 'aws-sdk-ruby', 'amazon-sqs', 'aws-cdk', 'amazon-athena', 'aws-direct-connect', 'aws-batch', 'amazon-lex', 'amazon-sns', 'amazon-ec2-spot-market', 'amazon-eks', 'aws-sam', 'aws-msk', 'aws-sso', 'aws-sdk-go', 'aws-snowball', 'alexa-smart-home-skill', 'aws-transfer-family', 'amazon-textract', 'amazon-inspector', 'aws-reserved-instances', 'aws-resource-group', 'aws-app-config', 'aws-cloudmap', 'amazon-ivs', 'aws-iot-core', 'aws-deeplens', 'amazon-web-services', 'amazon-kinesis-analytics', 'aws-application-load-balancer', 'amazon-guardduty', 'amazon-kms', 'aws-iam-identity-center', 'amazon-imagebuilder', 'aws-iot-greengrass', 'aws-device-farm', 'amazon-elastic-transcoder', 'amazon-rds', 'amazon-cloudwatchlogs', 'aws-fis', 'aws-global-accelerator', 'amazon-transcribe', 'aws-sdk-java-2.0', 'amazon-route53', 'aws-elb', 'amazon-cloudfront', 'amazon-cloudtrail', 'aws-mediatailor', 'amazon-redshift-spectrum', 'alexa-sdk-nodejs', 'alexa-sdk-python', 'amazon-keyspaces', 'aws-codebuild', 'aws-codecatalyst', 'aws-cloudshell', 'aws-nlb', 'aws-billing', 'aws-directory-services', 'amazon-quicksight', 'aws-appstream', 'aws-pinpoint', 'amazon-gamelift', 'amazon-s3', 'amazon-sagemaker-compilers', 'amazon-cloudwatch', 'alexa-skills-kit', 'aws-dms', 'aws-data-exchange', 'amazon-elasticsearch', 'aws-sct', 'aws-lambda-powertools', 'aws-event-bridge', 'aws-app-mesh', 'amazon-simpledb', 'alexa-smapi', 'amazon-dynamodb-dax', 'aws-iot-events', 'aws-appsync', 'aws-lambda-edge', 'amazon-cloudsearch', 'aws-control-tower', 'amazon-ecr', 'amazon-elasticache', 'amazon-workdocs', 'aws-sdk-go-v2', 'amazon-aurora', 'amazon-memory-db', 'amazon-lightsail', 'aws-step-functions', 'aws-sdk-java', 'aws-opsworks', 'aws-api-gateway', 'amazon-emr', 'amazon-cloudhsm', 'aws-sdk', 'aws-code-deploy', 'aws-lambda', 'amazon-redshift', 'elastic-ip', 'aws-elastictranscoder', 'amazon-fsx', 'amazon-iam', 'aws-codeartifact', 'aws-sdk-js', 'amazon-translate', 'aws-graviton', 'aws-security-hub', 'alexa-flash-briefing-skill', 'aws-private-link', 'aws-cloud9', 'amazon-waf', 'amazon-data-pipeline', 'aws-sdk-android', 'amazon-personalize', 'amazon-polly', 'aws-databrew', 'aws-secrets-manager', 'aws-backup', 'amazon-cognito', 'amazon-dynamodb', 'amazon-neptune', 'aws-chatbot', 'amazon-mq', 'amazon-ec2', 'amazon-vpc', 'aws-copilot', 'aws-fargate', 'aws-lake-formation', 'aws-cloudformation', 'aws-organizations', 'amazon-app-runner', 'amazon-ami', 'aws-config', 'aws-security-group', 'amazon-appflow', 'amazon-s3-select', 'aws-documentdb', 'aws-parameter-store', 'amazon-elb', 'amazon-bedrock', 'aws-mediapackage', 'amazon-glacier', 'aws-sdk-mock', 'amazon-honeycode', 'amazon-comprehend', 'aws-service-catalog'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 3029772, 'is_employee': False, 'last_modified_date': 1663420194, 'last_access_date': 1710946870, 'reputation_change_year': 180, 'reputation_change_quarter': 180, 'reputation_change_month': 70, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6679, 'creation_date': 1373467564, 'user_type': 'registered', 'user_id': 2569042, 'accept_rate': 78, 'location': 'Copenhagen, Denmark', 'website_url': 'https://www.linkedin.com/in/krzysztofslowinski/', 'link': 'https://stackoverflow.com/users/2569042/krzysztof-s%c5%82owi%c5%84ski', 'profile_image': 'https://i.stack.imgur.com/3pKWM.jpg?s=256&g=1', 'display_name': 'Krzysztof Słowiński'}","I am exporting a pandas to Excel, and since it contains a lot of rows and columns, it would be useful to keep the top row and the first column when browsing its contents. There is a feature present in Excel that allows for freezing the top row and the first column. Is accessible through when exporting DataFrames to excel?",DataFrame XlsxWriter,-2,3,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
84,48668125,48668307,20805,"Pandas, read CSV ignoring extra commas",4,<python><pandas><csv>,16,"<p>I am reading a CSV file with 8 columns into Pandas data frame. The final column contains an error message, some of which contain commas. This causes the file read to fail with the error <code>ParserError: Error tokenizing data. C error: Expected 8 fields in line 21922, saw 9</code></p>

<p>Is there a way to ignore all commas after the 8th field, rather than having to go through the file and remove excess commas?</p>

<p>Code to read file:</p>

<pre><code>import pandas as pd
df = pd.read_csv('C:\\somepath\\output.csv')
</code></pre>

<p>Line that works:</p>

<pre><code>061AE,Active,001,2017_02_24 15_18_01,00006,1,00013,some message
</code></pre>

<p>Line that fails: </p>

<pre><code>061AE,Active,001,2017_02_24 15_18_01,00006,1,00013,longer message, with commas
</code></pre>
",2623411,1944,07-02-2018 15:59,07-02-2018 16:08,0,1944,59,4,30,100,"{'badge_counts': {'bronze': 59, 'silver': 30, 'gold': 4}, 'account_id': 3098735, 'is_employee': False, 'last_modified_date': 1673282400, 'last_access_date': 1711111792, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1944, 'creation_date': 1374852878, 'user_type': 'registered', 'user_id': 2623411, 'accept_rate': 100, 'location': 'Reading, UK', 'website_url': 'https://procrastinatingengineer.uk', 'link': 'https://stackoverflow.com/users/2623411/mikes159', 'profile_image': 'https://i.stack.imgur.com/4K5cB.jpg?s=256&g=1', 'display_name': 'MikeS159'}","I am reading a CSV file with 8 columns into Pandas data frame. The final column contains an error message, some of which contain commas. This causes the file read to fail with the error Is there a way to ignore all commas after the 8th field, rather than having to go through the file and remove excess commas? Code to read file: Line that works: Line that fails:","ParserError: Error tokenizing data. C error: Expected 8 fields in line 21922, saw 9 import pandas as pd
df = pd.read_csv('C:\\somepath\\output.csv')
 061AE,Active,001,2017_02_24 15_18_01,00006,1,00013,some message
 061AE,Active,001,2017_02_24 15_18_01,00006,1,00013,longer message, with commas
",0,19,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
85,48619709,57326946,4807,PyCharm type hinting enum iteration,2,<python><enums><pycharm><type-hinting>,20,"<p>Python's enum class supports iteration, but PyCharm has trouble figuring this out.</p>

<pre><code>from enum import Enum

class Color(Enum):
    RED = 0
    BLUE = 1

for color in Color:
    # Warning: Expected 'collections.Iterable', got 'Type[Color]' instead
    print(color)
</code></pre>

<p>Though the method <code>EnumMeta.__iter__</code> exists, PyCharm has trouble figuring this out.</p>

<p>I don't mind manually adding type hinting to work around the problem, I'm just not sure what and where.</p>
",3161460,2139,05-02-2018 09:51,02-08-2019 12:56,543,2139,29,1,15,100,"{'badge_counts': {'bronze': 29, 'silver': 15, 'gold': 1}, 'account_id': 3810379, 'is_employee': False, 'last_modified_date': 1648734000, 'last_access_date': 1711020520, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2139, 'creation_date': 1388876996, 'user_type': 'registered', 'user_id': 3161460, 'accept_rate': 100, 'location': 'Tel Aviv-Yafo, Israel', 'website_url': '', 'link': 'https://stackoverflow.com/users/3161460/hetzroni', 'profile_image': 'https://graph.facebook.com/661774649/picture?type=large', 'display_name': 'Hetzroni'}","Python's enum class supports iteration, but PyCharm has trouble figuring this out. Though the method exists, PyCharm has trouble figuring this out. I don't mind manually adding type hinting to work around the problem, I'm just not sure what and where.","from enum import Enum

class Color(Enum):
    RED = 0
    BLUE = 1

for color in Color:
    # Warning: Expected 'collections.Iterable', got 'Type[Color]' instead
    print(color)
 EnumMeta.__iter__",7,16,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
86,48245987,48246009,5783,Efficient way to compute the Vandermonde matrix,2,<python><arrays><performance><numpy><linear-algebra>,12,"<p>I'm calculating <a href=""https://en.wikipedia.org/wiki/Vandermonde_matrix"" rel=""nofollow noreferrer""><code>Vandermonde matrix</code></a> for a fairly large 1D array. The natural and clean way to do this is using <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.vander.html"" rel=""nofollow noreferrer""><code>np.vander()</code></a>. However, I found that this is approx. <strong>2.5x slower</strong> than a list comprehension based approach.</p>

<pre><code>In [43]: x = np.arange(5000)
In [44]: N = 4

In [45]: %timeit np.vander(x, N, increasing=True)
155 µs ± 205 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

# one of the listed approaches from the documentation
In [46]: %timeit np.flip(np.column_stack([x**(N-1-i) for i in range(N)]), axis=1)
65.3 µs ± 235 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [47]: np.all(np.vander(x, N, increasing=True) == np.flip(np.column_stack([x**(N-1-i) for i in range(N)]), axis=1))
Out[47]: True
</code></pre>

<p>I'm trying to understand where the bottleneck is and the reason why does the implementation of native <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.vander.html"" rel=""nofollow noreferrer""><code>np.vander()</code></a> is ~ <strong>2.5x</strong> slower. </p>

<p>Efficiency matters for my implementation. So, even faster alternatives are also welcome!</p>
",2956066,59047,14-01-2018 01:45,14-01-2018 01:51,0,59187,155,15,168,74,"{'badge_counts': {'bronze': 155, 'silver': 168, 'gold': 15}, 'account_id': 3538323, 'is_employee': False, 'last_modified_date': 1710101100, 'last_access_date': 1711179539, 'reputation_change_year': 848, 'reputation_change_quarter': 848, 'reputation_change_month': 228, 'reputation_change_week': 60, 'reputation_change_day': 10, 'reputation': 59187, 'creation_date': 1383650723, 'user_type': 'registered', 'user_id': 2956066, 'accept_rate': 74, 'location': 'Deutschland', 'website_url': 'https://kmario23.github.io', 'link': 'https://stackoverflow.com/users/2956066/kmario23', 'profile_image': 'https://i.stack.imgur.com/Bqbcr.png?s=256&g=1', 'display_name': 'kmario23'}","I'm calculating for a fairly large 1D array. The natural and clean way to do this is using . However, I found that this is approx. 2.5x slower than a list comprehension based approach. I'm trying to understand where the bottleneck is and the reason why does the implementation of native is ~ 2.5x slower. Efficiency matters for my implementation. So, even faster alternatives are also welcome!","Vandermonde matrix np.vander() In [43]: x = np.arange(5000)
In [44]: N = 4

In [45]: %timeit np.vander(x, N, increasing=True)
155 µs ± 205 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

# one of the listed approaches from the documentation
In [46]: %timeit np.flip(np.column_stack([x**(N-1-i) for i in range(N)]), axis=1)
65.3 µs ± 235 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [47]: np.all(np.vander(x, N, increasing=True) == np.flip(np.column_stack([x**(N-1-i) for i in range(N)]), axis=1))
Out[47]: True
 np.vander()",8,19,0,3,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
87,49871030,49871423,14477,How to fetch the latest records using find_one in pymongo?,2,<python><mongodb><pymongo>,12,"<p>I have an existing program where I'm trying to fetch last inserted document which matches the key <code>aws_account_id</code> using <code>find_one</code> in pymongo.</p>

<p>I'm using this query to perform the fetching:</p>

<pre><code>report = securitydb.scout.find_one({'aws_account_id': aws_account.account_number})
</code></pre>

<p>But this query returns a wrong document. The image below demonstrates the expected result and the wrong one that I'm getting.</p>

<p><a href=""https://i.stack.imgur.com/rUtYm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rUtYm.png"" alt=""enter image description here""></a></p>

<p>In the image, both documents have the same aws_account_id but the red one is inserted last. So the expected result is the red marked document but it pulls the yellow marked document.</p>

<p>I'm not sure if I need to use sorting or things like that. I got some solution but those are based on pure mongo query. I need help doing this with pymongo.</p>

<p>Thanks In Advance,
Robin</p>
",3211949,456,17-04-2018 06:10,17-04-2018 06:39,0,466,24,2,5,75,"{'badge_counts': {'bronze': 24, 'silver': 5, 'gold': 2}, 'account_id': 3878747, 'is_employee': False, 'last_modified_date': 1573680792, 'last_access_date': 1685726498, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 466, 'creation_date': 1390129593, 'user_type': 'registered', 'user_id': 3211949, 'accept_rate': 75, 'location': 'Bangladesh', 'website_url': 'https://robinislam.me', 'link': 'https://stackoverflow.com/users/3211949/robin', 'profile_image': 'https://graph.facebook.com/100008295366079/picture?type=large', 'display_name': 'Robin'}","I have an existing program where I'm trying to fetch last inserted document which matches the key using in pymongo. I'm using this query to perform the fetching: But this query returns a wrong document. The image below demonstrates the expected result and the wrong one that I'm getting. In the image, both documents have the same aws_account_id but the red one is inserted last. So the expected result is the red marked document but it pulls the yellow marked document. I'm not sure if I need to use sorting or things like that. I got some solution but those are based on pure mongo query. I need help doing this with pymongo. Thanks In Advance, Robin","aws_account_id find_one report = securitydb.scout.find_one({'aws_account_id': aws_account.account_number})
",-2,17,1,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
88,48628510,48700914,9461,Efficiently download files asynchronously with requests,4,<python><python-3.x><performance><python-requests><requests-futures>,15,"<p>I want to download files as fast as possible with python.Here is my code</p>

<pre><code>import pandas as pd
import requests
from requests_futures.sessions import FuturesSession
import os
import pathlib
from timeit import default_timer as timer


class AsyncDownloader:
    """"""Download files asynchronously""""""

    __urls = set()
    __dest_path = None
    __user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:58.0) Gecko/20100101 Firefox/58.0'
    __read_timeout = 60
    __connection_timeout = 30
    __download_count = 0  # unlimited
    # http://www.browserscope.org/?category=network
    __worker_count = 17  # No of threads to spawn
    __chunk_size = 1024
    __download_time = -1
    __errors = []

    # TODO Fetch only content of a specific type from a csv
    # TODO Improve code structure so that it can be used as a commandline tool

    def set_source_csv(self, source_path, column_name):
        self.source_path = source_path
        self.column_name = column_name

        try:
            my_csv = pd.read_csv(source_path, usecols=[self.column_name], chunksize=10)
        except ValueError:
            print(""The column name doesn't exist"")
            return
        else:
            # No exception whatsoever
            for chunk in my_csv:
                AsyncDownloader.__urls.update(set(getattr(chunk, self.column_name)))

    def set_destination_path(self, dest_path):
        if dest_path.endswith('/'):
            dest_path = dest_path[:-1]
        self.dest_path = dest_path
        # TODO Add exception in case we can't create the directory
        pathlib.Path(self.dest_path).mkdir(parents=True, exist_ok=True)
        if os.access(self.dest_path, os.W_OK):
            AsyncDownloader.__dest_path = pathlib.Path(self.dest_path).resolve()

    def set_user_agent(self, useragent):
        self.useragent = useragent
        AsyncDownloader.__user_agent = self.useragent

    def set_connection_timeout(self, ctimeout_secs):
        self.timeout_secs = ctimeout_secs
        if self.timeout_secs &gt;= 0:
            AsyncDownloader.__connection_timeout = self.timeout_secs

    def set_read_timeout(self, rtimeout_secs):
        self.timeout_secs = rtimeout_secs
        if self.timeout_secs &gt;= 0:
            AsyncDownloader.__read_timeout = self.timeout_secs

    def set_download_count(self, file_count):
        self.file_count = file_count
        if self.file_count &gt; 0:
            AsyncDownloader.__download_count = self.file_count

    def set_worker_count(self, worker_count):
        self.worker_count = worker_count
        if self.worker_count &gt; 0:
            AsyncDownloader.__worker_count = self.worker_count

    def set_chunk_size(self, chunk_size):
        self.chunk_size = chunk_size
        if self.chunk_size &gt; 0:
            AsyncDownloader.__chunk_size = self.chunk_size

    def print_urls(self):
        print(AsyncDownloader.__urls)

    def get_download_time(self):
        return AsyncDownloader.__download_time

    def get_errors(self):
        return AsyncDownloader.__errors

    def download(self):
        start = timer()
        try:
            session = FuturesSession(max_workers=AsyncDownloader.__worker_count)
            session.headers.update({'user-agent': AsyncDownloader.__user_agent})
            session.request(AsyncDownloader.__connection_timeout,
                            AsyncDownloader.__connection_timeout, stream=True)

            results = []
            # Give an accurate file count even if we don't have to download it as it a;ready exist
            file_count = 0

            for url in AsyncDownloader.__urls:
                filename = os.path.basename(url)
                # check if we need only a limited number of files
                if AsyncDownloader.__download_count != 0:
                    # No need to download file if it already exist
                    if pathlib.Path(AsyncDownloader.__dest_path / filename).is_file():
                        file_count += 1
                        continue
                    else:
                        if file_count &lt; AsyncDownloader.__download_count:
                            file_count += 1
                            results.append(session.get(url))
                else:
                    if not pathlib.Path(AsyncDownloader.__dest_path / filename).is_file():
                        results.append(session.get(url))

            for result in results:
                # wait for the response to complete, if it hasn't already
                response = result.result()
                filename = os.path.basename(response.url)
                if response.status_code == 200:
                    with open(pathlib.Path(AsyncDownloader.__dest_path / filename).resolve(), 'wb') as fd:
                        for chunk in response.iter_content(chunk_size=AsyncDownloader.__chunk_size):
                            if chunk:  # filter out keep-alive new chunks
                                fd.write(chunk)

            end = timer()
            AsyncDownloader.__download_time = end - start

        except requests.exceptions.HTTPError as errh:
            AsyncDownloader.__errors.append(""Http Error:"" + errh)
            # print(""Http Error:"", errh)
        except requests.exceptions.ConnectionError as errc:
            AsyncDownloader.__errors.append(""Error Connecting:"" + errc)
            # print(""Error Connecting:"", errc)
        except requests.exceptions.Timeout as errt:
            AsyncDownloader.__errors.append(""Timeout Error:"" + errt)
            # print(""Timeout Error:"", errt)
        except requests.exceptions.RequestException as err:
            AsyncDownloader.__errors.append(""OOps: Something Else"" + err)
        else:
            return
</code></pre>

<p>The following code is making a very bad assumption.Indeed i am assuming that the first url will finish first which is of course not correct.</p>

<pre><code># wait for the response to complete, if it hasn't already
response = result.result()
</code></pre>

<p>How can i ensure that only requests that have been completed are processed instead of taking an assumption like above in an efficient manner ?</p>

<p>I would appreciate any other suggestion on how to improve performance.</p>

<p>Kind Regards</p>
",2650277,6441,05-02-2018 17:49,09-02-2018 07:41,4,6491,137,17,65,72,"{'badge_counts': {'bronze': 137, 'silver': 65, 'gold': 17}, 'account_id': 3133092, 'is_employee': False, 'last_modified_date': 1695814500, 'last_access_date': 1711137908, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 60, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 6491, 'creation_date': 1375613737, 'user_type': 'registered', 'user_id': 2650277, 'accept_rate': 72, 'link': 'https://stackoverflow.com/users/2650277/user2650277', 'profile_image': 'https://www.gravatar.com/avatar/6d7f954b8221b1affc0886a8900d53e9?s=256&d=identicon&r=PG', 'display_name': 'user2650277'}",I want to download files as fast as possible with python.Here is my code The following code is making a very bad assumption.Indeed i am assuming that the first url will finish first which is of course not correct. How can i ensure that only requests that have been completed are processed instead of taking an assumption like above in an efficient manner ? I would appreciate any other suggestion on how to improve performance. Kind Regards,"import pandas as pd
import requests
from requests_futures.sessions import FuturesSession
import os
import pathlib
from timeit import default_timer as timer


class AsyncDownloader:
    """"""Download files asynchronously""""""

    __urls = set()
    __dest_path = None
    __user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:58.0) Gecko/20100101 Firefox/58.0'
    __read_timeout = 60
    __connection_timeout = 30
    __download_count = 0  # unlimited
    # http://www.browserscope.org/?category=network
    __worker_count = 17  # No of threads to spawn
    __chunk_size = 1024
    __download_time = -1
    __errors = []

    # TODO Fetch only content of a specific type from a csv
    # TODO Improve code structure so that it can be used as a commandline tool

    def set_source_csv(self, source_path, column_name):
        self.source_path = source_path
        self.column_name = column_name

        try:
            my_csv = pd.read_csv(source_path, usecols=[self.column_name], chunksize=10)
        except ValueError:
            print(""The column name doesn't exist"")
            return
        else:
            # No exception whatsoever
            for chunk in my_csv:
                AsyncDownloader.__urls.update(set(getattr(chunk, self.column_name)))

    def set_destination_path(self, dest_path):
        if dest_path.endswith('/'):
            dest_path = dest_path[:-1]
        self.dest_path = dest_path
        # TODO Add exception in case we can't create the directory
        pathlib.Path(self.dest_path).mkdir(parents=True, exist_ok=True)
        if os.access(self.dest_path, os.W_OK):
            AsyncDownloader.__dest_path = pathlib.Path(self.dest_path).resolve()

    def set_user_agent(self, useragent):
        self.useragent = useragent
        AsyncDownloader.__user_agent = self.useragent

    def set_connection_timeout(self, ctimeout_secs):
        self.timeout_secs = ctimeout_secs
        if self.timeout_secs &gt;= 0:
            AsyncDownloader.__connection_timeout = self.timeout_secs

    def set_read_timeout(self, rtimeout_secs):
        self.timeout_secs = rtimeout_secs
        if self.timeout_secs &gt;= 0:
            AsyncDownloader.__read_timeout = self.timeout_secs

    def set_download_count(self, file_count):
        self.file_count = file_count
        if self.file_count &gt; 0:
            AsyncDownloader.__download_count = self.file_count

    def set_worker_count(self, worker_count):
        self.worker_count = worker_count
        if self.worker_count &gt; 0:
            AsyncDownloader.__worker_count = self.worker_count

    def set_chunk_size(self, chunk_size):
        self.chunk_size = chunk_size
        if self.chunk_size &gt; 0:
            AsyncDownloader.__chunk_size = self.chunk_size

    def print_urls(self):
        print(AsyncDownloader.__urls)

    def get_download_time(self):
        return AsyncDownloader.__download_time

    def get_errors(self):
        return AsyncDownloader.__errors

    def download(self):
        start = timer()
        try:
            session = FuturesSession(max_workers=AsyncDownloader.__worker_count)
            session.headers.update({'user-agent': AsyncDownloader.__user_agent})
            session.request(AsyncDownloader.__connection_timeout,
                            AsyncDownloader.__connection_timeout, stream=True)

            results = []
            # Give an accurate file count even if we don't have to download it as it a;ready exist
            file_count = 0

            for url in AsyncDownloader.__urls:
                filename = os.path.basename(url)
                # check if we need only a limited number of files
                if AsyncDownloader.__download_count != 0:
                    # No need to download file if it already exist
                    if pathlib.Path(AsyncDownloader.__dest_path / filename).is_file():
                        file_count += 1
                        continue
                    else:
                        if file_count &lt; AsyncDownloader.__download_count:
                            file_count += 1
                            results.append(session.get(url))
                else:
                    if not pathlib.Path(AsyncDownloader.__dest_path / filename).is_file():
                        results.append(session.get(url))

            for result in results:
                # wait for the response to complete, if it hasn't already
                response = result.result()
                filename = os.path.basename(response.url)
                if response.status_code == 200:
                    with open(pathlib.Path(AsyncDownloader.__dest_path / filename).resolve(), 'wb') as fd:
                        for chunk in response.iter_content(chunk_size=AsyncDownloader.__chunk_size):
                            if chunk:  # filter out keep-alive new chunks
                                fd.write(chunk)

            end = timer()
            AsyncDownloader.__download_time = end - start

        except requests.exceptions.HTTPError as errh:
            AsyncDownloader.__errors.append(""Http Error:"" + errh)
            # print(""Http Error:"", errh)
        except requests.exceptions.ConnectionError as errc:
            AsyncDownloader.__errors.append(""Error Connecting:"" + errc)
            # print(""Error Connecting:"", errc)
        except requests.exceptions.Timeout as errt:
            AsyncDownloader.__errors.append(""Timeout Error:"" + errt)
            # print(""Timeout Error:"", errt)
        except requests.exceptions.RequestException as err:
            AsyncDownloader.__errors.append(""OOps: Something Else"" + err)
        else:
            return
 # wait for the response to complete, if it hasn't already
response = result.result()
",141,156,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
89,49689550,49692185,12464,Simple hash of PIL image,2,<python><hash><python-imaging-library>,11,"<h2>Background</h2>

<p>I want to store information of PIL images in a key-value store. For that, I hash the image and use the hash as a key.</p>

<h2>What I tried</h2>

<p>I have been using the following code to compute the hash:</p>

<pre><code>def hash(img):
   return hashlib.md5(img.tobytes()).hexdigest()
</code></pre>

<p>But it seems like this is not stable. I have not figured out why, but for the same image on different machines, I get different hashes.</p>

<h2>Question</h2>

<p>Is there a simple way of <strong>hashing images that only depends on the image itself</strong> (not on timestamps, system architecture, etc.)?</p>

<p>Note that I do not need similar images to get a similar/same hash, as in <a href=""https://stackoverflow.com/questions/998662/what-is-image-hashing-used-for"">image hashing</a>. In fact, I want different images to have a different hash, e.g. changing the brightness of the image should change its hash.</p>
",2761174,403,06-04-2018 09:25,06-04-2018 11:50,0,403,17,1,5,,"{'badge_counts': {'bronze': 17, 'silver': 5, 'gold': 1}, 'account_id': 3280430, 'is_employee': False, 'last_modified_date': 1630135800, 'last_access_date': 1711066606, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 403, 'creation_date': 1378724412, 'user_type': 'registered', 'user_id': 2761174, 'link': 'https://stackoverflow.com/users/2761174/peter', 'profile_image': 'https://www.gravatar.com/avatar/69c50a3f9f4b1b9ae0f5557722f47634?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Peter'}","Background I want to store information of PIL images in a key-value store. For that, I hash the image and use the hash as a key. What I tried I have been using the following code to compute the hash: But it seems like this is not stable. I have not figured out why, but for the same image on different machines, I get different hashes. Question Is there a simple way of hashing images that only depends on the image itself (not on timestamps, system architecture, etc.)? Note that I do not need similar images to get a similar/same hash, as in image hashing. In fact, I want different images to have a different hash, e.g. changing the brightness of the image should change its hash.","def hash(img):
   return hashlib.md5(img.tobytes()).hexdigest()
",1,19,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
90,49635436,49635530,62566,Shapely point geometry in geopandas df to lat/lon columns,3,<python><gis><latitude-longitude><geopandas><shapely>,30,"<p>I have a geopandas df with a column of shapely point objects. I want to extract the coordinate (lat/lon) from the shapely point objects to generate latitude and longitude columns. There must be an easy way to do this, but I cannot figure it out.</p>

<p>I know you can extract the individual coordinates like this:</p>

<pre><code>lon = df.point_object[0].x
lat = df.point_object[0].y
</code></pre>

<p>And I could create a function that does this for the entire df, but I figured there was a more efficient/elegant way.</p>
",2970409,854,03-04-2018 16:56,03-04-2018 17:02,0,854,24,1,9,88,"{'badge_counts': {'bronze': 24, 'silver': 9, 'gold': 1}, 'account_id': 3556869, 'is_employee': False, 'last_modified_date': 1696038601, 'last_access_date': 1677520677, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 854, 'creation_date': 1383942266, 'user_type': 'registered', 'user_id': 2970409, 'accept_rate': 88, 'website_url': '', 'link': 'https://stackoverflow.com/users/2970409/jtam', 'profile_image': 'https://www.gravatar.com/avatar/f7eb7da9e53ddc7fd7265f71e4545b17?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jtam'}","I have a geopandas df with a column of shapely point objects. I want to extract the coordinate (lat/lon) from the shapely point objects to generate latitude and longitude columns. There must be an easy way to do this, but I cannot figure it out. I know you can extract the individual coordinates like this: And I could create a function that does this for the entire df, but I figured there was a more efficient/elegant way.","lon = df.point_object[0].x
lat = df.point_object[0].y
",1,9,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
91,49436367,49436763,11000,Access child class variable in parent class,3,<python><class><oop>,15,"<p>Is it correct to access variables from a child class in a parent class? Is this a good OOP approach? I don't need to create instances of Animal class, but if I will, the <code>make_sound</code> method will raise <code>AttributeError</code>, which is bothering me.</p>

<pre><code>class Animal:
    def make_sound(self):
        print(self.sound)

class Cat(Animal):
    sound = 'meow'

class Dog(Animal):
    sound = 'bark'

cat = Cat()
cat.make_sound()

dog = Dog()
dog.make_sound()
</code></pre>
",2977262,2024,22-03-2018 18:53,22-03-2018 19:17,0,2024,27,3,19,100,"{'badge_counts': {'bronze': 27, 'silver': 19, 'gold': 3}, 'account_id': 3565667, 'is_employee': False, 'last_modified_date': 1703334300, 'last_access_date': 1711118036, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2024, 'creation_date': 1384122341, 'user_type': 'registered', 'user_id': 2977262, 'accept_rate': 100, 'location': 'Minsk, Belarus', 'website_url': '', 'link': 'https://stackoverflow.com/users/2977262/ryche', 'profile_image': 'https://i.stack.imgur.com/aqFiC.jpg?s=256&g=1', 'display_name': 'ryche'}","Is it correct to access variables from a child class in a parent class? Is this a good OOP approach? I don't need to create instances of Animal class, but if I will, the method will raise , which is bothering me.","make_sound AttributeError class Animal:
    def make_sound(self):
        print(self.sound)

class Cat(Animal):
    sound = 'meow'

class Dog(Animal):
    sound = 'bark'

cat = Cat()
cat.make_sound()

dog = Dog()
dog.make_sound()
",12,18,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
92,48435348,48477752,346,The relationship between thread and process in multi-process program,1,<python><multithreading><python-multithreading>,12,"<p>OS: debian9.<br>
A simple multi-processes program named <code>mprocesses.py</code>.</p>

<pre><code>import os
import multiprocessing

def run_task(name):
    print(""task %s (pid = %s) is running""  %(name,os.getpid()))
    while True:
        pass

if __name__ == ""__main__"":
    print(""current process %s ."" %os.getpid())
    pool = multiprocessing.Pool(processes = 2)
    for i in range(2):
        pool.apply_async(run_task,args=(i,))
    pool.close()
    pool.join()
</code></pre>

<p>Run <code>python3 mprocesses.py</code> and get below output.</p>

<pre><code>python3 mprocesses.py
current process 6145 .
task 0 (pid = 6146) is running
task 1 (pid = 6147) is running
</code></pre>

<p>Get processes info.</p>

<pre><code>ps lax |grep 'python3 mprocesses.py' |grep -v grep 
0  1000  6145  5615  20   0 275428 14600 -      Sl+  pts/1      0:00 python3 mprocesses.py
1  1000  6146  6145  20   0  54232 10340 -      R+   pts/1      1:01 python3 mprocesses.py
1  1000  6147  6145  20   0  54232 10348 -      R+   pts/1      1:01 python3 mprocesses.py
</code></pre>

<p>Check processes tree view.</p>

<pre><code>pstree -p 5615
bash(5615)───python3(6145)─┬─python3(6146)
                           ├─python3(6147)
                           ├─{python3}(6148)
                           ├─{python3}(6149)
                           └─{python3}(6150)
</code></pre>

<p>What confused me is the three threads 6148,6149,6150.<br>
Does that mean every process contain one process ? 
Maybe my logical graph is better to express relationships between processes and threads here.</p>

<pre><code>bash(5615)───python3(6145)─┬─────────────────python3(6146)
                           |                    └─{python3}(6149)
                           |             
                           ├──────────────────python3(6147)
                           ├─{python3}(6148)     └─{python3}(6150)
</code></pre>

<p>1.bash(5615) is the <code>python3 mprocesses.py</code>(6145) 's father process.<br>
2.<code>python3 mprocesses.py</code>(6145)  contains two processes 6146 and 6147 created by <code>pool = multiprocessing.Pool(processes = 2)</code>.<br>
3.Process(6145) contain thread(6148),Process(6146) contain thread(6149),Process(6147) contain thread(6150).<br>
It does no matter which exact process id contain which thread id.<br>
Is my understanding right?    </p>
",1982032,420,25-01-2018 03:48,27-01-2018 16:15,2,440,308,44,150,85,"{'badge_counts': {'bronze': 308, 'silver': 150, 'gold': 44}, 'account_id': 2249472, 'is_employee': False, 'last_modified_date': 1711160100, 'last_access_date': 1710065219, 'reputation_change_year': -154, 'reputation_change_quarter': -154, 'reputation_change_month': 60, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 440, 'creation_date': 1358295088, 'user_type': 'registered', 'user_id': 1982032, 'accept_rate': 85, 'location': 'China', 'website_url': '', 'link': 'https://stackoverflow.com/users/1982032/showkey', 'profile_image': 'https://www.gravatar.com/avatar/20b10ce0ff1755b449c3b9f14c1f20ca?s=256&d=identicon&r=PG', 'display_name': 'showkey'}","OS: debian9. A simple multi-processes program named . Run and get below output. Get processes info. Check processes tree view. What confused me is the three threads 6148,6149,6150. Does that mean every process contain one process ? Maybe my logical graph is better to express relationships between processes and threads here. 1.bash(5615) is the (6145) 's father process. 2.(6145) contains two processes 6146 and 6147 created by . 3.Process(6145) contain thread(6148),Process(6146) contain thread(6149),Process(6147) contain thread(6150). It does no matter which exact process id contain which thread id. Is my understanding right?","mprocesses.py import os
import multiprocessing

def run_task(name):
    print(""task %s (pid = %s) is running""  %(name,os.getpid()))
    while True:
        pass

if __name__ == ""__main__"":
    print(""current process %s ."" %os.getpid())
    pool = multiprocessing.Pool(processes = 2)
    for i in range(2):
        pool.apply_async(run_task,args=(i,))
    pool.close()
    pool.join()
 python3 mprocesses.py python3 mprocesses.py
current process 6145 .
task 0 (pid = 6146) is running
task 1 (pid = 6147) is running
 ps lax |grep 'python3 mprocesses.py' |grep -v grep 
0  1000  6145  5615  20   0 275428 14600 -      Sl+  pts/1      0:00 python3 mprocesses.py
1  1000  6146  6145  20   0  54232 10340 -      R+   pts/1      1:01 python3 mprocesses.py
1  1000  6147  6145  20   0  54232 10348 -      R+   pts/1      1:01 python3 mprocesses.py
 pstree -p 5615
bash(5615)───python3(6145)─┬─python3(6146)
                           ├─python3(6147)
                           ├─{python3}(6148)
                           ├─{python3}(6149)
                           └─{python3}(6150)
 bash(5615)───python3(6145)─┬─────────────────python3(6146)
                           |                    └─{python3}(6149)
                           |             
                           ├──────────────────python3(6147)
                           ├─{python3}(6148)     └─{python3}(6150)
 python3 mprocesses.py python3 mprocesses.py pool = multiprocessing.Pool(processes = 2)",24,62,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
93,48726621,48733093,18314,Is it possible to lock versions of packages in Anaconda?,3,<python><anaconda><conda>,24,"<p>I installed Anaconda 3 2.3.0 on Windows xp. It is supposed to be the <a href=""https://conda.io/docs/user-guide/configuration/use-winxp-with-proxy.html"" rel=""noreferrer"" title=""ref"">last version of anaconda to support windows xp</a> as it contains python 3.4.3, and python 3.4.x is the last version of python to support windows xp.</p>

<p>When installing a package with </p>

<blockquote>
  <p>conda install</p>
</blockquote>

<p>,one of its dependencies was updating conda to conda 4.x. conda 4.x crashed in the commandline when running conda install . This made anaconda unusable that i uninstalled anaconda. </p>

<p>My question, is it possible to lock versions of packages ? For the two use cases:</p>

<ul>
<li>lock and forget : for packages I never want them to update, i need to run a command to lock them once so they will never update as a depency</li>
<li>ignore updating : install a package while ignoring to update certain package passed by name in this update.</li>
</ul>

<p>If only one of the 2 use cases is possible or is known or is easier, please write it as answer. </p>
",2008463,6610,10-02-2018 23:19,11-02-2018 15:27,1,6630,39,4,36,80,"{'badge_counts': {'bronze': 39, 'silver': 36, 'gold': 4}, 'account_id': 2283985, 'is_employee': False, 'last_modified_date': 1709601000, 'last_access_date': 1692847874, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6630, 'creation_date': 1359049310, 'user_type': 'registered', 'user_id': 2008463, 'accept_rate': 80, 'website_url': '', 'link': 'https://stackoverflow.com/users/2008463/mohamed-el-nakeep', 'profile_image': 'https://www.gravatar.com/avatar/5e1b0ed7b8d7d84cddace2f9f6fd88d4?s=256&d=identicon&r=PG', 'display_name': 'Mohamed El-Nakeep'}","I installed Anaconda 3 2.3.0 on Windows xp. It is supposed to be the last version of anaconda to support windows xp as it contains python 3.4.3, and python 3.4.x is the last version of python to support windows xp. When installing a package with conda install ,one of its dependencies was updating conda to conda 4.x. conda 4.x crashed in the commandline when running conda install . This made anaconda unusable that i uninstalled anaconda. My question, is it possible to lock versions of packages ? For the two use cases: lock and forget : for packages I never want them to update, i need to run a command to lock them once so they will never update as a depency ignore updating : install a package while ignoring to update certain package passed by name in this update. If only one of the 2 use cases is possible or is known or is easier, please write it as answer.",,0,18,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
94,48295246,48295351,17003,How to pass None keyword as command line argument,4,<python>,16,"<p>I am trying to pass <code>None</code> keyword as a command line parameter to a script as follows, if I explicity mention <code>Category=None</code> it works but the moment I switch to <code>sys.argv[1]</code> it fails, any pointers on how to fix this?</p>

<pre><code>category = None --&gt; works
#category=sys.argv[1] --&gt; doesn't work 
</code></pre>

<p>so I tried as below which still didn't work</p>

<pre><code>  if sys.argv[1].strip()==None:
    category = None
else:
    category=sys.argv[1].strip()
</code></pre>

<p>Command line passage:</p>

<pre><code> script.py None
</code></pre>
",2125827,11106,17-01-2018 06:55,17-01-2018 07:03,0,11136,67,16,47,74,"{'badge_counts': {'bronze': 67, 'silver': 47, 'gold': 16}, 'account_id': 2435912, 'is_employee': False, 'last_modified_date': 1687784400, 'last_access_date': 1641013023, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 50, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 11136, 'creation_date': 1362196629, 'user_type': 'registered', 'user_id': 2125827, 'accept_rate': 74, 'website_url': '', 'link': 'https://stackoverflow.com/users/2125827/carte-blanche', 'profile_image': 'https://www.gravatar.com/avatar/e2931c1ee2f6b4fefef20b2ef4a25082?s=256&d=identicon&r=PG', 'display_name': 'carte blanche'}","I am trying to pass keyword as a command line parameter to a script as follows, if I explicity mention it works but the moment I switch to it fails, any pointers on how to fix this? so I tried as below which still didn't work Command line passage:","None Category=None sys.argv[1] category = None --&gt; works
#category=sys.argv[1] --&gt; doesn't work 
   if sys.argv[1].strip()==None:
    category = None
else:
    category=sys.argv[1].strip()
  script.py None
",1,18,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
95,48502153,48618890,2942,Fit mixture of Gaussians with fixed covariance in Python,3,<python><machine-learning><scikit-learn><gmm>,13,"<p>I have some 2D data (GPS data) with clusters (stop locations) that I know resemble Gaussians with a characteristic standard deviation (proportional to the inherent noise of GPS samples). The figure below visualizes a sample that I expect has two such clusters. The image is 25 meters wide and 13 meters tall.</p>

<p><a href=""https://i.stack.imgur.com/cECqI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cECqI.png"" alt=""enter image description here""></a></p>

<p>The <code>sklearn</code> module has a function <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit"" rel=""noreferrer""><code>sklearn.mixture.GaussianMixture</code></a> which allows you to fit a mixture of Gaussians to data. The function has a parameter, <code>covariance_type</code>, that enables you to assume different things about the shape of the Gaussians. You can, for example, assume them to be uniform using the <code>'tied'</code> argument.</p>

<p>However, it does not appear directly possible to assume the covariance matrices to remain constant. From the <code>sklearn</code> source code it seems trivial to make a modification that enables this but it feels a bit excessive to make a pull request with an update that allows this (also I don't want to accidentally add bugs in <code>sklearn</code>). Is there a better way to fit a mixture to data where the covariance matrix of each Gaussian is fixed?</p>

<p>I want to assume that the SD should remain constant at around 3 meters for each component, since that is roughly the noise level of my GPS samples.</p>
",3986879,8234,29-01-2018 13:14,05-02-2018 09:03,7,8254,58,4,35,78,"{'badge_counts': {'bronze': 58, 'silver': 35, 'gold': 4}, 'account_id': 4953639, 'is_employee': False, 'last_modified_date': 1687782000, 'last_access_date': 1710938529, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 8254, 'creation_date': 1409231926, 'user_type': 'registered', 'user_id': 3986879, 'accept_rate': 78, 'location': 'Copenhagen', 'website_url': 'http://ulfaslak.com/', 'link': 'https://stackoverflow.com/users/3986879/ulf-aslak', 'profile_image': 'https://i.stack.imgur.com/dac0W.jpg?s=256&g=1', 'display_name': 'Ulf Aslak'}","I have some 2D data (GPS data) with clusters (stop locations) that I know resemble Gaussians with a characteristic standard deviation (proportional to the inherent noise of GPS samples). The figure below visualizes a sample that I expect has two such clusters. The image is 25 meters wide and 13 meters tall. The module has a function which allows you to fit a mixture of Gaussians to data. The function has a parameter, , that enables you to assume different things about the shape of the Gaussians. You can, for example, assume them to be uniform using the argument. However, it does not appear directly possible to assume the covariance matrices to remain constant. From the source code it seems trivial to make a modification that enables this but it feels a bit excessive to make a pull request with an update that allows this (also I don't want to accidentally add bugs in ). Is there a better way to fit a mixture to data where the covariance matrix of each Gaussian is fixed? I want to assume that the SD should remain constant at around 3 meters for each component, since that is roughly the noise level of my GPS samples.",sklearn sklearn.mixture.GaussianMixture covariance_type 'tied' sklearn sklearn,-6,9,1,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
96,50071841,50072113,30847,How to push local files to github using python? (or post a commit via Python),2,<python><git><subprocess><popen>,11,"<h2>What options are there for commiting and pushing files to github from python?</h2>
<p>Here are three methods I thought should be feasible so attempted in order:</p>
<ol>
<li><p><strong>Use <a href=""https://github.com/PyGithub/PyGithub"" rel=""noreferrer"">pygithub</a>:</strong> (Github's python API) to send push requests to my repository. Failed because I can find no push functions in the API. I can see edit files, but that doesn't help when I plan on replacing the file often.</p>
</li>
<li><p><strong>Use <code>git push</code> in command line from a python subprocess (HTTPS):</strong> This almost works, but I cannot figure out how to fill in the user and password fields required. Attempt:</p>
<pre><code>import subprocess
from pexpect import popen_spawn


user = 'GithubUsername'
password = '***********'

cmd = &quot;cd C:\\Users\Dropbox\git-test&quot;
returned_value = subprocess.call(cmd, shell=True)  # returns the exit code in unix

cmd = &quot;git add .&quot; 
subprocess.call(cmd, shell=True)

cmd = 'git commit -m &quot;python project update&quot;'
subprocess.call(cmd, shell=True)

cmd = &quot;git remote set-url origin https://github.com/Tehsurfer/git-test.git&quot;
subprocess.call(cmd, shell=True)

cmd = &quot;git push &quot;
child_process = popen_spawn.PopenSpawn(cmd)
child_process.expect('User')
child_process.sendline(user)
child_process.expect('Password')
child_process.sendline(password)
print('returned value:', returned_value)

print('end of commands')`
</code></pre>
</li>
<li><p><strong>Use <code>git push</code> in command line from a python subprocess (SSH):</strong> The problem I had here is that I cannot find a way to create a ssh agent in the windows command prompt. I have been able to create one in the MINGW64 terminal easily enough via <a href=""https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/"" rel=""noreferrer"">this tutorial</a> , but have no way of interacting with it via Python.</p>
</li>
</ol>
",2217801,3241,28-04-2018 00:59,28-04-2018 01:54,0,3251,53,4,37,,"{'badge_counts': {'bronze': 53, 'silver': 37, 'gold': 4}, 'account_id': 2555785, 'is_employee': False, 'last_modified_date': 1708010100, 'last_access_date': 1711077834, 'reputation_change_year': 61, 'reputation_change_quarter': 61, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 3251, 'creation_date': 1364426751, 'user_type': 'registered', 'user_id': 2217801, 'location': 'Auckland, New Zealand', 'website_url': '', 'link': 'https://stackoverflow.com/users/2217801/jesse-reza-khorasanee', 'profile_image': 'https://www.gravatar.com/avatar/68585b941fb0767728af81b80a18a1e7?s=256&d=identicon&r=PG', 'display_name': 'Jesse Reza Khorasanee'}","What options are there for commiting and pushing files to github from python? Here are three methods I thought should be feasible so attempted in order: Use pygithub: (Github's python API) to send push requests to my repository. Failed because I can find no push functions in the API. I can see edit files, but that doesn't help when I plan on replacing the file often. Use in command line from a python subprocess (HTTPS): This almost works, but I cannot figure out how to fill in the user and password fields required. Attempt: Use in command line from a python subprocess (SSH): The problem I had here is that I cannot find a way to create a ssh agent in the windows command prompt. I have been able to create one in the MINGW64 terminal easily enough via this tutorial , but have no way of interacting with it via Python.","git push import subprocess
from pexpect import popen_spawn


user = 'GithubUsername'
password = '***********'

cmd = &quot;cd C:\\Users\Dropbox\git-test&quot;
returned_value = subprocess.call(cmd, shell=True)  # returns the exit code in unix

cmd = &quot;git add .&quot; 
subprocess.call(cmd, shell=True)

cmd = 'git commit -m &quot;python project update&quot;'
subprocess.call(cmd, shell=True)

cmd = &quot;git remote set-url origin https://github.com/Tehsurfer/git-test.git&quot;
subprocess.call(cmd, shell=True)

cmd = &quot;git push &quot;
child_process = popen_spawn.PopenSpawn(cmd)
child_process.expect('User')
child_process.sendline(user)
child_process.expect('Password')
child_process.sendline(password)
print('returned value:', returned_value)

print('end of commands')`
 git push",25,39,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
97,48909110,48909165,70069,Python pandas: mean and sum groupby on different columns at the same time,2,<python><pandas>,45,"<p>I have a pandas dataframe which looks like the following:</p>

<pre><code>Name    Missed    Credit    Grade
A       1         3         10
A       1         1         12      
B       2         3         10
B       1         2         20
</code></pre>

<p>And my desired output is:</p>

<pre><code>Name    Sum1   Sum2    Average
A       2      4      11
B       3      5      15   
</code></pre>

<p>Basically to get the sum of column <code>Credit</code> and <code>Missed</code> and to do average on <code>Grade</code>. What I am doing right now is two groupby on <code>Name</code> and then get sum and average and finally merge the two output dataframes which does not seem to be the best way of doing this. I have also found this on SO which makes sense if I want to work only on one column:</p>

<pre><code>df.groupby('Name')['Credit'].agg(['sum','average'])
</code></pre>

<p>But not sure how to do a one-liner for both columns?   </p>
",2827771,13140,21-02-2018 15:03,21-02-2018 15:05,0,13150,123,31,81,86,"{'badge_counts': {'bronze': 123, 'silver': 81, 'gold': 31}, 'account_id': 3367658, 'is_employee': False, 'last_modified_date': 1698933000, 'last_access_date': 1709828155, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 13150, 'creation_date': 1380434657, 'user_type': 'registered', 'user_id': 2827771, 'accept_rate': 86, 'location': 'Orlando, FL, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/2827771/ahajib', 'profile_image': 'https://www.gravatar.com/avatar/e63c326aa3d4fa518d894063ed53318c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ahajib'}",I have a pandas dataframe which looks like the following: And my desired output is: Basically to get the sum of column and and to do average on . What I am doing right now is two groupby on and then get sum and average and finally merge the two output dataframes which does not seem to be the best way of doing this. I have also found this on SO which makes sense if I want to work only on one column: But not sure how to do a one-liner for both columns?,"Name    Missed    Credit    Grade
A       1         3         10
A       1         1         12      
B       2         3         10
B       1         2         20
 Name    Sum1   Sum2    Average
A       2      4      11
B       3      5      15   
 Credit Missed Grade Name df.groupby('Name')['Credit'].agg(['sum','average'])
",2,22,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
98,48761985,48762969,18912,How to mock aiohttp.client.ClientSession.get async context manager,4,<python><python-asyncio><python-unittest><aiohttp><asynctest>,33,"<p>I have some troubles with mocking aiohttp.client.ClientSession.get context manager. I found some articles and here is one example that seems was working: <a href=""http://pfertyk.me/2017/06/testing-asynchronous-context-managers-in-python/"" rel=""noreferrer"">article 1</a></p>

<p>So my code that I want to test:</p>

<p><strong>async_app.py</strong></p>

<pre><code>import random
from aiohttp.client import ClientSession

async def get_random_photo_url():
    while True:
        async with ClientSession() as session:
            async with session.get('random.photos') as resp:
                json = await resp.json()
        photos = json['photos']
        if not photos:
            continue
        return random.choice(photos)['img_src']
</code></pre>

<p>And test:</p>

<p><strong>test_async_app.py</strong></p>

<pre><code>from asynctest import CoroutineMock, MagicMock, patch

from asynctest import TestCase as TestCaseAsync

from async_app import get_random_photo_url


class AsyncContextManagerMock(MagicMock):
    async def __aenter__(self):
        return self.aenter

    async def __aexit__(self, *args):
        pass

class TestAsyncExample(TestCaseAsync):
    @patch('aiohttp.client.ClientSession.get', new_callable=AsyncContextManagerMock)
    async def test_call_api_again_if_photos_not_found(self, mock_get):
        mock_get.return_value.aenter.json = CoroutineMock(side_effect=[{'photos': []},
                                                                       {'photos': [{'img_src': 'a.jpg'}]}])

        image_url = await get_random_photo_url()

        assert mock_get.call_count == 2
        assert mock_get.return_value.aenter.json.call_count == 2
        assert image_url == 'a.jpg'
</code></pre>

<p>When I'm running test, I'm getting an error:</p>

<pre><code>(test-0zFWLpVX) ➜  test python -m unittest test_async_app.py -v
test_call_api_again_if_photos_not_found (test_async_app.TestAsyncExample) ... ERROR

======================================================================
ERROR: test_call_api_again_if_photos_not_found (test_async_app.TestAsyncExample)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/case.py"", line 294, in run
    self._run_test_method(testMethod)
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/case.py"", line 351, in _run_test_method
    self.loop.run_until_complete(result)
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/case.py"", line 221, in wrapper
    return method(*args, **kwargs)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 467, in run_until_complete
    return future.result()
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/_awaitable.py"", line 21, in wrapper
    return await coroutine(*args, **kwargs)
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/mock.py"", line 588, in __next__
    return self.gen.send(None)
  File ""/home/kamyanskiy/work/test/test_async_app.py"", line 23, in test_call_api_again_if_photos_not_found
    image_url = await get_random_photo_url()
  File ""/home/kamyanskiy/work/test/async_app.py"", line 9, in get_random_photo_url
    json = await resp.json()
TypeError: object MagicMock can't be used in 'await' expression

----------------------------------------------------------------------
Ran 1 test in 0.003s

FAILED (errors=1)
</code></pre>

<p>So I've tried to debug - here is what I can see:</p>

<pre><code>&gt; /home/kamyanskiy/work/test/async_app.py(10)get_random_photo_url()
      9                 import ipdb; ipdb.set_trace()
---&gt; 10                 json = await resp.json()
     11         photos = json['photos']

ipdb&gt; resp.__aenter__()
&lt;generator object CoroutineMock._mock_call.&lt;locals&gt;.&lt;lambda&gt; at 0x7effad980048&gt;
ipdb&gt; resp.aenter
&lt;MagicMock name='get().__aenter__().aenter' id='139636643357584'&gt;
ipdb&gt; resp.__aenter__().json()
*** AttributeError: 'generator' object has no attribute 'json'
ipdb&gt; resp.__aenter__()
&lt;generator object CoroutineMock._mock_call.&lt;locals&gt;.&lt;lambda&gt; at 0x7effad912468&gt;
ipdb&gt; resp.json()
&lt;MagicMock name='get().__aenter__().json()' id='139636593767928'&gt;
ipdb&gt; session
&lt;aiohttp.client.ClientSession object at 0x7effb15548d0&gt;
ipdb&gt; next(resp.__aenter__())
TypeError: object MagicMock can't be used in 'await' expression
</code></pre>

<p>So what is proper way to mock async context manager ?</p>
",2235755,482,13-02-2018 07:57,13-02-2018 09:04,0,482,12,1,4,,"{'badge_counts': {'bronze': 12, 'silver': 4, 'gold': 1}, 'account_id': 2578591, 'is_employee': False, 'last_modified_date': 1573681476, 'last_access_date': 1677755499, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 482, 'creation_date': 1364900020, 'user_type': 'registered', 'user_id': 2235755, 'location': 'Russia', 'website_url': 'https://kamyanskiy.github.io', 'link': 'https://stackoverflow.com/users/2235755/alexander-kamyanskiy', 'profile_image': 'https://www.gravatar.com/avatar/47d592700d37c307cc3a2f53f1e3fbb4?s=256&d=identicon&r=PG', 'display_name': 'Alexander Kamyanskiy'}","I have some troubles with mocking aiohttp.client.ClientSession.get context manager. I found some articles and here is one example that seems was working: article 1 So my code that I want to test: async_app.py And test: test_async_app.py When I'm running test, I'm getting an error: So I've tried to debug - here is what I can see: So what is proper way to mock async context manager ?","import random
from aiohttp.client import ClientSession

async def get_random_photo_url():
    while True:
        async with ClientSession() as session:
            async with session.get('random.photos') as resp:
                json = await resp.json()
        photos = json['photos']
        if not photos:
            continue
        return random.choice(photos)['img_src']
 from asynctest import CoroutineMock, MagicMock, patch

from asynctest import TestCase as TestCaseAsync

from async_app import get_random_photo_url


class AsyncContextManagerMock(MagicMock):
    async def __aenter__(self):
        return self.aenter

    async def __aexit__(self, *args):
        pass

class TestAsyncExample(TestCaseAsync):
    @patch('aiohttp.client.ClientSession.get', new_callable=AsyncContextManagerMock)
    async def test_call_api_again_if_photos_not_found(self, mock_get):
        mock_get.return_value.aenter.json = CoroutineMock(side_effect=[{'photos': []},
                                                                       {'photos': [{'img_src': 'a.jpg'}]}])

        image_url = await get_random_photo_url()

        assert mock_get.call_count == 2
        assert mock_get.return_value.aenter.json.call_count == 2
        assert image_url == 'a.jpg'
 (test-0zFWLpVX) ➜  test python -m unittest test_async_app.py -v
test_call_api_again_if_photos_not_found (test_async_app.TestAsyncExample) ... ERROR

======================================================================
ERROR: test_call_api_again_if_photos_not_found (test_async_app.TestAsyncExample)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/case.py"", line 294, in run
    self._run_test_method(testMethod)
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/case.py"", line 351, in _run_test_method
    self.loop.run_until_complete(result)
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/case.py"", line 221, in wrapper
    return method(*args, **kwargs)
  File ""/usr/lib/python3.6/asyncio/base_events.py"", line 467, in run_until_complete
    return future.result()
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/_awaitable.py"", line 21, in wrapper
    return await coroutine(*args, **kwargs)
  File ""/home/kamyanskiy/.local/share/virtualenvs/test-0zFWLpVX/lib/python3.6/site-packages/asynctest/mock.py"", line 588, in __next__
    return self.gen.send(None)
  File ""/home/kamyanskiy/work/test/test_async_app.py"", line 23, in test_call_api_again_if_photos_not_found
    image_url = await get_random_photo_url()
  File ""/home/kamyanskiy/work/test/async_app.py"", line 9, in get_random_photo_url
    json = await resp.json()
TypeError: object MagicMock can't be used in 'await' expression

----------------------------------------------------------------------
Ran 1 test in 0.003s

FAILED (errors=1)
 &gt; /home/kamyanskiy/work/test/async_app.py(10)get_random_photo_url()
      9                 import ipdb; ipdb.set_trace()
---&gt; 10                 json = await resp.json()
     11         photos = json['photos']

ipdb&gt; resp.__aenter__()
&lt;generator object CoroutineMock._mock_call.&lt;locals&gt;.&lt;lambda&gt; at 0x7effad980048&gt;
ipdb&gt; resp.aenter
&lt;MagicMock name='get().__aenter__().aenter' id='139636643357584'&gt;
ipdb&gt; resp.__aenter__().json()
*** AttributeError: 'generator' object has no attribute 'json'
ipdb&gt; resp.__aenter__()
&lt;generator object CoroutineMock._mock_call.&lt;locals&gt;.&lt;lambda&gt; at 0x7effad912468&gt;
ipdb&gt; resp.json()
&lt;MagicMock name='get().__aenter__().json()' id='139636593767928'&gt;
ipdb&gt; session
&lt;aiohttp.client.ClientSession object at 0x7effb15548d0&gt;
ipdb&gt; next(resp.__aenter__())
TypeError: object MagicMock can't be used in 'await' expression
",81,108,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
99,50219904,50220075,10579,django - prefetch only the newest record?,5,<python><django><django-queryset>,14,"<p>I am trying to prefetch only the latest record against the parent record.</p>

<p>my models are as such</p>

<pre><code>class LinkTargets(models.Model):
    device_circuit_subnet = models.ForeignKey(DeviceCircuitSubnets, verbose_name=""Device"", on_delete=models.PROTECT)
    interface_index = models.CharField(max_length=100, verbose_name='Interface index (SNMP)', blank=True, null=True)
    get_bgp = models.BooleanField(default=False, verbose_name=""get BGP Data?"")
    dashboard = models.BooleanField(default=False, verbose_name=""Display on monitoring dashboard?"")


class LinkData(models.Model):
    link_target = models.ForeignKey(LinkTargets, verbose_name=""Link Target"", on_delete=models.PROTECT)
    interface_description = models.CharField(max_length=200, verbose_name='Interface Description', blank=True, null=True)
...
</code></pre>

<p>The below query fails with the error</p>

<pre><code>AttributeError: 'LinkData' object has no attribute '_iterable_class'
</code></pre>

<p>Query: </p>

<pre><code>link_data = LinkTargets.objects.filter(dashboard=True) \
                            .prefetch_related(
                                Prefetch(
                                    'linkdata_set',
                                    queryset=LinkData.objects.all().order_by('-id')[0]
                                    )
                                )
</code></pre>

<p>I thought about getting LinkData instead and doing a select related but ive no idea how to get only 1 record for each link_target_id</p>

<pre><code>link_data = LinkData.objects.filter(link_target__dashboard=True) \
                            .select_related('link_target')..?   
</code></pre>

<p>EDIT:</p>

<p>using rtindru's solution, the pre fetched seems to be empty. there is 6 records in there currently, atest 1 record for each of the 3 LinkTargets</p>

<pre><code>&gt;&gt;&gt; link_data[0]
&lt;LinkTargets: LinkTargets object&gt;
&gt;&gt;&gt; link_data[0].linkdata_set.all()
&lt;QuerySet []&gt;
&gt;&gt;&gt;
</code></pre>
",2261950,2469,07-05-2018 17:54,07-05-2018 18:06,0,2469,161,14,75,86,"{'badge_counts': {'bronze': 161, 'silver': 75, 'gold': 14}, 'account_id': 2611998, 'is_employee': False, 'last_modified_date': 1703296800, 'last_access_date': 1704811636, 'reputation_change_year': -438, 'reputation_change_quarter': -438, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2469, 'creation_date': 1365514516, 'user_type': 'registered', 'user_id': 2261950, 'accept_rate': 86, 'website_url': '', 'link': 'https://stackoverflow.com/users/2261950/alexw', 'profile_image': 'https://www.gravatar.com/avatar/8c44aebbff7a2e5e4a75fc9e7341970d?s=256&d=identicon&r=PG', 'display_name': 'AlexW'}","I am trying to prefetch only the latest record against the parent record. my models are as such The below query fails with the error Query: I thought about getting LinkData instead and doing a select related but ive no idea how to get only 1 record for each link_target_id EDIT: using rtindru's solution, the pre fetched seems to be empty. there is 6 records in there currently, atest 1 record for each of the 3 LinkTargets","class LinkTargets(models.Model):
    device_circuit_subnet = models.ForeignKey(DeviceCircuitSubnets, verbose_name=""Device"", on_delete=models.PROTECT)
    interface_index = models.CharField(max_length=100, verbose_name='Interface index (SNMP)', blank=True, null=True)
    get_bgp = models.BooleanField(default=False, verbose_name=""get BGP Data?"")
    dashboard = models.BooleanField(default=False, verbose_name=""Display on monitoring dashboard?"")


class LinkData(models.Model):
    link_target = models.ForeignKey(LinkTargets, verbose_name=""Link Target"", on_delete=models.PROTECT)
    interface_description = models.CharField(max_length=200, verbose_name='Interface Description', blank=True, null=True)
...
 AttributeError: 'LinkData' object has no attribute '_iterable_class'
 link_data = LinkTargets.objects.filter(dashboard=True) \
                            .prefetch_related(
                                Prefetch(
                                    'linkdata_set',
                                    queryset=LinkData.objects.all().order_by('-id')[0]
                                    )
                                )
 link_data = LinkData.objects.filter(link_target__dashboard=True) \
                            .select_related('link_target')..?   
 &gt;&gt;&gt; link_data[0]
&lt;LinkTargets: LinkTargets object&gt;
&gt;&gt;&gt; link_data[0].linkdata_set.all()
&lt;QuerySet []&gt;
&gt;&gt;&gt;
",21,49,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
100,49620487,49620642,24735,Why changing start method to 'spawn' from 'fork' in Python multiprocessing does not allow me run my job anymore?,1,<python><python-3.x><multiprocessing><python-multiprocessing><spawn>,15,"<p>I am able to run a background function using <code>multiprocessing.Process</code> with the start method <code>fork</code>. For some reason, I need this child process to start a new environment when running. So I set the start method to <code>spawn</code> via <code>multiprocessing.set_start_method('spawn')</code> and run the job via <code>job.start()</code> I get the following error:</p>

<pre><code>Can't pickle &lt;class 'module'&gt;: attribute lookup module on builtins failed
</code></pre>

<p>However, I do not use pickle for anything within the function that I am calling. What could I be doing wrong? Is there a general rule of thumb that I should have followed when running processes in <code>spawn</code> mode?</p>

<p>FYI: I am on a machine with Ubuntu 16.04</p>
",2838606,10800,03-04-2018 00:17,03-04-2018 00:42,0,10808,75,10,49,66,"{'badge_counts': {'bronze': 75, 'silver': 49, 'gold': 10}, 'account_id': 3381683, 'is_employee': False, 'last_modified_date': 1711053900, 'last_access_date': 1702931293, 'reputation_change_year': 66, 'reputation_change_quarter': 66, 'reputation_change_month': 26, 'reputation_change_week': -2, 'reputation_change_day': 0, 'reputation': 10808, 'creation_date': 1380716718, 'user_type': 'registered', 'user_id': 2838606, 'accept_rate': 66, 'website_url': 'http://amir-arsalan.github.io/', 'link': 'https://stackoverflow.com/users/2838606/amir', 'profile_image': 'https://i.stack.imgur.com/OQM4l.jpg?s=256&g=1', 'display_name': 'Amir'}","I am able to run a background function using with the start method . For some reason, I need this child process to start a new environment when running. So I set the start method to via and run the job via I get the following error: However, I do not use pickle for anything within the function that I am calling. What could I be doing wrong? Is there a general rule of thumb that I should have followed when running processes in mode? FYI: I am on a machine with Ubuntu 16.04","multiprocessing.Process fork spawn multiprocessing.set_start_method('spawn') job.start() Can't pickle &lt;class 'module'&gt;: attribute lookup module on builtins failed
 spawn",-6,8,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
101,48910214,48910727,56340,"Celery, calling delay with countdown",2,<python><django><celery><django-celery>,25,"<p>I'm trying to understand how celery works</p>

<p>In my django application in <code>tasks.py</code> file I have created one task:</p>

<pre><code>@celery.shared_task(default_retry_delay=2 * 60, max_retries=2)
def my_task(param1, param2):
    # There are some operations
</code></pre>

<p>I call this task by using this code:</p>

<pre><code>my_task.delay(param1, param2)
</code></pre>

<p>Inside of this <code>my_task</code> there is one condition where this task should be started again <strong>but after one minute delay</strong></p>

<p>I have found that there are some kind of <a href=""http://docs.celeryproject.org/en/latest/userguide/calling.html#eta-and-countdown"" rel=""noreferrer"">ETA and countdown</a> for tasks, but their examples are only with <code>apply_async</code></p>

<p>Is it possible to use some kind countdown for <code>delay</code>?</p>
",3003432,7663,21-02-2018 15:57,21-02-2018 16:23,0,7683,122,14,66,86,"{'badge_counts': {'bronze': 122, 'silver': 66, 'gold': 14}, 'account_id': 3599915, 'is_employee': False, 'last_modified_date': 1709345100, 'last_access_date': 1710663700, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 60, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 7683, 'creation_date': 1384752061, 'user_type': 'registered', 'user_id': 3003432, 'accept_rate': 86, 'location': 'Jupiter', 'website_url': '', 'link': 'https://stackoverflow.com/users/3003432/mr-d', 'profile_image': 'https://www.gravatar.com/avatar/fee8735eb951c3625fe5a34fd8e6f37d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Mr.D'}","I'm trying to understand how celery works In my django application in file I have created one task: I call this task by using this code: Inside of this there is one condition where this task should be started again but after one minute delay I have found that there are some kind of ETA and countdown for tasks, but their examples are only with Is it possible to use some kind countdown for ?","tasks.py @celery.shared_task(default_retry_delay=2 * 60, max_retries=2)
def my_task(param1, param2):
    # There are some operations
 my_task.delay(param1, param2)
 my_task apply_async delay",-2,19,0,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
102,48549326,48549560,35240,PySpark filter using startswith from list,3,<python><list><apache-spark><pyspark><filter>,13,"<p>I have a list of elements that may start a couple of strings that are of record in an RDD. If I have and element list of &quot;yes&quot; and &quot;no&quot;, they should match &quot;yes23&quot; and &quot;no3&quot; but not &quot;35yes&quot; or &quot;41no&quot;. Using PySpark, how can I use <code>startswith</code> to match elements in list or tuple.</p>
<p>An example DF would be:</p>
<pre class=""lang-none prettyprint-override""><code>+-----+------+
|index| label|
+-----+------+
|    1|yes342|
|    2| 45yes|
|    3| no123|
|    4|  75no|
+-----+------+
</code></pre>
<p>When I try:</p>
<pre class=""lang-py prettyprint-override""><code>Element_List = ['yes', 'no']
filter_DF = DF.where(DF.label.startswith(tuple(Element_List)))
</code></pre>
<p>The resulting df should look something like:</p>
<pre class=""lang-none prettyprint-override""><code>+-----+------+
|index| label|
+-----+------+
|    1|yes342|
|    3| no123|
+-----+------+
</code></pre>
<p>Instead, I get the error:</p>
<blockquote>
<pre class=""lang-none prettyprint-override""><code>Py4JError: An error occurred while calling o250.startsWith. Trace:
py4j.Py4JException: Method startsWith([class java.util.ArrayList]) does not exist
at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
at py4j.Gateway.invoke(Gateway.java:272)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:214)
at java.lang.Thread.run(Thread.java:745)
</code></pre>
</blockquote>
<p>It looks like <code>startsWith</code> can't be used with any type of list. Is there a simple workaround?</p>
",3010960,2000,31-01-2018 18:43,31-01-2018 19:00,0,2000,27,3,20,73,"{'badge_counts': {'bronze': 27, 'silver': 20, 'gold': 3}, 'account_id': 3609640, 'is_employee': False, 'last_modified_date': 1621038900, 'last_access_date': 1643904426, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2000, 'creation_date': 1384902971, 'user_type': 'registered', 'user_id': 3010960, 'accept_rate': 73, 'website_url': '', 'link': 'https://stackoverflow.com/users/3010960/jenks', 'profile_image': 'https://www.gravatar.com/avatar/28322f85b131084c4f92dc09b55f0f10?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jenks'}","I have a list of elements that may start a couple of strings that are of record in an RDD. If I have and element list of &quot;yes&quot; and &quot;no&quot;, they should match &quot;yes23&quot; and &quot;no3&quot; but not &quot;35yes&quot; or &quot;41no&quot;. Using PySpark, how can I use to match elements in list or tuple. An example DF would be: When I try: The resulting df should look something like: Instead, I get the error: It looks like can't be used with any type of list. Is there a simple workaround?","startswith +-----+------+
|index| label|
+-----+------+
|    1|yes342|
|    2| 45yes|
|    3| no123|
|    4|  75no|
+-----+------+
 Element_List = ['yes', 'no']
filter_DF = DF.where(DF.label.startswith(tuple(Element_List)))
 +-----+------+
|index| label|
+-----+------+
|    1|yes342|
|    3| no123|
+-----+------+
 Py4JError: An error occurred while calling o250.startsWith. Trace:
py4j.Py4JException: Method startsWith([class java.util.ArrayList]) does not exist
at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
at py4j.Gateway.invoke(Gateway.java:272)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:214)
at java.lang.Thread.run(Thread.java:745)
 startsWith",19,37,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
103,50215824,50215974,12765,The tasks from asyncio.gather does not work concurrently,3,<python><python-asyncio>,18,"<p>I want to scrape data from a website concurrently, but I found that the following program is NOT executed concurrently.</p>

<pre><code>async def return_soup(url):
    r = requests.get(url)
    r.encoding = ""utf-8""
    soup = BeautifulSoup(r.text, ""html.parser"")

    future = asyncio.Future()
    future.set_result(soup)
    return future

async def parseURL_async(url):    
    print(""Started to download {0}"".format(url))
    soup = await return_soup(url)
    print(""Finished downloading {0}"".format(url))

    return soup

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)
t = [parseURL_async(url_1), parseURL_async(url_2)]
loop.run_until_complete(asyncio.gather(*t))
</code></pre>

<p>However, this program starts to download the second content only after the first one finishes. If my understanding is correct, the <code>await</code> keyword on the <code>await return_soup(url)</code> awaits for the function to be complete, and while waiting for the completion, it returns back the control to the event loop, which enables the loop to start the second download. </p>

<p>And once the function finally finishes the execution, the future instance within it gets the result value.</p>

<p>But why does this not work concurrently? What am I missing here?</p>
",2360798,31455,07-05-2018 13:46,07-05-2018 13:52,0,31465,236,53,153,80,"{'badge_counts': {'bronze': 236, 'silver': 153, 'gold': 53}, 'account_id': 2738792, 'is_employee': False, 'last_modified_date': 1710465300, 'last_access_date': 1707209427, 'reputation_change_year': 210, 'reputation_change_quarter': 210, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 31465, 'creation_date': 1367985900, 'user_type': 'registered', 'user_id': 2360798, 'accept_rate': 80, 'location': 'All over the world', 'website_url': 'http://None', 'link': 'https://stackoverflow.com/users/2360798/blaszard', 'profile_image': 'https://www.gravatar.com/avatar/72bd5e3f73d1cd1774d2235b6bfab11d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Blaszard'}","I want to scrape data from a website concurrently, but I found that the following program is NOT executed concurrently. However, this program starts to download the second content only after the first one finishes. If my understanding is correct, the keyword on the awaits for the function to be complete, and while waiting for the completion, it returns back the control to the event loop, which enables the loop to start the second download. And once the function finally finishes the execution, the future instance within it gets the result value. But why does this not work concurrently? What am I missing here?","async def return_soup(url):
    r = requests.get(url)
    r.encoding = ""utf-8""
    soup = BeautifulSoup(r.text, ""html.parser"")

    future = asyncio.Future()
    future.set_result(soup)
    return future

async def parseURL_async(url):    
    print(""Started to download {0}"".format(url))
    soup = await return_soup(url)
    print(""Finished downloading {0}"".format(url))

    return soup

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)
t = [parseURL_async(url_1), parseURL_async(url_2)]
loop.run_until_complete(asyncio.gather(*t))
 await await return_soup(url)",17,29,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
104,49817786,49823013,18136,How to preselect (set default) python interpreter in python visual code extension?,4,<python><visual-studio-code>,18,"<p>The small problem is, when I press <kbd>Ctrl</kbd>+<kbd>F5</kbd>, I want the code to be run immediately; but I have to</p>

<pre>Select environment:

Python
Python Exprimental</pre>

<p>all the time. Is there a way to set default env in settings so I don't have to choose at each run?</p>
",3036878,647,13-04-2018 13:07,13-04-2018 18:18,0,672,26,2,8,75,"{'badge_counts': {'bronze': 26, 'silver': 8, 'gold': 2}, 'account_id': 3643217, 'is_employee': False, 'last_modified_date': 1679428500, 'last_access_date': 1711115872, 'reputation_change_year': 35, 'reputation_change_quarter': 35, 'reputation_change_month': 25, 'reputation_change_week': 25, 'reputation_change_day': 0, 'reputation': 672, 'creation_date': 1385476619, 'user_type': 'registered', 'user_id': 3036878, 'accept_rate': 75, 'website_url': '', 'link': 'https://stackoverflow.com/users/3036878/emil', 'profile_image': 'https://graph.facebook.com/100000036507438/picture?type=large', 'display_name': 'Emil'}","The small problem is, when I press Ctrl+F5, I want the code to be run immediately; but I have to Select environment: Python Python Exprimental all the time. Is there a way to set default env in settings so I don't have to choose at each run?",,0,8,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
105,48172928,48174228,12196,Scale matplotlib.pyplot.Axes.scatter markersize by x-scale,1,<python><matplotlib><scatter>,17,"<p>I would like to scale the <code>markersize</code> of <code>matplotlib.pyplot.Axes.scatter</code> plot based on the number of points on the x/y-axis.</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

vmin = 1
vmax = 11

x = np.random.randint(vmin, vmax, 5)
y = np.random.randint(vmin, vmax, 5)

fig, ax = plt.subplots()
for v in np.arange(vmin, vmax):
    ax.axvline(v - 0.5)
    ax.axvline(v + 0.5)
    ax.axhline(v - 0.5)
    ax.axhline(v + 0.5)

ax.set_xlim(vmin - 0.5, vmax + 0.5)
ax.set_ylim(vmin - 0.5, vmax + 0.5)
ax.scatter(x, y)

ax.set_aspect(1)
plt.show()
</code></pre>

<p><code>ax</code> is always using an equal aspect ratio and both axes have the same <code>lim</code> values. </p>

<p>Currently, running the above generates the following plot ...
<a href=""https://i.stack.imgur.com/3RZEa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3RZEa.png"" alt=""enter image description here""></a></p>

<p>...and changing the value of <code>vmax = 41</code>
<a href=""https://i.stack.imgur.com/A7Fx4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/A7Fx4.png"" alt=""enter image description here""></a></p>

<p>The <code>markersize</code> in both plots is left to the default, i.e. <code>markersize=6</code>. </p>

<p>My question is, how could I compute the <code>markersize</code> value so the <code>marker</code>s touch the edges of each cell? (Each cell has a maximum of one data point.)</p>
",3046533,1098,09-01-2018 16:47,09-01-2018 18:05,0,1098,21,2,9,100,"{'badge_counts': {'bronze': 21, 'silver': 9, 'gold': 2}, 'account_id': 3386181, 'is_employee': False, 'last_modified_date': 1611363300, 'last_access_date': 1671810337, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1098, 'creation_date': 1385653047, 'user_type': 'registered', 'user_id': 3046533, 'accept_rate': 100, 'location': 'London, UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/3046533/fsimkovic', 'profile_image': 'https://www.gravatar.com/avatar/af9687e43660736eb65ecc95bd446511?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'fsimkovic'}","I would like to scale the of plot based on the number of points on the x/y-axis. is always using an equal aspect ratio and both axes have the same values. Currently, running the above generates the following plot ... ...and changing the value of The in both plots is left to the default, i.e. . My question is, how could I compute the value so the s touch the edges of each cell? (Each cell has a maximum of one data point.)","markersize matplotlib.pyplot.Axes.scatter import matplotlib.pyplot as plt
import numpy as np

vmin = 1
vmax = 11

x = np.random.randint(vmin, vmax, 5)
y = np.random.randint(vmin, vmax, 5)

fig, ax = plt.subplots()
for v in np.arange(vmin, vmax):
    ax.axvline(v - 0.5)
    ax.axvline(v + 0.5)
    ax.axhline(v - 0.5)
    ax.axhline(v + 0.5)

ax.set_xlim(vmin - 0.5, vmax + 0.5)
ax.set_ylim(vmin - 0.5, vmax + 0.5)
ax.scatter(x, y)

ax.set_aspect(1)
plt.show()
 ax lim vmax = 41 markersize markersize=6 markersize marker",12,37,2,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
106,49384682,49384823,80166,How to iterate 1d NumPy array with index and value,2,<python><arrays><numpy><indexing><iterator>,33,"<p>For python <code>dict</code>, I could use <code>iteritems()</code> to loop through key and value at the same time. But I cannot find such functionality for NumPy array. I have to manually track <code>idx</code> like this:</p>

<pre><code>idx = 0 
for j in theta:
   some_function(idx,j,theta)
   idx += 1
</code></pre>

<p>Is there a better way to do this?</p>
",3707564,1842,20-03-2018 12:50,20-03-2018 12:56,0,1842,51,8,30,79,"{'badge_counts': {'bronze': 51, 'silver': 30, 'gold': 8}, 'account_id': 3382713, 'is_employee': False, 'last_modified_date': 1707529500, 'last_access_date': 1710464516, 'reputation_change_year': 4, 'reputation_change_quarter': 4, 'reputation_change_month': -50, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1842, 'creation_date': 1401893262, 'user_type': 'registered', 'user_id': 3707564, 'accept_rate': 79, 'link': 'https://stackoverflow.com/users/3707564/user40780', 'profile_image': 'https://www.gravatar.com/avatar/6789da87cc115cbd19413540aad9efac?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user40780'}","For python , I could use to loop through key and value at the same time. But I cannot find such functionality for NumPy array. I have to manually track like this: Is there a better way to do this?","dict iteritems() idx idx = 0 
for j in theta:
   some_function(idx,j,theta)
   idx += 1
",0,9,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
107,49283336,49283823,7647,"What does it mean for a tensor to have shape [None, x] in TensorFlow?",1,<python><tensorflow>,11,"<p>If I define a tf placeholder as:</p>

<pre><code>tf.placeholder(""float"", [None, 10])
</code></pre>

<p>what does the <code>None</code> do? What is the linear algebra interpretation of this? A horizontal vector?</p>
",3128156,8208,14-03-2018 16:43,14-03-2018 17:09,0,8228,145,25,74,71,"{'badge_counts': {'bronze': 145, 'silver': 74, 'gold': 25}, 'account_id': 3764201, 'is_employee': False, 'last_modified_date': 1711158000, 'last_access_date': 1706098027, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 8228, 'creation_date': 1387758204, 'user_type': 'registered', 'user_id': 3128156, 'accept_rate': 71, 'location': 'Munich, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/3128156/sahand', 'profile_image': 'https://i.stack.imgur.com/h3FIS.jpg?s=256&g=1', 'display_name': 'Sahand'}",If I define a tf placeholder as: what does the do? What is the linear algebra interpretation of this? A horizontal vector?,"tf.placeholder(""float"", [None, 10])
 None",-1,6,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
108,49061521,49073142,20331,Projection of a point to a line segment Python Shapely,2,<python><gis><point><projection><shapely>,17,"<p>I have a LineString defined by two points, so essentially a straight line segment, and I wanted to project a point on to it. I am aware of <code>.project</code> and <code>.interpolate</code>. However when the point is ""outside"" the segment, I don't want the closest point on the segment, but I want to extend the segment and draw a line going through the point and is orthogonal to the (extended) line segment. I want the coordinate of the projection.</p>

<p><a href=""https://i.stack.imgur.com/s8bZ3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/s8bZ3.png"" alt=""enter image description here""></a></p>

<p>For example if the point is ""within"" the segment</p>

<pre><code>from shapely.geometry import Point
from shapely.geometry import LineString
point = Point(0.2, 0.5)
dist = LineString([(0, 1), (1, 1)]).project(point)
list(LineString([(0, 1), (1, 1)]).interpolate(dist).coords)
</code></pre>

<p>Anyone knows what to do when the point is outside of the segment?</p>
",3265397,275,02-03-2018 01:43,02-03-2018 16:17,0,275,6,1,2,,"{'badge_counts': {'bronze': 6, 'silver': 2, 'gold': 1}, 'account_id': 1789096, 'is_employee': False, 'last_modified_date': 1628933400, 'last_access_date': 1612025198, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 275, 'creation_date': 1391422812, 'user_type': 'registered', 'user_id': 3265397, 'location': 'Ithaca, NY, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/3265397/sofaruntitled', 'profile_image': 'https://www.gravatar.com/avatar/48a31e4bf77b913855537299d132ff5d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'sofaruntitled'}","I have a LineString defined by two points, so essentially a straight line segment, and I wanted to project a point on to it. I am aware of and . However when the point is ""outside"" the segment, I don't want the closest point on the segment, but I want to extend the segment and draw a line going through the point and is orthogonal to the (extended) line segment. I want the coordinate of the projection. For example if the point is ""within"" the segment Anyone knows what to do when the point is outside of the segment?",".project .interpolate from shapely.geometry import Point
from shapely.geometry import LineString
point = Point(0.2, 0.5)
dist = LineString([(0, 1), (1, 1)]).project(point)
list(LineString([(0, 1), (1, 1)]).interpolate(dist).coords)
",2,14,1,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
109,49330080,49330324,12205,NumPy 2D array: selecting indices in a circle,2,<python><arrays><numpy><indexing>,14,"<p>For some rectangular we can select all indices in a 2D array very efficiently:</p>

<pre><code>arr[y:y+height, x:x+width]
</code></pre>

<p>...where <code>(x, y)</code> is the upper-left corner of the rectangle and <code>height</code> and <code>width</code> the height (number of rows) and width (number of columns) of the rectangular selection.</p>

<p>Now, let's say we want to select all indices in a 2D array located in a certain circle given center coordinates <code>(cx, cy)</code> and radius <code>r</code>. Is there a numpy function to achieve this efficiently?</p>

<p>Currently I am pre-computing the indices manually by having a Python loop that adds indices into a buffer (list). Thus, this is pretty inefficent for large 2D arrays, since I need to queue up every integer lying in some circle.</p>

<pre><code># buffer for x &amp; y indices
indices_x = list()
indices_y = list()

# lower and upper index range
x_lower, x_upper = int(max(cx-r, 0)), int(min(cx+r, arr.shape[1]-1))
y_lower, y_upper = int(max(cy-r, 0)), int(min(cy+r, arr.shape[0]-1))
range_x = range(x_lower, x_upper)
range_y = range(y_lower, y_upper)

# loop over all indices
for y, x in product(range_y, range_x):
    # check if point lies within radius r
    if (x-cx)**2 + (y-cy)**2 &lt; r**2:
        indices_y.append(y)
        indices_x.append(x)

# circle indexing
arr[(indices_y, indices_x)]
</code></pre>

<p>As mentioned, this procedure gets quite inefficient for larger arrays / circles. Any ideas for speeding things up?</p>

<p>If there is a better way to index a circle, does this also apply for ""arbitrary"" 2D shapes? For example, could I somehow pass a function that expresses membership of points for an arbitrary shape to get the corresponding numpy indices of an array?</p>
",3314143,10764,16-03-2018 21:58,16-03-2018 22:27,0,10784,127,20,70,75,"{'badge_counts': {'bronze': 127, 'silver': 70, 'gold': 20}, 'account_id': 4025821, 'is_employee': False, 'last_modified_date': 1700442600, 'last_access_date': 1711029065, 'reputation_change_year': 68, 'reputation_change_quarter': 68, 'reputation_change_month': 50, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 10784, 'creation_date': 1392489820, 'user_type': 'registered', 'user_id': 3314143, 'accept_rate': 75, 'website_url': '', 'link': 'https://stackoverflow.com/users/3314143/daniel451', 'profile_image': 'https://i.stack.imgur.com/RpemN.png?s=256&g=1', 'display_name': 'daniel451'}","For some rectangular we can select all indices in a 2D array very efficiently: ...where is the upper-left corner of the rectangle and and the height (number of rows) and width (number of columns) of the rectangular selection. Now, let's say we want to select all indices in a 2D array located in a certain circle given center coordinates and radius . Is there a numpy function to achieve this efficiently? Currently I am pre-computing the indices manually by having a Python loop that adds indices into a buffer (list). Thus, this is pretty inefficent for large 2D arrays, since I need to queue up every integer lying in some circle. As mentioned, this procedure gets quite inefficient for larger arrays / circles. Any ideas for speeding things up? If there is a better way to index a circle, does this also apply for ""arbitrary"" 2D shapes? For example, could I somehow pass a function that expresses membership of points for an arbitrary shape to get the corresponding numpy indices of an array?","arr[y:y+height, x:x+width]
 (x, y) height width (cx, cy) r # buffer for x &amp; y indices
indices_x = list()
indices_y = list()

# lower and upper index range
x_lower, x_upper = int(max(cx-r, 0)), int(min(cx+r, arr.shape[1]-1))
y_lower, y_upper = int(max(cy-r, 0)), int(min(cy+r, arr.shape[0]-1))
range_x = range(x_lower, x_upper)
range_y = range(y_lower, y_upper)

# loop over all indices
for y, x in product(range_y, range_x):
    # check if point lies within radius r
    if (x-cx)**2 + (y-cy)**2 &lt; r**2:
        indices_y.append(y)
        indices_x.append(x)

# circle indexing
arr[(indices_y, indices_x)]
",13,35,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
110,48506460,48507121,120848,Python simple socket client/server using asyncio,2,<python><sockets><python-asyncio>,61,"<p>I would like to re-implement my code using asyncio coroutines instead of multi-threading.</p>

<p>server.py</p>

<pre><code>def handle_client(client):
    request = None
    while request != 'quit':
        request = client.recv(255).decode('utf8')
        response = cmd.run(request)
        client.send(response.encode('utf8'))
    client.close()

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(('localhost', 15555))
server.listen(8)

try:
    while True:
        client, _ = server.accept()
        threading.Thread(target=handle_client, args=(client,)).start()
except KeyboardInterrupt:
    server.close()
</code></pre>

<p>client.py</p>

<pre><code>server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.connect(('localhost', 15555))
request = None

try:
    while request != 'quit':
        request = input('&gt;&gt; ')
        if request:
            server.send(request.encode('utf8'))
            response = server.recv(255).decode('utf8')
            print(response)
except KeyboardInterrupt:
    server.close()
</code></pre>

<p>I know there are some appropriate asynchronous network librairies to do that. But I just want to only use asyncio core library on this case in order to have a better understanding of it.</p>

<p>It would have been so nice to only add async keyword before handle client definition... Here a piece of code which seems to work, but I'm still confused about the implementation.</p>

<p>asyncio_server.py</p>

<pre><code>def handle_client(client):
    request = None
    while request != 'quit':
        request = client.recv(255).decode('utf8')
        response = cmd.run(request)
        client.send(response.encode('utf8'))
    client.close()

def run_server(server):
    client, _ = server.accept()
    handle_client(client)

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(('localhost', 15555))
server.listen(8)

loop = asyncio.get_event_loop()
asyncio.async(run_server(server))
try:
    loop.run_forever()
except KeyboardInterrupt:
    server.close()
</code></pre>

<p>How adapt this in the best way and using async await keywords.</p>
",3316406,1009,29-01-2018 17:06,29-01-2018 17:47,0,1019,17,1,9,71,"{'badge_counts': {'bronze': 17, 'silver': 9, 'gold': 1}, 'account_id': 4029821, 'is_employee': False, 'last_modified_date': 1655512200, 'last_access_date': 1710846706, 'reputation_change_year': 59, 'reputation_change_quarter': 59, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1019, 'creation_date': 1392567706, 'user_type': 'registered', 'user_id': 3316406, 'accept_rate': 71, 'location': 'Paris', 'website_url': '', 'link': 'https://stackoverflow.com/users/3316406/srjjio', 'profile_image': 'https://i.stack.imgur.com/KqEUO.jpg?s=256&g=1', 'display_name': 'srjjio'}","I would like to re-implement my code using asyncio coroutines instead of multi-threading. server.py client.py I know there are some appropriate asynchronous network librairies to do that. But I just want to only use asyncio core library on this case in order to have a better understanding of it. It would have been so nice to only add async keyword before handle client definition... Here a piece of code which seems to work, but I'm still confused about the implementation. asyncio_server.py How adapt this in the best way and using async await keywords.","def handle_client(client):
    request = None
    while request != 'quit':
        request = client.recv(255).decode('utf8')
        response = cmd.run(request)
        client.send(response.encode('utf8'))
    client.close()

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(('localhost', 15555))
server.listen(8)

try:
    while True:
        client, _ = server.accept()
        threading.Thread(target=handle_client, args=(client,)).start()
except KeyboardInterrupt:
    server.close()
 server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.connect(('localhost', 15555))
request = None

try:
    while request != 'quit':
        request = input('&gt;&gt; ')
        if request:
            server.send(request.encode('utf8'))
            response = server.recv(255).decode('utf8')
            print(response)
except KeyboardInterrupt:
    server.close()
 def handle_client(client):
    request = None
    while request != 'quit':
        request = client.recv(255).decode('utf8')
        response = cmd.run(request)
        client.send(response.encode('utf8'))
    client.close()

def run_server(server):
    client, _ = server.accept()
    handle_client(client)

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(('localhost', 15555))
server.listen(8)

loop = asyncio.get_event_loop()
asyncio.async(run_server(server))
try:
    loop.run_forever()
except KeyboardInterrupt:
    server.close()
",50,72,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
111,48645942,48646058,29218,How do I format a multi-line TODO comment in PyCharm?,2,<python><pycharm><comments><multiline><todo>,25,"<p>I want to add a multi-line TODO comment to my PyCharm project.</p>

<pre><code># TODO: Multiple errors can be wrapped inside an exception.
#       WfcApiException should do recursive error checking to locate
#       and store an arbitrary number of nested errors.
</code></pre>

<p>Unfortunately, PyCharm only recognizes the first line as a TODO comment. Any following lines are viewed as standard Python comments.</p>

<p><a href=""https://i.stack.imgur.com/99qAn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/99qAn.png"" alt=""Failed attempt at a multi-line TODO statement in PyCharm""></a></p>

<p>What is the correct way to format a multi-line TODO comment in PyCharm?</p>
",3357935,25006,06-02-2018 15:00,06-02-2018 15:06,0,25082,230,29,128,82,"{'badge_counts': {'bronze': 230, 'silver': 128, 'gold': 29}, 'account_id': 4030991, 'is_employee': False, 'last_modified_date': 1703299500, 'last_access_date': 1711164785, 'reputation_change_year': 482, 'reputation_change_quarter': 482, 'reputation_change_month': 114, 'reputation_change_week': 26, 'reputation_change_day': 0, 'reputation': 25082, 'creation_date': 1393451089, 'user_type': 'registered', 'user_id': 3357935, 'accept_rate': 82, 'location': 'NY, USA', 'link': 'https://stackoverflow.com/users/3357935/stevoisiak', 'profile_image': 'https://i.stack.imgur.com/jaomO.png?s=256&g=1', 'display_name': 'Stevoisiak'}","I want to add a multi-line TODO comment to my PyCharm project. Unfortunately, PyCharm only recognizes the first line as a TODO comment. Any following lines are viewed as standard Python comments. What is the correct way to format a multi-line TODO comment in PyCharm?","# TODO: Multiple errors can be wrapped inside an exception.
#       WfcApiException should do recursive error checking to locate
#       and store an arbitrary number of nested errors.
",2,12,1,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
112,49072942,50650817,3988,"How can I add a ""show details"" button to a tkinter messagebox?",1,<python><tkinter><tkmessagebox>,11,"<p>I have a Python script which uses <code>tkinter.messagebox</code> to display an error message with traceback details if an unexpected exception occurs.</p>

<pre><code>import tkinter.messagebox as tm
import traceback

try:
    1/0
except Exception as error:
    tm.showerror(title=""Error"",
                 message=""An error has occurred: '"" + str(error) + ""'."",
                 detail=traceback.format_exc())
</code></pre>

<p><a href=""https://i.stack.imgur.com/XEmTp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XEmTp.png"" alt=""Standard tkinter error""></a></p>

<p>Displaying tracebacks this way has a few drawbacks.</p>

<ul>
<li>Traceback details <a href=""https://softwareengineering.stackexchange.com/a/245258/168744"">aren't helpful for the average user</a>.</li>
<li>Testers can't easily select and copy text from a messagebox</li>
<li>Complex errors can have <a href=""https://i.stack.imgur.com/iVEqE.png"" rel=""noreferrer"">large tracebacks</a> which span dozens of lines.</li>
</ul>

<p>Instead of displaying error details by default, I would like to add a ""show details"" button which would display more information in a read-only text field.</p>

<p><a href=""https://i.stack.imgur.com/P6YXH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/P6YXH.png"" alt=""Detailed error for &quot;division by zero&quot;""></a></p>

<p>How can I add a ""show details"" button to a tkinter messagebox?</p>
",3357935,25006,02-03-2018 16:06,01-06-2018 20:48,91,25082,230,29,128,82,"{'badge_counts': {'bronze': 230, 'silver': 128, 'gold': 29}, 'account_id': 4030991, 'is_employee': False, 'last_modified_date': 1703299500, 'last_access_date': 1711164785, 'reputation_change_year': 482, 'reputation_change_quarter': 482, 'reputation_change_month': 114, 'reputation_change_week': 26, 'reputation_change_day': 0, 'reputation': 25082, 'creation_date': 1393451089, 'user_type': 'registered', 'user_id': 3357935, 'accept_rate': 82, 'location': 'NY, USA', 'link': 'https://stackoverflow.com/users/3357935/stevoisiak', 'profile_image': 'https://i.stack.imgur.com/jaomO.png?s=256&g=1', 'display_name': 'Stevoisiak'}","I have a Python script which uses to display an error message with traceback details if an unexpected exception occurs. Displaying tracebacks this way has a few drawbacks. Traceback details aren't helpful for the average user. Testers can't easily select and copy text from a messagebox Complex errors can have large tracebacks which span dozens of lines. Instead of displaying error details by default, I would like to add a ""show details"" button which would display more information in a read-only text field. How can I add a ""show details"" button to a tkinter messagebox?","tkinter.messagebox import tkinter.messagebox as tm
import traceback

try:
    1/0
except Exception as error:
    tm.showerror(title=""Error"",
                 message=""An error has occurred: '"" + str(error) + ""'."",
                 detail=traceback.format_exc())
",7,28,2,4,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
113,48522609,48522820,37186,How to retrieve model estimates from statsmodels?,2,<python><statsmodels>,19,"<p>From a dataset like this:</p>

<pre><code>import pandas as pd
import numpy as np
import statsmodels.api as sm

# A dataframe with two variables
np.random.seed(123)
rows = 12
rng = pd.date_range('1/1/2017', periods=rows, freq='D')
df = pd.DataFrame(np.random.randint(100,150,size=(rows, 2)), columns=['y', 'x']) 
df = df.set_index(rng)
</code></pre>

<p><a href=""https://i.stack.imgur.com/XaK90.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XaK90.png"" alt=""enter image description here""></a></p>

<p>...and a linear regression model like this:</p>

<pre><code>x = sm.add_constant(df['x'])
model = sm.OLS(df['y'], x).fit()
</code></pre>

<p>... you can easily retrieve some model coefficients this way:</p>

<pre><code>print(model.params)
</code></pre>

<p><a href=""https://i.stack.imgur.com/jHV2z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jHV2z.png"" alt=""enter image description here""></a></p>

<p>But I just can't find out how to retrieve all other parameters from the model summary:</p>

<pre><code>print(str(model.summary()))
</code></pre>

<p><a href=""https://i.stack.imgur.com/TdmHU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TdmHU.png"" alt=""enter image description here""></a></p>

<p>As stated in the question, I'm particularly interested in <strong>R-squared</strong>.</p>

<p>From the post <a href=""https://stackoverflow.com/questions/37508158/how-to-extract-a-particular-value-from-the-ols-summary-in-pandas"">How to extract a particular value from the OLS-summary in Pandas?</a> I learned that you could just use <code>print(model.r2)</code> to do the same thing there. But that does not seem to work for statsmodels.</p>

<p>Any suggestions?</p>
",3437787,58000,30-01-2018 13:27,30-01-2018 13:38,0,58150,321,39,198,100,"{'badge_counts': {'bronze': 321, 'silver': 198, 'gold': 39}, 'collectives': [{'collective': {'tags': ['stringr', 'r-package', 'dtplyr', 'tidyr', 'r-raster', 'plyr', 'quantmod', 'dplyr', 'purrr', 'shiny', 'forcats', 'r-caret', 'shiny-server', 'zoo', 'rstudio', 'knitr', 'shinydashboard', 'lubridate', 'rvest', 'shinyapps', 'r', 'ggplot2', 'data.table', 'rlang', 'readr', 'tidyverse', 'tibble'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where data scientists and AI researchers gather to find, share, and learn about R and other subtags like knitr and dplyr.', 'link': '/collectives/r-language', 'name': 'R Language', 'slug': 'r-language'}, 'role': 'member'}, {'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 4196502, 'is_employee': False, 'last_modified_date': 1703311200, 'last_access_date': 1711133929, 'reputation_change_year': 1406, 'reputation_change_quarter': 1406, 'reputation_change_month': 380, 'reputation_change_week': 60, 'reputation_change_day': 0, 'reputation': 58150, 'creation_date': 1395235213, 'user_type': 'registered', 'user_id': 3437787, 'accept_rate': 100, 'location': 'Norway', 'website_url': '', 'link': 'https://stackoverflow.com/users/3437787/vestland', 'profile_image': 'https://i.stack.imgur.com/W8tmU.jpg?s=256&g=1', 'display_name': 'vestland'}","From a dataset like this: ...and a linear regression model like this: ... you can easily retrieve some model coefficients this way: But I just can't find out how to retrieve all other parameters from the model summary: As stated in the question, I'm particularly interested in R-squared. From the post How to extract a particular value from the OLS-summary in Pandas? I learned that you could just use to do the same thing there. But that does not seem to work for statsmodels. Any suggestions?","import pandas as pd
import numpy as np
import statsmodels.api as sm

# A dataframe with two variables
np.random.seed(123)
rows = 12
rng = pd.date_range('1/1/2017', periods=rows, freq='D')
df = pd.DataFrame(np.random.randint(100,150,size=(rows, 2)), columns=['y', 'x']) 
df = df.set_index(rng)
 x = sm.add_constant(df['x'])
model = sm.OLS(df['y'], x).fit()
 print(model.params)
 print(str(model.summary()))
 print(model.r2)",9,41,3,4,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Intermediate**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
114,49195189,49199132,18904,Error loading the saved optimizer. keras python raspberry,2,<python><tensorflow><raspberry-pi><keras>,11,"<p>I have trained a keras sequential model in a linux 64 machine and saved to a .h5 file.</p>

<p>It this PC I can load the model and do predictions without problems.</p>

<p>Now I'm implementing the prediction in a Raspberry Pi 3 that have installed keras, tensorflow, h5py and python3.</p>

<p>when I load the model</p>

<pre><code>from keras.models import load_model
model = load_model('model-0.6358.h5')
</code></pre>

<p>, I'm getting: </p>

<pre><code>usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
return f(*args, **kwds)

/usr/local/lib/python3.4/dist-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.
warnings.warn('Error in loading the saved optimizer '
</code></pre>

<p>But... it looks like it predicts right.</p>

<p>How can I avoid that warning message?</p>
",3574571,1064,09-03-2018 13:46,09-03-2018 17:30,0,1064,32,1,12,85,"{'badge_counts': {'bronze': 32, 'silver': 12, 'gold': 1}, 'account_id': 4383688, 'is_employee': False, 'last_modified_date': 1684548600, 'last_access_date': 1708947736, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1064, 'creation_date': 1398460785, 'user_type': 'registered', 'user_id': 3574571, 'accept_rate': 85, 'location': 'Madrid', 'website_url': '', 'link': 'https://stackoverflow.com/users/3574571/mquinteiro', 'profile_image': 'https://i.stack.imgur.com/1CulI.jpg?s=256&g=1', 'display_name': 'Mquinteiro'}","I have trained a keras sequential model in a linux 64 machine and saved to a .h5 file. It this PC I can load the model and do predictions without problems. Now I'm implementing the prediction in a Raspberry Pi 3 that have installed keras, tensorflow, h5py and python3. when I load the model , I'm getting: But... it looks like it predicts right. How can I avoid that warning message?","from keras.models import load_model
model = load_model('model-0.6358.h5')
 usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
return f(*args, **kwds)

/usr/local/lib/python3.4/dist-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.
warnings.warn('Error in loading the saved optimizer '
",5,24,0,0,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Advanced**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
115,49505080,49505103,24519,Python Pandas: How to set the name of MultiIndex?,2,<python><python-3.x><pandas><dataframe><multi-index>,26,"<p>My dataframe looks like below:</p>
<p><a href=""https://i.stack.imgur.com/FxR4F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FxR4F.png"" alt=""enter image description here"" /></a></p>
<p>I want to set the name of red box in image as 'Ticker'. How can I do that?</p>
",3595632,5530,27-03-2018 05:36,27-03-2018 05:38,0,5550,116,10,58,41,"{'badge_counts': {'bronze': 116, 'silver': 58, 'gold': 10}, 'account_id': 4413354, 'is_employee': False, 'last_modified_date': 1711159500, 'last_access_date': 1711000033, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 20, 'reputation_change_week': 12, 'reputation_change_day': 2, 'reputation': 5550, 'creation_date': 1399021468, 'user_type': 'registered', 'user_id': 3595632, 'accept_rate': 41, 'link': 'https://stackoverflow.com/users/3595632/user3595632', 'profile_image': 'https://www.gravatar.com/avatar/c3f306ae24e9ad913fbf990ee233f099?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user3595632'}",My dataframe looks like below: I want to set the name of red box in image as 'Ticker'. How can I do that?,,0,3,1,1,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
116,49932759,49933085,72389,"pip 10 and apt: how to avoid ""Cannot uninstall X"" errors for distutils packages",4,<python><docker><pip><distutils><apt>,65,"<p>I am dealing with a legacy Dockerfile. Here is a <em>very simplified</em> version of what I am dealing with:</p>
<pre><code>FROM ubuntu:14.04

RUN apt-get -y update &amp;&amp; apt-get -y install \
    python-pip \
    python-numpy # ...and many other packages

RUN pip install -U pip

RUN pip install -r /tmp/requirements1.txt # includes e.g., numpy==1.13.0
RUN pip install -r /tmp/requirements2.txt
RUN pip install -r /tmp/requirements3.txt
</code></pre>
<p>First, several packages are installed using <code>apt</code>, and then several packages are installed using <code>pip</code>. <code>pip</code> version 10 has been released, and <a href=""https://pip.pypa.io/en/stable/news/#deprecations-and-removals"" rel=""noreferrer"">part of the release</a> is this new restriction:</p>
<blockquote>
<p>Removed support for uninstalling projects which have been installed using distutils. distutils installed projects do not include metadata indicating what files belong to that install and thus it is impossible to actually uninstall them rather than just remove the metadata saying they've been installed while leaving all of the actual files behind.</p>
</blockquote>
<p>This leads to the following problem in my setup. For example, first <code>apt</code> installs <code>python-numpy</code>. Later <code>pip</code> tries to install a newer version of <code>numpy</code> from e.g., <code>/tmp/requirements1.txt</code>, and tries to uninstall the older version, but because of the new restriction, it cannot remove this version:</p>
<pre><code>Installing collected packages: numpy
  Found existing installation: numpy 1.8.2
Cannot uninstall 'numpy'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.
</code></pre>
<p>Now I know at this point there are several solutions.</p>
<p>I could not install <code>python-numpy</code> through <code>apt</code>. However, this causes issues because <code>python-numpy</code> installs a few different packages as requirements, and I do not know if another part of the system relies on these packages. And in reality, there are several <code>apt</code> packages installed through the Dockerfile, and each one I remove seems to reveal another <code>Cannot uninstall X</code> error, and removes a number of other packages along with it, that our app may or may not rely on.</p>
<p>I could also use the <code>--ignore-installed</code> option when I try to <code>pip</code> install things that have already been installed through <code>apt</code>, but then again I have the same problem of every <code>--ignore-installed</code> argument revealing yet another thing that needs to be ignored.</p>
<p>I could pin <code>pip</code> at an older version that does not have this restriction, but I don't want to be stuck using an outdated version of <code>pip</code> forever.</p>
<p>I have been going around in circles trying to come up with a good solution that involves minimal changes to this legacy Dockerfile, and allows the app we deploy with that file to continue to function as it has been. Any suggestions as to how I can safely get around this problem of <code>pip</code> 10 not being able to install newer versions of <code>distutils</code> packages? Thank you!</p>
<h1>UPDATE:</h1>
<p>I did not realize that <code>--ignore-installed</code> could be used without a package as an argument to ignore all installed packages. I am considering whether or not this might be a good option for me, and have asked about it <a href=""https://stackoverflow.com/q/49932926/3642398"">here</a>.</p>
",3642398,16743,20-04-2018 01:58,20-04-2018 02:41,0,16763,91,8,65,74,"{'badge_counts': {'bronze': 91, 'silver': 65, 'gold': 8}, 'account_id': 4477732, 'is_employee': False, 'last_modified_date': 1703094600, 'last_access_date': 1710940272, 'reputation_change_year': 160, 'reputation_change_quarter': 160, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 16763, 'creation_date': 1400183916, 'user_type': 'registered', 'user_id': 3642398, 'accept_rate': 74, 'location': 'Kansas City, MO, United States', 'website_url': 'https://ethanskinner.com', 'link': 'https://stackoverflow.com/users/3642398/elethan', 'profile_image': 'https://i.stack.imgur.com/Y6cZb.jpg?s=256&g=1', 'display_name': 'elethan'}","I am dealing with a legacy Dockerfile. Here is a very simplified version of what I am dealing with: First, several packages are installed using , and then several packages are installed using . version 10 has been released, and part of the release is this new restriction: Removed support for uninstalling projects which have been installed using distutils. distutils installed projects do not include metadata indicating what files belong to that install and thus it is impossible to actually uninstall them rather than just remove the metadata saying they've been installed while leaving all of the actual files behind. This leads to the following problem in my setup. For example, first installs . Later tries to install a newer version of from e.g., , and tries to uninstall the older version, but because of the new restriction, it cannot remove this version: Now I know at this point there are several solutions. I could not install through . However, this causes issues because installs a few different packages as requirements, and I do not know if another part of the system relies on these packages. And in reality, there are several packages installed through the Dockerfile, and each one I remove seems to reveal another error, and removes a number of other packages along with it, that our app may or may not rely on. I could also use the option when I try to install things that have already been installed through , but then again I have the same problem of every argument revealing yet another thing that needs to be ignored. I could pin at an older version that does not have this restriction, but I don't want to be stuck using an outdated version of forever. I have been going around in circles trying to come up with a good solution that involves minimal changes to this legacy Dockerfile, and allows the app we deploy with that file to continue to function as it has been. Any suggestions as to how I can safely get around this problem of 10 not being able to install newer versions of packages? Thank you! UPDATE: I did not realize that could be used without a package as an argument to ignore all installed packages. I am considering whether or not this might be a good option for me, and have asked about it here.","FROM ubuntu:14.04

RUN apt-get -y update &amp;&amp; apt-get -y install \
    python-pip \
    python-numpy # ...and many other packages

RUN pip install -U pip

RUN pip install -r /tmp/requirements1.txt # includes e.g., numpy==1.13.0
RUN pip install -r /tmp/requirements2.txt
RUN pip install -r /tmp/requirements3.txt
 apt pip pip apt python-numpy pip numpy /tmp/requirements1.txt Installing collected packages: numpy
  Found existing installation: numpy 1.8.2
Cannot uninstall 'numpy'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.
 python-numpy apt python-numpy apt Cannot uninstall X --ignore-installed pip apt --ignore-installed pip pip pip distutils --ignore-installed",-10,29,0,2,"response:
GenerateContentResponse(
    done=True,
    iterator=None,
    result=glm.GenerateContentResponse({'candidates': [{'content': {'parts': [{'text': '**Basic**'}], 'role': 'model'}, 'finish_reason': 1, 'index': 0, 'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'token_count': 0, 'grounding_attributions': []}], 'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}}),
)"
117,49600379,49600761,11752,How to ignore a warning inside a test using pytest?,1,<python><testing><warnings><decorator><pytest>,14,"<p>I am trying to test a function that uses <code>DatetimeFields</code>. The function I want to test is the following:</p>

<pre><code>def get_pledge_frequency(last_week_pledges):
    """"""Returns two lists:
    pledge_frequency: containing the number of pledges per day of the last week
    weekdays: containing a letter that represents the day
    It assumes that last_week_pledges are pledges made within the last week.
    """"""
    pledge_frequency = []
    weekdays = []

    if last_week_pledges:
        last_7_days = [timezone.now() - timedelta(days=i) for i in range(7)]
        last_7_days.reverse()
        day_names = 'MTWTFSS'

        for day in last_7_days:
            pledge_frequency.append(
                last_week_pledges.filter(timestamp__date=day).count())
            weekdays.append(day_names[day.weekday()])

    return pledge_frequency, weekdays
</code></pre>

<p>I am using <code>pytest</code> for testing, so the test that I have implemented is the following:</p>

<pre><code>pledge_frequency_ids = ['no_pledges', 'one_pledge_today',
                        'one_pledge_not_today', 'two_pledges_same_day',
                        'two_pledges_not_same_day', 'multiple_pledges_a',
                        'multiple_pledges_b']

pledge_data = [
    ('2018-03-30', [], ([], [])),
    ('2018-03-30', ['2018-03-30'], ([0] * 6 + [1], 'SSMTWTF')),
    ('2018-03-30', ['2018-03-27'], ([0, 0, 0, 1, 0, 0, 0], 'SSMTWTF')),
    ('2018-03-31', ['2018-03-29', '2018-03-29'], ([0, 0, 0, 0, 2, 0, 0], 'SMTWTFS')),
    ('2018-03-28', ['2018-03-26', '2018-03-28'], ([0, 0, 0, 0, 1, 0, 1], 'TFSSMTW')),
    ('2018-04-01', ['2018-03-26', '2018-03-26', '2018-03-27', '2018-03-28'], ([2, 1, 1, 0, 0, 0, 0], 'MTWTFSS',)),
    ('2018-03-29', ['2018-03-25', '2018-03-26', '2018-03-27', '2018-03-28'], ([0, 0, 1, 1, 1, 1, 0], 'FSSMTWT'))]

@pytest.mark.parametrize('today, pledge_information, pledge_frequency',
                         pledge_data, ids=pledge_frequency_ids)
@pytest.mark.django_db
@mock.patch('django.utils.timezone.now')
@mock.patch('pledges.models.Pledge')
def test_get_pledge_frequency(_, mock_now, social_user, today,
                              pledge_information, pledge_frequency):
    """"""Tests to verify correctness of get_pledge_frequency() function.
    Covering the following cases:
    * No pledges
    * One pledge today
    * One pledge not today
    * Two pledges the same day
    * Two pledges not the same day
    * Multiple pledges particular case 0
    * Multiple pledges particular case 1""""""
    mock_now.return_value = timezone.datetime.strptime(today, '%Y-%m-%d')
    for pledge_info in pledge_information:
        pledge = Pledge()
        pledge.user = social_user
        pledge.save()
        pledge.timestamp = timezone.datetime.strptime(pledge_info, '%Y-%m-%d')
        pledge.save()

    last_week_pledges = Pledge.objects.all()
    expected_frequency, expected_weekdays = pledge_frequency
    expected_weekdays = list(expected_weekdays)
    actual_frequency, actual_weekdays = get_pledge_frequency(last_week_pledges)

    assert expected_frequency == actual_frequency
    assert expected_weekdays == actual_weekdays
</code></pre>

<p>The tests passes, but the problem is that I am getting the following warning:</p>

<blockquote>
  <p>RuntimeWarning: DateTimeField Pledge.timestamp received a naive datetime (2018-03-29 00:00:00) while time zone support is active.</p>
</blockquote>

<p>Actually, I get several <code>RuntimeWarning</code> which notify the use of a naive datetime while time zone support is active.</p>

<p>How can I disable warnings just for this test? I found that using <code>@pytest.mark.filterwarnings</code> might be useful, and I have added the tag as this: <code>@pytest.mark.filterwarnings('ignore:RuntimeWarning')</code>. However, that didn't work, and after running the test I still have those warnings.</p>

<p>Does the order of where I put the decorator matters? I have tried several combinations, but it does't work yet.</p>

<p>In the documentation I found that I can add <code>addopts = -p no:warnings</code> to my <code>pytest.ini</code> file, but I don't want to follow this approach in case I get another test generating this warning.</p>
",3705840,66027,01-04-2018 16:50,01-04-2018 17:31,0,66245,230,47,225,97,"{'badge_counts': {'bronze': 230, 'silver': 225, 'gold': 47}, 'account_id': 4564466, 'is_employee': False, 'last_modified_date': 1708606819, 'last_access_date': 1711035017, 'reputation_change_year': 1336, 'reputation_change_quarter': 1336, 'reputation_change_month': 348, 'reputation_change_week': 100, 'reputation_change_day': 0, 'reputation': 66245, 'creation_date': 1401864143, 'user_type': 'registered', 'user_id': 3705840, 'accept_rate': 97, 'location': 'Ecuador', 'website_url': '', 'link': 'https://stackoverflow.com/users/3705840/lmiguelvargasf', 'profile_image': 'https://i.stack.imgur.com/a7ybc.png?s=256&g=1', 'display_name': 'lmiguelvargasf'}","I am trying to test a function that uses . The function I want to test is the following: I am using for testing, so the test that I have implemented is the following: The tests passes, but the problem is that I am getting the following warning: RuntimeWarning: DateTimeField Pledge.timestamp received a naive datetime (2018-03-29 00:00:00) while time zone support is active. Actually, I get several which notify the use of a naive datetime while time zone support is active. How can I disable warnings just for this test? I found that using might be useful, and I have added the tag as this: . However, that didn't work, and after running the test I still have those warnings. Does the order of where I put the decorator matters? I have tried several combinations, but it does't work yet. In the documentation I found that I can add to my file, but I don't want to follow this approach in case I get another test generating this warning.","DatetimeFields def get_pledge_frequency(last_week_pledges):
    """"""Returns two lists:
    pledge_frequency: containing the number of pledges per day of the last week
    weekdays: containing a letter that represents the day
    It assumes that last_week_pledges are pledges made within the last week.
    """"""
    pledge_frequency = []
    weekdays = []

    if last_week_pledges:
        last_7_days = [timezone.now() - timedelta(days=i) for i in range(7)]
        last_7_days.reverse()
        day_names = 'MTWTFSS'

        for day in last_7_days:
            pledge_frequency.append(
                last_week_pledges.filter(timestamp__date=day).count())
            weekdays.append(day_names[day.weekday()])

    return pledge_frequency, weekdays
 pytest pledge_frequency_ids = ['no_pledges', 'one_pledge_today',
                        'one_pledge_not_today', 'two_pledges_same_day',
                        'two_pledges_not_same_day', 'multiple_pledges_a',
                        'multiple_pledges_b']

pledge_data = [
    ('2018-03-30', [], ([], [])),
    ('2018-03-30', ['2018-03-30'], ([0] * 6 + [1], 'SSMTWTF')),
    ('2018-03-30', ['2018-03-27'], ([0, 0, 0, 1, 0, 0, 0], 'SSMTWTF')),
    ('2018-03-31', ['2018-03-29', '2018-03-29'], ([0, 0, 0, 0, 2, 0, 0], 'SMTWTFS')),
    ('2018-03-28', ['2018-03-26', '2018-03-28'], ([0, 0, 0, 0, 1, 0, 1], 'TFSSMTW')),
    ('2018-04-01', ['2018-03-26', '2018-03-26', '2018-03-27', '2018-03-28'], ([2, 1, 1, 0, 0, 0, 0], 'MTWTFSS',)),
    ('2018-03-29', ['2018-03-25', '2018-03-26', '2018-03-27', '2018-03-28'], ([0, 0, 1, 1, 1, 1, 0], 'FSSMTWT'))]

@pytest.mark.parametrize('today, pledge_information, pledge_frequency',
                         pledge_data, ids=pledge_frequency_ids)
@pytest.mark.django_db
@mock.patch('django.utils.timezone.now')
@mock.patch('pledges.models.Pledge')
def test_get_pledge_frequency(_, mock_now, social_user, today,
                              pledge_information, pledge_frequency):
    """"""Tests to verify correctness of get_pledge_frequency() function.
    Covering the following cases:
    * No pledges
    * One pledge today
    * One pledge not today
    * Two pledges the same day
    * Two pledges not the same day
    * Multiple pledges particular case 0
    * Multiple pledges particular case 1""""""
    mock_now.return_value = timezone.datetime.strptime(today, '%Y-%m-%d')
    for pledge_info in pledge_information:
        pledge = Pledge()
        pledge.user = social_user
        pledge.save()
        pledge.timestamp = timezone.datetime.strptime(pledge_info, '%Y-%m-%d')
        pledge.save()

    last_week_pledges = Pledge.objects.all()
    expected_frequency, expected_weekdays = pledge_frequency
    expected_weekdays = list(expected_weekdays)
    actual_frequency, actual_weekdays = get_pledge_frequency(last_week_pledges)

    assert expected_frequency == actual_frequency
    assert expected_weekdays == actual_weekdays
 RuntimeWarning @pytest.mark.filterwarnings @pytest.mark.filterwarnings('ignore:RuntimeWarning') addopts = -p no:warnings pytest.ini",56,86,0,0,
118,49161652,49253755,9673,How to get around in place operation error if index leaf variable for gradient update?,3,<python><neural-network><deep-learning><gradient-descent><pytorch>,11,"<p>I am encountering In place operation error when I am trying to index a leaf variable to update gradients with customized Shrink function. I cannot work around it. Any help is highly appreciated!</p>



<pre class=""lang-python prettyprint-override""><code>import torch.nn as nn
import torch
import numpy as np
from torch.autograd import Variable, Function

# hyper parameters
batch_size = 100 # batch size of images
ld = 0.2 # sparse penalty
lr = 0.1 # learning rate

x = Variable(torch.from_numpy(np.random.normal(0,1,(batch_size,10,10))), requires_grad=False)  # original

# depends on size of the dictionary, number of atoms.
D = Variable(torch.from_numpy(np.random.normal(0,1,(500,10,10))), requires_grad=True)

# hx sparse representation
ht = Variable(torch.from_numpy(np.random.normal(0,1,(batch_size,500,1,1))), requires_grad=True)

# Dictionary loss function
loss = nn.MSELoss()

# customized shrink function to update gradient
shrink_ht = lambda x: torch.stack([torch.sign(i)*torch.max(torch.abs(i)-lr*ld,0)[0] for i in x])

### sparse reprsentation optimizer_ht single image.
optimizer_ht = torch.optim.SGD([ht], lr=lr, momentum=0.9) # optimizer for sparse representation

## update for the batch
for idx in range(len(x)):
    optimizer_ht.zero_grad() # clear up gradients
    loss_ht = 0.5*torch.norm((x[idx]-(D*ht[idx]).sum(dim=0)),p=2)**2
    loss_ht.backward() # back propogation and calculate gradients
    optimizer_ht.step() # update parameters with gradients
    ht[idx] = shrink_ht(ht[idx])  # customized shrink function.

RuntimeError Traceback (most recent call last) in ()
15 loss_ht.backward() # back propogation and calculate gradients
16 optimizer_ht.step() # update parameters with gradients
—&gt; 17 ht[idx] = shrink_ht(ht[idx]) # customized shrink function.
18
19

/home/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py in setitem(self, key, value)
85 return MaskedFill.apply(self, key, value, True)
86 else:
—&gt; 87 return SetItem.apply(self, key, value)
88
89 def deepcopy(self, memo):

RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.
</code></pre>

<p>Specifically, this line of code below seems give error as it index and update leaf variable at the same time. </p>

<pre class=""lang-python prettyprint-override""><code>ht[idx] = shrink_ht(ht[idx])  # customized shrink function.
</code></pre>

<p>Thanks.</p>

<p>W.S.</p>
",4345535,647,07-03-2018 21:39,13-03-2018 10:27,6,647,19,1,6,86,"{'badge_counts': {'bronze': 19, 'silver': 6, 'gold': 1}, 'account_id': 5464306, 'is_employee': False, 'last_modified_date': 1573679920, 'last_access_date': 1684178935, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 647, 'creation_date': 1418212478, 'user_type': 'registered', 'user_id': 4345535, 'accept_rate': 86, 'link': 'https://stackoverflow.com/users/4345535/w-s', 'profile_image': 'https://www.gravatar.com/avatar/09ae0d87dfa4aa07a3a7b52987ed7760?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'W.S.'}","I am encountering In place operation error when I am trying to index a leaf variable to update gradients with customized Shrink function. I cannot work around it. Any help is highly appreciated! Specifically, this line of code below seems give error as it index and update leaf variable at the same time. Thanks. W.S.","import torch.nn as nn
import torch
import numpy as np
from torch.autograd import Variable, Function

# hyper parameters
batch_size = 100 # batch size of images
ld = 0.2 # sparse penalty
lr = 0.1 # learning rate

x = Variable(torch.from_numpy(np.random.normal(0,1,(batch_size,10,10))), requires_grad=False)  # original

# depends on size of the dictionary, number of atoms.
D = Variable(torch.from_numpy(np.random.normal(0,1,(500,10,10))), requires_grad=True)

# hx sparse representation
ht = Variable(torch.from_numpy(np.random.normal(0,1,(batch_size,500,1,1))), requires_grad=True)

# Dictionary loss function
loss = nn.MSELoss()

# customized shrink function to update gradient
shrink_ht = lambda x: torch.stack([torch.sign(i)*torch.max(torch.abs(i)-lr*ld,0)[0] for i in x])

### sparse reprsentation optimizer_ht single image.
optimizer_ht = torch.optim.SGD([ht], lr=lr, momentum=0.9) # optimizer for sparse representation

## update for the batch
for idx in range(len(x)):
    optimizer_ht.zero_grad() # clear up gradients
    loss_ht = 0.5*torch.norm((x[idx]-(D*ht[idx]).sum(dim=0)),p=2)**2
    loss_ht.backward() # back propogation and calculate gradients
    optimizer_ht.step() # update parameters with gradients
    ht[idx] = shrink_ht(ht[idx])  # customized shrink function.

RuntimeError Traceback (most recent call last) in ()
15 loss_ht.backward() # back propogation and calculate gradients
16 optimizer_ht.step() # update parameters with gradients
—&gt; 17 ht[idx] = shrink_ht(ht[idx]) # customized shrink function.
18
19

/home/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py in setitem(self, key, value)
85 return MaskedFill.apply(self, key, value, True)
86 else:
—&gt; 87 return SetItem.apply(self, key, value)
88
89 def deepcopy(self, memo):

RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.
 ht[idx] = shrink_ht(ht[idx])  # customized shrink function.
",49,64,0,0,
119,49375054,49466811,6756,Alexa Skill Development using flask-ask and ngrok,4,<python><alexa-skills-kit><ngrok>,11,"<p>I'm trying to begin developing a skill for alexa using flask-ask and ngrok in python. Following is my code:</p>

<pre><code>from flask import Flask
from flask_ask import Ask, statement, question, session
import json
import requests
import time
import unidecode

app = Flask(__name__)
ask = Ask(app, ""/reddit_reader"")

def get_headlines():

    titles = 'is this working'
    return titles  

@app.route('/')
def homepage():
    return ""hi there, how ya doin?""

@ask.launch
def start_skill():
    welcome_message = 'Hello there, would you like the news?'
    return question(welcome_message)

@ask.intent(""YesIntent"")
def share_headlines():
    headlines = get_headlines()
    headline_msg = 'The current world news headlines are 
{}'.format(headlines)
    return statement(headline_msg)

@ask.intent(""NoIntent"")
def no_intent():
    bye_text = 'I am not sure why you asked me to run then, but okay... bye'
    return statement(bye_text)

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>

<p>The code runs fine on my machine and returns the correct output if I print it out. But the skill gives a HTTP 500 internal error when I deploy it on amazon using ngrok. I get the same 500 internal error both in the text as well as json simulator in the development console.</p>

<p>This is my intent schema:</p>

<pre><code>{
  ""intents"": [
    {
      ""intent"": ""YesIntent""
    },
    {
      ""intent"": ""NoIntent""
    }
  ]
}
</code></pre>

<p>I get the following error in my python prompt:
<code>AttributeError: module 'lib' has no attribute 'X509V3_EXT_get</code></p>

<p>The stacktrace is as follows:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1997, in __call__
    return self.wsgi_app(environ, start_response)
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1985, in wsgi_app
    response = self.handle_exception(e)
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1540, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\Python36\lib\site-packages\flask\_compat.py"", line 33, in reraise
    raise value
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1982, in wsgi_app
    response = self.full_dispatch_request()
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1614, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1517, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\Python36\lib\site-packages\flask\_compat.py"", line 33, in reraise
    raise value
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1612, in full_dispatch_request
    rv = self.dispatch_request()
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1598, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""C:\Python36\lib\site-packages\flask_ask\core.py"", line 728, in _flask_view_func
    ask_payload = self._alexa_request(verify=self.ask_verify_requests)
  File ""C:\Python36\lib\site-packages\flask_ask\core.py"", line 662, in _alexa_request
    cert = verifier.load_certificate(cert_url)
  File ""C:\Python36\lib\site-packages\flask_ask\verifier.py"", line 21, in load_certificate
    if not _valid_certificate(cert):
  File ""C:\Python36\lib\site-packages\flask_ask\verifier.py"", line 63, in _valid_certificate
    value = str(extension)
  File ""C:\Python36\lib\site-packages\OpenSSL\crypto.py"", line 779, in __str__
    return self._subjectAltNameString()
  File ""C:\Python36\lib\site-packages\OpenSSL\crypto.py"", line 740, in _subjectAltNameString
    method = _lib.X509V3_EXT_get(self._extension)
AttributeError: module 'lib' has no attribute 'X509V3_EXT_get'
</code></pre>

<p>Pip freeze output:</p>

<pre><code>aniso8601==1.2.0
asn1crypto==0.24.0
certifi==2018.1.18
cffi==1.11.5
chardet==3.0.4
click==6.7
cryptography==2.2
Flask==0.12.1
Flask-Ask==0.9.8
idna==2.6
itsdangerous==0.24
Jinja2==2.10
MarkupSafe==1.0
pycparser==2.18
pyOpenSSL==17.0.0
python-dateutil==2.7.0
PyYAML==3.12
requests==2.18.4
six==1.11.0
Unidecode==1.0.22
urllib3==1.22
Werkzeug==0.14.1
</code></pre>

<p>I've tried running it on both python 2.7 and python 3.6. Any help is appreciated</p>
",3745949,197,20-03-2018 01:36,24-03-2018 15:59,4,197,7,0,1,,"{'badge_counts': {'bronze': 7, 'silver': 1, 'gold': 0}, 'account_id': 4620773, 'is_employee': False, 'last_modified_date': 1573680303, 'last_access_date': 1709944758, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 197, 'creation_date': 1402942491, 'user_type': 'registered', 'user_id': 3745949, 'location': 'India', 'website_url': '', 'link': 'https://stackoverflow.com/users/3745949/rogerthat', 'profile_image': 'https://www.gravatar.com/avatar/c797f23039e1655c2a4af7866a8329bc?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'RogerThat'}",I'm trying to begin developing a skill for alexa using flask-ask and ngrok in python. Following is my code: The code runs fine on my machine and returns the correct output if I print it out. But the skill gives a HTTP 500 internal error when I deploy it on amazon using ngrok. I get the same 500 internal error both in the text as well as json simulator in the development console. This is my intent schema: I get the following error in my python prompt: The stacktrace is as follows: Pip freeze output: I've tried running it on both python 2.7 and python 3.6. Any help is appreciated,"from flask import Flask
from flask_ask import Ask, statement, question, session
import json
import requests
import time
import unidecode

app = Flask(__name__)
ask = Ask(app, ""/reddit_reader"")

def get_headlines():

    titles = 'is this working'
    return titles  

@app.route('/')
def homepage():
    return ""hi there, how ya doin?""

@ask.launch
def start_skill():
    welcome_message = 'Hello there, would you like the news?'
    return question(welcome_message)

@ask.intent(""YesIntent"")
def share_headlines():
    headlines = get_headlines()
    headline_msg = 'The current world news headlines are 
{}'.format(headlines)
    return statement(headline_msg)

@ask.intent(""NoIntent"")
def no_intent():
    bye_text = 'I am not sure why you asked me to run then, but okay... bye'
    return statement(bye_text)

if __name__ == '__main__':
    app.run(debug=True)
 {
  ""intents"": [
    {
      ""intent"": ""YesIntent""
    },
    {
      ""intent"": ""NoIntent""
    }
  ]
}
 AttributeError: module 'lib' has no attribute 'X509V3_EXT_get Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1997, in __call__
    return self.wsgi_app(environ, start_response)
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1985, in wsgi_app
    response = self.handle_exception(e)
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1540, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\Python36\lib\site-packages\flask\_compat.py"", line 33, in reraise
    raise value
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1982, in wsgi_app
    response = self.full_dispatch_request()
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1614, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1517, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\Python36\lib\site-packages\flask\_compat.py"", line 33, in reraise
    raise value
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1612, in full_dispatch_request
    rv = self.dispatch_request()
  File ""C:\Python36\lib\site-packages\flask\app.py"", line 1598, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""C:\Python36\lib\site-packages\flask_ask\core.py"", line 728, in _flask_view_func
    ask_payload = self._alexa_request(verify=self.ask_verify_requests)
  File ""C:\Python36\lib\site-packages\flask_ask\core.py"", line 662, in _alexa_request
    cert = verifier.load_certificate(cert_url)
  File ""C:\Python36\lib\site-packages\flask_ask\verifier.py"", line 21, in load_certificate
    if not _valid_certificate(cert):
  File ""C:\Python36\lib\site-packages\flask_ask\verifier.py"", line 63, in _valid_certificate
    value = str(extension)
  File ""C:\Python36\lib\site-packages\OpenSSL\crypto.py"", line 779, in __str__
    return self._subjectAltNameString()
  File ""C:\Python36\lib\site-packages\OpenSSL\crypto.py"", line 740, in _subjectAltNameString
    method = _lib.X509V3_EXT_get(self._extension)
AttributeError: module 'lib' has no attribute 'X509V3_EXT_get'
 aniso8601==1.2.0
asn1crypto==0.24.0
certifi==2018.1.18
cffi==1.11.5
chardet==3.0.4
click==6.7
cryptography==2.2
Flask==0.12.1
Flask-Ask==0.9.8
idna==2.6
itsdangerous==0.24
Jinja2==2.10
MarkupSafe==1.0
pycparser==2.18
pyOpenSSL==17.0.0
python-dateutil==2.7.0
PyYAML==3.12
requests==2.18.4
six==1.11.0
Unidecode==1.0.22
urllib3==1.22
Werkzeug==0.14.1
",99,126,0,0,
120,49555991,49557099,7342,Can I create a local numpy random seed?,2,<python><numpy><random><scope>,16,"<p>There is a function, <code>foo</code>, that uses the <code>np.random</code> functionality.
I want to control the seed that <code>foo</code> uses, but without actually changing the function itself.
How do I do this?</p>

<p>Essentially I want something like this:</p>

<pre><code>bar() # should have normal seed
with np.random.seed(0): # Doesn't work
    foo()
bar() # should have normal seed
</code></pre>

<hr>

<p>Solutions like 
<a href=""https://stackoverflow.com/questions/37355985/how-to-use-pythons-random-number-generator-with-a-local-seed"">this</a>:</p>

<pre><code>rng = random.Random(42)
number = rng.randint(10, 20)
</code></pre>

<p>doesn't work in this case, as I don't have access to the inner workings of <code>foo</code> (or am I missing something??).</p>
",3747801,5878,29-03-2018 12:22,29-03-2018 13:16,0,5878,63,9,42,65,"{'badge_counts': {'bronze': 63, 'silver': 42, 'gold': 9}, 'account_id': 4623325, 'is_employee': False, 'last_modified_date': 1650019983, 'last_access_date': 1689697639, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5878, 'creation_date': 1402996078, 'user_type': 'registered', 'user_id': 3747801, 'accept_rate': 65, 'location': 'Copenhagen, Denmark', 'website_url': 'http://IntelXT.com', 'link': 'https://stackoverflow.com/users/3747801/toke-faurby', 'profile_image': 'https://i.stack.imgur.com/kHyXk.png?s=256&g=1', 'display_name': 'Toke Faurby'}","There is a function, , that uses the functionality. I want to control the seed that uses, but without actually changing the function itself. How do I do this? Essentially I want something like this: Solutions like this: doesn't work in this case, as I don't have access to the inner workings of (or am I missing something??).","foo np.random foo bar() # should have normal seed
with np.random.seed(0): # Doesn't work
    foo()
bar() # should have normal seed
 rng = random.Random(42)
number = rng.randint(10, 20)
 foo",0,22,0,1,
121,48432029,48432979,37676,Mocking requests.post and requests.json decoder python,4,<python><unit-testing><python-requests>,16,"<p>I'm creating a test suite for my module that uses the requests library quite a bit. However, I'm trying to mock several different return values for a specific request, and I'm having trouble doing so. Here is my code snippet that doesn't work:</p>

<pre><code>class MyTests(unittest.TestCase):

    @patch('mypackage.mymodule.requests.post') 
    def test_change_nested_dict_function(self, mock_post):
        mock_post.return_value.status_code = 200
        mock_post.return_value.json = nested_dictionary
        modified_dict = mymodule.change_nested_dict()
        self.assertEqual(modified_dict['key1']['key2'][0]['key3'], 'replaced_value')
</code></pre>

<p>The function I am attempting to mock:</p>

<pre><code>import requests

def change_nested_dict():
    uri = 'http://this_is_the_endpoint/I/am/hitting'
    payload = {'param1': 'foo', 'param2': 'bar'}
    r = requests.post(uri, params=payload)

    # This function checks to make sure the response is giving the 
    # correct status code, hence why I need to mock the status code above
    raise_error_if_bad_status_code(r)

    dict_to_be_changed = r.json()

    def _internal_fxn_to_change_nested_value(dict):
        ''' This goes through the dict and finds the correct key to change the value. 
            This is the actual function I am trying to test above'''
        return changed_dict


    modified_dict = _internal_fxn_to_change_nested_value(dict_to_be_changed)

    return modified_dict
</code></pre>

<p>I know a simple way of doing this would be to not have a nested function, but I am only showing you part of the entire function's code. Trust me, the nested function is necessary and I really do not want to change that part of it.</p>

<p>My issue is, I don't understand how to mock requests.post and then set the return value for both the status code and the internal json decoder. I also can't seem to find a way around this issue since I can't seem to patch the internal function either, which also would solve this problem. Does anyone have any suggestions/ideas? Thanks a bunch.</p>
",3798666,173,24-01-2018 21:38,24-01-2018 22:48,0,173,6,1,2,,"{'badge_counts': {'bronze': 6, 'silver': 2, 'gold': 1}, 'account_id': 4693041, 'is_employee': False, 'last_modified_date': 1647461400, 'last_access_date': 1541192072, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 173, 'creation_date': 1404320641, 'user_type': 'registered', 'user_id': 3798666, 'location': 'San Francisco, CA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/3798666/the-deuce', 'profile_image': 'https://www.gravatar.com/avatar/190e3d07edb0d06f65590e840645e09d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'the_deuce'}","I'm creating a test suite for my module that uses the requests library quite a bit. However, I'm trying to mock several different return values for a specific request, and I'm having trouble doing so. Here is my code snippet that doesn't work: The function I am attempting to mock: I know a simple way of doing this would be to not have a nested function, but I am only showing you part of the entire function's code. Trust me, the nested function is necessary and I really do not want to change that part of it. My issue is, I don't understand how to mock requests.post and then set the return value for both the status code and the internal json decoder. I also can't seem to find a way around this issue since I can't seem to patch the internal function either, which also would solve this problem. Does anyone have any suggestions/ideas? Thanks a bunch.","class MyTests(unittest.TestCase):

    @patch('mypackage.mymodule.requests.post') 
    def test_change_nested_dict_function(self, mock_post):
        mock_post.return_value.status_code = 200
        mock_post.return_value.json = nested_dictionary
        modified_dict = mymodule.change_nested_dict()
        self.assertEqual(modified_dict['key1']['key2'][0]['key3'], 'replaced_value')
 import requests

def change_nested_dict():
    uri = 'http://this_is_the_endpoint/I/am/hitting'
    payload = {'param1': 'foo', 'param2': 'bar'}
    r = requests.post(uri, params=payload)

    # This function checks to make sure the response is giving the 
    # correct status code, hence why I need to mock the status code above
    raise_error_if_bad_status_code(r)

    dict_to_be_changed = r.json()

    def _internal_fxn_to_change_nested_value(dict):
        ''' This goes through the dict and finds the correct key to change the value. 
            This is the actual function I am trying to test above'''
        return changed_dict


    modified_dict = _internal_fxn_to_change_nested_value(dict_to_be_changed)

    return modified_dict
",28,41,0,0,
122,50027635,50027898,8299,Show more images in Tensorboard - Tensorflow object detection,3,<python><tensorflow><object-detection><tensorboard>,11,"<p>I am using <a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""noreferrer"">Tensorflow's object detection framework</a>. Training and evaluation jobs are going well, but in tensorboard I am only able to see 10 images for the evaluation job. Is there a way to increase this number to look at more images? I tried changing the config file:</p>

<pre><code>eval_config: {
  num_examples: 1000
  max_evals: 50
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""xxx/eval.record""
  }
  label_map_path: ""xxx/label_map.pbtxt""
  shuffle: false
  num_readers: 1
}
</code></pre>

<p>I thought the <code>max_eval</code> parameter would change this but it doesn't.</p>

<p>This is the command i'm running for the evaluation job:</p>

<pre><code>python ../models/research/object_detection/eval.py \
    --logtostderr \
    --pipeline_config_path=xxx/ssd.config \
    --checkpoint_dir=""xxx/train/"" \
    --eval_dir=""xxx/eval""
</code></pre>
",3808402,2247,25-04-2018 16:58,25-04-2018 17:13,0,2247,32,3,21,85,"{'badge_counts': {'bronze': 32, 'silver': 21, 'gold': 3}, 'account_id': 4706431, 'is_employee': False, 'last_modified_date': 1607614624, 'last_access_date': 1711070360, 'reputation_change_year': 9, 'reputation_change_quarter': 9, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2247, 'creation_date': 1404591117, 'user_type': 'registered', 'user_id': 3808402, 'accept_rate': 85, 'location': 'Sydney NSW, Australia', 'website_url': 'https://clemkoa.github.io', 'link': 'https://stackoverflow.com/users/3808402/jood', 'profile_image': 'https://i.stack.imgur.com/KKKCj.png?s=256&g=1', 'display_name': 'jood'}","I am using Tensorflow's object detection framework. Training and evaluation jobs are going well, but in tensorboard I am only able to see 10 images for the evaluation job. Is there a way to increase this number to look at more images? I tried changing the config file: I thought the parameter would change this but it doesn't. This is the command i'm running for the evaluation job:","eval_config: {
  num_examples: 1000
  max_evals: 50
}

eval_input_reader: {
  tf_record_input_reader {
    input_path: ""xxx/eval.record""
  }
  label_map_path: ""xxx/label_map.pbtxt""
  shuffle: false
  num_readers: 1
}
 max_eval python ../models/research/object_detection/eval.py \
    --logtostderr \
    --pipeline_config_path=xxx/ssd.config \
    --checkpoint_dir=""xxx/train/"" \
    --eval_dir=""xxx/eval""
",15,27,0,1,
123,49459661,49459879,14925,Differences between Numpy divide and Python divide?,1,<python><numpy><division><elementwise-operations>,14,"<p>What are the similarities and differences between <strong>numpy.divide</strong> and the Python slash <strong>/</strong> operator? As far as I can tell they behave the same, both implementing an element-wise division. The <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.divide.html"" rel=""noreferrer"">Numpy documentation</a> mentions:</p>

<blockquote>
  <p>numpy.divide(x1, x2) ... Equivalent to x1 / x2 in terms of array-broadcasting. ...</p>
</blockquote>

<p>Implying that np.divide(x1, x2) is not <strong>completely</strong> equivalent to x1 / x2.
I have run the following snippet to compare their speed:</p>

<pre><code>import numpy as np
import time

a = np.random.rand(10000, 10000)
b = np.random.rand(10000, 10000)

tic = time.time()
c = a / b
toc = time.time()
print(""Python divide took: "", toc - tic)

tic = time.time()
c = np.divide(a, b)
toc = time.time()
print(""Numpy divide took: "", toc - tic)
</code></pre>

<p>It appears that the Python divide generally runs faster which leads me to believe the Numpy divide implements some additional bells and whistles.</p>

<p>Any help is much appreciated!</p>
",3847255,2310,23-03-2018 23:38,24-03-2018 00:08,1,2320,33,3,24,,"{'badge_counts': {'bronze': 33, 'silver': 24, 'gold': 3}, 'collectives': [{'collective': {'tags': ['twilio-programmable-chat', 'twilio', 'twilio-javascript', 'twilio-programmable-voice', 'twilio-python', 'sendgrid-api-v2', 'sendgrid-api-v3', 'twilio-studio', 'twilio-video', 'twilio-conversations', 'twilio-click-to-call', 'twilioflexwebchat', 'twilio-functions', 'twilio-flex', 'twilio-api', 'twilio-node', 'segment-io', 'twilio-twiml', 'twilio-conference', 'twimlet', 'twilio-php', 'twilio-cli', 'twilio-taskrouter', 'authy', 'sendgrid-rails', 'sendgrid-templates', 'sendgrid', 'twilio-amd', 'sendgrid-ruby'], 'external_links': [{'type': 'website', 'link': 'https://www.twilio.com/'}, {'type': 'support', 'link': 'mailto:stackoverflow@twilio.com'}, {'type': 'twitter', 'link': 'https://twitter.com/TwilioDevs'}, {'type': 'github', 'link': 'https://github.com/twilio'}, {'type': 'facebook', 'link': 'https://facebook.com/TeamTwilio'}, {'type': 'instagram', 'link': 'https://instagram.com/twilio'}], 'description': 'Twilio has democratized channels like voice, text, chat, video, and email by virtualizing the world’s communications infrastructure through APIs that are simple enough for any developer, yet robust enough to power the world’s most demanding applications.', 'link': '/collectives/twilio', 'name': 'Twilio', 'slug': 'twilio'}, 'role': 'member'}, {'collective': {'tags': ['cicd', 'octopus-deploy', 'jenkins-plugins', 'bitbucket-pipelines', 'continuous-delivery', 'continuous-testing', 'github-actions', 'argocd', 'azure-pipelines', 'jenkins-groovy', 'jenkins', 'teamcity', 'hudson', 'jenkins-pipeline', 'continuous-deployment', 'gitlab-ci', 'gitlab-ci-runner', 'codemagic', 'tfsbuild', 'google-cloud-build', 'continuous-integration', 'circleci'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where developers focused on continuous integration, delivery, and deployment can find, share, and learn about simultaneous development.', 'link': '/collectives/ci-cd', 'name': 'CI/CD', 'slug': 'ci-cd'}, 'role': 'member'}, {'collective': {'tags': ['azure-sdk-ruby', 'azure-lab-services', 'azure-devtest-labs', 'azure-stream-analytics', 'azureadgraph-deprecation', 'azure-configuration', 'azure-cosmosdb-cassandra-api', 'azure-eventgrid', 'azure-functions-proxies', 'azure-dns', 'azureclicredential', 'azure-ad-msal', 'sql-azure-federations', 'azure-log-analytics', 'azure-git-deployment', 'azure-queues', 'azure-auto-ml', 'azure-acr', 'azure-nsg', 'azure-integration-account', 'azure-managed-app', 'azure-sql-reporting', 'azure-public-ip', 'azure-sdk', 'adal', 'azure-service-plan', 'azure-appservice', 'azure-blob-trigger', 'azure-keyvault', 'azure-ddos', 'azure-ilb', 'azure-logic-app-standard', 'azure-functions', 'azure-iot-hub-device-update', 'azure-web-roles', 'azure-cloud-shell', 'azure-android-sdk', 'azure-acs', 'azure-alerts', 'azure-web-app-service', 'azure-batch', 'adal.js', 'azure-xplat-cli', 'azure-functions-runtime', 'azure-databricks', 'azure-sql', 'azure-site-recovery', 'azure-rbac', 'azure-analysis-services', 'azure-service-principal', 'azure-sentinel', 'azure-diagnostics', 'azure-databoxfamily', 'azure-media-player', 'azure-appfabric', 'azure-compute-emulator', 'azure-ai-translator', 'azure-traffic-manager', 'azure-sql-database', 'azure-clouddrive', 'azure-billing', 'azure-affinity-group', 'azure-blueprints', 'azure-text-translation', 'azure-caching', 'azure-ai', 'azure', 'azure-sas', 'azure-sphere', 'azure-promptflow', 'azure-free-services', 'azure-servicebus-topics', 'azure-iot-hub', 'azureshell', 'azure-sql-edge', 'azure-app-api', 'azure-security-center', 'azure-static-website-routing', 'azure-ad-powershell-v2', 'azure-ad-role', 'azure-spatial-anchors', 'azure-iot-central', 'azure-adf', 'azure-dsvm', 'azure-active-directory', 'azure-update-management-center', 'azure-service-fabric', 'azure-web-app-for-containers', 'azure-secrets', 'azure-workflow-automation', 'azure-mobile-engagement', 'azure-defender', 'azure-ad-graph-api', 'kitchen-azurerm', 'azure-china', 'azure-log-analytics-workspace', 'azure-data-factory', 'azure-batch-account', 'azure-adal-deprecation', 'microsoft-entra-id', 'azure-calculator', 'azure-remote-rendering', 'azure-image-builder', 'azure-bot-service', 'azure-anomaly-detector', 'azure-virtual-network', 'spring-cloud-azure', 'microsoft-entra-internet-access', 'microsoft-custom-vision', 'azure-database-postgresql', 'azure-maps', 'azure-hdinsight', 'azure-feature-manager', 'azure-ad-b2c-custom-policy', 'azure-private-dns', 'azure-management-portal', 'azure-cosmosdb-gremlinapi', 'azure-function-async', 'django-pyodbc-azure', 'azure-signalr', 'azure-billing-api', 'azure-scheduler', 'azure-anomaly-detection', 'azure-app-service-envrmnt', 'azure-front-door', 'azure-elasticpool', 'azure-role-environment', 'azure-vm-templates', 'azure-redis-cache', 'azure-waf', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-tablequery', 'azure-blob-storage', 'azure-postgresql', 'azure-servicebusrelay', 'azure-security', 'azure-automation', 'azure-quantum', 'azure-web-pubsub', 'azure-blockchain-service', 'azure-notificationhub', 'azure-load-testing', 'azure-object-anchors', 'azure-media-services', 'azure-dashboard', 'azure-managed-disk', 'azure-metrics-advisor', 'azure-emulator', 'azure-mobile-services', 'azure-static-web-app-routing', 'azure-webjobssdk', 'azure-zulu', 'azure-iot-suite', 'azure-app-service-plans', 'azure-application-proxy', 'azure-identity', 'azure-data-lake-gen2', 'azure-availability-set', 'azure-function-app-proxy', 'azure-container-service', 'azure-mcd', 'azure-analytics', 'azure-debugger', 'azure-gov', 'azure-vm', 'azure-iot-dps', 'defaultazurecredential', 'azure-triggers', 'azure-purview', 'azure-service-fabric-mesh', 'azure-notebooks', 'azure-autoscaling-block', 'azure-python-sdk', 'azure-node-sdk', 'azure-aks', 'azure-cosmosdb-mongoapi', 'azuremlsdk', 'azure-oauth', 'rebus-azureservicebus', 'azure-file-copy', 'azure-dev-spaces', 'azure-monitor', 'azure-sql-managed-instance', 'azure-database-mysql', 'azure-cosmosdb-tables', 'azure-relay', 'fhir-server-for-azure', 'azure-packaging', 'azurekinect', 'azure-webjobs', 'azure-java-tools', 'azure-api-management', 'azure-sdk-.net', 'azure-arc', 'azure-functions-core-tools', 'azure-cloud-services', 'azure-video-indexer', 'azure-worker-roles', 'azure-servicebus-queues', 'azure-powershell', 'sql-server-azure', 'pulumi-azure', 'azure-rest-api', 'azure-compliance-policy', 'azurerm-app-service', 'azure-regions', 'azure-java-sdk', 'azure-private-dns-zone', 'azure-function-queue', 'azure-data-share', 'azure-functions-docker', 'azure-advisor', 'azure-deployment', 'azure-managed-database', 'azure-mapping-data-flow', 'azure-application-roles', 'azure-deployment-slots', 'azure-cosmosdb-changefeed', 'azure-webhooks', 'azure-communication-services', 'azure-synapse-link', 'azure-storage-account', 'azure-storage-explorer', 'passport-azure-ad', 'azure-durable-functions', 'azure-search-.net-sdk', 'azure-language-understanding', 'azure-iot-hub-device-management', 'azure-api-apps', 'azure-application-gateway', 'azure-pack', 'azure-disk', 'azure-resource-group', 'azure-sdk-for-java', 'azureservicebus', 'azure-storage-queues', 'azure-resource-graph', 'azure-storage-files', 'azure-bicep', 'azure-data-sync', 'azure-management', 'azure-rm', 'azure-spring-cloud', 'azure-performancecounters', 'azure-pipelines-release-pipeline', 'azure-management-api', 'azure-management-groups', 'azure-policy', 'azure-servicebus-subscriptions', 'azure-files', 'microsoft-entra-external-id', 'azure-industrial-iot', 'azure-load-balancer', 'azure-elastic-scale', 'azure-application-insights-profiler', 'azure-information-protection', 'azure-managed-grafana', 'azure-container-apps', 'azure-blockchain-workbench', 'azure-static-web-app', 'azure-sdk-for-go', 'azure-monitoring', 'azure-function-http', 'azure-runbook', 'azure-eventhub', 'azure-service-runtime', 'azure-ad-b2b', 'azure-ml-component', 'azure-webjobs-continuous', 'azure-static-website-hosting', 'azure-sdk-php', 'azure-private-link', 'azure-digital-twins', 'azure-availability-zones', 'azure-agent', 'azure-subscription', 'azure-data-catalog', 'azure-migrate', 'azure-linux', 'azure.data.tables', 'azure-vpn', 'azure-oms', 'azure-application-settings', 'azure-app-configuration', 'azure-form-recognizer', 'kql', 'azure-http-trigger', 'azure-backup-vault', 'azure-synapse', 'azure-table-storage', 'azure-custom-providers', 'azure-iot-sdk', 'azure-container-registry', 'azure-authentication', 'spark-bash-azure-databricks', 'azure-data-studio', 'azure-sdk-js', 'azure-machine-learning-service', 'azure-rm-template', 'azure-custom-domain', 'azure-bastion', 'azure-sdk-go', 'azure-cli2', 'azure-ad-b2c', 'azure-cosmosdb-sqlapi', 'sitecore-azure', 'azure-application-insights', 'azure-cosmosdb-emulator', 'azure-timeseries-insights', 'azure-service-hooks', 'azure-fluent-api', 'azure-monitor-workbooks', 'azure-web-app-firewall', 'azure-cost-calculation', 'azure-cosmosdb-mongovcore', 'azure-connect', 'azureml-python-sdk', 'azure-ad-v2', 'azure-storage', 'azure-iot-edge', 'azure-cdn', 'microsoft-entra-private-access', 'azure-in-role-cache', 'azure-hybrid-connections', 'azure-data-explorer', 'azure-ad-domain-services', 'azure-speech', 'azure-store', 'azure-webjobs-triggered', 'azure-function-app', 'azureportal', 'azure-stack', 'sql-azure-alerts', 'azure-virtual-machine', 'terraform-provider-azure', 'azure-webapps', 'azure-sdk-for-ruby', 'azure-vm-scale-set', 'azure-rtos', 'azure-hub', 'azure-cli', 'azure-qna-maker', 'azure-marketplace', 'azure-logic-apps', 'azure-cosmosdb', 'azure-app-registration', 'azure-application-registration', 'azure-managed-identity', 'azure-cognitive-search', 'azure-ml-pipelines', 'azure-cognitive-services', 'azure-mysql-database', 'azure-sdk-python', 'azure-container-instances', 'azure-ase', 'azure-spring-boot', 'azure-storage-emulator', 'azure-sql-server', 'azure-data-lake', 'azure-ad-verifiable-credentials'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}], 'account_id': 4759475, 'is_employee': False, 'last_modified_date': 1656727800, 'last_access_date': 1711173546, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2320, 'creation_date': 1405561164, 'user_type': 'registered', 'user_id': 3847255, 'link': 'https://stackoverflow.com/users/3847255/pooya13', 'profile_image': 'https://www.gravatar.com/avatar/bc491d80914456ec7c6ff2fceafc5668?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'pooya13'}","What are the similarities and differences between numpy.divide and the Python slash / operator? As far as I can tell they behave the same, both implementing an element-wise division. The Numpy documentation mentions: numpy.divide(x1, x2) ... Equivalent to x1 / x2 in terms of array-broadcasting. ... Implying that np.divide(x1, x2) is not completely equivalent to x1 / x2. I have run the following snippet to compare their speed: It appears that the Python divide generally runs faster which leads me to believe the Numpy divide implements some additional bells and whistles. Any help is much appreciated!","import numpy as np
import time

a = np.random.rand(10000, 10000)
b = np.random.rand(10000, 10000)

tic = time.time()
c = a / b
toc = time.time()
print(""Python divide took: "", toc - tic)

tic = time.time()
c = np.divide(a, b)
toc = time.time()
print(""Numpy divide took: "", toc - tic)
",14,29,0,1,
124,49622575,49962590,19836,Schedule to start an EC2 instance and run a python script within it,3,<python><amazon-web-services><amazon-ec2>,17,"<p>I am trying to schedule my python script in AWS, however I don't want the instances to be running all the time. So, trying to automate the process of:</p>

<ol>
<li>Start the EC2 instance on a specific time</li>
<li>Run the python script within it</li>
<li>Stop the EC2 instance once the job is completed.</li>
</ol>

<p>I cannot run this script directly as a Lambda function because the script does some parallel processing which requires more RAM, so choosing a bigger AWS instance rather than writing it as a lambda function. Also, don't want this instance to be running all the time as it is expensive. </p>

<p>So far, I followed <a href=""https://matoski.com/article/aws-automatic-start-stop-instances-lambda/"" rel=""noreferrer"">Automatic starting and stopping of AWS EC2 instances with Lambda and CloudWatch · matoski.com</a> and created a Lambda function to start and stop the instance at specific time, however I couldn't find a way to run the python script once the instance is started.</p>

<p>Can anyone point me in the right direction?</p>
",3875107,2159,03-04-2018 05:02,22-04-2018 04:52,19,2159,71,5,39,84,"{'badge_counts': {'bronze': 71, 'silver': 39, 'gold': 5}, 'account_id': 4264769, 'is_employee': False, 'last_modified_date': 1711175700, 'last_access_date': 1634156094, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2159, 'creation_date': 1406247793, 'user_type': 'registered', 'user_id': 3875107, 'accept_rate': 84, 'location': 'New Zealand', 'website_url': '', 'link': 'https://stackoverflow.com/users/3875107/ds-user', 'profile_image': 'https://www.gravatar.com/avatar/1afb9529d80743b9f1d8dffe1679ad70?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ds_user'}","I am trying to schedule my python script in AWS, however I don't want the instances to be running all the time. So, trying to automate the process of: Start the EC2 instance on a specific time Run the python script within it Stop the EC2 instance once the job is completed. I cannot run this script directly as a Lambda function because the script does some parallel processing which requires more RAM, so choosing a bigger AWS instance rather than writing it as a lambda function. Also, don't want this instance to be running all the time as it is expensive. So far, I followed Automatic starting and stopping of AWS EC2 instances with Lambda and CloudWatch · matoski.com and created a Lambda function to start and stop the instance at specific time, however I couldn't find a way to run the python script once the instance is started. Can anyone point me in the right direction?",,0,13,0,1,
125,48686945,48690064,98033,reshaping a tensor with padding in pytorch,6,<python><pytorch>,45,"<p>How do I reshape a tensor with dimensions <code>(30, 35, 49)</code> to <code>(30, 35, 512)</code> by padding it?</p>
",4393898,3661,08-02-2018 13:43,08-02-2018 16:18,0,3661,86,8,47,67,"{'badge_counts': {'bronze': 86, 'silver': 47, 'gold': 8}, 'account_id': 5535357, 'is_employee': False, 'last_modified_date': 1693165200, 'last_access_date': 1710266998, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3661, 'creation_date': 1419519384, 'user_type': 'registered', 'user_id': 4393898, 'accept_rate': 67, 'location': 'Tokyo, Japan', 'website_url': '', 'link': 'https://stackoverflow.com/users/4393898/yusuf', 'profile_image': 'https://www.gravatar.com/avatar/d02bece884458461a08731fc107aa4d1?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'yusuf'}",How do I reshape a tensor with dimensions to by padding it?,"(30, 35, 49) (30, 35, 512)",-2,1,0,0,
126,48347515,48347516,4928,Modulo operation on a python negative decimal.Decimal and a positive int,2,<python><python-3.x><decimal>,13,"<p>With simple <code>int</code>s:</p>

<pre><code>&gt;&gt;&gt; -45 % 360
315
</code></pre>

<p>Whereas, using a <code>decimal.Decimal</code>:</p>

<pre><code>&gt;&gt;&gt; from decimal import Decimal
&gt;&gt;&gt; Decimal('-45') % 360
Decimal('-45')
</code></pre>

<p>I would expect to get <code>Decimal('315')</code>. </p>

<p>Is there any reason for this? Is there a way to get a consistent behaviour (without patching <code>decimal.Decimal</code>)? (I did not change the context, and cannot find how it could be changed to solve this situation).</p>
",3926735,4816,19-01-2018 18:35,19-01-2018 18:35,0,4826,59,6,30,73,"{'badge_counts': {'bronze': 59, 'silver': 30, 'gold': 6}, 'account_id': 4869804, 'is_employee': False, 'last_modified_date': 1622750700, 'last_access_date': 1710966895, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4826, 'creation_date': 1407665399, 'user_type': 'registered', 'user_id': 3926735, 'accept_rate': 73, 'location': 'Marvejols, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/3926735/zezollo', 'profile_image': 'https://www.gravatar.com/avatar/cd07f4b7ab3002264309b8dbf5eecfa7?s=256&d=identicon&r=PG', 'display_name': 'zezollo'}","With simple s: Whereas, using a : I would expect to get . Is there any reason for this? Is there a way to get a consistent behaviour (without patching )? (I did not change the context, and cannot find how it could be changed to solve this situation).","int &gt;&gt;&gt; -45 % 360
315
 decimal.Decimal &gt;&gt;&gt; from decimal import Decimal
&gt;&gt;&gt; Decimal('-45') % 360
Decimal('-45')
 Decimal('315') decimal.Decimal",-1,16,0,0,
127,49563445,49563946,8896,Type annotation for boto3 resources like DynamoDB.Table,4,<python><python-3.x><amazon-web-services><aws-sdk><boto3>,23,"<p>The <code>boto3</code> library provides several factory methods that returns resources. For example:</p>

<pre><code>dynamo = (
    boto3
    .resource('dynamodb')
    .Table(os.environ['DYNAMODB_TABLE'])
)
</code></pre>

<p>I want to annotate those resources so I can get better type checking and completion, but the only type alike I could find was <code>from boto3.dynamodb.table import TableResource</code>.</p>

<p>When I add that annotation:</p>

<pre><code>dynamo: TableResource = (
    boto3
    .resource('dynamodb')
    .Table(os.environ['DYNAMODB_TABLE'])
)
</code></pre>

<p>The only method offered by auto-completion is <code>batch_writer(self, overwrite_by_pkeys)</code>, even though the docs <a href=""http://boto3.readthedocs.io/en/latest/reference/services/dynamodb.html#table"" rel=""noreferrer"">lists several others</a>.</p>

<p>Is this the wrong class to use as annotation? Inspecting that variable type in the terminal I could see that it was <code>&lt;class 'boto3.resources.factory.dynamodb.Table'&gt;</code>, but it doesn't seem to be possible to get that type statically.</p>
",3935325,6513,29-03-2018 19:10,29-03-2018 19:47,0,6533,82,3,45,86,"{'badge_counts': {'bronze': 82, 'silver': 45, 'gold': 3}, 'account_id': 4881680, 'is_employee': False, 'last_modified_date': 1694299804, 'last_access_date': 1711031735, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 6533, 'creation_date': 1407882537, 'user_type': 'registered', 'user_id': 3935325, 'accept_rate': 86, 'location': 'S&#227;o Paulo, State of S&#227;o Paulo, Brazil', 'website_url': 'http://victor.villas', 'link': 'https://stackoverflow.com/users/3935325/villasv', 'profile_image': 'https://i.stack.imgur.com/jjwPe.jpg?s=256&g=1', 'display_name': 'villasv'}","The library provides several factory methods that returns resources. For example: I want to annotate those resources so I can get better type checking and completion, but the only type alike I could find was . When I add that annotation: The only method offered by auto-completion is , even though the docs lists several others. Is this the wrong class to use as annotation? Inspecting that variable type in the terminal I could see that it was , but it doesn't seem to be possible to get that type statically.","boto3 dynamo = (
    boto3
    .resource('dynamodb')
    .Table(os.environ['DYNAMODB_TABLE'])
)
 from boto3.dynamodb.table import TableResource dynamo: TableResource = (
    boto3
    .resource('dynamodb')
    .Table(os.environ['DYNAMODB_TABLE'])
)
 batch_writer(self, overwrite_by_pkeys) &lt;class 'boto3.resources.factory.dynamodb.Table'&gt;",4,23,0,1,
128,48543460,48543552,7223,How to use user-defined class object as a networkx node?,3,<python><class><nodes><networkx><user-defined>,15,"<p>Class point is defined as (there are also some methods, atributes, and stuff in it, but this is minimal part):</p>

<pre><code>class point():
    def ___init___(self, x, y):
        self.x = x
        self.y = y
</code></pre>

<p>So, I saw <a href=""https://softwareengineering.stackexchange.com/questions/136216/networkx-python-is-using-a-class-for-a-node-better-practice-than-defining-mu/136217#136217"">this question</a>, but when I tried applying it, it returns an error:</p>

<pre><code>G = nx.Graph()
p = point(0,0)
G.add_node(0, p)
</code></pre>

<p>NetworkXError: The attr_dict argument must be a dictionary.</p>

<p>If i use </p>

<pre><code>G = nx.Graph()
p = point(0,0)
G.add_node(0, data = p)
</code></pre>

<p>I don't get an error, but when i try to access the x-coordinate, it turns out it didn't save it as a point.</p>

<pre><code>G[0].x
</code></pre>

<p>returns: AttributeError: 'dict' object has no attribute 'x'</p>

<p>doing </p>

<pre><code>G = nx.Graph()
G.add_node(0, data = point(0,0))
G[0]
</code></pre>

<p>returns:
{}</p>

<p>which means it still saves it as a dictionary.</p>

<p>I saw I can make my points hashable, and use these objects as nodes, so i added atribute id, since points are going to move. I added this to the class, and __repr__ for nice drawing of the graphs:</p>

<pre><code>def __hash__(self):
    return self.id_n
def __cmp__(self, p):
    if self.id_n &lt; p.id_n: return -1
    elif self.id_n == p.id_n: return 0
    else: return 1
def __eq__(self, p):
    if p.id_n == self.id_n: return True
    else: return False
def __repr__(self):
    return str(self.id_n) 
</code></pre>

<p>but that is a bit wierd, since I don't understand how to select a node then, by </p>

<pre><code>G[&lt;what should i put here?&gt;]
</code></pre>

<p>So, question is, what is a proper way to do this?</p>

<p>I hoped to be able to use something like</p>

<pre><code>G[node_id].some_method(some_args)
</code></pre>
",3960696,339,31-01-2018 13:34,31-01-2018 13:39,0,339,9,0,3,100,"{'badge_counts': {'bronze': 9, 'silver': 3, 'gold': 0}, 'account_id': 4917012, 'is_employee': False, 'last_modified_date': 1658918673, 'last_access_date': 1521740367, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 339, 'creation_date': 1408544939, 'user_type': 'registered', 'user_id': 3960696, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/3960696/luka-petrovi%c4%87', 'profile_image': 'https://www.gravatar.com/avatar/ec69cc38185d3e5d440b493916a958d8?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Luka Petrović'}","Class point is defined as (there are also some methods, atributes, and stuff in it, but this is minimal part): So, I saw this question, but when I tried applying it, it returns an error: NetworkXError: The attr_dict argument must be a dictionary. If i use I don't get an error, but when i try to access the x-coordinate, it turns out it didn't save it as a point. returns: AttributeError: 'dict' object has no attribute 'x' doing returns: {} which means it still saves it as a dictionary. I saw I can make my points hashable, and use these objects as nodes, so i added atribute id, since points are going to move. I added this to the class, and __repr__ for nice drawing of the graphs: but that is a bit wierd, since I don't understand how to select a node then, by So, question is, what is a proper way to do this? I hoped to be able to use something like","class point():
    def ___init___(self, x, y):
        self.x = x
        self.y = y
 G = nx.Graph()
p = point(0,0)
G.add_node(0, p)
 G = nx.Graph()
p = point(0,0)
G.add_node(0, data = p)
 G[0].x
 G = nx.Graph()
G.add_node(0, data = point(0,0))
G[0]
 def __hash__(self):
    return self.id_n
def __cmp__(self, p):
    if self.id_n &lt; p.id_n: return -1
    elif self.id_n == p.id_n: return 0
    else: return 1
def __eq__(self, p):
    if p.id_n == self.id_n: return True
    else: return False
def __repr__(self):
    return str(self.id_n) 
 G[&lt;what should i put here?&gt;]
 G[node_id].some_method(some_args)
",19,69,0,1,
129,48274890,48275247,1226,Treat an emoji as one character in a regex,4,<python><regex><python-2.7><python-unicode><unicode-literals>,11,"<p>Here's a small example:</p>

<pre><code>reg = ur""((?P&lt;initial&gt;[+\-👍])(?P&lt;rest&gt;.+?))$""
</code></pre>

<p>(In both cases the file has <code>-*- coding: utf-8 -*-</code>)</p>

<p>In Python 2:</p>

<pre><code>re.match(reg, u""👍hello"").groupdict()
# =&gt; {u'initial': u'\ud83d', u'rest': u'\udc4dhello'}
# unicode why must you do this
</code></pre>

<p>Whereas, in Python 3:</p>

<pre><code>re.match(reg, ""👍hello"").groupdict()
# =&gt; {'initial': '👍', 'rest': 'hello'}
</code></pre>

<p>The above behaviour is 100% perfect, but switching to Python 3 is currently not an option. What's the best way to replicate 3's results in 2, that works in both narrow and wide Python builds? The 👍 appears to be coming to me in the format ""\ud83d\udc4d"", which is what's making this tricky.</p>
",4014075,702,16-01-2018 05:53,16-01-2018 06:29,0,712,44,2,16,80,"{'badge_counts': {'bronze': 44, 'silver': 16, 'gold': 2}, 'collectives': [{'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 4973385, 'is_employee': False, 'last_modified_date': 1682979363, 'last_access_date': 1711075447, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 712, 'creation_date': 1409989867, 'user_type': 'registered', 'user_id': 4014075, 'accept_rate': 80, 'location': 'Iselin, NJ, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/4014075/naiveai', 'profile_image': 'https://www.gravatar.com/avatar/dfac6f770d9e6b2a82d909382d7da9c6?s=256&d=identicon&r=PG', 'display_name': 'naiveai'}","Here's a small example: (In both cases the file has ) In Python 2: Whereas, in Python 3: The above behaviour is 100% perfect, but switching to Python 3 is currently not an option. What's the best way to replicate 3's results in 2, that works in both narrow and wide Python builds? The 👍 appears to be coming to me in the format ""\ud83d\udc4d"", which is what's making this tricky.","reg = ur""((?P&lt;initial&gt;[+\-👍])(?P&lt;rest&gt;.+?))$""
 -*- coding: utf-8 -*- re.match(reg, u""👍hello"").groupdict()
# =&gt; {u'initial': u'\ud83d', u'rest': u'\udc4dhello'}
# unicode why must you do this
 re.match(reg, ""👍hello"").groupdict()
# =&gt; {'initial': '👍', 'rest': 'hello'}
",2,21,0,0,
130,48343669,53135936,13000,How to install Python packages over SSH Port Forwarding?,3,<python><ssh><pip>,11,"<p>I am controlling a remote unit over SSH and OPENVPN.</p>

<p>On the remote unit I want to install some Python packages using <code>pip</code> but:</p>

<ul>
<li>the remote company firewall allows only traffic on port 22 (and not 443, needed by <code>pip</code>);</li>
<li>DNS is not installed on the remote unit;</li>
<li>I cannot modify any OPENVPN settings (or I would like to avoid this option as it means to access some remote sysadmin and try to convince him that the configuration must be changed);</li>
<li>all systems are Linux (Ubuntu + Debian). Non Windows involved.</li>
</ul>

<p>Stripping down hours of attempts (<strong>I am not a system admin</strong> and my knowledge on this subject is very limited), the idea was to open an obvious SSH port forwarding:</p>

<p><code>ssh -R 9999:pypi.python.org:443 xxxx@XX.XX.XX.XX</code></p>

<p>and then, on the remote unit play with <code>pip install</code>:</p>

<p><code>pip install pymodbus==1.3.2 --proxy localhost:9999</code></p>

<p>But this command returns:</p>

<pre><code>Cannot fetch index base URL https://pypi.python.org/simple/
Could not find any downloads that satisfy the requirement pymodbus==1.3.2
</code></pre>

<p><code>/root/.pip/pip.log</code> is: </p>

<pre><code>  Getting page https://pypi.python.org/simple/pymodbus/
  Could not fetch URL https://pypi.python.org/simple/pymodbus/: connection error: ('Connection aborted.', BadStatusLine(""''"",))
  Will skip URL https://pypi.python.org/simple/pymodbus/ when looking for download links for pymodbus==1.3.2
  Getting page https://pypi.python.org/simple/
  Could not fetch URL https://pypi.python.org/simple/: connection error: ('Connection aborted.', BadStatusLine(""''"",))
  Will skip URL https://pypi.python.org/simple/ when looking for download links for pymodbus==1.3.2
  Cannot fetch index base URL https://pypi.python.org/simple/
  URLs to search for versions for pymodbus==1.3.2:
  * https://pypi.python.org/simple/pymodbus/1.3.2
  * https://pypi.python.org/simple/pymodbus/
  Getting page https://pypi.python.org/simple/pymodbus/1.3.2
  Could not fetch URL https://pypi.python.org/simple/pymodbus/1.3.2: connection error: ('Connection aborted.', BadStatusLine(""''"",))
  Will skip URL https://pypi.python.org/simple/pymodbus/1.3.2 when looking for download links for pymodbus==1.3.2
  Getting page https://pypi.python.org/simple/pymodbus/
</code></pre>

<p>It is obvious the remote unit cannot read the index page on pypi.pthon.org because the connection is refused.</p>

<p>What is the <strong>correct syntax</strong> for what I am trying to achieve?</p>
",4106261,2478,19-01-2018 14:48,03-11-2018 22:08,288,2506,52,4,27,97,"{'badge_counts': {'bronze': 52, 'silver': 27, 'gold': 4}, 'account_id': 5123323, 'is_employee': False, 'last_modified_date': 1680091200, 'last_access_date': 1711020022, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2506, 'creation_date': 1412348948, 'user_type': 'registered', 'user_id': 4106261, 'accept_rate': 97, 'website_url': '', 'link': 'https://stackoverflow.com/users/4106261/alex-poca', 'profile_image': 'https://lh5.googleusercontent.com/-iJUgSCa5y0A/AAAAAAAAAAI/AAAAAAAADY0/Kjs5T9umLVE/photo.jpg?sz=256', 'display_name': 'Alex Poca'}","I am controlling a remote unit over SSH and OPENVPN. On the remote unit I want to install some Python packages using but: the remote company firewall allows only traffic on port 22 (and not 443, needed by ); DNS is not installed on the remote unit; I cannot modify any OPENVPN settings (or I would like to avoid this option as it means to access some remote sysadmin and try to convince him that the configuration must be changed); all systems are Linux (Ubuntu + Debian). Non Windows involved. Stripping down hours of attempts (I am not a system admin and my knowledge on this subject is very limited), the idea was to open an obvious SSH port forwarding: and then, on the remote unit play with : But this command returns: is: It is obvious the remote unit cannot read the index page on pypi.pthon.org because the connection is refused. What is the correct syntax for what I am trying to achieve?","pip pip ssh -R 9999:pypi.python.org:443 xxxx@XX.XX.XX.XX pip install pip install pymodbus==1.3.2 --proxy localhost:9999 Cannot fetch index base URL https://pypi.python.org/simple/
Could not find any downloads that satisfy the requirement pymodbus==1.3.2
 /root/.pip/pip.log   Getting page https://pypi.python.org/simple/pymodbus/
  Could not fetch URL https://pypi.python.org/simple/pymodbus/: connection error: ('Connection aborted.', BadStatusLine(""''"",))
  Will skip URL https://pypi.python.org/simple/pymodbus/ when looking for download links for pymodbus==1.3.2
  Getting page https://pypi.python.org/simple/
  Could not fetch URL https://pypi.python.org/simple/: connection error: ('Connection aborted.', BadStatusLine(""''"",))
  Will skip URL https://pypi.python.org/simple/ when looking for download links for pymodbus==1.3.2
  Cannot fetch index base URL https://pypi.python.org/simple/
  URLs to search for versions for pymodbus==1.3.2:
  * https://pypi.python.org/simple/pymodbus/1.3.2
  * https://pypi.python.org/simple/pymodbus/
  Getting page https://pypi.python.org/simple/pymodbus/1.3.2
  Could not fetch URL https://pypi.python.org/simple/pymodbus/1.3.2: connection error: ('Connection aborted.', BadStatusLine(""''"",))
  Will skip URL https://pypi.python.org/simple/pymodbus/1.3.2 when looking for download links for pymodbus==1.3.2
  Getting page https://pypi.python.org/simple/pymodbus/
",8,46,0,0,
131,48204780,48205096,34363,How to plot multiple figures in a row using seaborn,1,<python><matplotlib><data-visualization><seaborn>,12,"<p>I have a dataframe <code>df</code> that looks like this:</p>

<pre><code>df.head()
id        feedback        nlp_model        similarity_score
0xijh4    1               tfidf            0.36
0sdnj7    -1              lda              0.89
kjh458    1               doc2vec          0.78
....
</code></pre>

<p>I want to plot <code>similairty_score</code> versus feedback in a boxplot form using seaborn for each of the unique values in the <code>model</code> column: <code>tfidf</code>, <code>lda</code>, <code>doc2vec</code>. My code for this is as follows:</p>

<pre><code>fig, ax = plt.subplots(figsize=(10,8))
ax = sns.boxplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='tfidf'])
ax = sns.swarmplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='tfidf'], color=""0.25"")

fig, ax = plt.subplots(figsize=(10,8))
ax = sns.boxplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='lda'])
ax = sns.swarmplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='lda'], color=""0.25"")

fig, ax = plt.subplots(figsize=(10,8))
ax = sns.boxplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='doc2vec'])
ax = sns.swarmplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='doc2vec'], color=""0.25"")

plt.show()
</code></pre>

<p>The problem is this creates 3 plots one on top of the other.</p>

<p><a href=""https://i.stack.imgur.com/J7H64.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/J7H64.png"" alt=""enter image description here""></a></p>

<p>How can I generate these same plots but all on a single line, with one axis marking ""Similarity Score"" on the left most plot only, and ""Feedback"" axis label directly below each plot?</p>
",4139143,7110,11-01-2018 10:25,11-01-2018 10:40,0,7120,86,12,54,79,"{'badge_counts': {'bronze': 86, 'silver': 54, 'gold': 12}, 'account_id': 5170471, 'is_employee': False, 'last_modified_date': 1698456900, 'last_access_date': 1710603083, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 7120, 'creation_date': 1413235420, 'user_type': 'registered', 'user_id': 4139143, 'accept_rate': 79, 'website_url': '', 'link': 'https://stackoverflow.com/users/4139143/pyrsquared', 'profile_image': 'https://i.stack.imgur.com/J8cH3.png?s=256&g=1', 'display_name': 'PyRsquared'}","I have a dataframe that looks like this: I want to plot versus feedback in a boxplot form using seaborn for each of the unique values in the column: , , . My code for this is as follows: The problem is this creates 3 plots one on top of the other. How can I generate these same plots but all on a single line, with one axis marking ""Similarity Score"" on the left most plot only, and ""Feedback"" axis label directly below each plot?","df df.head()
id        feedback        nlp_model        similarity_score
0xijh4    1               tfidf            0.36
0sdnj7    -1              lda              0.89
kjh458    1               doc2vec          0.78
....
 similairty_score model tfidf lda doc2vec fig, ax = plt.subplots(figsize=(10,8))
ax = sns.boxplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='tfidf'])
ax = sns.swarmplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='tfidf'], color=""0.25"")

fig, ax = plt.subplots(figsize=(10,8))
ax = sns.boxplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='lda'])
ax = sns.swarmplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='lda'], color=""0.25"")

fig, ax = plt.subplots(figsize=(10,8))
ax = sns.boxplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='doc2vec'])
ax = sns.swarmplot(x=""feedback"", y=""similarity_score"", data=df[df.nlp_model=='doc2vec'], color=""0.25"")

plt.show()
",11,32,1,1,
132,49390682,49390941,20642,Convert pandas.core.groupby.SeriesGroupBy to a DataFrame,1,<python><pandas><dataframe><apply><pandas-groupby>,11,"<p>This <a href=""https://stackoverflow.com/questions/46807334/converting-pandas-core-groupby-seriesgroupby-to-dataframe"">question</a> didn't have a satisfactory answer, so I'm asking it again. </p>

<p>Suppose I have the following Pandas DataFrame:</p>

<pre><code>df1 = pd.DataFrame({'group': ['a', 'a', 'b', 'b'], 'values': [1, 1, 2, 2]})
</code></pre>

<p>I group by the first column 'group':</p>

<pre><code>g1 = df1.groupby('group')
</code></pre>

<p>I've now created a ""<strong>DataFrame</strong>GroupBy"". Then I extract the first column from the GroupBy object:</p>

<pre><code>g1_1st_column = g1['group']
</code></pre>

<p>The type of g1_1st_column is ""pandas.core.groupby.<strong>Series</strong>GroupBy"". Notice it's not a ""<strong>DataFrame</strong>GroupBy"" anymore. </p>

<p>My question is, how can I convert the SeriesGroupBy object back to a DataFrame object? I tried using the .to_frame() method, and got the following error:</p>

<pre><code>g1_1st_column = g1['group'].to_frame()
</code></pre>

<p><strong>AttributeError</strong>: Cannot access callable attribute 'to_frame' of 'SeriesGroupBy' objects, try using the 'apply' method. </p>

<p>How would I use the apply method, or some other method, to convert to a DataFrame?</p>
",3385948,5148,20-03-2018 17:31,20-03-2018 17:45,0,5158,66,9,42,86,"{'badge_counts': {'bronze': 66, 'silver': 42, 'gold': 9}, 'account_id': 3951462, 'is_employee': False, 'last_modified_date': 1700273100, 'last_access_date': 1711124886, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5158, 'creation_date': 1394064325, 'user_type': 'registered', 'user_id': 3385948, 'accept_rate': 86, 'location': 'Calgary, Alberta, Canada', 'website_url': 'https://myijack.com', 'link': 'https://stackoverflow.com/users/3385948/sean-mccarthy', 'profile_image': 'https://i.stack.imgur.com/YJIjJ.png?s=256&g=1', 'display_name': 'Sean McCarthy'}","This question didn't have a satisfactory answer, so I'm asking it again. Suppose I have the following Pandas DataFrame: I group by the first column 'group': I've now created a ""DataFrameGroupBy"". Then I extract the first column from the GroupBy object: The type of g1_1st_column is ""pandas.core.groupby.SeriesGroupBy"". Notice it's not a ""DataFrameGroupBy"" anymore. My question is, how can I convert the SeriesGroupBy object back to a DataFrame object? I tried using the .to_frame() method, and got the following error: AttributeError: Cannot access callable attribute 'to_frame' of 'SeriesGroupBy' objects, try using the 'apply' method. How would I use the apply method, or some other method, to convert to a DataFrame?","df1 = pd.DataFrame({'group': ['a', 'a', 'b', 'b'], 'values': [1, 1, 2, 2]})
 g1 = df1.groupby('group')
 g1_1st_column = g1['group']
 g1_1st_column = g1['group'].to_frame()
",0,27,0,1,
133,49323439,49323656,10972,Plot the Geometry of one row of a GeoDataFrame,1,<python><pandas><geopandas>,13,"<p>I would like to plot the geometry contained in a single row of a geopandas dataframe, but I am having problems. Here an example</p>

<pre><code>import geopandas as gpd
import numpy as np
from shapely.geometry import Polygon

p1 = Polygon([(0, 0), (1, 0), (1, 1)])
p2 = Polygon([(2, 0), (3, 0), (3, 1), (2, 1)])
p3 = Polygon([(1, 1), (2, 1), (2, 2), (1, 2)])
index = np.random.random(3)
df = gpd.GeoDataFrame()
df['index'] = index
df['geometry'] = [p1,p2,p3]
df = df.set_geometry('geometry')
</code></pre>

<p>Now if I plot using the command <code>df.plot()</code> I get </p>

<p><a href=""https://i.stack.imgur.com/wEkMf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wEkMf.png"" alt=""enter image description here""></a></p>

<p>but if I try to plot only one row, <code>df.loc[:,0].plot()</code> </p>

<p>I get the following error </p>

<p><code>TypeError: Empty 'DataFrame': no numeric data to plot</code>, </p>

<p>while if if i try </p>

<p><code>df.loc[:,'geometry'].plot()</code></p>

<p>I get <code>AttributeError: 'Polygon' object has no attribute 'plot'</code></p>

<p>What is the correct way of doing this ?</p>
",4521095,1430,16-03-2018 14:43,16-03-2018 14:53,0,1430,27,3,15,64,"{'badge_counts': {'bronze': 27, 'silver': 15, 'gold': 3}, 'account_id': 5723287, 'is_employee': False, 'last_modified_date': 1662490033, 'last_access_date': 1711105299, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1430, 'creation_date': 1422900542, 'user_type': 'registered', 'user_id': 4521095, 'accept_rate': 64, 'location': 'Rome', 'website_url': '', 'link': 'https://stackoverflow.com/users/4521095/duccio-piovani', 'profile_image': 'https://i.stack.imgur.com/AaiLE.jpg?s=256&g=1', 'display_name': 'Duccio Piovani'}","I would like to plot the geometry contained in a single row of a geopandas dataframe, but I am having problems. Here an example Now if I plot using the command I get but if I try to plot only one row, I get the following error , while if if i try I get What is the correct way of doing this ?","import geopandas as gpd
import numpy as np
from shapely.geometry import Polygon

p1 = Polygon([(0, 0), (1, 0), (1, 1)])
p2 = Polygon([(2, 0), (3, 0), (3, 1), (2, 1)])
p3 = Polygon([(1, 1), (2, 1), (2, 2), (1, 2)])
index = np.random.random(3)
df = gpd.GeoDataFrame()
df['index'] = index
df['geometry'] = [p1,p2,p3]
df = df.set_geometry('geometry')
 df.plot() df.loc[:,0].plot() TypeError: Empty 'DataFrame': no numeric data to plot df.loc[:,'geometry'].plot() AttributeError: 'Polygon' object has no attribute 'plot'",6,33,1,1,
134,49554139,49554843,211662,Boxplot of Multiple Columns of a Pandas Dataframe on the Same Figure (seaborn),4,<python><pandas><seaborn>,68,"<p>I feel I am probably not thinking of something obvious. I want to put in the same figure, the box plot of every column of a dataframe, where on the x-axis I have the columns' names. In the <code>seaborn.boxplot()</code> this would be equal to <code>groupby</code> by every column. </p>

<p>In pandas I would do </p>

<pre><code>df = pd.DataFrame(data = np.random.random(size=(4,4)), columns = ['A','B','C','D'])
df.boxplot()
</code></pre>

<p>which yields</p>

<p><a href=""https://i.stack.imgur.com/6NHvp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6NHvp.png"" alt=""enter image description here""></a></p>

<p>Now I would like to get the same thing in seaborn. But when I try <code>sns.boxplot(df)</code>, I get only one grouped boxplot. How do I reproduce the same figure in seaborn? </p>
",4521095,1430,29-03-2018 10:48,29-03-2018 11:23,0,1430,27,3,15,64,"{'badge_counts': {'bronze': 27, 'silver': 15, 'gold': 3}, 'account_id': 5723287, 'is_employee': False, 'last_modified_date': 1662490033, 'last_access_date': 1711105299, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1430, 'creation_date': 1422900542, 'user_type': 'registered', 'user_id': 4521095, 'accept_rate': 64, 'location': 'Rome', 'website_url': '', 'link': 'https://stackoverflow.com/users/4521095/duccio-piovani', 'profile_image': 'https://i.stack.imgur.com/AaiLE.jpg?s=256&g=1', 'display_name': 'Duccio Piovani'}","I feel I am probably not thinking of something obvious. I want to put in the same figure, the box plot of every column of a dataframe, where on the x-axis I have the columns' names. In the this would be equal to by every column. In pandas I would do which yields Now I would like to get the same thing in seaborn. But when I try , I get only one grouped boxplot. How do I reproduce the same figure in seaborn?","seaborn.boxplot() groupby df = pd.DataFrame(data = np.random.random(size=(4,4)), columns = ['A','B','C','D'])
df.boxplot()
 sns.boxplot(df)",-2,13,1,1,
135,50242968,50243108,236120,Check for duplicate values in Pandas dataframe column,4,<python><pandas><dataframe><duplicates>,90,"<p><strong>Is there a way in pandas to check if a dataframe column has duplicate values, without actually dropping rows?</strong> I have a function that will remove duplicate rows, however, I only want it to run if there are actually duplicates in a specific column.</p>

<p>Currently I compare the number of unique values in the column to the number of rows: if there are less unique values than rows then there are duplicates and the code runs.</p>

<pre><code> if len(df['Student'].unique()) &lt; len(df.index):
    # Code to remove duplicates based on Date column runs
</code></pre>

<p>Is there an easier or more efficient way to check if duplicate values exist in a specific column, using pandas?</p>

<p>Some of the sample data I am working with (only two columns shown). If duplicates are found then another function identifies which row to keep (row with oldest date):</p>

<pre><code>    Student Date
0   Joe     December 2017
1   James   January 2018
2   Bob     April 2018
3   Joe     December 2017
4   Jack    February 2018
5   Jack    March 2018
</code></pre>
",4340630,1097,08-05-2018 22:11,08-05-2018 22:28,0,1097,16,1,8,,"{'badge_counts': {'bronze': 16, 'silver': 8, 'gold': 1}, 'account_id': 5457685, 'is_employee': False, 'last_modified_date': 1702084500, 'last_access_date': 1710978879, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1097, 'creation_date': 1418115231, 'user_type': 'registered', 'user_id': 4340630, 'location': 'Auckland, New Zealand', 'website_url': 'http://www.coachgrowth.wordpress.com', 'link': 'https://stackoverflow.com/users/4340630/jeff-mitchell', 'profile_image': 'https://www.gravatar.com/avatar/21750a83421b1eb1ffe30711699428b6?s=256&d=identicon&r=PG', 'display_name': 'Jeff Mitchell'}","Is there a way in pandas to check if a dataframe column has duplicate values, without actually dropping rows? I have a function that will remove duplicate rows, however, I only want it to run if there are actually duplicates in a specific column. Currently I compare the number of unique values in the column to the number of rows: if there are less unique values than rows then there are duplicates and the code runs. Is there an easier or more efficient way to check if duplicate values exist in a specific column, using pandas? Some of the sample data I am working with (only two columns shown). If duplicates are found then another function identifies which row to keep (row with oldest date):"," if len(df['Student'].unique()) &lt; len(df.index):
    # Code to remove duplicates based on Date column runs
     Student Date
0   Joe     December 2017
1   James   January 2018
2   Bob     April 2018
3   Joe     December 2017
4   Jack    February 2018
5   Jack    March 2018
",7,20,0,0,
136,48714769,48715014,76517,Python flask-cors ImportError: No module named 'flask-cors' Raspberry pi,10,<python><flask><raspberry-pi><flask-cors>,15,"<p>I'm following the flask-cors tutorial from the documentation here:
<a href=""https://pypi.python.org/pypi/Flask-Cors"" rel=""noreferrer"">https://pypi.python.org/pypi/Flask-Cors</a></p>

<p>but when i installed it on my raspberry pi and run my python app i'm getting this error</p>

<p><code>Traceback (most recent call last):
  File ""app.py"", line 3, in &lt;module&gt;
    from flask_cors import CORS, cross_origin
ImportError: No module named 'flask_cors'</code></p>

<p>here is my python script:</p>

<pre><code>from flask import Flask
from Main import main
from flask_cors import CORS, cross_origin    
app = Flask(__name__)
CORS(app)
main = main() 

@app.route('/turn' ,methods=['GET', 'OPTIONS'])
def index():
  return main.turn()

if __name__ == '__main__': 
  app.run(debug=True, host='0.0.0.0')
</code></pre>
",4664894,359,09-02-2018 22:03,09-02-2018 22:25,0,359,17,1,5,40,"{'badge_counts': {'bronze': 17, 'silver': 5, 'gold': 1}, 'account_id': 5930160, 'is_employee': False, 'last_modified_date': 1627387694, 'last_access_date': 1697465700, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 359, 'creation_date': 1426196698, 'user_type': 'registered', 'user_id': 4664894, 'accept_rate': 40, 'location': 'Barcelona, Spain', 'link': 'https://stackoverflow.com/users/4664894/nelson-candia', 'profile_image': 'https://www.gravatar.com/avatar/f364c9f9cf36c8a912f4b490dc536590?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nelson Candia'}",I'm following the flask-cors tutorial from the documentation here: https://pypi.python.org/pypi/Flask-Cors but when i installed it on my raspberry pi and run my python app i'm getting this error here is my python script:,"Traceback (most recent call last):
  File ""app.py"", line 3, in &lt;module&gt;
    from flask_cors import CORS, cross_origin
ImportError: No module named 'flask_cors' from flask import Flask
from Main import main
from flask_cors import CORS, cross_origin    
app = Flask(__name__)
CORS(app)
main = main() 

@app.route('/turn' ,methods=['GET', 'OPTIONS'])
def index():
  return main.turn()

if __name__ == '__main__': 
  app.run(debug=True, host='0.0.0.0')
",14,26,0,1,
137,49754137,49754261,4056,Empty class with comment same as pass?,2,<python>,14,"<p>Are these equivalent?</p>

<pre><code>class Empty : pass
</code></pre>

<p>and</p>

<pre><code>class Empty:
    '''
    This class intentionally left blank
    '''
</code></pre>

<p>The second one seems better for readability and one could put <code>pass</code> at the end but it does not seem necessary.</p>

<p>Is the comment treated as a <code>pass</code>?</p>
",4360746,5575,10-04-2018 12:47,10-04-2018 12:53,0,5605,69,4,35,53,"{'badge_counts': {'bronze': 69, 'silver': 35, 'gold': 4}, 'account_id': 5486002, 'is_employee': False, 'last_modified_date': 1668700805, 'last_access_date': 1695806972, 'reputation_change_year': 140, 'reputation_change_quarter': 140, 'reputation_change_month': 80, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 5605, 'creation_date': 1418606642, 'user_type': 'registered', 'user_id': 4360746, 'accept_rate': 53, 'location': 'Boston, MA', 'website_url': 'http://www.raydanielmystery.com', 'link': 'https://stackoverflow.com/users/4360746/ray-salemi', 'profile_image': 'https://www.gravatar.com/avatar/1daf5b4cd599a49ba6464490ee2a7dd1?s=256&d=identicon&r=PG', 'display_name': 'Ray Salemi'}",Are these equivalent? and The second one seems better for readability and one could put at the end but it does not seem necessary. Is the comment treated as a ?,"class Empty : pass
 class Empty:
    '''
    This class intentionally left blank
    '''
 pass pass",1,16,0,0,
138,49110112,49110123,8986,concatenate multiindex into single index in pandas series,1,<python><pandas>,16,"<p>I have a <code>pandas.Series</code> with multiindex:</p>

<pre><code>index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),
                                   ('two', 'a'), ('two', 'b')])
s = pd.Series(np.arange(1.0, 5.0), index=index)
print(s)
one  a   1.0
     b   2.0
two  a   3.0
     b   4.0
dtype: float64
</code></pre>

<p>I want to merge the multiindex into a single index in the following form:</p>

<pre><code>one_a   1.0
one_b   2.0
two_a   3.0
two_b   4.0
dtype: float64
</code></pre>

<p>Is there a nice way to do this?</p>
",4374441,1499,05-03-2018 12:03,05-03-2018 12:04,0,1499,23,0,18,,"{'badge_counts': {'bronze': 23, 'silver': 18, 'gold': 0}, 'account_id': 5506801, 'is_employee': False, 'last_modified_date': 1607614587, 'last_access_date': 1711119015, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1499, 'creation_date': 1418909719, 'user_type': 'registered', 'user_id': 4374441, 'link': 'https://stackoverflow.com/users/4374441/jean-paul', 'profile_image': 'https://www.gravatar.com/avatar/75dd31a77a32c735775338c8b7504a93?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jean Paul'}",I have a with multiindex: I want to merge the multiindex into a single index in the following form: Is there a nice way to do this?,"pandas.Series index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),
                                   ('two', 'a'), ('two', 'b')])
s = pd.Series(np.arange(1.0, 5.0), index=index)
print(s)
one  a   1.0
     b   2.0
two  a   3.0
     b   4.0
dtype: float64
 one_a   1.0
one_b   2.0
two_a   3.0
two_b   4.0
dtype: float64
",11,23,0,0,
139,48394486,48394619,47959,pyinstaller No module named grpc,2,<python><pyinstaller><grpc>,12,"<p>My goal is to build an executable using pyinstaller. The python script I am trying to build imports grpc. The following is an example that illustrates the problem called hello.py.</p>

<pre><code>  import grpc
  if __name__ == '__main__':
     print ""hello world""
</code></pre>

<p>I do <code>pyinstaller hello.py</code> and that produces the expected dist directory. Then I run it like <code>./dist/hello/hello</code> and I get error <code>ImportError: No module named grpc.</code> </p>

<p>So then I installed grpc using <code>pip install grpc</code>. When I rebuild the artifact I now get error <code>Import grpc:No module named gevent.socket</code>. </p>

<p>Reading online indicated that the correct items to install were actually grpcio and grpcio-tools. So I tried <code>pip uninstall grpc</code> <code>pip install grpcio</code> and <code>pip install grpcio-tools</code>. Doing this and rebuilding the artifact gave me error <code>ImportError: No module named pkg_resources</code>. Trying to <code>pip install pkg_resources</code> gives error: <code>Could not find a version that satisfies the requirement pkg_resources</code></p>

<p>Having all <code>grpcio</code> <code>grpcio-tools</code> and <code>grpc</code> install gives the same error: <code>Import grpc:No module named gevent.socket</code></p>

<p>This seems like it should be a very simple task. I simply want to use pyinstaller to build an artifact that has a dependency on grpc, how do I do this?</p>
",4021691,155,23-01-2018 04:55,23-01-2018 05:08,0,155,8,1,2,,"{'badge_counts': {'bronze': 8, 'silver': 2, 'gold': 1}, 'account_id': 5003400, 'is_employee': False, 'last_modified_date': 1688721300, 'last_access_date': 1698289895, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 155, 'creation_date': 1410243770, 'user_type': 'registered', 'user_id': 4021691, 'link': 'https://stackoverflow.com/users/4021691/andrew-dawson', 'profile_image': 'https://lh6.googleusercontent.com/-1mgnRi1ocS4/AAAAAAAAAAI/AAAAAAAAADc/4tIbU_06tOo/photo.jpg?sz=256', 'display_name': 'Andrew Dawson'}","My goal is to build an executable using pyinstaller. The python script I am trying to build imports grpc. The following is an example that illustrates the problem called hello.py. I do and that produces the expected dist directory. Then I run it like and I get error So then I installed grpc using . When I rebuild the artifact I now get error . Reading online indicated that the correct items to install were actually grpcio and grpcio-tools. So I tried and . Doing this and rebuilding the artifact gave me error . Trying to gives error: Having all and install gives the same error: This seems like it should be a very simple task. I simply want to use pyinstaller to build an artifact that has a dependency on grpc, how do I do this?","  import grpc
  if __name__ == '__main__':
     print ""hello world""
 pyinstaller hello.py ./dist/hello/hello ImportError: No module named grpc. pip install grpc Import grpc:No module named gevent.socket pip uninstall grpc pip install grpcio pip install grpcio-tools ImportError: No module named pkg_resources pip install pkg_resources Could not find a version that satisfies the requirement pkg_resources grpcio grpcio-tools grpc Import grpc:No module named gevent.socket",-13,16,0,0,
140,49603498,49606866,13125,Convolution2D + LSTM versus ConvLSTM2D,2,<python><tensorflow><keras>,25,"<p>Are <code>1</code> and <code>2</code> the same?</p>

<ol>
<li>Use <code>Convolution2D</code> layers and <code>LSTM</code> layers </li>
<li>Use <code>ConvLSTM2D</code></li>
</ol>

<p>If there is any difference, could you explain it for me?</p>
",4088201,3147,01-04-2018 23:14,02-04-2018 07:11,1,3157,57,8,27,95,"{'badge_counts': {'bronze': 57, 'silver': 27, 'gold': 8}, 'account_id': 5097348, 'is_employee': False, 'last_modified_date': 1632573300, 'last_access_date': 1634648383, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3157, 'creation_date': 1411905275, 'user_type': 'registered', 'user_id': 4088201, 'accept_rate': 95, 'website_url': '', 'link': 'https://stackoverflow.com/users/4088201/roman', 'profile_image': 'https://www.gravatar.com/avatar/2b2ea265b86ce0977f3ada8286ba9451?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Roman'}","Are and the same? Use layers and layers Use If there is any difference, could you explain it for me?",1 2 Convolution2D LSTM ConvLSTM2D,-5,8,0,0,
141,48216473,48216491,11877,Using assertTrue(==) vs assertEqual in unittest,2,<python><unit-testing><python-unittest>,17,"<p>In the Python <code>unittest</code> module, are there any advantages or disadvantages of using <code>assertTrue()</code> vs. <code>assertEqual()</code> in the following case?</p>

<pre><code>self.assertTrue(a == b)
self.assertEqual(a, b)
</code></pre>
",4143840,3499,11-01-2018 21:58,11-01-2018 22:00,0,3509,46,4,29,88,"{'badge_counts': {'bronze': 46, 'silver': 29, 'gold': 4}, 'account_id': 4240697, 'is_employee': False, 'last_modified_date': 1695085200, 'last_access_date': 1709840254, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3509, 'creation_date': 1413339549, 'user_type': 'registered', 'user_id': 4143840, 'accept_rate': 88, 'website_url': '', 'link': 'https://stackoverflow.com/users/4143840/jersey-bean', 'profile_image': 'https://www.gravatar.com/avatar/858371d9a93692e3a207bf6ddaa21fd1?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jersey bean'}","In the Python module, are there any advantages or disadvantages of using vs. in the following case?","unittest assertTrue() assertEqual() self.assertTrue(a == b)
self.assertEqual(a, b)
",-2,5,0,0,
142,48382575,48382895,27600,Sort graph nodes according to their degree,1,<python><networkx>,26,"<p>I have a graph G in networkx and would like to sort the nodes according to their degree. However, the following code does not work in latest networkx versions:</p>

<pre><code>sorted(set(G.degree().values()))
</code></pre>

<p>and the following seems a bit clunky as it requires converting the networkx DegreeView to a python list of tuples</p>

<pre><code>degrees = [(node,val) for (node, val) in G.degree()]
sorted(degrees, key=lambda x: x[1], reverse=True)
</code></pre>

<p>is there any better way?</p>
",4181677,3358,22-01-2018 13:21,22-01-2018 13:37,0,3358,46,7,29,100,"{'badge_counts': {'bronze': 46, 'silver': 29, 'gold': 7}, 'account_id': 5231514, 'is_employee': False, 'last_modified_date': 1628914500, 'last_access_date': 1711095914, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3358, 'creation_date': 1414272127, 'user_type': 'registered', 'user_id': 4181677, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/4181677/famargar', 'profile_image': 'https://www.gravatar.com/avatar/5bee9421ccdba1d74dd832a18aca20e4?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'famargar'}","I have a graph G in networkx and would like to sort the nodes according to their degree. However, the following code does not work in latest networkx versions: and the following seems a bit clunky as it requires converting the networkx DegreeView to a python list of tuples is there any better way?","sorted(set(G.degree().values()))
 degrees = [(node,val) for (node, val) in G.degree()]
sorted(degrees, key=lambda x: x[1], reverse=True)
",1,12,0,0,
143,49965774,49966781,1724,What exactly is the optimization `functools.partial` is making?,1,<python><performance><cpython><python-internals><functools>,13,"<p>CPython 3.6.4:</p>

<pre><code>from functools import partial

def add(x, y, z, a):
    return x + y + z + a

list_of_as = list(range(10000))

def max1():
    return max(list_of_as , key=lambda a: add(10, 20, 30, a))

def max2():
    return max(list_of_as , key=partial(add, 10, 20, 30))
</code></pre>

<p>now:</p>

<pre><code>In [2]: %timeit max1()
4.36 ms ± 42.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [3]: %timeit max2()
3.67 ms ± 25.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>

<p>I thought <code>partial</code> just remembers part of parameters and then forwards them to the original function when called with the rest of the parameters (so it's nothing more than a shortcut), but it seems it makes some optimization. In my case the whole <code>max2</code> function gets optimized by 15% compared to the <code>max1</code>, which is pretty nice.</p>

<p>It would be great to know what the optimization is, so I could use it in a more efficient way. <a href=""https://docs.python.org/3.6/library/functools.html#functools.partial"" rel=""noreferrer"">Docs</a> are silent regarding any optimization. Not surprisingly, ""roughly equivalent to"" implementation (given in docs), does not optimize at all:</p>

<pre><code>In [3]: %timeit max2()  # using `partial` implementation from docs 
10.7 ms ± 267 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
",4263161,562,22-04-2018 12:14,22-04-2018 14:02,0,572,15,1,4,,"{'badge_counts': {'bronze': 15, 'silver': 4, 'gold': 1}, 'account_id': 5348254, 'is_employee': False, 'last_modified_date': 1687977600, 'last_access_date': 1711108946, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 572, 'creation_date': 1416263309, 'user_type': 'registered', 'user_id': 4263161, 'website_url': '', 'link': 'https://stackoverflow.com/users/4263161/pawelswiecki', 'profile_image': 'https://i.stack.imgur.com/gPVRj.jpg?s=256&g=1', 'display_name': 'pawelswiecki'}","CPython 3.6.4: now: I thought just remembers part of parameters and then forwards them to the original function when called with the rest of the parameters (so it's nothing more than a shortcut), but it seems it makes some optimization. In my case the whole function gets optimized by 15% compared to the , which is pretty nice. It would be great to know what the optimization is, so I could use it in a more efficient way. Docs are silent regarding any optimization. Not surprisingly, ""roughly equivalent to"" implementation (given in docs), does not optimize at all:","from functools import partial

def add(x, y, z, a):
    return x + y + z + a

list_of_as = list(range(10000))

def max1():
    return max(list_of_as , key=lambda a: add(10, 20, 30, a))

def max2():
    return max(list_of_as , key=partial(add, 10, 20, 30))
 In [2]: %timeit max1()
4.36 ms ± 42.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [3]: %timeit max2()
3.67 ms ± 25.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
 partial max2 max1 In [3]: %timeit max2()  # using `partial` implementation from docs 
10.7 ms ± 267 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
",13,32,0,1,
144,50185399,50244319,12420,Multiple-output Gaussian Process regression in scikit-learn,2,<python><machine-learning><scikit-learn><regression><gaussian-process>,25,"<p>I am using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html"" rel=""noreferrer"">scikit learn</a> for Gaussian process regression (GPR) operation to predict data. My training data are as follows:</p>

<pre class=""lang-python prettyprint-override""><code>x_train = np.array([[0,0],[2,2],[3,3]]) #2-D cartesian coordinate points

y_train = np.array([[200,250, 155],[321,345,210],[417,445,851]]) #observed output from three different datasources at respective input data points (x_train)
</code></pre>
<p>The test points (2-D) where mean and variance/standard deviation need to be predicted are:</p>
<pre class=""lang-python prettyprint-override""><code>xvalues = np.array([0,1,2,3])
yvalues = np.array([0,1,2,3])

x,y = np.meshgrid(xvalues,yvalues) #Total 16 locations (2-D)
positions = np.vstack([x.ravel(), y.ravel()]) 
x_test = (np.array(positions)).T
</code></pre>
<p>Now, after running the GPR (<code>GausianProcessRegressor</code>) fit (Here, the product of ConstantKernel and RBF is used as Kernel in <code>GaussianProcessRegressor</code>), mean and variance/standard deviation can be predicted by following the line of code:</p>
<pre class=""lang-python prettyprint-override""><code>y_pred_test, sigma = gp.predict(x_test, return_std =True)
</code></pre>
<p>While printing the predicted mean (<code>y_pred_test</code>) and variance (<code>sigma</code>), I get following output printed in the console:</p>
<p><a href=""https://i.stack.imgur.com/86wov.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/86wov.jpg"" alt=""enter image description here"" /></a></p>
<p>In the predicted values (mean), the 'nested array' with three objects inside the inner array is printed. It can be presumed that the inner arrays are the predicted mean values of each data source at each 2-D test point locations. However, the printed variance contains only a single array with 16 objects (perhaps for 16 test location points). I know that the variance provides an indication of the uncertainty of the estimation. Hence, I was expecting the predicted variance for each data source at each test point. Is my expectation wrong? How can I get the predicted variance for each data source at each test points? Is it due to wrong code?</p>
",4336593,829,05-05-2018 03:22,09-05-2018 01:36,4,829,39,3,19,89,"{'badge_counts': {'bronze': 39, 'silver': 19, 'gold': 3}, 'account_id': 5452123, 'is_employee': False, 'last_modified_date': 1696511700, 'last_access_date': 1707559341, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 829, 'creation_date': 1418028962, 'user_type': 'registered', 'user_id': 4336593, 'accept_rate': 89, 'location': 'Nepal', 'website_url': '', 'link': 'https://stackoverflow.com/users/4336593/santobedi', 'profile_image': 'https://www.gravatar.com/avatar/a5618320895edb8ba8c38adf87803e19?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'santobedi'}","I am using scikit learn for Gaussian process regression (GPR) operation to predict data. My training data are as follows: The test points (2-D) where mean and variance/standard deviation need to be predicted are: Now, after running the GPR () fit (Here, the product of ConstantKernel and RBF is used as Kernel in ), mean and variance/standard deviation can be predicted by following the line of code: While printing the predicted mean () and variance (), I get following output printed in the console: In the predicted values (mean), the 'nested array' with three objects inside the inner array is printed. It can be presumed that the inner arrays are the predicted mean values of each data source at each 2-D test point locations. However, the printed variance contains only a single array with 16 objects (perhaps for 16 test location points). I know that the variance provides an indication of the uncertainty of the estimation. Hence, I was expecting the predicted variance for each data source at each test point. Is my expectation wrong? How can I get the predicted variance for each data source at each test points? Is it due to wrong code?","x_train = np.array([[0,0],[2,2],[3,3]]) #2-D cartesian coordinate points

y_train = np.array([[200,250, 155],[321,345,210],[417,445,851]]) #observed output from three different datasources at respective input data points (x_train)
 xvalues = np.array([0,1,2,3])
yvalues = np.array([0,1,2,3])

x,y = np.meshgrid(xvalues,yvalues) #Total 16 locations (2-D)
positions = np.vstack([x.ravel(), y.ravel()]) 
x_test = (np.array(positions)).T
 GausianProcessRegressor GaussianProcessRegressor y_pred_test, sigma = gp.predict(x_test, return_std =True)
 y_pred_test sigma",3,20,1,2,
145,48054761,48054994,12878,Using multiple conditions in Django's Case When expressions,2,<python><django><django-orm>,14,"<p>As per the Django documentation, its possible to use multiple conditions with the When clause.</p>

<pre class=""lang-py prettyprint-override""><code>When(
    registered_on__gt=date(2014, 1, 1),
    registered_on__lt=date(2015, 1, 1),
    then='account_type'
)
</code></pre>

<p>However, I am unable to use the same when using the Case clause.</p>

<pre class=""lang-py prettyprint-override""><code>Case(
    When(
        registered_on__gt=date(2014, 1, 1),
        registered_on__lt=date(2015, 1, 1), 
        then='account_type'
    ),
    default='default'
)
</code></pre>

<p>I end up getting the following error:</p>

<p><code>TypeError: __init__() got multiple values for keyword argument 'then'</code></p>

<p>Is there some way I can achieve this? Am I missing something here?</p>
",4597273,195,02-01-2018 02:00,02-01-2018 02:40,0,195,9,1,3,,"{'badge_counts': {'bronze': 9, 'silver': 3, 'gold': 1}, 'account_id': 5833462, 'is_employee': False, 'last_modified_date': 1639694701, 'last_access_date': 1668597879, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 195, 'creation_date': 1424701138, 'user_type': 'registered', 'user_id': 4597273, 'website_url': '', 'link': 'https://stackoverflow.com/users/4597273/rahul', 'profile_image': 'https://www.gravatar.com/avatar/485aa6990dc22c068619cd6248200ac5?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Rahul'}","As per the Django documentation, its possible to use multiple conditions with the When clause. However, I am unable to use the same when using the Case clause. I end up getting the following error: Is there some way I can achieve this? Am I missing something here?","When(
    registered_on__gt=date(2014, 1, 1),
    registered_on__lt=date(2015, 1, 1),
    then='account_type'
)
 Case(
    When(
        registered_on__gt=date(2014, 1, 1),
        registered_on__lt=date(2015, 1, 1), 
        then='account_type'
    ),
    default='default'
)
 TypeError: __init__() got multiple values for keyword argument 'then'",10,26,0,0,
146,48578906,49611892,4318,Flask caching multiple files in project,2,<python><caching><flask>,11,"<p>I'm relatively new to Flask. I have multiple files in my flask project. Up until now, I was using <code>current_app</code> if I wanted to access <code>app</code> object from outside of <code>app.py</code> file. </p>

<p>Now I am trying to add cache to my app with <a href=""https://flask-caching.readthedocs.io/en/latest/"" rel=""noreferrer"">flask-caching</a> extension. I initialize this in my <code>app.py</code></p>

<pre><code>from flask_caching import Cache
...
cache = Cache(app, config={'CACHE_TYPE': 'simple'})
</code></pre>

<p>However I'm having troubles with uisng it with <code>views.py</code> file.</p>

<p>I have a resource class:</p>

<pre><code>class MyEndpoint(Resource):
    def get(self):
        do_stuff_here
</code></pre>

<p>I don't know how to get <code>cache</code> object here to achieve this:</p>

<pre><code>class MyEndpoint(Resource):
    @cache.cached(timeout=600)
    def get(self):
        do_stuff_here
</code></pre>

<p>I tried to do:</p>

<ul>
<li><code>from app import cache</code> -> <code>ImportError: cannot import name 'cache'</code></li>
<li><code>@current_app.cache.cached</code> -> <code>RuntimeError: Working outside of application context.</code></li>
</ul>

<p>Part of the structure of my project:</p>

<pre><code>|
-app.py
|
--api
  |
  -__init__.py
  -views.py
</code></pre>
",4638248,5016,02-02-2018 08:47,02-04-2018 13:06,59,5046,60,7,35,44,"{'badge_counts': {'bronze': 60, 'silver': 35, 'gold': 7}, 'account_id': 5891909, 'is_employee': False, 'last_modified_date': 1691807101, 'last_access_date': 1711132782, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5046, 'creation_date': 1425583550, 'user_type': 'registered', 'user_id': 4638248, 'accept_rate': 44, 'location': 'Poznań, Poland', 'website_url': 'http://jakubsemik.com', 'link': 'https://stackoverflow.com/users/4638248/quba', 'profile_image': 'https://i.stack.imgur.com/DSxUb.png?s=256&g=1', 'display_name': 'Quba'}","I'm relatively new to Flask. I have multiple files in my flask project. Up until now, I was using if I wanted to access object from outside of file. Now I am trying to add cache to my app with flask-caching extension. I initialize this in my However I'm having troubles with uisng it with file. I have a resource class: I don't know how to get object here to achieve this: I tried to do: -> -> Part of the structure of my project:","current_app app app.py app.py from flask_caching import Cache
...
cache = Cache(app, config={'CACHE_TYPE': 'simple'})
 views.py class MyEndpoint(Resource):
    def get(self):
        do_stuff_here
 cache class MyEndpoint(Resource):
    @cache.cached(timeout=600)
    def get(self):
        do_stuff_here
 from app import cache ImportError: cannot import name 'cache' @current_app.cache.cached RuntimeError: Working outside of application context. |
-app.py
|
--api
  |
  -__init__.py
  -views.py
",3,43,0,1,
147,49511753,49517948,46083,Python - byte image to NumPy array using OpenCV,2,<python><python-3.x><numpy><opencv>,28,"<p>I have an image in bytes:</p>

<p><code>print(image_bytes)</code></p>

<p><code>b'\xff\xd8\xff\xfe\x00\x10Lavc57.64.101\x00\xff\xdb\x00C\x00\x08\x04\x04\x04\x04\x04\x05\x05\x05\x05\x05\x05\x06\x06\x06\x06\x06\x06\x06\x06\x06\x06\x06\x06\x06\x07\x07\x07\x08\x08\x08\x07\x07\x07\x06\x06\x07\x07\x08\x08\x08\x08\t\t\t\x08\x08\x08\x08\t\t\n\n\n\x0c\x0c\x0b\x0b\x0e\x0e\x0e\x11\x11\x14\xff\xc4\x01\xa2\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\ ... some other stuff
</code></p>

<p>I am able to convert it to a NumPy array using <code>Pillow</code>:</p>

<pre><code>image = numpy.array(Image.open(io.BytesIO(image_bytes))) 
</code></pre>

<p>But I don't really like using Pillow. Is there a way to use clear OpenCV, or directly NumPy even better, or some other faster library?</p>
",5123537,3962,27-03-2018 11:30,27-03-2018 16:22,0,3972,51,12,38,82,"{'badge_counts': {'bronze': 51, 'silver': 38, 'gold': 12}, 'account_id': 6636503, 'is_employee': False, 'last_modified_date': 1626517800, 'last_access_date': 1710363428, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3972, 'creation_date': 1437049157, 'user_type': 'registered', 'user_id': 5123537, 'accept_rate': 82, 'location': 'Slovakia', 'website_url': '', 'link': 'https://stackoverflow.com/users/5123537/martin-brisiak', 'profile_image': 'https://lh3.googleusercontent.com/-lkIKX0kr-SM/AAAAAAAAAAI/AAAAAAAAAfU/5p3jsAt1vHE/photo.jpg?sz=256', 'display_name': 'Martin Brisiak'}","I have an image in bytes: I am able to convert it to a NumPy array using : But I don't really like using Pillow. Is there a way to use clear OpenCV, or directly NumPy even better, or some other faster library?","print(image_bytes) b'\xff\xd8\xff\xfe\x00\x10Lavc57.64.101\x00\xff\xdb\x00C\x00\x08\x04\x04\x04\x04\x04\x05\x05\x05\x05\x05\x05\x06\x06\x06\x06\x06\x06\x06\x06\x06\x06\x06\x06\x06\x07\x07\x07\x08\x08\x08\x07\x07\x07\x06\x06\x07\x07\x08\x08\x08\x08\t\t\t\x08\x08\x08\x08\t\t\n\n\n\x0c\x0c\x0b\x0b\x0e\x0e\x0e\x11\x11\x14\xff\xc4\x01\xa2\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\ ... some other stuff
 Pillow image = numpy.array(Image.open(io.BytesIO(image_bytes))) 
",-2,13,0,0,
148,50203106,52974437,62937,"ValueError: pos_label=1 is not a valid label: array(['neg', 'pos'], dtype='<U3')",4,<python><machine-learning><precision><precision-recall>,42,"<p>I recieve this error while trying to obtain the recall score. </p>

<pre><code>X_test = test_pos_vec + test_neg_vec
Y_test = [""pos""] * len(test_pos_vec) + [""neg""] * len(test_neg_vec)

recall_average = recall_score(Y_test, y_predict, average=""binary"")

print(recall_average)
</code></pre>

<p>This will give me:</p>

<pre><code>    C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py:1030: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  if pos_label not in present_labels:
Traceback (most recent call last):
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 812, in &lt;module&gt;
    main()
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 91, in main
    evaluate_model(model, train_pos_vec, train_neg_vec, test_pos_vec, test_neg_vec, False)
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 648, in evaluate_model
    recall_average = recall_score(Y_test, y_predict, average=""binary"")
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py"", line 1359, in recall_score
    sample_weight=sample_weight)
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py"", line 1036, in precision_recall_fscore_support
    (pos_label, present_labels))
ValueError: pos_label=1 is not a valid label: array(['neg', 'pos'],
      dtype='&lt;U3')
</code></pre>

<p>I tried to transform 'pos' in 1 and 'neg' in 0 this way:</p>

<pre><code>for i in range(len(Y_test)):
     if 'neg' in Y_test[i]:
         Y_test[i] = 0
     else:
         Y_test[i] = 1
</code></pre>

<p>But this is giving me another error:</p>

<pre><code>    C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py:181: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  score = y_true == y_pred
Traceback (most recent call last):
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 812, in &lt;module&gt;
    main()
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 91, in main
    evaluate_model(model, train_pos_vec, train_neg_vec, test_pos_vec, test_neg_vec, False)
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 648, in evaluate_model
    recall_average = recall_score(Y_test, y_predict, average=""binary"")
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py"", line 1359, in recall_score
    sample_weight=sample_weight)
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py"", line 1026, in precision_recall_fscore_support
    present_labels = unique_labels(y_true, y_pred)
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\utils\multiclass.py"", line 103, in unique_labels
    raise ValueError(""Mix of label input types (string and number)"")
ValueError: Mix of label input types (string and number)
</code></pre>

<p>What I am trying to do is to obtain the metrics: accuracy, precision, recall, f_measure. With <code>average='weighted'</code>, I obtain the same result: accuracy=recall. I guess this is not correct, so I changed the <code>average='binary'</code>, but I have those errors. Any ideas?</p>
",5128388,1123,06-05-2018 18:21,24-10-2018 17:02,171,1143,19,1,12,100,"{'badge_counts': {'bronze': 19, 'silver': 12, 'gold': 1}, 'account_id': 6643378, 'is_employee': False, 'last_modified_date': 1607614545, 'last_access_date': 1710680730, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1143, 'creation_date': 1437152749, 'user_type': 'registered', 'user_id': 5128388, 'accept_rate': 100, 'location': 'aasdffaas', 'website_url': '', 'link': 'https://stackoverflow.com/users/5128388/mr-wizard', 'profile_image': 'https://i.stack.imgur.com/sZjbh.jpg?s=256&g=1', 'display_name': 'Mr. Wizard'}","I recieve this error while trying to obtain the recall score. This will give me: I tried to transform 'pos' in 1 and 'neg' in 0 this way: But this is giving me another error: What I am trying to do is to obtain the metrics: accuracy, precision, recall, f_measure. With , I obtain the same result: accuracy=recall. I guess this is not correct, so I changed the , but I have those errors. Any ideas?","X_test = test_pos_vec + test_neg_vec
Y_test = [""pos""] * len(test_pos_vec) + [""neg""] * len(test_neg_vec)

recall_average = recall_score(Y_test, y_predict, average=""binary"")

print(recall_average)
     C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py:1030: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  if pos_label not in present_labels:
Traceback (most recent call last):
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 812, in &lt;module&gt;
    main()
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 91, in main
    evaluate_model(model, train_pos_vec, train_neg_vec, test_pos_vec, test_neg_vec, False)
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 648, in evaluate_model
    recall_average = recall_score(Y_test, y_predict, average=""binary"")
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py"", line 1359, in recall_score
    sample_weight=sample_weight)
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py"", line 1036, in precision_recall_fscore_support
    (pos_label, present_labels))
ValueError: pos_label=1 is not a valid label: array(['neg', 'pos'],
      dtype='&lt;U3')
 for i in range(len(Y_test)):
     if 'neg' in Y_test[i]:
         Y_test[i] = 0
     else:
         Y_test[i] = 1
     C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py:181: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  score = y_true == y_pred
Traceback (most recent call last):
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 812, in &lt;module&gt;
    main()
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 91, in main
    evaluate_model(model, train_pos_vec, train_neg_vec, test_pos_vec, test_neg_vec, False)
  File ""G:/PyCharmProjects/NB/accuracy/script.py"", line 648, in evaluate_model
    recall_average = recall_score(Y_test, y_predict, average=""binary"")
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py"", line 1359, in recall_score
    sample_weight=sample_weight)
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\metrics\classification.py"", line 1026, in precision_recall_fscore_support
    present_labels = unique_labels(y_true, y_pred)
  File ""C:\Users\anca_elena.moisa\AppData\Local\Programs\Python\Python36\lib\site-packages\sklearn\utils\multiclass.py"", line 103, in unique_labels
    raise ValueError(""Mix of label input types (string and number)"")
ValueError: Mix of label input types (string and number)
 average='weighted' average='binary'",36,59,0,0,
149,48104097,48104215,33914,How to convert a PDF from base64 string to a file?,6,<python><pdf><base64><file-writing>,16,"<p>I have a PDF as a base64 string and I need to write it to file using Python.
I tried this:</p>

<pre><code>import base64

base64String = ""data:application/pdf;base64,JVBERi0xLjQKJeHp69MKMSAwIG9iago8PC9Qcm9kdWNlciAoU2tpYS9...""

with open('temp.pdf', 'wb') as theFile:
  theFile.write(base64.b64decode(base64String))
</code></pre>

<p>But it didn't create a valid PDF file.
What am I missing?</p>
",5131567,305,04-01-2018 21:50,04-01-2018 22:00,0,305,13,1,3,,"{'badge_counts': {'bronze': 13, 'silver': 3, 'gold': 1}, 'account_id': 6648427, 'is_employee': False, 'last_modified_date': 1692816866, 'last_access_date': 1707180691, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 305, 'creation_date': 1437279043, 'user_type': 'registered', 'user_id': 5131567, 'link': 'https://stackoverflow.com/users/5131567/rafael-miller', 'profile_image': 'https://graph.facebook.com/10204994546929500/picture?type=large', 'display_name': 'Rafael Miller'}",I have a PDF as a base64 string and I need to write it to file using Python. I tried this: But it didn't create a valid PDF file. What am I missing?,"import base64

base64String = ""data:application/pdf;base64,JVBERi0xLjQKJeHp69MKMSAwIG9iago8PC9Qcm9kdWNlciAoU2tpYS9...""

with open('temp.pdf', 'wb') as theFile:
  theFile.write(base64.b64decode(base64String))
",5,13,0,0,
150,50211546,50214281,7155,Converting Byte to String and Back Properly in Python3?,2,<python><type-conversion><byte>,11,"<p>Given a random byte (i.e. not only numbers/characters!), I need to <strong>convert it to a string</strong> and then <strong>back to the inital byte</strong> without loosing information. This seems like a basic task, but I ran in to the following problems:</p>

<p>Assuming:</p>

<pre><code>rnd_bytes = b'w\x12\x96\xb8'
len(rnd_bytes)
</code></pre>

<p>prints: <code>4</code></p>

<p>Now, converting it to a string. Note: I need to set <code>backslashreplace</code> as it otherwise returns a 'UnicodeDecodeError' or would loose information setting it to another flag value.</p>

<pre><code>my_str = rnd_bytes.decode('utf-8' , 'backslashreplace')
</code></pre>

<p>Now, I have the string. 
I want to convert it back to exactly the original byte (size 4!):</p>

<p>According to  python ressources and this <a href=""https://stackoverflow.com/questions/7585435/best-way-to-convert-string-to-bytes-in-python-3"">answer</a>, there are different possibilities:</p>

<pre><code>conv_bytes = bytes(my_str, 'utf-8')
conv_bytes = my_str.encode('utf-8')
</code></pre>

<p>But len(conv_bytes) returns <code>10</code>. </p>

<p>I tried to analyse the outcome:</p>

<pre><code>&gt;&gt;&gt; repr(rnd_bytes)
""b'w\\x12\\x96\\xb8'""
&gt;&gt;&gt; repr(my_str)
""'w\\x12\\\\x96\\\\xb8'""
&gt;&gt;&gt; repr(conv_bytes)
""b'w\\x12\\\\x96\\\\xb8'""
</code></pre>

<p>It would make sense to replace <code>'\\\\'</code>. <code>my_str.replace('\\\\','\\')</code> doesn't change anything. Probably, because four backslashes represent only two. So, <code>my_str.replace('\\','\')</code> would find the <code>'\\\\'</code>, but leads to</p>

<blockquote>
  <p>SyntaxError: EOL while scanning string literal</p>
</blockquote>

<p>due to the last argument <code>'\'</code>. This had been discussed <a href=""https://stackoverflow.com/a/14453186/4673476"">here</a>, where the following suggestion came up:</p>

<pre><code>&gt;&gt;&gt; my_str2=my_str.encode('utf_8').decode('unicode_escape')
&gt;&gt;&gt; repr(my_str2)
""'w\\x12\\x96¸'""
</code></pre>

<p>This replaces the <code>'\\\\'</code> but seems to add / change some other characters:</p>

<pre><code>&gt;&gt;&gt; conv_bytes2 = my_str2.encode('utf8')
&gt;&gt;&gt; len(conv_bytes2)
6
&gt;&gt;&gt; repr(conv_bytes2)
""b'w\\x12\\xc2\\x96\\xc2\\xb8'""
</code></pre>

<p>There <em>must</em> be a prober way to convert a (complex) byte to a string and back. How can I achieve that?</p>
",4673476,1191,07-05-2018 10:00,07-05-2018 12:29,0,1201,50,4,20,79,"{'badge_counts': {'bronze': 50, 'silver': 20, 'gold': 4}, 'account_id': 5942931, 'is_employee': False, 'last_modified_date': 1706531700, 'last_access_date': 1711119138, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1201, 'creation_date': 1426433558, 'user_type': 'registered', 'user_id': 4673476, 'accept_rate': 79, 'location': 'Bay Area, USA', 'link': 'https://stackoverflow.com/users/4673476/black', 'profile_image': 'https://i.stack.imgur.com/Omlfj.jpg?s=256&g=1', 'display_name': 'black'}","Given a random byte (i.e. not only numbers/characters!), I need to convert it to a string and then back to the inital byte without loosing information. This seems like a basic task, but I ran in to the following problems: Assuming: prints: Now, converting it to a string. Note: I need to set as it otherwise returns a 'UnicodeDecodeError' or would loose information setting it to another flag value. Now, I have the string. I want to convert it back to exactly the original byte (size 4!): According to python ressources and this answer, there are different possibilities: But len(conv_bytes) returns . I tried to analyse the outcome: It would make sense to replace . doesn't change anything. Probably, because four backslashes represent only two. So, would find the , but leads to SyntaxError: EOL while scanning string literal due to the last argument . This had been discussed here, where the following suggestion came up: This replaces the but seems to add / change some other characters: There must be a prober way to convert a (complex) byte to a string and back. How can I achieve that?","rnd_bytes = b'w\x12\x96\xb8'
len(rnd_bytes)
 4 backslashreplace my_str = rnd_bytes.decode('utf-8' , 'backslashreplace')
 conv_bytes = bytes(my_str, 'utf-8')
conv_bytes = my_str.encode('utf-8')
 10 &gt;&gt;&gt; repr(rnd_bytes)
""b'w\\x12\\x96\\xb8'""
&gt;&gt;&gt; repr(my_str)
""'w\\x12\\\\x96\\\\xb8'""
&gt;&gt;&gt; repr(conv_bytes)
""b'w\\x12\\\\x96\\\\xb8'""
 '\\\\' my_str.replace('\\\\','\\') my_str.replace('\\','\') '\\\\' '\' &gt;&gt;&gt; my_str2=my_str.encode('utf_8').decode('unicode_escape')
&gt;&gt;&gt; repr(my_str2)
""'w\\x12\\x96¸'""
 '\\\\' &gt;&gt;&gt; conv_bytes2 = my_str2.encode('utf8')
&gt;&gt;&gt; len(conv_bytes2)
6
&gt;&gt;&gt; repr(conv_bytes2)
""b'w\\x12\\xc2\\x96\\xc2\\xb8'""
",4,59,0,2,
151,50173283,50173320,43471,Pandas - get first n-rows based on percentage,6,<python><pandas><percentage>,21,"<p>I have a dataframe i want to pop certain number of records, instead on number I want to pass as a percentage value. </p>

<p>for example,</p>

<p><code>df.head(n=10)</code></p>

<p>Pops out first 10 records from data set. I want a small change instead of 10 records i want to pop <strong>first</strong> 5% of record from my data set.
How to do this in pandas.</p>

<p>I'm looking for a code like this,</p>

<p><code>df.head(frac=0.05)</code></p>

<p>Is there any simple way to get this? </p>
",4684861,10962,04-05-2018 10:54,04-05-2018 10:56,0,10962,116,11,58,86,"{'badge_counts': {'bronze': 116, 'silver': 58, 'gold': 11}, 'collectives': [{'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 5959197, 'is_employee': False, 'last_modified_date': 1707348900, 'last_access_date': 1711103755, 'reputation_change_year': 48, 'reputation_change_quarter': 48, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 10962, 'creation_date': 1426675778, 'user_type': 'registered', 'user_id': 4684861, 'accept_rate': 86, 'location': 'Coimbatore, Tamil Nadu, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/4684861/mohamed-thasin-ah', 'profile_image': 'https://i.stack.imgur.com/VKrAz.jpg?s=256&g=1', 'display_name': 'Mohamed Thasin ah'}","I have a dataframe i want to pop certain number of records, instead on number I want to pass as a percentage value. for example, Pops out first 10 records from data set. I want a small change instead of 10 records i want to pop first 5% of record from my data set. How to do this in pandas. I'm looking for a code like this, Is there any simple way to get this?",df.head(n=10) df.head(frac=0.05),-2,14,0,0,
152,48756249,48756562,13840,Django REST: Uploading and serializing multiple images,2,<python><django><serialization><django-rest-framework>,17,"<p>I have 2 models <code>Task</code> and <code>TaskImage</code> which is a collection of images belonging to <code>Task</code> object.</p>

<p>What I want is to be able to add multiple images to my <code>Task</code> object, but I can only do it using 2 models. Currently, when I add images, it doesn't let me upload them and save new objects.</p>

<p><strong>settings.py</strong></p>

<pre><code>MEDIA_ROOT = os.path.join(BASE_DIR, 'media')
MEDIA_URL = '/media/'
</code></pre>

<p><strong>serializers.py</strong> </p>

<pre><code>class TaskImageSerializer(serializers.ModelSerializer):
    class Meta:
        model = TaskImage
        fields = ('image',)


class TaskSerializer(serializers.HyperlinkedModelSerializer):
    user = serializers.ReadOnlyField(source='user.username')
    images = TaskImageSerializer(source='image_set', many=True, read_only=True)

    class Meta:
        model = Task
        fields = '__all__'

    def create(self, validated_data):
        images_data = validated_data.pop('images')
        task = Task.objects.create(**validated_data)
        for image_data in images_data:
            TaskImage.objects.create(task=task, **image_data)
        return task
</code></pre>

<p><strong>models.py</strong></p>

<pre><code>class Task(models.Model):
    title = models.CharField(max_length=100, blank=False)
    user = models.ForeignKey(User)

    def save(self, *args, **kwargs):
        super(Task, self).save(*args, **kwargs)

class TaskImage(models.Model):
    task = models.ForeignKey(Task, on_delete=models.CASCADE)
    image = models.FileField(blank=True)
</code></pre>

<p>However, when I do a post request:</p>

<p><a href=""https://i.stack.imgur.com/NndxK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NndxK.png"" alt=""enter image description here""></a></p>

<p>I get the following traceback:</p>

<blockquote>
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/django/core/handlers/exception.py""
  in inner
    41.             response = get_response(request)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/django/core/handlers/base.py""
  in _get_response
    187.                 response = self.process_exception_by_middleware(e, request)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/django/core/handlers/base.py""
  in _get_response
    185.                 response = wrapped_callback(request, *callback_args, **callback_kwargs)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/django/views/decorators/csrf.py""
  in wrapped_view
    58.         return view_func(*args, **kwargs)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/viewsets.py""
  in view
    95.             return self.dispatch(request, *args, **kwargs)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/views.py""
  in dispatch
    494.             response = self.handle_exception(exc)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/views.py""
  in handle_exception
    454.             self.raise_uncaught_exception(exc)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/views.py""
  in dispatch
    491.             response = handler(request, *args, **kwargs)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/mixins.py""
  in create
    21.         self.perform_create(serializer)</p>
  
  <p>File ""/Users/gr/Desktop/PycharmProjects/godo/api/views.py"" in
  perform_create
    152.         serializer.save(user=self.request.user)</p>
  
  <p>File
  ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/serializers.py""
  in save
    214.             self.instance = self.create(validated_data)</p>
  
  <p>File ""/Users/gr/Desktop/PycharmProjects/godo/api/serializers.py"" in
  create
    67.         images_data = validated_data.pop('images')</p>
  
  <p>Exception Type: KeyError at /api/tasks/ Exception Value: 'images'</p>
</blockquote>
",4729764,2892,12-02-2018 22:02,12-02-2018 22:26,0,2892,81,4,35,68,"{'badge_counts': {'bronze': 81, 'silver': 35, 'gold': 4}, 'account_id': 5825373, 'is_employee': False, 'last_modified_date': 1711156800, 'last_access_date': 1710756702, 'reputation_change_year': 45, 'reputation_change_quarter': 45, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2892, 'creation_date': 1427719981, 'user_type': 'registered', 'user_id': 4729764, 'accept_rate': 68, 'link': 'https://stackoverflow.com/users/4729764/grs', 'profile_image': 'https://www.gravatar.com/avatar/aa45447daa10fb1175ae9f3261edd01b?s=256&d=identicon&r=PG', 'display_name': 'GRS'}","I have 2 models and which is a collection of images belonging to object. What I want is to be able to add multiple images to my object, but I can only do it using 2 models. Currently, when I add images, it doesn't let me upload them and save new objects. settings.py serializers.py models.py However, when I do a post request: I get the following traceback: File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/django/core/handlers/exception.py"" in inner 41. response = get_response(request) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/django/core/handlers/base.py"" in _get_response 187. response = self.process_exception_by_middleware(e, request) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/django/core/handlers/base.py"" in _get_response 185. response = wrapped_callback(request, *callback_args, **callback_kwargs) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/django/views/decorators/csrf.py"" in wrapped_view 58. return view_func(*args, **kwargs) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/viewsets.py"" in view 95. return self.dispatch(request, *args, **kwargs) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/views.py"" in dispatch 494. response = self.handle_exception(exc) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/views.py"" in handle_exception 454. self.raise_uncaught_exception(exc) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/views.py"" in dispatch 491. response = handler(request, *args, **kwargs) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/mixins.py"" in create 21. self.perform_create(serializer) File ""/Users/gr/Desktop/PycharmProjects/godo/api/views.py"" in perform_create 152. serializer.save(user=self.request.user) File ""/Applications/Anaconda/anaconda/envs/godo/lib/python3.6/site-packages/rest_framework/serializers.py"" in save 214. self.instance = self.create(validated_data) File ""/Users/gr/Desktop/PycharmProjects/godo/api/serializers.py"" in create 67. images_data = validated_data.pop('images') Exception Type: KeyError at /api/tasks/ Exception Value: 'images'","Task TaskImage Task Task MEDIA_ROOT = os.path.join(BASE_DIR, 'media')
MEDIA_URL = '/media/'
 class TaskImageSerializer(serializers.ModelSerializer):
    class Meta:
        model = TaskImage
        fields = ('image',)


class TaskSerializer(serializers.HyperlinkedModelSerializer):
    user = serializers.ReadOnlyField(source='user.username')
    images = TaskImageSerializer(source='image_set', many=True, read_only=True)

    class Meta:
        model = Task
        fields = '__all__'

    def create(self, validated_data):
        images_data = validated_data.pop('images')
        task = Task.objects.create(**validated_data)
        for image_data in images_data:
            TaskImage.objects.create(task=task, **image_data)
        return task
 class Task(models.Model):
    title = models.CharField(max_length=100, blank=False)
    user = models.ForeignKey(User)

    def save(self, *args, **kwargs):
        super(Task, self).save(*args, **kwargs)

class TaskImage(models.Model):
    task = models.ForeignKey(Task, on_delete=models.CASCADE)
    image = models.FileField(blank=True)
",25,115,1,1,
153,48558007,48571146,19035,Where is dumped file in Google Colab?,7,<python><file><google-colaboratory>,12,"<p>When I wrote this code in google colab:</p>

<pre><code>import pickle
x=10;
output = open('data.pkl', 'wb')
pickle.dump(x,output)
</code></pre>

<p>x is saved and also in another window in Google Colab I can access this file and read it but I don't know where is the file. Does anybody know where is it?</p>
",4747755,150,01-02-2018 08:08,01-02-2018 20:25,0,150,10,1,1,100,"{'badge_counts': {'bronze': 10, 'silver': 1, 'gold': 1}, 'account_id': 6082445, 'is_employee': False, 'last_modified_date': 1693486200, 'last_access_date': 1693482258, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 150, 'creation_date': 1428095761, 'user_type': 'registered', 'user_id': 4747755, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/4747755/saber-malekzadeh', 'profile_image': 'https://lh3.googleusercontent.com/-ooeiDjM_E8U/AAAAAAAAAAI/AAAAAAAAA6U/2XVGpuFE-HQ/photo.jpg?sz=256', 'display_name': 'Saber MalekzadeH'}",When I wrote this code in google colab: x is saved and also in another window in Google Colab I can access this file and read it but I don't know where is the file. Does anybody know where is it?,"import pickle
x=10;
output = open('data.pkl', 'wb')
pickle.dump(x,output)
",3,9,0,0,
154,48721582,48721872,13436,How to choose proper variable names for long names in python,1,<python><pep8>,21,"<p>I need help choosing proper names for variables with long actual names. I have read pep8 docs, but I couln't find addressed such issue.</p>

<p>Would you rename <code>very_long_variable_name</code> to something like <code>vry_lng_var_nm</code> or would you leave it as it is. I notice that pythons build in libraries have very short names and I'd like to follow the conventions if it exists for this case.</p>

<p>I know I can name it something not very descriptive and add description, which would explain its meaning, but what do you think should be the variables name.</p>
",4769572,533,10-02-2018 14:01,10-02-2018 14:36,0,533,22,1,5,82,"{'badge_counts': {'bronze': 22, 'silver': 5, 'gold': 1}, 'account_id': 6113939, 'is_employee': False, 'last_modified_date': 1665193800, 'last_access_date': 1709706139, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 533, 'creation_date': 1428589068, 'user_type': 'registered', 'user_id': 4769572, 'accept_rate': 82, 'link': 'https://stackoverflow.com/users/4769572/sargsyan-grigor', 'profile_image': 'https://graph.facebook.com/1574580922822775/picture?type=large', 'display_name': 'Sargsyan Grigor'}","I need help choosing proper names for variables with long actual names. I have read pep8 docs, but I couln't find addressed such issue. Would you rename to something like or would you leave it as it is. I notice that pythons build in libraries have very short names and I'd like to follow the conventions if it exists for this case. I know I can name it something not very descriptive and add description, which would explain its meaning, but what do you think should be the variables name.",very_long_variable_name vry_lng_var_nm,-2,5,0,0,
155,48181073,48181294,21486,How to glob two patterns with pathlib?,8,<python><pathlib>,20,"<p>I want find two types of files with two different extensions: <code>.jl</code> and <code>.jsonlines</code>. I use </p>

<pre><code>from pathlib import Path
p1 = Path(""/path/to/dir"").joinpath().glob(""*.jl"")
p2 = Path(""/path/to/dir"").joinpath().glob(""*.jsonlines"")
</code></pre>

<p>but I want <code>p1</code> and <code>p2</code> as one variable not two. Should I merge <code>p1</code> and <code>p2</code> in first place? Are there other ways to concatinate glob's patterns?</p>
",4838661,607,10-01-2018 05:51,10-01-2018 06:11,0,607,18,1,7,83,"{'badge_counts': {'bronze': 18, 'silver': 7, 'gold': 1}, 'account_id': 5327393, 'is_employee': False, 'last_modified_date': 1598975981, 'last_access_date': 1710738639, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 607, 'creation_date': 1430152839, 'user_type': 'registered', 'user_id': 4838661, 'accept_rate': 83, 'link': 'https://stackoverflow.com/users/4838661/gmosy-gnaq', 'profile_image': 'https://www.gravatar.com/avatar/761785b0fcc8380010a0ca2310a6c89c?s=256&d=identicon&r=PG', 'display_name': 'Gmosy Gnaq'}",I want find two types of files with two different extensions: and . I use but I want and as one variable not two. Should I merge and in first place? Are there other ways to concatinate glob's patterns?,".jl .jsonlines from pathlib import Path
p1 = Path(""/path/to/dir"").joinpath().glob(""*.jl"")
p2 = Path(""/path/to/dir"").joinpath().glob(""*.jsonlines"")
 p1 p2 p1 p2",-4,8,0,0,
156,49450396,49547582,735,How to reflect an oracle database with BINARY_DOUBLE type columns,3,<python><oracle><python-3.x><sqlalchemy>,11,"<p>I tried to reflect an existing oracle database into sqlalchemy metadata:</p>

<pre><code>from sqlalchemy import create_engine
from sqlalchemy import MetaData
from sqlalchemy import Table

db_uri = 'oracle://USER:PASS@MYDBTNSNAME'
engine = create_engine(db_uri)

# create a MetaData instance
metadata = MetaData()

# reflect db schema to MetaData
metadata.reflect(bind=engine)
</code></pre>

<p>This returns the following:</p>

<pre><code>SAWarning: Did not recognize type 'BINARY_DOUBLE' of column 'column_1'(coltype, colname))
</code></pre>

<p>I have tried to import native types and also the types from dialect oracle using</p>

<pre><code>from sqlalchemy.types import *
from sqlalchemy.dialects.oracle import *
</code></pre>

<p>but it seems it does not recognize <code>BINARY_DOUBLE</code> type</p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-13-b69d481f6a4e&gt; in &lt;module&gt;()
      1 from sqlalchemy.types import *
----&gt; 2 from sqlalchemy.dialects.oracle import *

AttributeError: module 'sqlalchemy.dialects.oracle' has no attribute 'BINARY_DOUBLE'
</code></pre>

<p>I am using SQLAlchemy, version '1.2.1'</p>
",4919040,966,23-03-2018 13:06,29-03-2018 03:33,6,966,31,1,15,,"{'badge_counts': {'bronze': 31, 'silver': 15, 'gold': 1}, 'account_id': 341932, 'is_employee': False, 'last_modified_date': 1601112600, 'last_access_date': 1711098523, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 966, 'creation_date': 1432102712, 'user_type': 'registered', 'user_id': 4919040, 'location': 'Madrid, Spain', 'website_url': '', 'link': 'https://stackoverflow.com/users/4919040/aturegano', 'profile_image': 'https://i.stack.imgur.com/DbL3L.jpg?s=256&g=1', 'display_name': 'aturegano'}","I tried to reflect an existing oracle database into sqlalchemy metadata: This returns the following: I have tried to import native types and also the types from dialect oracle using but it seems it does not recognize type I am using SQLAlchemy, version '1.2.1'","from sqlalchemy import create_engine
from sqlalchemy import MetaData
from sqlalchemy import Table

db_uri = 'oracle://USER:PASS@MYDBTNSNAME'
engine = create_engine(db_uri)

# create a MetaData instance
metadata = MetaData()

# reflect db schema to MetaData
metadata.reflect(bind=engine)
 SAWarning: Did not recognize type 'BINARY_DOUBLE' of column 'column_1'(coltype, colname))
 from sqlalchemy.types import *
from sqlalchemy.dialects.oracle import *
 BINARY_DOUBLE ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-13-b69d481f6a4e&gt; in &lt;module&gt;()
      1 from sqlalchemy.types import *
----&gt; 2 from sqlalchemy.dialects.oracle import *

AttributeError: module 'sqlalchemy.dialects.oracle' has no attribute 'BINARY_DOUBLE'
",17,39,0,0,
157,49094597,49094955,54787,Illegal instruction (core dumped) after running import tensorflow,9,<python><tensorflow><pip><virtualenv>,38,"<p>I created a fresh virtual environment: <code>virtualenv -p python2 test_venv/</code>
And installed tensorflow: <code>pip install --upgrade --no-cache-dir tensorflow</code></p>

<p><code>import tensorflow</code> gives me <code>Illegal instruction (core dumped)</code></p>

<p>Please help me understand what's going on and how I can fix it. Thank you.</p>

<h3>CPU information:</h3>

<pre><code>-cpu
          description: CPU
          product: Intel(R) Core(TM) i3 CPU       M 330  @ 2.13GHz
          bus info: cpu@0
          version: CPU Version
          capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm arat cpufreq
</code></pre>

<h3>Stacktrace obtained with gdb:</h3>

<pre><code>#0  0x00007fffe5793880 in std::pair&lt;std::__detail::_Node_iterator&lt;std::pair&lt;tensorflow::StringPiece const, std::function&lt;bool (tensorflow::Variant*)&gt; &gt;, false, true&gt;, bool&gt; std::_Hashtable&lt;tensorflow::StringPiece, std::pair&lt;tensorflow::StringPiece const, std::function&lt;bool (tensorflow::Variant*)&gt; &gt;, std::allocator&lt;std::pair&lt;tensorflow::StringPiece const, std::function&lt;bool (tensorflow::Variant*)&gt; &gt; &gt;, std::__detail::_Select1st, std::equal_to&lt;tensorflow::StringPiece&gt;, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits&lt;true, false, true&gt; &gt;::_M_emplace&lt;std::pair&lt;tensorflow::StringPiece, std::function&lt;bool (tensorflow::Variant*)&gt; &gt; &gt;(std::integral_constant&lt;bool, true&gt;, std::pair&lt;tensorflow::StringPiece, std::function&lt;bool (tensorflow::Variant*)&gt; &gt;&amp;&amp;) ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#1  0x00007fffe5795735 in tensorflow::UnaryVariantOpRegistry::RegisterDecodeFn(std::string const&amp;, std::function&lt;bool (tensorflow::Variant*)&gt; const&amp;) () from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#2  0x00007fffe5770a7c in tensorflow::variant_op_registry_fn_registration::UnaryVariantDecodeRegistration&lt;tensorflow::Tensor&gt;::UnaryVariantDecodeRegistration(std::string const&amp;) ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fffe56ea165 in _GLOBAL__sub_I_tensor.cc ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007ffff7de76ba in call_init (l=&lt;optimized out&gt;, argc=argc@entry=2, argv=argv@entry=0x7fffffffd5c8, env=env@entry=0xa7b4d0)
    at dl-init.c:72
#5  0x00007ffff7de77cb in call_init (env=0xa7b4d0, argv=0x7fffffffd5c8, argc=2, l=&lt;optimized out&gt;) at dl-init.c:30
#6  _dl_init (main_map=main_map@entry=0xa11920, argc=2, argv=0x7fffffffd5c8, env=0xa7b4d0) at dl-init.c:120
#7  0x00007ffff7dec8e2 in dl_open_worker (a=a@entry=0x7fffffffb5c0) at dl-open.c:575
#8  0x00007ffff7de7564 in _dl_catch_error (objname=objname@entry=0x7fffffffb5b0, errstring=errstring@entry=0x7fffffffb5b8, 
    mallocedp=mallocedp@entry=0x7fffffffb5af, operate=operate@entry=0x7ffff7dec4d0 &lt;dl_open_worker&gt;, args=args@entry=0x7fffffffb5c0)
    at dl-error.c:187
#9  0x00007ffff7debda9 in _dl_open (
    file=0x7fffea7cbc34 ""/media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"", mode=-2147483646, caller_dlopen=0x51ad19 &lt;_PyImport_GetDynLoadFunc+233&gt;, nsid=-2, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, env=0xa7b4d0)
    at dl-open.c:660
#10 0x00007ffff75ecf09 in dlopen_doit (a=a@entry=0x7fffffffb7f0) at dlopen.c:66
#11 0x00007ffff7de7564 in _dl_catch_error (objname=0x9b1870, errstring=0x9b1878, mallocedp=0x9b1868, operate=0x7ffff75eceb0 &lt;dlopen_doit&gt;, 
    args=0x7fffffffb7f0) at dl-error.c:187
#12 0x00007ffff75ed571 in _dlerror_run (operate=operate@entry=0x7ffff75eceb0 &lt;dlopen_doit&gt;, args=args@entry=0x7fffffffb7f0) at dlerror.c:163
#13 0x00007ffff75ecfa1 in __dlopen (file=&lt;optimized out&gt;, mode=&lt;optimized out&gt;) at dlopen.c:87
#14 0x000000000051ad19 in _PyImport_GetDynLoadFunc ()
#15 0x000000000051a8e4 in _PyImport_LoadDynamicModule ()
#16 0x00000000005b7b1b in ?? ()
#17 0x00000000004bc3fa in PyEval_EvalFrameEx ()
#18 0x00000000004c136f in PyEval_EvalFrameEx ()
#19 0x00000000004b9ab6 in PyEval_EvalCodeEx ()
#20 0x00000000004b97a6 in PyEval_EvalCode ()
#21 0x00000000004b96df in PyImport_ExecCodeModuleEx ()
#22 0x00000000004b2b06 in ?? ()
#23 0x00000000004a4ae1 in ?? ()
</code></pre>
",4405942,2018,04-03-2018 10:54,04-03-2018 11:36,0,2018,26,4,20,90,"{'badge_counts': {'bronze': 26, 'silver': 20, 'gold': 4}, 'account_id': 5553739, 'is_employee': False, 'last_modified_date': 1671534000, 'last_access_date': 1711098962, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2018, 'creation_date': 1419954996, 'user_type': 'registered', 'user_id': 4405942, 'accept_rate': 90, 'link': 'https://stackoverflow.com/users/4405942/gerry', 'profile_image': 'https://www.gravatar.com/avatar/7c1d9c9e346d01262f46c7234ad21dce?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Gerry'}",I created a fresh virtual environment: And installed tensorflow: gives me Please help me understand what's going on and how I can fix it. Thank you. CPU information: Stacktrace obtained with gdb:,"virtualenv -p python2 test_venv/ pip install --upgrade --no-cache-dir tensorflow import tensorflow Illegal instruction (core dumped) -cpu
          description: CPU
          product: Intel(R) Core(TM) i3 CPU       M 330  @ 2.13GHz
          bus info: cpu@0
          version: CPU Version
          capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm arat cpufreq
 #0  0x00007fffe5793880 in std::pair&lt;std::__detail::_Node_iterator&lt;std::pair&lt;tensorflow::StringPiece const, std::function&lt;bool (tensorflow::Variant*)&gt; &gt;, false, true&gt;, bool&gt; std::_Hashtable&lt;tensorflow::StringPiece, std::pair&lt;tensorflow::StringPiece const, std::function&lt;bool (tensorflow::Variant*)&gt; &gt;, std::allocator&lt;std::pair&lt;tensorflow::StringPiece const, std::function&lt;bool (tensorflow::Variant*)&gt; &gt; &gt;, std::__detail::_Select1st, std::equal_to&lt;tensorflow::StringPiece&gt;, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits&lt;true, false, true&gt; &gt;::_M_emplace&lt;std::pair&lt;tensorflow::StringPiece, std::function&lt;bool (tensorflow::Variant*)&gt; &gt; &gt;(std::integral_constant&lt;bool, true&gt;, std::pair&lt;tensorflow::StringPiece, std::function&lt;bool (tensorflow::Variant*)&gt; &gt;&amp;&amp;) ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#1  0x00007fffe5795735 in tensorflow::UnaryVariantOpRegistry::RegisterDecodeFn(std::string const&amp;, std::function&lt;bool (tensorflow::Variant*)&gt; const&amp;) () from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#2  0x00007fffe5770a7c in tensorflow::variant_op_registry_fn_registration::UnaryVariantDecodeRegistration&lt;tensorflow::Tensor&gt;::UnaryVariantDecodeRegistration(std::string const&amp;) ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fffe56ea165 in _GLOBAL__sub_I_tensor.cc ()
   from /media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007ffff7de76ba in call_init (l=&lt;optimized out&gt;, argc=argc@entry=2, argv=argv@entry=0x7fffffffd5c8, env=env@entry=0xa7b4d0)
    at dl-init.c:72
#5  0x00007ffff7de77cb in call_init (env=0xa7b4d0, argv=0x7fffffffd5c8, argc=2, l=&lt;optimized out&gt;) at dl-init.c:30
#6  _dl_init (main_map=main_map@entry=0xa11920, argc=2, argv=0x7fffffffd5c8, env=0xa7b4d0) at dl-init.c:120
#7  0x00007ffff7dec8e2 in dl_open_worker (a=a@entry=0x7fffffffb5c0) at dl-open.c:575
#8  0x00007ffff7de7564 in _dl_catch_error (objname=objname@entry=0x7fffffffb5b0, errstring=errstring@entry=0x7fffffffb5b8, 
    mallocedp=mallocedp@entry=0x7fffffffb5af, operate=operate@entry=0x7ffff7dec4d0 &lt;dl_open_worker&gt;, args=args@entry=0x7fffffffb5c0)
    at dl-error.c:187
#9  0x00007ffff7debda9 in _dl_open (
    file=0x7fffea7cbc34 ""/media/gerry/hdd_1/ws_hdd/test_venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"", mode=-2147483646, caller_dlopen=0x51ad19 &lt;_PyImport_GetDynLoadFunc+233&gt;, nsid=-2, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, env=0xa7b4d0)
    at dl-open.c:660
#10 0x00007ffff75ecf09 in dlopen_doit (a=a@entry=0x7fffffffb7f0) at dlopen.c:66
#11 0x00007ffff7de7564 in _dl_catch_error (objname=0x9b1870, errstring=0x9b1878, mallocedp=0x9b1868, operate=0x7ffff75eceb0 &lt;dlopen_doit&gt;, 
    args=0x7fffffffb7f0) at dl-error.c:187
#12 0x00007ffff75ed571 in _dlerror_run (operate=operate@entry=0x7ffff75eceb0 &lt;dlopen_doit&gt;, args=args@entry=0x7fffffffb7f0) at dlerror.c:163
#13 0x00007ffff75ecfa1 in __dlopen (file=&lt;optimized out&gt;, mode=&lt;optimized out&gt;) at dlopen.c:87
#14 0x000000000051ad19 in _PyImport_GetDynLoadFunc ()
#15 0x000000000051a8e4 in _PyImport_LoadDynamicModule ()
#16 0x00000000005b7b1b in ?? ()
#17 0x00000000004bc3fa in PyEval_EvalFrameEx ()
#18 0x00000000004c136f in PyEval_EvalFrameEx ()
#19 0x00000000004b9ab6 in PyEval_EvalCodeEx ()
#20 0x00000000004b97a6 in PyEval_EvalCode ()
#21 0x00000000004b96df in PyImport_ExecCodeModuleEx ()
#22 0x00000000004b2b06 in ?? ()
#23 0x00000000004a4ae1 in ?? ()
",33,53,0,0,
158,49039436,49039535,172462,How to import a module from a different folder?,3,<python><python-3.x><python-import><python-module><python-packaging>,67,"<p>I have a project which I want to structure like this:</p>
<pre><code>myproject
├── api
│   ├── __init__.py
│   └── api.py
├── backend
│   ├── __init__.py
│   └── backend.py
├── models
│   ├── __init__.py
│   └── some_model.py
└── __init__.py
</code></pre>
<p>Now, I want to import the module <code>some_model.py</code> in both <code>api.py</code> and <code>backend.py</code>. How do I properly do this?</p>
<p>I tried:</p>
<pre><code>from models import some_model
</code></pre>
<p>but that fails with <code>ModuleNotFoundError: No module named 'models'</code>.</p>
<p>I also tried:</p>
<pre><code>from ..models import some_model
</code></pre>
<p>which gave me <code>ValueError: attempted relative import beyond top-level package</code>.</p>
<p>What am I doing wrong here? How can I import a file from a different directory, which is not a subdirectory?</p>
",4437417,1319,28-02-2018 22:09,28-02-2018 22:16,0,1319,32,2,13,70,"{'badge_counts': {'bronze': 32, 'silver': 13, 'gold': 2}, 'account_id': 5601220, 'is_employee': False, 'last_modified_date': 1707529500, 'last_access_date': 1694086786, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1319, 'creation_date': 1420817405, 'user_type': 'registered', 'user_id': 4437417, 'accept_rate': 70, 'location': 'Aachen, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/4437417/gasp0de', 'profile_image': 'https://www.gravatar.com/avatar/abb67d07a8d21b6db84e37d12217ccd9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Gasp0de'}","I have a project which I want to structure like this: Now, I want to import the module in both and . How do I properly do this? I tried: but that fails with . I also tried: which gave me . What am I doing wrong here? How can I import a file from a different directory, which is not a subdirectory?","myproject
├── api
│   ├── __init__.py
│   └── api.py
├── backend
│   ├── __init__.py
│   └── backend.py
├── models
│   ├── __init__.py
│   └── some_model.py
└── __init__.py
 some_model.py api.py backend.py from models import some_model
 ModuleNotFoundError: No module named 'models' from ..models import some_model
 ValueError: attempted relative import beyond top-level package",5,23,0,0,
159,48698536,48698875,58290,(Tensorflow-GPU) import tensorflow ImportError: Could not find 'cudnn64_7.dll',4,<python><tensorflow>,19,"<p>After created tensorflow environment under anaconda, I installed tensorflow-gpu. Then I was trying to import tensorflow to verify if it's correctly installed, but got this error:</p>

<pre><code>ImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn
</code></pre>

<p>Setup is:</p>

<pre><code>NVIDIA GTX 1080
CUDA 9.0
cuDNN 6.0
tensorflow-gpu 1.5
</code></pre>

<p>Environment Variables are:</p>

<pre><code>CUDA_PAT: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0
CUDA_PATH_V9_0: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0
</code></pre>

<p>The %Path% variables are:</p>

<pre><code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp
C:\Users\yshen\AppData\Local\cudnn-8.0-windows10-x64-v6.0\cuda\bin
</code></pre>

<p>it is obvious that I installed cuDNN6.0, don't why the error shows ""Could not find 'cudnn64_7.dll' "". Why it automatically searches cudnn64_7.dll instead of cudnn64_6.dll?</p>
",4942590,409,09-02-2018 03:58,09-02-2018 04:45,0,409,10,1,4,,"{'badge_counts': {'bronze': 10, 'silver': 4, 'gold': 1}, 'account_id': 6369829, 'is_employee': False, 'last_modified_date': 1622812165, 'last_access_date': 1554083489, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 409, 'creation_date': 1432685452, 'user_type': 'registered', 'user_id': 4942590, 'location': 'Auckland, New Zealand', 'website_url': '', 'link': 'https://stackoverflow.com/users/4942590/jshen', 'profile_image': 'https://www.gravatar.com/avatar/b198cf98ebd90c8dd61b3ce783a31046?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'JShen'}","After created tensorflow environment under anaconda, I installed tensorflow-gpu. Then I was trying to import tensorflow to verify if it's correctly installed, but got this error: Setup is: Environment Variables are: The %Path% variables are: it is obvious that I installed cuDNN6.0, don't why the error shows ""Could not find 'cudnn64_7.dll' "". Why it automatically searches cudnn64_7.dll instead of cudnn64_6.dll?","ImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn
 NVIDIA GTX 1080
CUDA 9.0
cuDNN 6.0
tensorflow-gpu 1.5
 CUDA_PAT: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0
CUDA_PATH_V9_0: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0
 C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp
C:\Users\yshen\AppData\Local\cudnn-8.0-windows10-x64-v6.0\cuda\bin
",7,28,0,0,
160,48760472,48807811,15358,How to use a Keras RNN model to forecast for future dates or events?,3,<python><tensorflow><keras>,18,"<p>Here is my code fore training the complete model and saving it:   </p>

<pre><code>num_units = 2
activation_function = 'sigmoid'
optimizer = 'adam'
loss_function = 'mean_squared_error'
batch_size = 10
num_epochs = 100

# Initialize the RNN
regressor = Sequential()

# Adding the input layer and the LSTM layer
regressor.add(LSTM(units = num_units, activation = activation_function, input_shape=(None, 1)))

# Adding the output layer
regressor.add(Dense(units = 1))

# Compiling the RNN
regressor.compile(optimizer = optimizer, loss = loss_function)

# Using the training set to train the model
regressor.fit(x_train, y_train, batch_size = batch_size, epochs = num_epochs)
regressor.save('model.h5')
</code></pre>

<p>After that I have seen that most of the time people our suggesting the test dataset for checking the prediction which I have attempted as well and got good result.     </p>

<p>But the problem is in the usage of the model that I have created. I want to have a forecast for next 30 days or every minute whatsoever. Now I have the trained model but I am not getting what I can do or what code do I use to use the model and forecast the prices for next 30 days or one minute.    </p>

<p>Please suggest me the way out. I am stuck at this problem since a week and not able to make any successful attempts.   </p>

<p>Here is the link of the repository where one can find the complete runnable code, the model, and the dataset:  <a href=""https://github.com/JafferWilson/forecastbtc/"" rel=""nofollow noreferrer"">My repository link</a></p>
",4948889,7185,13-02-2018 06:09,15-02-2018 12:53,2,7185,151,10,66,55,"{'badge_counts': {'bronze': 151, 'silver': 66, 'gold': 10}, 'account_id': 6378723, 'is_employee': False, 'last_modified_date': 1711163111, 'last_access_date': 1711173414, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7185, 'creation_date': 1432812505, 'user_type': 'registered', 'user_id': 4948889, 'accept_rate': 55, 'location': 'Boston, MA, United States', 'website_url': 'http://www.webresponsivedesigns.com/', 'link': 'https://stackoverflow.com/users/4948889/jaffer-wilson', 'profile_image': 'https://www.gravatar.com/avatar/7f1b540c13a4b2663a9bf668f6a50145?s=256&d=identicon&r=PG', 'display_name': 'Jaffer Wilson'}","Here is my code fore training the complete model and saving it: After that I have seen that most of the time people our suggesting the test dataset for checking the prediction which I have attempted as well and got good result. But the problem is in the usage of the model that I have created. I want to have a forecast for next 30 days or every minute whatsoever. Now I have the trained model but I am not getting what I can do or what code do I use to use the model and forecast the prices for next 30 days or one minute. Please suggest me the way out. I am stuck at this problem since a week and not able to make any successful attempts. Here is the link of the repository where one can find the complete runnable code, the model, and the dataset: My repository link","num_units = 2
activation_function = 'sigmoid'
optimizer = 'adam'
loss_function = 'mean_squared_error'
batch_size = 10
num_epochs = 100

# Initialize the RNN
regressor = Sequential()

# Adding the input layer and the LSTM layer
regressor.add(LSTM(units = num_units, activation = activation_function, input_shape=(None, 1)))

# Adding the output layer
regressor.add(Dense(units = 1))

# Compiling the RNN
regressor.compile(optimizer = optimizer, loss = loss_function)

# Using the training set to train the model
regressor.fit(x_train, y_train, batch_size = batch_size, epochs = num_epochs)
regressor.save('model.h5')
",21,33,0,1,
161,49000803,49000841,3858,Python iterate through array while finding the mean of the top k elements,15,<python><algorithm>,19,"<p>Suppose I have a Python array <code>a=[3, 5, 2, 7, 5, 3, 6, 8, 4]</code>. My goal is to iterate through this array 3 elements at a time returning the mean of the top 2 of the three elements.</p>

<p>Using the above array, during my iteration step, the first three elements are <code>[3, 5, 2]</code> and the mean of the top 2 elements is 4. The next three elements are <code>[5, 2, 7]</code> and the mean of the top 2 elements is 6. The next three elements are <code>[2, 7, 5]</code> and the mean of the top 2 elements is again 6. ...</p>

<p>Hence, the result for the above array would be <code>[4, 6, 6, 6, 5.5, 7, 7]</code>.</p>

<p>What is the nicest way to write such a function?</p>
",4972352,369,27-02-2018 03:49,27-02-2018 03:53,0,369,11,0,2,80,"{'badge_counts': {'bronze': 11, 'silver': 2, 'gold': 0}, 'account_id': 3472292, 'is_employee': False, 'last_modified_date': 1681954743, 'last_access_date': 1704292101, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 369, 'creation_date': 1433396967, 'user_type': 'registered', 'user_id': 4972352, 'accept_rate': 80, 'link': 'https://stackoverflow.com/users/4972352/darkgbm', 'profile_image': 'https://www.gravatar.com/avatar/e8a16b74feacf85e6740b2625458f0cf?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'darkgbm'}","Suppose I have a Python array . My goal is to iterate through this array 3 elements at a time returning the mean of the top 2 of the three elements. Using the above array, during my iteration step, the first three elements are and the mean of the top 2 elements is 4. The next three elements are and the mean of the top 2 elements is 6. The next three elements are and the mean of the top 2 elements is again 6. ... Hence, the result for the above array would be . What is the nicest way to write such a function?","a=[3, 5, 2, 7, 5, 3, 6, 8, 4] [3, 5, 2] [5, 2, 7] [2, 7, 5] [4, 6, 6, 6, 5.5, 7, 7]",-5,7,0,0,
162,48616694,48616798,5305,Remove combinations that contains some values before even calculated,7,<python><python-3.x><combinations><python-itertools>,20,"<p>given a list and exclusions elements, is it possible to ignore calculation of combinations that contains these elements ?</p>

<h1>Example 1</h1>

<p>Given <code>l = [1, 2, 3, 4, 5]</code>, I want to calculate all combinations of <code>size 4</code> and excluding combinations that contains <code>(1, 3)</code> before even calculated. </p>

<p>The results would be :</p>

<pre><code>    All results:            Wanted results:

    [1, 2, 3, 4]            [1, 2, 4, 5]
    [1, 2, 3, 5]            [2, 3, 4, 5]
    [1, 2, 4, 5]
    [1, 3, 4, 5]
    [2, 3, 4, 5]
</code></pre>

<p>All combinations that contained <strong>1 and 3</strong> have been removed.</p>

<h1>Example 2</h1>

<p>suggested by @Eric Duminil</p>

<p>the result for <code>l = [1, 2, 3, 4, 5, 6]</code>, <code>size 4</code> and </p>

<ul>
<li>excluding <code>(1, 2, 3)</code> in second column</li>
<li><p>excluding <code>(1, 2)</code> in third column</p>

<pre><code>All results:        Wanted results 1            Wanted results 2
                    (Excluding [1, 2, 3]):      (Excluding [1, 2])

[1, 2, 3, 4]        [1, 2, 4, 5]                [1, 3, 4, 5]
[1, 2, 3, 5]        [1, 2, 4, 6]                [1, 3, 4, 6]
[1, 2, 3, 6]        [1, 2, 5, 6]                [1, 3, 5, 6]
[1, 2, 4, 5]        [1, 3, 4, 5]                [1, 4, 5, 6]
[1, 2, 4, 6]        [1, 3, 4, 6]                [2, 3, 4, 5]
[1, 2, 5, 6]        [1, 3, 5, 6]                [2, 3, 4, 6]
[1, 3, 4, 5]        [1, 4, 5, 6]                [2, 3, 5, 6]
[1, 3, 4, 6]        [2, 3, 4, 5]                [2, 4, 5, 6]
[1, 3, 5, 6]        [2, 3, 4, 6]                [3, 4, 5, 6]
[1, 4, 5, 6]        [2, 3, 5, 6]                                
[2, 3, 4, 5]        [2, 4, 5, 6]                                
[2, 3, 4, 6]        [3, 4, 5, 6]                                
[2, 3, 5, 6]           
[2, 4, 5, 6]           
[3, 4, 5, 6]        
</code></pre></li>
</ul>

<p>All combinations that contained <strong>1 and 2 and 3</strong> have been removed from wanted results 1</p>

<p>All combinations that contained <strong>1 and 2</strong> have been removed from wanted results 2</p>

<p>I have a much bigger combinations to compute but it takes a lot of time and I want to reduce this time using these exclusions.</p>

<h1>Tried solutions</h1>

<p>With method 1, the combinations are still calculated</p>

<p>With method 2, I tried to modify the <a href=""https://docs.python.org/3/library/itertools.html#itertools.combinations"" rel=""nofollow noreferrer"">combinations function</a> but I could not find a proper way to ignore my exclusion list before calculated.</p>

<pre><code>            Method 1                    |               Method 2
                                        |               
def main():                             |   def combinations(iterable, r):
    l = list(range(1, 6))               |       pool = tuple(iterable)
    comb = combinations(l, 4)           |       n = len(pool)
                                        |       if r &gt; n:
    for i in comb:                      |           return
        if set([1, 3]).issubset(i):     |       indices = list(range(r))
            continue                    |       yield tuple(pool[i] for i in indices)
        else                            |       while True:
            process()                   |           for i in reversed(range(r)):
                                        |               if indices[i] != i + n - r:
                                        |                   break
                                        |               else:
                                        |                   return
                                        |           indices[i] += 1
                                        |           for j in range(i+1, r):
                                        |               indices[j] = indices[j-1] + 1
                                        |           yield tuple(pool[i] for i in indices)
</code></pre>

<h1>EDIT:</h1>

<p>First of all, thank you all for your help, I forgot to give more details about constraints.</p>

<ul>
<li><p>The order of the ouputs is not relevant, from example, if result is <code>[1, 2, 4, 5] [2, 3, 4, 5]</code> or <code>[2, 3, 4, 5] [1, 2, 4, 5]</code>, it is not important.</p></li>
<li><p>The elements of the combinations should be (if possible) sorted, <code>[1, 2, 4, 5] [2, 3, 4, 5]</code> and not <code>[2, 1, 5, 4] [3, 2, 4, 5]</code> but it is not important since the combinations could be sorted after.</p></li>
<li><p>The exclusions list is a list of all items that <strong>should not appear</strong> in the combinations <strong>together</strong>. e.g If my exclusion list is <code>(1, 2, 3)</code>, all combinations that contains <strong>1 and 2 and 3</strong> should not be calculated. However, combinations with <strong>1 and 2 and not 3</strong> are allowed. In that case, if I exclude combinations that contains <code>(1, 2)</code> and <code>(1, 2, 3)</code> it is completely useless since all combinations that will be filtered by <code>(1, 2, 3)</code> are already filtered by <code>(1, 2)</code></p></li>
<li><p><strong>Multiple exclude lists</strong> must be possible because I use multiple constraints on my combinations.</p></li>
</ul>

<h1>Tested answers</h1>

<p>@tobias_k
This solution considers the exclusion list <code>(1, 2, 3)</code> as OR exclusion meaning <code>(1, 2), (2, 3) and (1, 3)</code> will be excluded if I understood well, this is useful in a case but not in my current problem, I modified the question to give more details, sorry for confusion. In your answer, I can't use only lists <code>(1, 2)</code> and <code>(1, 3)</code> as exclusion as you specified it. However the big advantage of this solution is to permit multiple exclusions.</p>

<p>@Kasramvd and @mikuszefski
Your solution is really close to what I want, if it does include multiple exclusion lists, it would be the answer.</p>

<p>Thanks</p>
",4997158,1300,05-02-2018 06:04,05-02-2018 06:15,0,1300,32,2,19,60,"{'badge_counts': {'bronze': 32, 'silver': 19, 'gold': 2}, 'account_id': 6026090, 'is_employee': False, 'last_modified_date': 1643322830, 'last_access_date': 1710425345, 'reputation_change_year': 15, 'reputation_change_quarter': 15, 'reputation_change_month': 15, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1300, 'creation_date': 1433982921, 'user_type': 'registered', 'user_id': 4997158, 'accept_rate': 60, 'location': 'Seattle, WA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/4997158/syedelec', 'profile_image': 'https://i.stack.imgur.com/nmhsE.png?s=256&g=1', 'display_name': 'syedelec'}","given a list and exclusions elements, is it possible to ignore calculation of combinations that contains these elements ? Example 1 Given , I want to calculate all combinations of and excluding combinations that contains before even calculated. The results would be : All combinations that contained 1 and 3 have been removed. Example 2 suggested by @Eric Duminil the result for , and excluding in second column excluding in third column All combinations that contained 1 and 2 and 3 have been removed from wanted results 1 All combinations that contained 1 and 2 have been removed from wanted results 2 I have a much bigger combinations to compute but it takes a lot of time and I want to reduce this time using these exclusions. Tried solutions With method 1, the combinations are still calculated With method 2, I tried to modify the combinations function but I could not find a proper way to ignore my exclusion list before calculated. EDIT: First of all, thank you all for your help, I forgot to give more details about constraints. The order of the ouputs is not relevant, from example, if result is or , it is not important. The elements of the combinations should be (if possible) sorted, and not but it is not important since the combinations could be sorted after. The exclusions list is a list of all items that should not appear in the combinations together. e.g If my exclusion list is , all combinations that contains 1 and 2 and 3 should not be calculated. However, combinations with 1 and 2 and not 3 are allowed. In that case, if I exclude combinations that contains and it is completely useless since all combinations that will be filtered by are already filtered by Multiple exclude lists must be possible because I use multiple constraints on my combinations. Tested answers @tobias_k This solution considers the exclusion list as OR exclusion meaning will be excluded if I understood well, this is useful in a case but not in my current problem, I modified the question to give more details, sorry for confusion. In your answer, I can't use only lists and as exclusion as you specified it. However the big advantage of this solution is to permit multiple exclusions. @Kasramvd and @mikuszefski Your solution is really close to what I want, if it does include multiple exclusion lists, it would be the answer. Thanks","l = [1, 2, 3, 4, 5] size 4 (1, 3)     All results:            Wanted results:

    [1, 2, 3, 4]            [1, 2, 4, 5]
    [1, 2, 3, 5]            [2, 3, 4, 5]
    [1, 2, 4, 5]
    [1, 3, 4, 5]
    [2, 3, 4, 5]
 l = [1, 2, 3, 4, 5, 6] size 4 (1, 2, 3) (1, 2) All results:        Wanted results 1            Wanted results 2
                    (Excluding [1, 2, 3]):      (Excluding [1, 2])

[1, 2, 3, 4]        [1, 2, 4, 5]                [1, 3, 4, 5]
[1, 2, 3, 5]        [1, 2, 4, 6]                [1, 3, 4, 6]
[1, 2, 3, 6]        [1, 2, 5, 6]                [1, 3, 5, 6]
[1, 2, 4, 5]        [1, 3, 4, 5]                [1, 4, 5, 6]
[1, 2, 4, 6]        [1, 3, 4, 6]                [2, 3, 4, 5]
[1, 2, 5, 6]        [1, 3, 5, 6]                [2, 3, 4, 6]
[1, 3, 4, 5]        [1, 4, 5, 6]                [2, 3, 5, 6]
[1, 3, 4, 6]        [2, 3, 4, 5]                [2, 4, 5, 6]
[1, 3, 5, 6]        [2, 3, 4, 6]                [3, 4, 5, 6]
[1, 4, 5, 6]        [2, 3, 5, 6]                                
[2, 3, 4, 5]        [2, 4, 5, 6]                                
[2, 3, 4, 6]        [3, 4, 5, 6]                                
[2, 3, 5, 6]           
[2, 4, 5, 6]           
[3, 4, 5, 6]        
             Method 1                    |               Method 2
                                        |               
def main():                             |   def combinations(iterable, r):
    l = list(range(1, 6))               |       pool = tuple(iterable)
    comb = combinations(l, 4)           |       n = len(pool)
                                        |       if r &gt; n:
    for i in comb:                      |           return
        if set([1, 3]).issubset(i):     |       indices = list(range(r))
            continue                    |       yield tuple(pool[i] for i in indices)
        else                            |       while True:
            process()                   |           for i in reversed(range(r)):
                                        |               if indices[i] != i + n - r:
                                        |                   break
                                        |               else:
                                        |                   return
                                        |           indices[i] += 1
                                        |           for j in range(i+1, r):
                                        |               indices[j] = indices[j-1] + 1
                                        |           yield tuple(pool[i] for i in indices)
 [1, 2, 4, 5] [2, 3, 4, 5] [2, 3, 4, 5] [1, 2, 4, 5] [1, 2, 4, 5] [2, 3, 4, 5] [2, 1, 5, 4] [3, 2, 4, 5] (1, 2, 3) (1, 2) (1, 2, 3) (1, 2, 3) (1, 2) (1, 2, 3) (1, 2), (2, 3) and (1, 3) (1, 2) (1, 3)",21,103,0,1,
163,48170086,48171833,28078,What to use instead of bokeh.charts,1,<python><charts><bokeh>,17,"<p>I am trying to run some code written by someone else, which contains the line</p>

<pre><code>from bokeh.charts import Bar
</code></pre>

<p>When I run this in the Anaconda Prompt, I get the message ""No module named 'bokeh.charts'"".</p>

<p>I have installed bokeh 0.12.13, so the problem isn't that I haven't installed it. Indeed, other bokeh modules run fine. </p>

<p>I have noticed on the bokeh website that the 'charts' module says that it refers to a previous version (see <a href=""https://docs.bokeh.org/en/0.12.4/docs/reference/charts.html"" rel=""noreferrer"">https://docs.bokeh.org/en/0.12.4/docs/reference/charts.html</a>).</p>

<p>Does the latest version not have bokeh.charts? If so, is there an alternative?</p>
",4451315,8562,09-01-2018 14:13,09-01-2018 15:44,0,8704,78,5,35,100,"{'badge_counts': {'bronze': 78, 'silver': 35, 'gold': 5}, 'account_id': 5619481, 'is_employee': False, 'last_modified_date': 1708132800, 'last_access_date': 1711128085, 'reputation_change_year': 1243, 'reputation_change_quarter': 1243, 'reputation_change_month': 314, 'reputation_change_week': 50, 'reputation_change_day': 0, 'reputation': 8704, 'creation_date': 1421191268, 'user_type': 'registered', 'user_id': 4451315, 'accept_rate': 100, 'location': 'Quox', 'website_url': '', 'link': 'https://stackoverflow.com/users/4451315/ignoring-gravity', 'profile_image': 'https://www.gravatar.com/avatar/ca1f6c6db3c9da1ebd9e3bcbfed2cc23?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ignoring_gravity'}","I am trying to run some code written by someone else, which contains the line When I run this in the Anaconda Prompt, I get the message ""No module named 'bokeh.charts'"". I have installed bokeh 0.12.13, so the problem isn't that I haven't installed it. Indeed, other bokeh modules run fine. I have noticed on the bokeh website that the 'charts' module says that it refers to a previous version (see https://docs.bokeh.org/en/0.12.4/docs/reference/charts.html). Does the latest version not have bokeh.charts? If so, is there an alternative?","from bokeh.charts import Bar
",0,12,0,1,
164,48560138,48709863,28509,plotly.offline.iplot gives a large blank field as its output in Jupyter Notebook/Lab,15,<python><jupyter-notebook><plotly><jupyter><jupyter-lab>,36,"<p>I am trying to create a Sankey chart in a Jupyter notebook, basing my code on <a href=""https://plot.ly/~alishobeiri/1257/plotly-sankey-diagrams/"" rel=""noreferrer"">the first example shown here</a>.</p>

<p>I ended up with this, which I can run without getting any errors:</p>

<pre><code>import numpy as npy
import pandas as pd
import plotly as ply

ply.offline.init_notebook_mode(connected=True)

df = pd.read_csv('C:\\Users\\a245401\\Desktop\\Test.csv',sep=';')

print(df.head())
print(ply.__version__)

data_trace = dict(
    type='sankey',
    domain = dict(
      x =  [0,1],
      y =  [0,1]
    ),
    orientation = ""h"",
    valueformat = "".0f"",
    node = dict(
      pad = 10,
      thickness = 30,
      line = dict(
        color = ""black"",
        width = 0.5
      ),
      label =  df['Node, Label'].dropna(axis=0, how='any'),
      color = df['Color']
    ),
    link = dict(
      source = df['Source'].dropna(axis=0, how='any'),
      target = df['Target'].dropna(axis=0, how='any'),
      value = df['Value'].dropna(axis=0, how='any'),
  )
)
print(data_trace)

layout =  dict(
    title = ""Test"",
    height = 772,
    width = 950,
    font = dict(
      size = 10
    ),    
)
print(layout)

fig = dict(data=[data_trace], layout=layout)
ply.offline.iplot(fig, filename='Test')
</code></pre>

<p>With the csv-file looking like this:</p>

<pre><code>Source;Target;Value;Color;Node, Label
0;2;2958.5;#262C46;Test 1
0;2;236.7;#262C46;Test 2
0;2;1033.4;#262C46;Test 3
0;2;58.8;#262C46;Test 4
0;2;5.2;#262C46;Test 5
0;2;9.4;#262C46;Test 6
0;2;3.4;#262C46;Test 7
</code></pre>

<p>It seems to run fine, with the various outputs <em>looking</em> right at a first glance, but the final output from <code>ply.offline.iplot(fig, filename='Test')</code> just shows a large blank field:
<a href=""https://i.stack.imgur.com/AQz6L.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/AQz6L.png"" alt=""enter image description here""></a>
The terminal looks like this after having run all the cells in the notebook once:
<a href=""https://i.stack.imgur.com/OvLxk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OvLxk.png"" alt=""enter image description here""></a></p>

<p>Can someone please point me to where I am going wrong here?</p>

<ul>
<li>edit: I also posted this question on the plotly forums: <a href=""https://community.plot.ly/t/no-output-from-plotly-offline-iplot/8086"" rel=""noreferrer"">https://community.plot.ly/t/no-output-from-plotly-offline-iplot/8086</a> -</li>
</ul>
",4497791,3164,01-02-2018 10:10,09-02-2018 16:12,8,3164,53,6,27,65,"{'badge_counts': {'bronze': 53, 'silver': 27, 'gold': 6}, 'account_id': 5689271, 'is_employee': False, 'last_modified_date': 1700877000, 'last_access_date': 1709980548, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3164, 'creation_date': 1422346349, 'user_type': 'registered', 'user_id': 4497791, 'accept_rate': 65, 'location': 'Norway', 'website_url': '', 'link': 'https://stackoverflow.com/users/4497791/eirikdaude', 'profile_image': 'https://i.stack.imgur.com/vCgRs.png?s=256&g=1', 'display_name': 'eirikdaude'}","I am trying to create a Sankey chart in a Jupyter notebook, basing my code on the first example shown here. I ended up with this, which I can run without getting any errors: With the csv-file looking like this: It seems to run fine, with the various outputs looking right at a first glance, but the final output from just shows a large blank field: The terminal looks like this after having run all the cells in the notebook once: Can someone please point me to where I am going wrong here? edit: I also posted this question on the plotly forums: https://community.plot.ly/t/no-output-from-plotly-offline-iplot/8086 -","import numpy as npy
import pandas as pd
import plotly as ply

ply.offline.init_notebook_mode(connected=True)

df = pd.read_csv('C:\\Users\\a245401\\Desktop\\Test.csv',sep=';')

print(df.head())
print(ply.__version__)

data_trace = dict(
    type='sankey',
    domain = dict(
      x =  [0,1],
      y =  [0,1]
    ),
    orientation = ""h"",
    valueformat = "".0f"",
    node = dict(
      pad = 10,
      thickness = 30,
      line = dict(
        color = ""black"",
        width = 0.5
      ),
      label =  df['Node, Label'].dropna(axis=0, how='any'),
      color = df['Color']
    ),
    link = dict(
      source = df['Source'].dropna(axis=0, how='any'),
      target = df['Target'].dropna(axis=0, how='any'),
      value = df['Value'].dropna(axis=0, how='any'),
  )
)
print(data_trace)

layout =  dict(
    title = ""Test"",
    height = 772,
    width = 950,
    font = dict(
      size = 10
    ),    
)
print(layout)

fig = dict(data=[data_trace], layout=layout)
ply.offline.iplot(fig, filename='Test')
 Source;Target;Value;Color;Node, Label
0;2;2958.5;#262C46;Test 1
0;2;236.7;#262C46;Test 2
0;2;1033.4;#262C46;Test 3
0;2;58.8;#262C46;Test 4
0;2;5.2;#262C46;Test 5
0;2;9.4;#262C46;Test 6
0;2;3.4;#262C46;Test 7
 ply.offline.iplot(fig, filename='Test')",54,77,2,4,
165,48350850,48351036,182099,Subtract two columns in dataframe,8,<python><pandas><dataframe>,34,"<p>My df looks as follows:</p>

<pre><code>Index    Country    Val1  Val2 ... Val10
1        Australia  1     3    ... 5
2        Bambua     12    33   ... 56
3        Tambua     14    34   ... 58
</code></pre>

<p>I'd like to substract Val10 from Val1 for each country, so output looks like:</p>

<pre><code>Country    Val10-Val1
Australia  4
Bambua     23
Tambua     24
</code></pre>

<p>So far I've got:</p>

<pre><code>def myDelta(row):
    data = row[['Val10', 'Val1']]
    return pd.Series({'Delta': np.subtract(data)})

def runDeltas():
    myDF = getDF() \
        .apply(myDelta, axis=1) \
        .sort_values(by=['Delta'], ascending=False)
    return myDF
</code></pre>

<p>runDeltas results in this error:</p>

<pre><code>ValueError: ('invalid number of arguments', u'occurred at index 9')
</code></pre>

<p>What's the proper way to fix this?</p>
",4502870,593,19-01-2018 23:12,19-01-2018 23:37,0,593,16,3,7,69,"{'badge_counts': {'bronze': 16, 'silver': 7, 'gold': 3}, 'account_id': 5696535, 'is_employee': False, 'last_modified_date': 1607614580, 'last_access_date': 1517913284, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 593, 'creation_date': 1422447636, 'user_type': 'registered', 'user_id': 4502870, 'accept_rate': 69, 'link': 'https://stackoverflow.com/users/4502870/peter', 'profile_image': 'https://www.gravatar.com/avatar/28845d56a1fd62608371013369eead16?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Peter'}","My df looks as follows: I'd like to substract Val10 from Val1 for each country, so output looks like: So far I've got: runDeltas results in this error: What's the proper way to fix this?","Index    Country    Val1  Val2 ... Val10
1        Australia  1     3    ... 5
2        Bambua     12    33   ... 56
3        Tambua     14    34   ... 58
 Country    Val10-Val1
Australia  4
Bambua     23
Tambua     24
 def myDelta(row):
    data = row[['Val10', 'Val1']]
    return pd.Series({'Delta': np.subtract(data)})

def runDeltas():
    myDF = getDF() \
        .apply(myDelta, axis=1) \
        .sort_values(by=['Delta'], ascending=False)
    return myDF
 ValueError: ('invalid number of arguments', u'occurred at index 9')
",14,35,0,0,
166,49239516,49285362,3524,Spotipy Refreshing a token with authorization code flow,2,<python><authorization><refresh><spotipy>,11,"<p>I have a long-running script using spotipy. After an hour (per the Spotify API), my access token expires. I am catching this successfully, but I don't know where to go from there in regards to actually refreshing the token. I am using the authorization code flow, not client credentials. Here's how I authorize:</p>

<pre><code>token = util.prompt_for_user_token(username,scope=scopes,client_id=client_id,client_secret=client_secret, redirect_uri=redirect_uri)

sp = spotipy.Spotify(auth=token)
</code></pre>

<p>All refresh examples I've seen involve an <code>oauth2</code> object (ex. <code>oauth.refresh_access_token()</code>), and the docs list only that function as a method of refreshing your token. It's my understanding that with authorization code flow, you don't need an <code>oauth</code> object (because you authenticate with <code>prompt_for_user_token()</code>). If that's the case, how do I refresh my token?</p>
",5112620,474,12-03-2018 15:57,14-03-2018 18:35,2,474,18,0,4,,"{'badge_counts': {'bronze': 18, 'silver': 4, 'gold': 0}, 'account_id': 6620883, 'is_employee': False, 'last_modified_date': 1631183108, 'last_access_date': 1680993303, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 474, 'creation_date': 1436818579, 'user_type': 'registered', 'user_id': 5112620, 'location': 'Eugene, OR, USA', 'website_url': 'https://nhend.github.io/', 'link': 'https://stackoverflow.com/users/5112620/gos', 'profile_image': 'https://lh3.googleusercontent.com/-wckgHx_XXMY/AAAAAAAAAAI/AAAAAAAAABo/ItFUv0bJHIc/photo.jpg?sz=256', 'display_name': 'gos'}","I have a long-running script using spotipy. After an hour (per the Spotify API), my access token expires. I am catching this successfully, but I don't know where to go from there in regards to actually refreshing the token. I am using the authorization code flow, not client credentials. Here's how I authorize: All refresh examples I've seen involve an object (ex. ), and the docs list only that function as a method of refreshing your token. It's my understanding that with authorization code flow, you don't need an object (because you authenticate with ). If that's the case, how do I refresh my token?","token = util.prompt_for_user_token(username,scope=scopes,client_id=client_id,client_secret=client_secret, redirect_uri=redirect_uri)

sp = spotipy.Spotify(auth=token)
 oauth2 oauth.refresh_access_token() oauth prompt_for_user_token()",-2,8,0,0,
167,49684811,49745820,11054,Display matplotlib graph in browser,1,<python><matplotlib>,14,"<p>I am using matplotlib library of python for plotting graphs. My program is to detect the outliers in the system. I am making an interface for this. I have to show this graph in browser on clicking the button. Here is my php file code :- </p>

<pre><code>&lt;?php
    if (isset($_POST['button']))
    {
         echo shell_exec(""python3 /var/www/html/python/anomalies.py 2&gt;&amp;1"");
    }
?&gt;
&lt;html&gt;

&lt;body&gt;
&lt;center&gt;


    &lt;form method=""post""&gt;
    &lt;table&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Enter CSV file path:
            &lt;/td&gt;
            &lt;td&gt;
                &lt;input type=""text"" name=""path""&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/table&gt;
    &lt;p&gt;
        &lt;button name=""button""&gt;Identify Anomalies&lt;/button&gt;
    &lt;/p&gt;
    &lt;/form&gt;
    &lt;/center&gt;
&lt;/body&gt;
</code></pre>

<p>on clicking this button a python script runs which gives the output as graph. The problem is that if i run it by command line, the script generates the graph as output. but when i run this script through browser button click, it generates the following error:</p>

<pre><code>Traceback (most recent call last): File ""/var/www/html/python/anomalies.py"", line 72, in plt.xlabel('RelativeTime (ms)') File ""/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py"", line 1512, in xlabel return gca().set_xlabel(s, *args, **kwargs) File ""/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py"", line 984, in gca return gcf().gca(**kwargs) File ""/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py"", line 601, in gcf return figure() File ""/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py"", line 548, in figure **kwargs) File ""/usr/local/lib/python3.4/dist-packages/matplotlib/backend_bases.py"", line 161, in new_figure_manager return cls.new_figure_manager_given_figure(num, fig) File ""/usr/local/lib/python3.4/dist-packages/matplotlib/backends/_backend_tk.py"", line 1044, in new_figure_manager_given_figure window = Tk.Tk(className=""matplotlib"") File ""/usr/lib/python3.4/tkinter/__init__.py"", line 1854, in __init__ self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use) _tkinter.TclError: no display name and no $DISPLAY environment variable
</code></pre>

<p>Here is my python code snippet:- </p>

<pre><code>plt.xlabel('R_Time (ms)')
plt.ylabel('A_Time (ms)')
plt.title('R-A Combinations')
plt.plot(tr_data[:,0],tr_data[:,1],'bx')
plt.plot(tr_data[outliers,0],tr_data[outliers,1],'ro')

plt.show()
</code></pre>

<p>I tried using <code>mpld3</code> but found no resolution.
Please suggest where i am missing it in.</p>
",5118701,1189,06-04-2018 03:28,10-04-2018 05:17,4,1189,38,4,19,44,"{'badge_counts': {'bronze': 38, 'silver': 19, 'gold': 4}, 'account_id': 6629634, 'is_employee': False, 'last_modified_date': 1607614545, 'last_access_date': 1705553807, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1189, 'creation_date': 1436952218, 'user_type': 'registered', 'user_id': 5118701, 'accept_rate': 44, 'location': 'Gurgaon, Haryana, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/5118701/hitttt', 'profile_image': 'https://www.gravatar.com/avatar/0dcbf846ead021959dc95dfaf95556eb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'hitttt'}","I am using matplotlib library of python for plotting graphs. My program is to detect the outliers in the system. I am making an interface for this. I have to show this graph in browser on clicking the button. Here is my php file code :- on clicking this button a python script runs which gives the output as graph. The problem is that if i run it by command line, the script generates the graph as output. but when i run this script through browser button click, it generates the following error: Here is my python code snippet:- I tried using but found no resolution. Please suggest where i am missing it in.","&lt;?php
    if (isset($_POST['button']))
    {
         echo shell_exec(""python3 /var/www/html/python/anomalies.py 2&gt;&amp;1"");
    }
?&gt;
&lt;html&gt;

&lt;body&gt;
&lt;center&gt;


    &lt;form method=""post""&gt;
    &lt;table&gt;
        &lt;tr&gt;
            &lt;td&gt;
                Enter CSV file path:
            &lt;/td&gt;
            &lt;td&gt;
                &lt;input type=""text"" name=""path""&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/table&gt;
    &lt;p&gt;
        &lt;button name=""button""&gt;Identify Anomalies&lt;/button&gt;
    &lt;/p&gt;
    &lt;/form&gt;
    &lt;/center&gt;
&lt;/body&gt;
 Traceback (most recent call last): File ""/var/www/html/python/anomalies.py"", line 72, in plt.xlabel('RelativeTime (ms)') File ""/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py"", line 1512, in xlabel return gca().set_xlabel(s, *args, **kwargs) File ""/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py"", line 984, in gca return gcf().gca(**kwargs) File ""/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py"", line 601, in gcf return figure() File ""/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py"", line 548, in figure **kwargs) File ""/usr/local/lib/python3.4/dist-packages/matplotlib/backend_bases.py"", line 161, in new_figure_manager return cls.new_figure_manager_given_figure(num, fig) File ""/usr/local/lib/python3.4/dist-packages/matplotlib/backends/_backend_tk.py"", line 1044, in new_figure_manager_given_figure window = Tk.Tk(className=""matplotlib"") File ""/usr/lib/python3.4/tkinter/__init__.py"", line 1854, in __init__ self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use) _tkinter.TclError: no display name and no $DISPLAY environment variable
 plt.xlabel('R_Time (ms)')
plt.ylabel('A_Time (ms)')
plt.title('R-A Combinations')
plt.plot(tr_data[:,0],tr_data[:,1],'bx')
plt.plot(tr_data[outliers,0],tr_data[outliers,1],'ro')

plt.show()
 mpld3",33,51,0,0,
168,49806790,49808760,75752,"Iterable over raw text documents expected, string object received",2,<python><string><iterator>,32,"<p>I am currently trying to build a naive Bayes classifier as mentioned in <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""noreferrer"">this link.</a>
Referring to the line</p>

<pre><code>X_new_tfidf = tfidf_transformer.transform(X_new_counts)
</code></pre>

<p>under the <code>Training the Classifier</code> subheading, I had a similar line, <code>X_new_counts = count_vect.transform(input.plot_movie)</code> in my code which should take an iterable as an input to the transform function. The  <code>input</code> is a record from a DataFrame and is of type <code>pd.Series</code> and contains the following entries, out of which I send <code>input.plot_movie</code> as the input to the transform function:</p>

<p><a href=""https://i.stack.imgur.com/I4m2D.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/I4m2D.png"" alt=""enter image description here""></a></p>

<p>However, I get the following error: <code>Iterable over raw text documents expected, string object received</code></p>

<p>How do I fix this error? I also referred to <a href=""https://stackoverflow.com/a/9884501/5157633"">this</a> answer where the person says that <code>s</code> is an iterable because it was assigned a string. I also came across <a href=""http://www.pythonabc.com/iterable-and-iterator/"" rel=""noreferrer"">this link</a> where a <code>TypeError: 'String' object is not iterable</code> is encountered. Am I missing something here? The links seem to contradict each other.</p>

<p><strong>EDIT:</strong>
I just realized that <code>input.plot_movie</code> is of type unicode and decided to convert it to a string. I encounter the same error again.</p>
",5157633,2242,12-04-2018 22:30,13-04-2018 03:13,1,2242,28,3,19,100,"{'badge_counts': {'bronze': 28, 'silver': 19, 'gold': 3}, 'account_id': 6686642, 'is_employee': False, 'last_modified_date': 1698455100, 'last_access_date': 1707155975, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2242, 'creation_date': 1437919643, 'user_type': 'registered', 'user_id': 5157633, 'accept_rate': 100, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/5157633/thegreatcoder', 'profile_image': 'https://www.gravatar.com/avatar/e7fa17d60ad63c730f389196465cc478?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'thegreatcoder'}","I am currently trying to build a naive Bayes classifier as mentioned in this link. Referring to the line under the subheading, I had a similar line, in my code which should take an iterable as an input to the transform function. The is a record from a DataFrame and is of type and contains the following entries, out of which I send as the input to the transform function: However, I get the following error: How do I fix this error? I also referred to this answer where the person says that is an iterable because it was assigned a string. I also came across this link where a is encountered. Am I missing something here? The links seem to contradict each other. EDIT: I just realized that is of type unicode and decided to convert it to a string. I encounter the same error again.","X_new_tfidf = tfidf_transformer.transform(X_new_counts)
 Training the Classifier X_new_counts = count_vect.transform(input.plot_movie) input pd.Series input.plot_movie Iterable over raw text documents expected, string object received s TypeError: 'String' object is not iterable input.plot_movie",-9,16,1,4,
169,48211784,48249474,13182,"Best way to use python-dotenv with pytest, or best way to have a pytest test/dev-environment with seperate configs",4,<python><python-3.x><testing><environment-variables><pytest>,14,"<p>I want to use the python dotenv-lib at my python project. My dev-environment should use .env-file and the test-suite (pytest) should use .env.test automatically. </p>

<p>Until now I didn't find a satisfying solution.</p>

<p>I'm not very familiar with python. Maybe somebody can point me in the right direction.</p>

<p>Should I load the .env.test file in a pytest hook?</p>
",5662805,3639,11-01-2018 16:29,14-01-2018 12:15,3,3659,33,3,19,,"{'badge_counts': {'bronze': 33, 'silver': 19, 'gold': 3}, 'account_id': 7446951, 'is_employee': False, 'last_modified_date': 1699593300, 'last_access_date': 1711032810, 'reputation_change_year': 170, 'reputation_change_quarter': 170, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 3659, 'creation_date': 1449731647, 'user_type': 'registered', 'user_id': 5662805, 'website_url': '', 'link': 'https://stackoverflow.com/users/5662805/wuarmin', 'profile_image': 'https://i.stack.imgur.com/nBb38.png?s=256&g=1', 'display_name': 'wuarmin'}",I want to use the python dotenv-lib at my python project. My dev-environment should use .env-file and the test-suite (pytest) should use .env.test automatically. Until now I didn't find a satisfying solution. I'm not very familiar with python. Maybe somebody can point me in the right direction. Should I load the .env.test file in a pytest hook?,,0,7,0,0,
170,48767965,48768349,10228,DBSCAN with custom metric,2,<python><scikit-learn><cluster-analysis>,15,"<p>I have the following given:</p>

<ul>
<li><p>a dataset in the range of thousands</p></li>
<li><p>a way of computing the similarity, but the datapoints themselves I cannot plot them in euclidian space</p></li>
</ul>

<p>I know that DBSCAN should support custom distance metric but I dont know how to use it.</p>

<p>say I have a function </p>

<pre><code>def similarity(x,y):
    return  similarity ... 
</code></pre>

<p>and I have a list of data that can be passed pairwise into that function, how do I specify this when using the DBSCAN implementation of scikit-learn ?</p>

<p>Ideally what I want to do is to get a list of the clusters but I cant figure out how to get started in the first place. </p>

<p>There is a lot of terminology that still confuses me: </p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"" rel=""noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html</a></p>

<p>How do I pass a feature array and what is it ? How do I fit this implementation to my needs ? How will I be able to get my ""sublists"" from this algorithm ?</p>
",5669167,1206,13-02-2018 13:29,13-02-2018 13:48,0,1206,51,6,24,94,"{'badge_counts': {'bronze': 51, 'silver': 24, 'gold': 6}, 'account_id': 7455869, 'is_employee': False, 'last_modified_date': 1679238600, 'last_access_date': 1675426334, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1206, 'creation_date': 1449843582, 'user_type': 'registered', 'user_id': 5669167, 'accept_rate': 94, 'location': 'Germany', 'link': 'https://stackoverflow.com/users/5669167/zython', 'profile_image': 'https://i.stack.imgur.com/2cc0p.jpg?s=256&g=1', 'display_name': 'zython'}","I have the following given: a dataset in the range of thousands a way of computing the similarity, but the datapoints themselves I cannot plot them in euclidian space I know that DBSCAN should support custom distance metric but I dont know how to use it. say I have a function and I have a list of data that can be passed pairwise into that function, how do I specify this when using the DBSCAN implementation of scikit-learn ? Ideally what I want to do is to get a list of the clusters but I cant figure out how to get started in the first place. There is a lot of terminology that still confuses me: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html How do I pass a feature array and what is it ? How do I fit this implementation to my needs ? How will I be able to get my ""sublists"" from this algorithm ?","def similarity(x,y):
    return  similarity ... 
",1,24,0,1,
171,49130343,49131723,11273,Is there a way to get the error in fitting parameters from scipy.stats.norm.fit?,2,<python><statistics><curve-fitting><gaussian><data-fitting>,11,"<p>I have some data which I have fitted a normal distribution to using the scipy.stats.normal objects fit function like so: </p>

<pre><code>import numpy as np                                                                                                                                                                                                                       
import matplotlib.pyplot as plt                                                                                                                                                                                                          
from scipy.stats import norm                                                                                                                                                                                                             
import matplotlib.mlab as mlab                                                                                                                                                                                                           

x = np.random.normal(size=50000)                                                                                                                                                                                                         

fig, ax = plt.subplots()                                                                                                                                                                                                                 

nbins = 75                                                                                                                                                                                                                               
mu, sigma = norm.fit(x)                                                                                                                                                                                                                  
n, bins, patches = ax.hist(x,nbins,normed=1,facecolor = 'grey', alpha = 0.5, label='before');                                                                                                                                            
y0 = mlab.normpdf(bins, mu, sigma) # Line of best fit                                                                                                                                                                                    
ax.plot(bins,y0,'k--',linewidth = 2, label='fit before')                                                                                                                                                                                 
ax.set_title('$\mu$={}, $\sigma$={}'.format(mu, sigma))                                                                                                                                                                                  

plt.show()                                                                                                                                                                                                                               
</code></pre>

<p>I would now like to extract the uncertainty/error in the fitted mu and sigma values. How can I go about this? </p>
",5711995,1559,06-03-2018 12:01,06-03-2018 13:13,0,1559,44,4,21,94,"{'badge_counts': {'bronze': 44, 'silver': 21, 'gold': 4}, 'account_id': 7519608, 'is_employee': False, 'last_modified_date': 1704506700, 'last_access_date': 1710769040, 'reputation_change_year': 8, 'reputation_change_quarter': 8, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1559, 'creation_date': 1450889620, 'user_type': 'registered', 'user_id': 5711995, 'accept_rate': 94, 'location': 'Southampton', 'website_url': 'https://www.linkedin.com/in/ashleysetter/', 'link': 'https://stackoverflow.com/users/5711995/somerandomphysicist', 'profile_image': 'https://i.stack.imgur.com/yVQIi.png?s=256&g=1', 'display_name': 'SomeRandomPhysicist'}",I have some data which I have fitted a normal distribution to using the scipy.stats.normal objects fit function like so: I would now like to extract the uncertainty/error in the fitted mu and sigma values. How can I go about this?,"import numpy as np                                                                                                                                                                                                                       
import matplotlib.pyplot as plt                                                                                                                                                                                                          
from scipy.stats import norm                                                                                                                                                                                                             
import matplotlib.mlab as mlab                                                                                                                                                                                                           

x = np.random.normal(size=50000)                                                                                                                                                                                                         

fig, ax = plt.subplots()                                                                                                                                                                                                                 

nbins = 75                                                                                                                                                                                                                               
mu, sigma = norm.fit(x)                                                                                                                                                                                                                  
n, bins, patches = ax.hist(x,nbins,normed=1,facecolor = 'grey', alpha = 0.5, label='before');                                                                                                                                            
y0 = mlab.normpdf(bins, mu, sigma) # Line of best fit                                                                                                                                                                                    
ax.plot(bins,y0,'k--',linewidth = 2, label='fit before')                                                                                                                                                                                 
ax.set_title('$\mu$={}, $\sigma$={}'.format(mu, sigma))                                                                                                                                                                                  

plt.show()                                                                                                                                                                                                                               
",16,22,0,0,
172,49074021,49074286,80148,Repeat Rows in Data Frame n Times,1,<python><pandas>,68,"<p>consider a data frame defined like so:</p>
<pre><code>import pandas as pd
test = pd.DataFrame({
    'id' : ['a', 'b', 'c', 'd'],
    'times' : [2, 3, 1, 5]
})
</code></pre>
<p>Is it possible to create a new data frame from this in which each row is repeated <code>times</code> times, such that the result looks like this:</p>
<pre><code>&gt;&gt;&gt; result
   id  times
0   a      2
1   a      2
2   b      3
3   b      3
4   b      3
5   c      1
6   d      5
7   d      5
8   d      5
9   d      5
10  d      5
</code></pre>
",5724749,1186,02-03-2018 17:08,02-03-2018 17:25,0,1186,15,1,9,100,"{'badge_counts': {'bronze': 15, 'silver': 9, 'gold': 1}, 'account_id': 7539694, 'is_employee': False, 'last_modified_date': 1607614514, 'last_access_date': 1603741147, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1186, 'creation_date': 1451338536, 'user_type': 'registered', 'user_id': 5724749, 'accept_rate': 100, 'location': 'Philadelphia, PA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/5724749/93i7hdjb', 'profile_image': 'https://www.gravatar.com/avatar/8dfeb8b4fd3010043c424f2f54925c60?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': '93i7hdjb'}","consider a data frame defined like so: Is it possible to create a new data frame from this in which each row is repeated times, such that the result looks like this:","import pandas as pd
test = pd.DataFrame({
    'id' : ['a', 'b', 'c', 'd'],
    'times' : [2, 3, 1, 5]
})
 times &gt;&gt;&gt; result
   id  times
0   a      2
1   a      2
2   b      3
3   b      3
4   b      3
5   c      1
6   d      5
7   d      5
8   d      5
9   d      5
10  d      5
",15,22,0,0,
173,48814211,48814225,12988,Python3 - How does one define an abstract subclass from an existing abstract class?,2,<python><python-3.x><oop><inheritance><abstract-class>,27,"<p>I initially defined the following abstract class:</p>

<pre><code>from abc import ABC, abstractmethod    
class Primitive(ABC):
</code></pre>

<p>Now I want to create another abstract class that inherits from Primitive:</p>

<pre><code>class InstrumentName(Primitive)
</code></pre>

<p>I need this class to be abstract since I ultimately want to create the following two concrete classes:</p>

<pre><code>class CurrencyInstrumentName(InstrumentName)
class MetalInstrumentName(InstrumentName)
</code></pre>

<p>I have read the documentation and searched SO, but they mostly pertain to sublcassing concrete classes from abstract classes, or discussing how Python handles abstraction</p>
",5163606,4014,15-02-2018 18:42,15-02-2018 18:43,0,4034,14,2,14,,"{'badge_counts': {'bronze': 14, 'silver': 14, 'gold': 2}, 'account_id': 6695484, 'is_employee': False, 'last_modified_date': 1645957800, 'last_access_date': 1685932230, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4034, 'creation_date': 1438066612, 'user_type': 'registered', 'user_id': 5163606, 'location': 'Singapore', 'website_url': '', 'link': 'https://stackoverflow.com/users/5163606/abhay-nainan', 'profile_image': 'https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=256', 'display_name': 'Abhay Nainan'}","I initially defined the following abstract class: Now I want to create another abstract class that inherits from Primitive: I need this class to be abstract since I ultimately want to create the following two concrete classes: I have read the documentation and searched SO, but they mostly pertain to sublcassing concrete classes from abstract classes, or discussing how Python handles abstraction","from abc import ABC, abstractmethod    
class Primitive(ABC):
 class InstrumentName(Primitive)
 class CurrencyInstrumentName(InstrumentName)
class MetalInstrumentName(InstrumentName)
",2,18,0,0,
174,49898742,49898817,21668,Pandas reading csv files with partial wildcard,6,<python><pandas>,13,"<p>I'm trying to write a script that imports a file, then does something with the file and outputs the result into another file.</p>

<p><code>df = pd.read_csv('somefile2018.csv')</code></p>

<p>The above code works perfectly fine. However, I'd like to avoid hardcoding the file name in the code.</p>

<p>The script will be run in a folder (directory) that contains the <code>script.py</code> and several csv files.</p>

<p>I've tried the following:</p>

<p><code>somefile_path = glob.glob('somefile*.csv')</code></p>

<p><code>df = pd.read_csv(somefile_path)</code></p>

<p>But I get the following error:</p>

<p><code>ValueError: Invalid file path or buffer object type: &lt;class 'list'&gt;</code></p>
",5236124,1371,18-04-2018 11:36,18-04-2018 11:40,0,1371,33,7,21,91,"{'badge_counts': {'bronze': 33, 'silver': 21, 'gold': 7}, 'account_id': 6803578, 'is_employee': False, 'last_modified_date': 1652469900, 'last_access_date': 1692085756, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1371, 'creation_date': 1439829628, 'user_type': 'registered', 'user_id': 5236124, 'accept_rate': 91, 'link': 'https://stackoverflow.com/users/5236124/kvothe', 'profile_image': 'https://www.gravatar.com/avatar/95b4fb7a5b98355f4866cbc4334b2071?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Kvothe'}","I'm trying to write a script that imports a file, then does something with the file and outputs the result into another file. The above code works perfectly fine. However, I'd like to avoid hardcoding the file name in the code. The script will be run in a folder (directory) that contains the and several csv files. I've tried the following: But I get the following error:",df = pd.read_csv('somefile2018.csv') script.py somefile_path = glob.glob('somefile*.csv') df = pd.read_csv(somefile_path) ValueError: Invalid file path or buffer object type: &lt;class 'list'&gt;,-5,17,0,0,
175,49534901,49535091,14739,Is there a way to use `json.dump` with `gzip`?,2,<python><json><python-3.x><gzip>,27,"<p><a href=""https://stackoverflow.com/questions/39450065/python-3-read-write-compressed-json-objects-from-to-gzip-file"">Here</a> is a great answer about how to use <code>json.dumps</code> to write to a gzip file. What I would like to do is to use the <code>dump</code> method instead to serialize the json directly into a <code>GzipFile</code> object.</p>

<p>Example code:</p>

<pre><code>import gzip, json

data = # a dictionary of data here
with gzip.open(write_file, 'w') as zipfile:
   json.dump(data, zipfile)
</code></pre>

<p>The error raised is </p>

<pre><code>TypeError: memoryview: a bytes-like objet is required, not 'str'
</code></pre>

<p>I believe this is caused because the gzip write() method wants a bytes object passed to it. Per the <a href=""https://docs.python.org/3.5/library/json.html#basic-usage"" rel=""noreferrer"">documentation</a>, </p>

<blockquote>
  <p>The json module always produces str objects, not bytes objects.
  Therefore, fp.write() must support str input.</p>
</blockquote>

<p>Is there a way to wrap the <code>json</code> string output as bytes so that <code>GzipFile</code>'s <code>write()</code> will handle it? Or is the only way to do this to use <code>json.dumps</code> and <code>encode()</code> the resulting string into a bytes object, as in the other linked answer?</p>
",5061485,2373,28-03-2018 12:43,28-03-2018 12:52,0,2383,40,3,27,59,"{'badge_counts': {'bronze': 40, 'silver': 27, 'gold': 3}, 'account_id': 6545286, 'is_employee': False, 'last_modified_date': 1652436300, 'last_access_date': 1705581674, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2383, 'creation_date': 1435584787, 'user_type': 'registered', 'user_id': 5061485, 'accept_rate': 59, 'location': 'Minneapolis, MN, USA', 'link': 'https://stackoverflow.com/users/5061485/kingledion', 'profile_image': 'https://i.stack.imgur.com/2Ncmd.jpg?s=256&g=1', 'display_name': 'kingledion'}","Here is a great answer about how to use to write to a gzip file. What I would like to do is to use the method instead to serialize the json directly into a object. Example code: The error raised is I believe this is caused because the gzip write() method wants a bytes object passed to it. Per the documentation, The json module always produces str objects, not bytes objects. Therefore, fp.write() must support str input. Is there a way to wrap the string output as bytes so that 's will handle it? Or is the only way to do this to use and the resulting string into a bytes object, as in the other linked answer?","json.dumps dump GzipFile import gzip, json

data = # a dictionary of data here
with gzip.open(write_file, 'w') as zipfile:
   json.dump(data, zipfile)
 TypeError: memoryview: a bytes-like objet is required, not 'str'
 json GzipFile write() json.dumps encode()",-4,24,0,2,
176,49772151,54672690,115109,Download a folder from S3 using Boto3,11,<python><amazon-web-services><amazon-s3><download><boto3>,63,"<p>Using <code>Boto3 Python SDK</code>, I was able to download files using the method <code>bucket.download_file()</code></p>

<p>Is there a way to download an entire folder?</p>
",5063171,1641,11-04-2018 10:02,13-02-2019 14:34,308,1641,26,2,19,,"{'badge_counts': {'bronze': 26, 'silver': 19, 'gold': 2}, 'account_id': 6547737, 'is_employee': False, 'last_modified_date': 1666091400, 'last_access_date': 1711017069, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1641, 'creation_date': 1435616078, 'user_type': 'registered', 'user_id': 5063171, 'website_url': '', 'link': 'https://stackoverflow.com/users/5063171/el-fadel-anas', 'profile_image': 'https://www.gravatar.com/avatar/a3d5a8a33799f966d1caa1ee03b92eca?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'El Fadel Anas'}","Using , I was able to download files using the method Is there a way to download an entire folder?",Boto3 Python SDK bucket.download_file(),-2,3,0,0,
177,49118277,49118318,55767,What is the best way to Install Conda on MacOS (Apple/Mac)?,5,<python><macos><installation><conda>,38,"<p>What is the recommended approach for installing Anaconda on Mac?</p>

<p>I tried with <code>brew cask install anaconda</code> <br>which after a while returns <code>anaconda was successfully installed!</code>.</p>

<p>After that - trying <code>conda</code> command returns <code>command not found: conda</code>.</p>

<p>Is there any post step installation that needs to be done?<br>
And what is recommended way to install Conda on MacOS?</p>
",5285068,12393,05-03-2018 19:44,05-03-2018 19:47,0,12463,193,34,113,35,"{'badge_counts': {'bronze': 193, 'silver': 113, 'gold': 34}, 'account_id': 6876934, 'is_employee': False, 'last_modified_date': 1661877300, 'last_access_date': 1650067826, 'reputation_change_year': 250, 'reputation_change_quarter': 250, 'reputation_change_month': 80, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 12463, 'creation_date': 1441031394, 'user_type': 'registered', 'user_id': 5285068, 'accept_rate': 35, 'link': 'https://stackoverflow.com/users/5285068/joe', 'profile_image': 'https://www.gravatar.com/avatar/5310de3ec35b4809b588e119098964d9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Joe'}",What is the recommended approach for installing Anaconda on Mac? I tried with which after a while returns . After that - trying command returns . Is there any post step installation that needs to be done? And what is recommended way to install Conda on MacOS?,brew cask install anaconda anaconda was successfully installed! conda command not found: conda,-4,8,0,0,
178,50127959,50163150,39992,win32.Dispatch vs win32.gencache in Python. What are the pros and cons?,2,<python><win32com>,26,"<p>I have been recently using win32com.client from python as an API for windows applications but am struggling to understand some basic things.</p>

<p>I had been using it with a program called WEAP, in the following way</p>

<pre><code>import win32com.client
win32com.client.Dispatch(""WEAP.WEAPApplication"")
</code></pre>

<p>Now, I want to use it with Excel and have found alternatives to the previous lines, one of them as follows (taken from <a href=""https://stackoverflow.com/questions/39877278/python-open-excel-workbook-using-win32-com-api"">Python: Open Excel Workbook using Win32 COM Api</a>)</p>

<pre><code>import win32com.client as win32
excel = win32.gencache.EnsureDispatch('Excel.Application')
</code></pre>

<p>Does anyone know the difference between using </p>

<pre><code>win32.Dispatch 
</code></pre>

<p>and </p>

<pre><code>win32.gencache.EnsureDispatch
</code></pre>

<p>and other alternatives? Does anyone know pros and cons of each one? or some advice regarding when one or another should be used?</p>

<p>I have looked for advice and i have found some useful answers, eg:</p>

<p><a href=""https://stackoverflow.com/questions/39877278/python-open-excel-workbook-using-win32-com-api"">Python: Open Excel Workbook using Win32 COM Api</a></p>

<p><a href=""https://stackoverflow.com/questions/11623461/win32com-client-dispatch-works-but-not-win32com-client-gencache-ensuredispatch"">win32com.client.Dispatch works but not win32com.client.gencache.EnsureDispatch</a></p>

<p><a href=""http://pythonexcels.com/python-excel-mini-cookbook/"" rel=""noreferrer"">http://pythonexcels.com/python-excel-mini-cookbook/</a></p>

<p><a href=""https://mail.python.org/pipermail/python-win32/2011-August/011738.html"" rel=""noreferrer"">https://mail.python.org/pipermail/python-win32/2011-August/011738.html</a></p>

<p>However, they are usually focused on answering specific issues, and not describing the bigger picture of the differences between Dispatch, gencache.EnsureDispatch, and perhaps further alternatives, which is what i want.</p>

<p>Any advice would be greatly appreciated.</p>
",5100990,1173,02-05-2018 05:55,03-05-2018 20:11,1,1173,14,1,10,,"{'badge_counts': {'bronze': 14, 'silver': 10, 'gold': 1}, 'account_id': 6603502, 'is_employee': False, 'last_modified_date': 1665797700, 'last_access_date': 1691202243, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1173, 'creation_date': 1436493258, 'user_type': 'registered', 'user_id': 5100990, 'location': 'Brisbane QLD, Australia', 'website_url': '', 'link': 'https://stackoverflow.com/users/5100990/juan-ossa', 'profile_image': 'https://www.gravatar.com/avatar/497379c2d153b31bf3aef01c22c40668?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Juan Ossa'}","I have been recently using win32com.client from python as an API for windows applications but am struggling to understand some basic things. I had been using it with a program called WEAP, in the following way Now, I want to use it with Excel and have found alternatives to the previous lines, one of them as follows (taken from Python: Open Excel Workbook using Win32 COM Api) Does anyone know the difference between using and and other alternatives? Does anyone know pros and cons of each one? or some advice regarding when one or another should be used? I have looked for advice and i have found some useful answers, eg: Python: Open Excel Workbook using Win32 COM Api win32com.client.Dispatch works but not win32com.client.gencache.EnsureDispatch http://pythonexcels.com/python-excel-mini-cookbook/ https://mail.python.org/pipermail/python-win32/2011-August/011738.html However, they are usually focused on answering specific issues, and not describing the bigger picture of the differences between Dispatch, gencache.EnsureDispatch, and perhaps further alternatives, which is what i want. Any advice would be greatly appreciated.","import win32com.client
win32com.client.Dispatch(""WEAP.WEAPApplication"")
 import win32com.client as win32
excel = win32.gencache.EnsureDispatch('Excel.Application')
 win32.Dispatch 
 win32.gencache.EnsureDispatch
",2,39,0,5,
179,50074690,50075525,39414,"ImproperlyConfigured: Requested setting INSTALLED_APPS, but settings are not configured",7,<python><django><django-settings><restframeworkmongoengine>,29,"<p>I'm trying to make MongoDB and Django get on with each other the way I want them to.
That's the error I'm getting when trying to import viewsets from rest_framework_mongoengine.</p>

<p>The whole error looks like this:</p>

<p><code>
ImproperlyConfigured: Requested setting INSTALLED_APPS, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.
</code></p>

<p>It doesn't find <code>settings.py</code> or what? If so I can't figure out why! Namely, say, why did this problem not appear with other modules then?</p>

<p>Here are my <code>INSTALLED APPS</code></p>

<pre><code>INSTALLED_APPS = [
    'rest_framework',
    'rest_framework_mongoengine',
    'mongoengine.django.mongo_auth',
    'rest_framework.authtoken',
    'corsheaders',
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'django_extensions',
    'core',
    'core.essences.user',
    'core.essences.user.authentication',
    'core.essences.test_result',
]
</code></pre>
",5862455,2316,28-04-2018 08:53,28-04-2018 10:31,0,2356,57,10,36,67,"{'badge_counts': {'bronze': 57, 'silver': 36, 'gold': 10}, 'account_id': 7743821, 'is_employee': False, 'last_modified_date': 1676319600, 'last_access_date': 1644104695, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 50, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 2356, 'creation_date': 1454194127, 'user_type': 'registered', 'user_id': 5862455, 'accept_rate': 67, 'link': 'https://stackoverflow.com/users/5862455/albert', 'profile_image': 'https://www.gravatar.com/avatar/59ce546da6ff494a36da83a61fb27530?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Albert'}","I'm trying to make MongoDB and Django get on with each other the way I want them to. That's the error I'm getting when trying to import viewsets from rest_framework_mongoengine. The whole error looks like this: It doesn't find or what? If so I can't figure out why! Namely, say, why did this problem not appear with other modules then? Here are my","
ImproperlyConfigured: Requested setting INSTALLED_APPS, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.
 settings.py INSTALLED APPS INSTALLED_APPS = [
    'rest_framework',
    'rest_framework_mongoengine',
    'mongoengine.django.mongo_auth',
    'rest_framework.authtoken',
    'corsheaders',
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'django_extensions',
    'core',
    'core.essences.user',
    'core.essences.user.authentication',
    'core.essences.test_result',
]
",16,32,0,0,
180,48628417,48628442,36693,How to select rows in Pandas dataframe where value appears more than once,5,<python><pandas>,31,"<p>Let's say I have the Pandas dataframe with columns of different measurement attributes and corresponding measurement values.</p>

<pre><code>ID     Parameter     Value
0      'A'           4.3
1      'B'           3.1
2      'C'           8.9
3      'A'           2.1
4      'A'           3.9
.      .             .
.      .             .
.      .             .
100    'B'           3.8
</code></pre>

<p>How can I filter this dataframe to only have measurements that appear more than X number of times? For example, for this dataframe I want to get all rows with more than 5 measurements (lets say only parameters 'A' and 'B' appear more than 5 times) to get a dataframe like below. </p>

<pre><code>ID     Parameter     Value
0      'A'           4.3
1      'B'           3.1
3      'A'           2.1
.      .             .
.      .             .
.      .             .
100    'B'           3.8
</code></pre>
",5431568,1695,05-02-2018 17:43,05-02-2018 17:45,0,1695,40,7,29,57,"{'badge_counts': {'bronze': 40, 'silver': 29, 'gold': 7}, 'account_id': 7099288, 'is_employee': False, 'last_modified_date': 1573679392, 'last_access_date': 1688876012, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1695, 'creation_date': 1444498273, 'user_type': 'registered', 'user_id': 5431568, 'accept_rate': 57, 'website_url': '', 'link': 'https://stackoverflow.com/users/5431568/char', 'profile_image': 'https://www.gravatar.com/avatar/810a79acb2632eb475818a0db5d04ec6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Char'}","Let's say I have the Pandas dataframe with columns of different measurement attributes and corresponding measurement values. How can I filter this dataframe to only have measurements that appear more than X number of times? For example, for this dataframe I want to get all rows with more than 5 measurements (lets say only parameters 'A' and 'B' appear more than 5 times) to get a dataframe like below.","ID     Parameter     Value
0      'A'           4.3
1      'B'           3.1
2      'C'           8.9
3      'A'           2.1
4      'A'           3.9
.      .             .
.      .             .
.      .             .
100    'B'           3.8
 ID     Parameter     Value
0      'A'           4.3
1      'B'           3.1
3      'A'           2.1
.      .             .
.      .             .
.      .             .
100    'B'           3.8
",16,25,0,0,
181,49111859,49111901,20193,how to merge two dataframes and sum the values of columns,2,<python><pandas><dataframe><data-analysis>,20,"<p>I have two dataframes</p>

<pre><code>df1
Name class value
Sri   1     5
Ram   2     8
viv   3     4

df2
Name class value
Sri   1     5
viv   4     4
</code></pre>

<p>My desired output is,</p>

<pre><code>df,

Name class value
Sri   2     10
Ram   2     8
viv   7     8
</code></pre>

<p>Please help, thanks in advance!</p>
",5439546,6107,05-03-2018 13:44,05-03-2018 13:46,0,6107,112,19,54,,"{'badge_counts': {'bronze': 112, 'silver': 54, 'gold': 19}, 'collectives': [{'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 7112037, 'is_employee': False, 'last_modified_date': 1711170600, 'last_access_date': 1711106868, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6107, 'creation_date': 1444718913, 'user_type': 'registered', 'user_id': 5439546, 'location': 'india', 'website_url': '', 'link': 'https://stackoverflow.com/users/5439546/pyd', 'profile_image': 'https://www.gravatar.com/avatar/251bf263ab842ddf9aa397ac7c5c2291?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Pyd'}","I have two dataframes My desired output is, Please help, thanks in advance!","df1
Name class value
Sri   1     5
Ram   2     8
viv   3     4

df2
Name class value
Sri   1     5
viv   4     4
 df,

Name class value
Sri   2     10
Ram   2     8
viv   7     8
",14,25,0,0,
182,48152674,48152675,1133450,How do I check if PyTorch is using the GPU?,20,<python><memory-management><gpu><nvidia><pytorch>,564,"<p>How do I check if PyTorch is using the GPU? The <code>nvidia-smi</code> command can detect GPU activity, but I want to check it directly from inside a Python script.</p>
",5446749,27883,08-01-2018 14:50,08-01-2018 14:50,0,27981,91,19,54,80,"{'badge_counts': {'bronze': 91, 'silver': 54, 'gold': 19}, 'account_id': 7122736, 'is_employee': False, 'last_modified_date': 1710101100, 'last_access_date': 1711115992, 'reputation_change_year': 911, 'reputation_change_quarter': 911, 'reputation_change_month': 203, 'reputation_change_week': 70, 'reputation_change_day': 10, 'reputation': 27981, 'creation_date': 1444852672, 'user_type': 'registered', 'user_id': 5446749, 'accept_rate': 80, 'location': 'Paris, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/5446749/vvvvv', 'profile_image': 'https://i.stack.imgur.com/fzsuf.jpg?s=256&g=1', 'display_name': 'vvvvv'}","How do I check if PyTorch is using the GPU? The command can detect GPU activity, but I want to check it directly from inside a Python script.",nvidia-smi,-1,1,0,0,
183,48123074,48242260,3335,Fatal error in extension: PyThreadState_Get: no current thread,3,<python><c++><python-3.x><macos><anaconda>,17,"<p>I've seen several posts that have stated the same error, but looking and trying out the answers in those posts have not helped. I was wondering if someone could look at this and see if something pops out?</p>

<p>I'm building a Python extension for a CPP application, and there are no errors during the compilation and build step. However, when I import the module I get the error mentioned in the title. Other stackoverflow answers have claimed that this is because of being linked with one library while compilation and using a different interpreter. As far as I can tell, I'm using the same Python interpreter. I'm going to describe now why I think I'm using the same Python in the linking process and for the interpreter.</p>

<p>This is the comand I'm using to build the Python extension</p>

<pre><code>$ gcc -shared helicsPYTHON_wrap.c $(python-config3 --includes) -I/path/to/helics-0.9/includes -L/path/to/helics-0.9/lib -lhelicsSharedLib -L$(python3-config --prefix)/lib -lpython3.6m -o _helics.so

$ which python3-config
/Users/$USER/miniconda3/bin/python3-config

$ python3-config --prefix
/Users/$USER/miniconda3
</code></pre>

<p>If I try to import the python file that imports the shared library, it throws the fatal error. If I use <code>otool -L</code> on the shared library, I get the following. This is what I expect to get.</p>

<pre><code>$ otool -L _helics.so
_helics.so:
        @rpath/libhelicsSharedLib.dylib (compatibility version 0.0.0, current version 0.0.0)
        @rpath/libpython3.6m.dylib (compatibility version 3.6.0, current version 3.6.0)
        /usr/local/opt/zeromq/lib/libzmq.5.dylib (compatibility version 7.0.0, current version 7.3.0)
        libboost_program_options.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_date_time.dylib (compatibility version 0.0.0, current version 0.0.0)
        /usr/local/opt/gcc/lib/gcc/7/libstdc++.6.dylib (compatibility version 7.0.0, current version 7.24.0)
        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.60.2)
        /usr/local/lib/gcc/7/libgcc_s.1.dylib (compatibility version 1.0.0, current version 1.0.0)
</code></pre>

<p>I also tried <code>install_name_tool</code> to add the full path of the libpython3.6m.dylib.</p>

<pre><code>$ install_name_tool -change @rpath/libpython3.6m.dylib /Users/$USER/miniconda3/envs/py3/lib/libpython3.6m.dylib _helics.so
</code></pre>

<p>I still get the same fatal error. My hypothesis is that my Mac System Python 2.7 installation is having an effect on this process at some stage. I'm unable to identify where though.</p>

<p>Is there a way to add more debug statements to find out why there is a Fatal Python error. Currently, the error message is very short.</p>

<pre><code>$ python helics.py
Fatal Python error: PyThreadState_Get: no current thread

[1]    64481 abort      python helics.py
</code></pre>

<p>Curiously, if I use a conda environment and use Python 2.7, I'm able to load the extension fine! This is why I think that when I'm using Python 3.6, it is somehow picking up something from the default mac system python 2.7 installation and working fine. It is picking the same thing up when I use the conda 2.7 python environment, but because they are both Python 2.7 (though conda is 2.7.14 and system python is 2.7.10) it seems to work. This is the <code>otool -L</code> output when I use a conda environment. </p>

<pre><code>$ otool -L _helics.so
_helics.so:
        @rpath/libhelicsSharedLib.dylib (compatibility version 0.0.0, current version 0.0.0)
        @rpath/libpython2.7.dylib (compatibility version 2.7.0, current version 2.7.0)
        /usr/local/opt/zeromq/lib/libzmq.5.dylib (compatibility version 7.0.0, current version 7.3.0)
        libboost_program_options.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_date_time.dylib (compatibility version 0.0.0, current version 0.0.0)
        /usr/local/opt/gcc/lib/gcc/7/libstdc++.6.dylib (compatibility version 7.0.0, current version 7.24.0)
        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.60.2)
        /usr/local/lib/gcc/7/libgcc_s.1.dylib (compatibility version 1.0.0, current version 1.0.0)
</code></pre>

<p>The questions I have are 1) how do I get more debug information out of the error from Python. I have tried <code>python -vvv</code> and that does not give me enough information. I tried using gdb but that also did not give me any information. I believe it requires recompiling Python itself using debug symbols. 2) Do you have any recommendations on how to solve this problem or debug further? </p>

<p>Also, I'm not sure if this is useful information, but I am able to use ctypes and load the shared library after I create it. I'm just unable to import it as a python module.</p>

<p>This is the original issue if one is interested - <a href=""https://github.com/GMLC-TDC/HELICS-src/issues/59"" rel=""nofollow noreferrer"">https://github.com/GMLC-TDC/HELICS-src/issues/59</a></p>

<p>Edit: I tried this using zsh and bash, and still got the same error. I also tried setting the following <code>export PATH=""/Users/$USER/miniconda3/bin:/Users/$USER/miniconda3/lib""</code> temporarily in the shell and running and I STILL get the same error. This should have excluded my Mac System Python 2.7.10, so I'm really not sure what is going on.</p>

<p>Edit again: I've also tried reinstalling miniconda with Python2. And if I use Python2 everything works fine. I'm just unable to use Python3 using miniconda. Oddly enough, if I use homebrew and install Python3 that seems to work fine.</p>

<p>Edit again: <strike>This is possibly an issue with High Sierra. I currently don't have access to another mac, but I'm on the latest operating system which has SIP. I'm not sure if this is causing this issue.</strike> Additionally, I've tried using Anaconda3 and had no luck.</p>

<p>Edit again: This does not seem to be related to the operating system. I'm able to run this successfully on another computer with High Sierra. </p>

<p>Edit again: I tested this on other fresh OS installs, and they don't work. But they do work on two of my machines. Are there other tools that tell you what dependency a library requires or where Python throws a fatal error? My best guess at the moment is that I've installed something on my other machines in the past that allows this to work. I need to identify what that was and make sure I can document it.</p>

<p>Edit again: I've added a <a href=""https://gist.github.com/kdheepak/9baf2e7641a3b2e1493fd8927afecaa2"" rel=""nofollow noreferrer"">gist</a> of the output of the version of Python that I'm using. </p>

<p>Edit again: I've added the tags for miniconda and anaconda since I don't experience this issue when using homebrew python3, but only just when I'm using miniconda3 or anaconda2 with a python3 environment. This always appears to work with Python2, regardless of whether it is homebrew, anaconda or miniconda.</p>

<p>Edit again:</p>

<p>These are the steps if someone else wants to replicate on their machine.</p>

<pre><code>git clone https://github.com/GMLC-TDC/HELICS-src
mkdir build-osx
brew install boost
brew install cmake
brew install swig
cmake -DBUILD_PYTHON=ON -DPYTHON_LIBRARY=$(python3-config --prefix)/lib/libpython3.6m.dylib -DPYTHON_INCLUDE_DIR=$(python3-config --prefix)/include/python3.6m/ ..
make
cd ./swig/python/
python helics.py # Error
</code></pre>
",5451769,1304,06-01-2018 00:22,13-01-2018 17:10,7,1314,23,0,11,72,"{'badge_counts': {'bronze': 23, 'silver': 11, 'gold': 0}, 'account_id': 7130245, 'is_employee': False, 'last_modified_date': 1588176300, 'last_access_date': 1710960421, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 10, 'reputation': 1314, 'creation_date': 1444950461, 'user_type': 'registered', 'user_id': 5451769, 'accept_rate': 72, 'website_url': '', 'link': 'https://stackoverflow.com/users/5451769/kdheepak', 'profile_image': 'https://www.gravatar.com/avatar/942b954a769ceeb649ae8e61d1d10f74?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'kdheepak'}","I've seen several posts that have stated the same error, but looking and trying out the answers in those posts have not helped. I was wondering if someone could look at this and see if something pops out? I'm building a Python extension for a CPP application, and there are no errors during the compilation and build step. However, when I import the module I get the error mentioned in the title. Other stackoverflow answers have claimed that this is because of being linked with one library while compilation and using a different interpreter. As far as I can tell, I'm using the same Python interpreter. I'm going to describe now why I think I'm using the same Python in the linking process and for the interpreter. This is the comand I'm using to build the Python extension If I try to import the python file that imports the shared library, it throws the fatal error. If I use on the shared library, I get the following. This is what I expect to get. I also tried to add the full path of the libpython3.6m.dylib. I still get the same fatal error. My hypothesis is that my Mac System Python 2.7 installation is having an effect on this process at some stage. I'm unable to identify where though. Is there a way to add more debug statements to find out why there is a Fatal Python error. Currently, the error message is very short. Curiously, if I use a conda environment and use Python 2.7, I'm able to load the extension fine! This is why I think that when I'm using Python 3.6, it is somehow picking up something from the default mac system python 2.7 installation and working fine. It is picking the same thing up when I use the conda 2.7 python environment, but because they are both Python 2.7 (though conda is 2.7.14 and system python is 2.7.10) it seems to work. This is the output when I use a conda environment. The questions I have are 1) how do I get more debug information out of the error from Python. I have tried and that does not give me enough information. I tried using gdb but that also did not give me any information. I believe it requires recompiling Python itself using debug symbols. 2) Do you have any recommendations on how to solve this problem or debug further? Also, I'm not sure if this is useful information, but I am able to use ctypes and load the shared library after I create it. I'm just unable to import it as a python module. This is the original issue if one is interested - https://github.com/GMLC-TDC/HELICS-src/issues/59 Edit: I tried this using zsh and bash, and still got the same error. I also tried setting the following temporarily in the shell and running and I STILL get the same error. This should have excluded my Mac System Python 2.7.10, so I'm really not sure what is going on. Edit again: I've also tried reinstalling miniconda with Python2. And if I use Python2 everything works fine. I'm just unable to use Python3 using miniconda. Oddly enough, if I use homebrew and install Python3 that seems to work fine. Edit again: This is possibly an issue with High Sierra. I currently don't have access to another mac, but I'm on the latest operating system which has SIP. I'm not sure if this is causing this issue. Additionally, I've tried using Anaconda3 and had no luck. Edit again: This does not seem to be related to the operating system. I'm able to run this successfully on another computer with High Sierra. Edit again: I tested this on other fresh OS installs, and they don't work. But they do work on two of my machines. Are there other tools that tell you what dependency a library requires or where Python throws a fatal error? My best guess at the moment is that I've installed something on my other machines in the past that allows this to work. I need to identify what that was and make sure I can document it. Edit again: I've added a gist of the output of the version of Python that I'm using. Edit again: I've added the tags for miniconda and anaconda since I don't experience this issue when using homebrew python3, but only just when I'm using miniconda3 or anaconda2 with a python3 environment. This always appears to work with Python2, regardless of whether it is homebrew, anaconda or miniconda. Edit again: These are the steps if someone else wants to replicate on their machine.","$ gcc -shared helicsPYTHON_wrap.c $(python-config3 --includes) -I/path/to/helics-0.9/includes -L/path/to/helics-0.9/lib -lhelicsSharedLib -L$(python3-config --prefix)/lib -lpython3.6m -o _helics.so

$ which python3-config
/Users/$USER/miniconda3/bin/python3-config

$ python3-config --prefix
/Users/$USER/miniconda3
 otool -L $ otool -L _helics.so
_helics.so:
        @rpath/libhelicsSharedLib.dylib (compatibility version 0.0.0, current version 0.0.0)
        @rpath/libpython3.6m.dylib (compatibility version 3.6.0, current version 3.6.0)
        /usr/local/opt/zeromq/lib/libzmq.5.dylib (compatibility version 7.0.0, current version 7.3.0)
        libboost_program_options.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_date_time.dylib (compatibility version 0.0.0, current version 0.0.0)
        /usr/local/opt/gcc/lib/gcc/7/libstdc++.6.dylib (compatibility version 7.0.0, current version 7.24.0)
        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.60.2)
        /usr/local/lib/gcc/7/libgcc_s.1.dylib (compatibility version 1.0.0, current version 1.0.0)
 install_name_tool $ install_name_tool -change @rpath/libpython3.6m.dylib /Users/$USER/miniconda3/envs/py3/lib/libpython3.6m.dylib _helics.so
 $ python helics.py
Fatal Python error: PyThreadState_Get: no current thread

[1]    64481 abort      python helics.py
 otool -L $ otool -L _helics.so
_helics.so:
        @rpath/libhelicsSharedLib.dylib (compatibility version 0.0.0, current version 0.0.0)
        @rpath/libpython2.7.dylib (compatibility version 2.7.0, current version 2.7.0)
        /usr/local/opt/zeromq/lib/libzmq.5.dylib (compatibility version 7.0.0, current version 7.3.0)
        libboost_program_options.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0)
        libboost_date_time.dylib (compatibility version 0.0.0, current version 0.0.0)
        /usr/local/opt/gcc/lib/gcc/7/libstdc++.6.dylib (compatibility version 7.0.0, current version 7.24.0)
        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.60.2)
        /usr/local/lib/gcc/7/libgcc_s.1.dylib (compatibility version 1.0.0, current version 1.0.0)
 python -vvv export PATH=""/Users/$USER/miniconda3/bin:/Users/$USER/miniconda3/lib"" git clone https://github.com/GMLC-TDC/HELICS-src
mkdir build-osx
brew install boost
brew install cmake
brew install swig
cmake -DBUILD_PYTHON=ON -DPYTHON_LIBRARY=$(python3-config --prefix)/lib/libpython3.6m.dylib -DPYTHON_INCLUDE_DIR=$(python3-config --prefix)/include/python3.6m/ ..
make
cd ./swig/python/
python helics.py # Error
",34,96,0,2,
184,48228060,48228107,10680,Filter pandas dataframe from tuples,1,<python><pandas>,16,"<pre><code>AB_col = [(0,230), (10,215), (15, 200), (20, 185), (40, 177), 
                (0,237), (10,222), (15, 207), (20, 192), (40, 184)]

sales = [{'account': 'Jones LLC', 'A': 0, 'B': 230, 'C': 140},
         {'account': 'Alpha Co',  'A': 20, 'B': 192, 'C': 215},
         {'account': 'Blue Inc',  'A': 50,  'B': 90,  'C': 95 }]
df = pd.DataFrame(sales)
print df
</code></pre>

<blockquote>
  <blockquote>
    <p><strong>result</strong><br>
    <a href=""https://i.stack.imgur.com/5tFuv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5tFuv.png"" alt=""df""></a></p>
  </blockquote>
</blockquote>

<p>Now the above dataframe has to be filtered by the AB_col list of tuples. I tried something like </p>

<pre><code>df[df[""A"",""B""].zip.isin(AB_col)]
</code></pre>

<p>But it did not work, How to filter the above dataframe to the one like below</p>

<p><a href=""https://i.stack.imgur.com/jXa7a.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jXa7a.png"" alt=""dfexpected""></a></p>
",5465168,1571,12-01-2018 14:15,12-01-2018 14:18,0,1571,47,6,27,97,"{'badge_counts': {'bronze': 47, 'silver': 27, 'gold': 6}, 'account_id': 7151181, 'is_employee': False, 'last_modified_date': 1697851200, 'last_access_date': 1711118707, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1571, 'creation_date': 1445306348, 'user_type': 'registered', 'user_id': 5465168, 'accept_rate': 97, 'location': 'Sindelfingen, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/5465168/yogi', 'profile_image': 'https://i.stack.imgur.com/cQ5U7.jpg?s=256&g=1', 'display_name': 'Yogi'}","result Now the above dataframe has to be filtered by the AB_col list of tuples. I tried something like But it did not work, How to filter the above dataframe to the one like below","AB_col = [(0,230), (10,215), (15, 200), (20, 185), (40, 177), 
                (0,237), (10,222), (15, 207), (20, 192), (40, 184)]

sales = [{'account': 'Jones LLC', 'A': 0, 'B': 230, 'C': 140},
         {'account': 'Alpha Co',  'A': 20, 'B': 192, 'C': 215},
         {'account': 'Blue Inc',  'A': 50,  'B': 90,  'C': 95 }]
df = pd.DataFrame(sales)
print df
 df[df[""A"",""B""].zip.isin(AB_col)]
",7,25,2,2,
185,50185926,50186267,189282,"ValueError: Shape of passed values is (1, 6), indices imply (6, 6)",4,<python><pandas><flask><valueerror>,65,"<p>I am passing a list from flask function to another function, and getting this value error.</p>

<p>My code at sending end:</p>

<pre><code>@app.route('/process', methods=['POST'])
def process():
    name = request.form['name']
    comment = request.form['comment']
    wickets = request.form['wickets']
    ga = request.form['ga']
    ppballs = request.form['ppballs']
    overs = request.form['overs']

    score = [name,comment,wickets,ga,ppballs,overs]
    results = []
    results = eval_score(score)
    print results
</code></pre>

<p>Receiver end :</p>

<pre><code>def ml_model(data):
    col = pd.DataFrame(data,columns=['runs','balls', 'wickets', 'ground_average', 'pp_balls_left', 'total_overs'])
    predicted = predictor(col)
</code></pre>

<p>Trace of Error:</p>

<pre><code> ...
 line 1598, in dispatch_request
 return self.view_functions[rule.endpoint](**req.view_args)

 File ""/Users/sbk/guestbook/guestbook.py"", line 26, in process
 results = eval_score(score)

 File ""/Users/sbk/guestbook/eval_score.py"", line 6, in eval_score
 col = pd.DataFrame(data,columns=['runs','balls', 'wickets',  'ground_average', 'pp_balls_left', 'total_overs'])

 File ""/Users/sbk/anaconda2/lib/python2.7/site-  packages/pandas/core/frame.py"", line 385, in __init__
 copy=copy)

 File ""/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py"", line 533, in _init_ndarray
 return create_block_manager_from_blocks([values], [columns, index])

 File ""/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/internals.py"", line 4631, in  create_block_manager_from_blocks
 construction_error(tot_items, blocks[0].shape[1:], axes, e)

 File ""/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/internals.py"", line 4608, in construction_error
 Open an interactive python shell in this framepassed, implied))
</code></pre>

<p>Please let me know where I am going wrong.</p>
",5918382,1269,05-05-2018 05:08,05-05-2018 06:03,0,1269,24,1,14,12,"{'badge_counts': {'bronze': 24, 'silver': 14, 'gold': 1}, 'account_id': 7827203, 'is_employee': False, 'last_modified_date': 1691863502, 'last_access_date': 1664493640, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1269, 'creation_date': 1455279241, 'user_type': 'registered', 'user_id': 5918382, 'accept_rate': 12, 'location': 'San Jose, CA, United States', 'website_url': 'https://shantagouda.com', 'link': 'https://stackoverflow.com/users/5918382/sbk3824', 'profile_image': 'https://lh4.googleusercontent.com/-gzrc5sH3cJQ/AAAAAAAAAAI/AAAAAAAAR7Q/YrNCvoI5Lf0/photo.jpg?sz=256', 'display_name': 'Sbk3824'}","I am passing a list from flask function to another function, and getting this value error. My code at sending end: Receiver end : Trace of Error: Please let me know where I am going wrong.","@app.route('/process', methods=['POST'])
def process():
    name = request.form['name']
    comment = request.form['comment']
    wickets = request.form['wickets']
    ga = request.form['ga']
    ppballs = request.form['ppballs']
    overs = request.form['overs']

    score = [name,comment,wickets,ga,ppballs,overs]
    results = []
    results = eval_score(score)
    print results
 def ml_model(data):
    col = pd.DataFrame(data,columns=['runs','balls', 'wickets', 'ground_average', 'pp_balls_left', 'total_overs'])
    predicted = predictor(col)
  ...
 line 1598, in dispatch_request
 return self.view_functions[rule.endpoint](**req.view_args)

 File ""/Users/sbk/guestbook/guestbook.py"", line 26, in process
 results = eval_score(score)

 File ""/Users/sbk/guestbook/eval_score.py"", line 6, in eval_score
 col = pd.DataFrame(data,columns=['runs','balls', 'wickets',  'ground_average', 'pp_balls_left', 'total_overs'])

 File ""/Users/sbk/anaconda2/lib/python2.7/site-  packages/pandas/core/frame.py"", line 385, in __init__
 copy=copy)

 File ""/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py"", line 533, in _init_ndarray
 return create_block_manager_from_blocks([values], [columns, index])

 File ""/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/internals.py"", line 4631, in  create_block_manager_from_blocks
 construction_error(tot_items, blocks[0].shape[1:], axes, e)

 File ""/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/internals.py"", line 4608, in construction_error
 Open an interactive python shell in this framepassed, implied))
",34,52,0,0,
186,49479083,49480052,12420,Print layer outputs in Keras during training,2,<python><tensorflow><keras>,12,"<p>I am new to Keras. How can I print the outputs of a layer, both intermediate or final, during the training phase?</p>

<p>I am trying to debug my neural network and wanted to know how the layers behave during training. To do so I am trying to exact input and output of a layer during training, for every step.</p>

<p>The FAQ (<a href=""https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer"" rel=""noreferrer"">https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer</a>) has a method to extract output of intermediate layer for building another model but that is not what I want. I don't need to use the intermediate layer output as input to other layer, I just need to print their values out and perhaps graph/chart/visualize it.</p>

<p>I am using Keras 2.1.4</p>
",5553468,179,25-03-2018 18:00,25-03-2018 19:30,0,179,9,1,1,,"{'badge_counts': {'bronze': 9, 'silver': 1, 'gold': 1}, 'account_id': 6347368, 'is_employee': False, 'last_modified_date': 1601079000, 'last_access_date': 1676214973, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 179, 'creation_date': 1447308884, 'user_type': 'registered', 'user_id': 5553468, 'link': 'https://stackoverflow.com/users/5553468/eric', 'profile_image': 'https://www.gravatar.com/avatar/3f61fb2ce18109bbbc72322f848fb645?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Eric'}","I am new to Keras. How can I print the outputs of a layer, both intermediate or final, during the training phase? I am trying to debug my neural network and wanted to know how the layers behave during training. To do so I am trying to exact input and output of a layer during training, for every step. The FAQ (https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer) has a method to extract output of intermediate layer for building another model but that is not what I want. I don't need to use the intermediate layer output as input to other layer, I just need to print their values out and perhaps graph/chart/visualize it. I am using Keras 2.1.4",,0,7,0,1,
187,49791246,49791402,40372,"Drop Columns with more than 60 Percent of ""empty"" Values in Pandas",3,<python><pandas>,18,"<p>I have got a dataframe like this:</p>

<pre><code>import pandas as pd
data = {
    'c1': ['Test1','Test2','NULL','Test3',' ','Test4','Test4','Test1',""Test3""],
    'c2': [' ','Test1',' ','NULL',' ','NULL','NULL','NULL','NULL'],
    'c3': [0,0,0,0,0,1,5,0,0],
    'c4': ['NULL', 'Test2', 'Test1','Test1', 'Test2', 'Test2','Test1','Test1','Test2']
}
df = pd.DataFrame(data)
df
</code></pre>

<p>The dataframe looks like this:</p>

<pre><code>    c1      c2      c3      c4
0   Test1           0       NULL
1   Test2   Test1   0       Test2
2   NULL            0       Test1
3   Test3   NULL    0       Test1
4                   0       Test2
5   Test4   NULL    1       Test2
6   Test4   NULL    5       Test1
7   Test1   NULL    0       Test1
8   Test3   NULL    0       Test2
</code></pre>

<p>I want to drop all columns, that have more than 60 % of ""empty"" values. ""Empty"" means in my case that the values are for example: ' ', 'NULL' or 0. There are strings (c1, c2, c4) as well as integers (c3). </p>

<p>The result should be a dataframe with columns c1 and c4 only.</p>

<pre><code>    c1      c4
0   Test1   NULL
1   Test2   Test2
2   NULL    Test1
3   Test3   Test1
4           Test2
5   Test4   Test2
6   Test4   Test1
7   Test1   Test1
8   Test3   Test2
</code></pre>

<p>I have no idea how to handle that problem. Only thing that comes to my mind is something like </p>

<pre><code>df.loc[:, (df != 0).any(axis=0)]
</code></pre>

<p>to delete all columns where all values are 0, 'NULL' and so on.</p>
",5558763,411,12-04-2018 08:11,12-04-2018 08:20,0,411,8,1,3,100,"{'badge_counts': {'bronze': 8, 'silver': 3, 'gold': 1}, 'account_id': 7292947, 'is_employee': False, 'last_modified_date': 1600708438, 'last_access_date': 1599464061, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 411, 'creation_date': 1447417031, 'user_type': 'registered', 'user_id': 5558763, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/5558763/krypt', 'profile_image': 'https://www.gravatar.com/avatar/ba3432b55f45c9bc9b51b57e5936616f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Krypt'}","I have got a dataframe like this: The dataframe looks like this: I want to drop all columns, that have more than 60 % of ""empty"" values. ""Empty"" means in my case that the values are for example: ' ', 'NULL' or 0. There are strings (c1, c2, c4) as well as integers (c3). The result should be a dataframe with columns c1 and c4 only. I have no idea how to handle that problem. Only thing that comes to my mind is something like to delete all columns where all values are 0, 'NULL' and so on.","import pandas as pd
data = {
    'c1': ['Test1','Test2','NULL','Test3',' ','Test4','Test4','Test1',""Test3""],
    'c2': [' ','Test1',' ','NULL',' ','NULL','NULL','NULL','NULL'],
    'c3': [0,0,0,0,0,1,5,0,0],
    'c4': ['NULL', 'Test2', 'Test1','Test1', 'Test2', 'Test2','Test1','Test1','Test2']
}
df = pd.DataFrame(data)
df
     c1      c2      c3      c4
0   Test1           0       NULL
1   Test2   Test1   0       Test2
2   NULL            0       Test1
3   Test3   NULL    0       Test1
4                   0       Test2
5   Test4   NULL    1       Test2
6   Test4   NULL    5       Test1
7   Test1   NULL    0       Test1
8   Test3   NULL    0       Test2
     c1      c4
0   Test1   NULL
1   Test2   Test2
2   NULL    Test1
3   Test3   Test1
4           Test2
5   Test4   Test2
6   Test4   Test1
7   Test1   Test1
8   Test3   Test2
 df.loc[:, (df != 0).any(axis=0)]
",26,49,0,0,
188,49392845,49392996,8211,Python parameter annotations unresolved reference,3,<python><python-3.x><annotations>,28,"<p>Why does it say that it can not find my class? Why should I create another class with the same name in order to make make it not complain?</p>

<pre><code>from typing import Dict


class WeekDay:

    def __init__(self, day_number, day_name):
        self.day_name = day_name
        self.day_number = day_number

    @staticmethod
    def get_week_days() -&gt; Dict[str, WeekDay]:  # WeekDay unresolved reference error
        weekdays = {
            ""monday"": WeekDay(1, ""Monday""),
            ""tuesday"": WeekDay(2, ""Tuesday""),
            ""wednesday"": WeekDay(3, ""Wednesday""),
            ""thursday"": WeekDay(4, ""Thursday""),
            ""friday"": WeekDay(5, ""Friday""),
            ""saturday"": WeekDay(6, ""Saturday""),
            ""sunday"": WeekDay(7, ""Sunday"")
        }
        return weekdays
</code></pre>
",5572411,3337,20-03-2018 19:41,20-03-2018 19:50,0,3337,49,2,29,54,"{'badge_counts': {'bronze': 49, 'silver': 29, 'gold': 2}, 'account_id': 7314198, 'is_employee': False, 'last_modified_date': 1679924109, 'last_access_date': 1711102386, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3337, 'creation_date': 1447769764, 'user_type': 'registered', 'user_id': 5572411, 'accept_rate': 54, 'link': 'https://stackoverflow.com/users/5572411/laimonas-sutkus', 'profile_image': 'https://graph.facebook.com/714167175350732/picture?type=large', 'display_name': 'Laimonas Sutkus'}",Why does it say that it can not find my class? Why should I create another class with the same name in order to make make it not complain?,"from typing import Dict


class WeekDay:

    def __init__(self, day_number, day_name):
        self.day_name = day_name
        self.day_number = day_number

    @staticmethod
    def get_week_days() -&gt; Dict[str, WeekDay]:  # WeekDay unresolved reference error
        weekdays = {
            ""monday"": WeekDay(1, ""Monday""),
            ""tuesday"": WeekDay(2, ""Tuesday""),
            ""wednesday"": WeekDay(3, ""Wednesday""),
            ""thursday"": WeekDay(4, ""Thursday""),
            ""friday"": WeekDay(5, ""Friday""),
            ""saturday"": WeekDay(6, ""Saturday""),
            ""sunday"": WeekDay(7, ""Sunday"")
        }
        return weekdays
",20,24,0,0,
189,49671693,49671919,27513,pandas DataFrame: normalize one JSON column and merge with other columns,2,<python><json><pandas><dataframe>,19,"<p>I have a pandas DataFrame containing one column with multiple JSON data items as list of dicts. I want to normalize the JSON column and duplicate the non-JSON columns:</p>

<pre><code># creating dataframe
df_actions = pd.DataFrame(columns=['id', 'actions'])
rows = [[12,json.loads('[{""type"": ""a"",""value"": ""17""},{""type"": ""b"",""value"": ""19""}]')],
   [15, json.loads('[{""type"": ""a"",""value"": ""1""},{""type"": ""b"",""value"": ""3""},{""type"": ""c"",""value"": ""5""}]')]]
df_actions.loc[0] = rows[0]
df_actions.loc[1] = rows[1]

&gt;&gt;&gt;df_actions
   id                                            actions
0  12  [{'type': 'a', 'value': '17'}, {'type': 'b', '...
1  15  [{'type': 'a', 'value': '1'}, {'type': 'b', 'v...
</code></pre>

<p>I want</p>

<pre><code>&gt;&gt;&gt;df_actions_parsed
   id      type    value
   12      a        17
   12      b        19
   15      a        1
   15      b        3
   15      c        5
</code></pre>

<p>I can normalize JSON data using:</p>

<pre><code>pd.concat([pd.DataFrame(json_normalize(x)) for x in df_actions['actions']],ignore_index=True)
</code></pre>

<p>but I don't know how to join that back to the id column of the original DataFrame.</p>
",5931892,1010,05-04-2018 11:51,05-04-2018 12:02,0,1010,28,3,11,100,"{'badge_counts': {'bronze': 28, 'silver': 11, 'gold': 3}, 'account_id': 7847219, 'is_employee': False, 'last_modified_date': 1643405700, 'last_access_date': 1698047262, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1010, 'creation_date': 1455568652, 'user_type': 'registered', 'user_id': 5931892, 'accept_rate': 100, 'location': 'Berlin, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/5931892/stack-lech', 'profile_image': 'https://www.gravatar.com/avatar/bf070390e3e9403c9a6cebc3fcfebd4c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'stack_lech'}",I have a pandas DataFrame containing one column with multiple JSON data items as list of dicts. I want to normalize the JSON column and duplicate the non-JSON columns: I want I can normalize JSON data using: but I don't know how to join that back to the id column of the original DataFrame.,"# creating dataframe
df_actions = pd.DataFrame(columns=['id', 'actions'])
rows = [[12,json.loads('[{""type"": ""a"",""value"": ""17""},{""type"": ""b"",""value"": ""19""}]')],
   [15, json.loads('[{""type"": ""a"",""value"": ""1""},{""type"": ""b"",""value"": ""3""},{""type"": ""c"",""value"": ""5""}]')]]
df_actions.loc[0] = rows[0]
df_actions.loc[1] = rows[1]

&gt;&gt;&gt;df_actions
   id                                            actions
0  12  [{'type': 'a', 'value': '17'}, {'type': 'b', '...
1  15  [{'type': 'a', 'value': '1'}, {'type': 'b', 'v...
 &gt;&gt;&gt;df_actions_parsed
   id      type    value
   12      a        17
   12      b        19
   15      a        1
   15      b        3
   15      c        5
 pd.concat([pd.DataFrame(json_normalize(x)) for x in df_actions['actions']],ignore_index=True)
",16,32,0,0,
190,48815872,48815977,22209,testing postgres db python,1,<python><postgresql><python-unittest>,22,"<p>I don't understand how to test my repositories.  </p>

<p>I want to be sure that I really saved object with all of it parameters into database, and when I execute my SQL statement I really received what I am supposed to.</p>

<p>But, I cannot put <code>""CREATE TABLE test_table</code>"" in <code>setUp</code> method of unittest case because it will be created multiple times (tests of the same testcase are runned in parallel). So, as long as I create 2 methods in the same class which needs to work on the same table, it won't work (name clash of tables)</p>

<p>Same, I cannot put <code>""CREATE TABLE test_table""</code> <code>setUpModule</code>, because, now the table is created once, but since tests are runned in parallel, there is nothing which prevents from inserting the same object multiple times into my table, which breakes the unicity constraint of some field.</p>

<p>Same, I cannot <code>""CREATE SCHEMA some_random_schema_name""</code> in every method, because I need to globally ""SET search_path TO ..."" for a given Database, so every method runned in parallel will be affected.</p>

<p>The only way I see is to create to <code>""CREATE DATABASE""</code> for each  test, and with unique name, and establish a invidual connection to each database.. This looks extreeeemly wasteful. Is there a better way?</p>

<p>Also, I cannot use SQLite in memory because I need to test PostgreSQL.</p>
",5688082,1463,15-02-2018 20:39,15-02-2018 20:47,0,1473,31,1,18,48,"{'badge_counts': {'bronze': 31, 'silver': 18, 'gold': 1}, 'account_id': 6723365, 'is_employee': False, 'last_modified_date': 1708617900, 'last_access_date': 1711178859, 'reputation_change_year': 52, 'reputation_change_quarter': 52, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1473, 'creation_date': 1450291745, 'user_type': 'registered', 'user_id': 5688082, 'accept_rate': 48, 'website_url': '', 'link': 'https://stackoverflow.com/users/5688082/%d0%9f%d1%83%d0%b9%d0%bb%d0%be-%d0%a5%d1%83%d0%b9%d0%bb%d0%be-%d1%81%d0%b4%d0%be%d1%85%d0%bd%d0%b8-%d0%b3%d0%bd%d0%b8%d0%b4%d0%b0', 'profile_image': 'https://www.gravatar.com/avatar/11606d449a86bb6bdfe7abbaa3f40f10?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Пуйло-Хуйло сдохни гнида'}","I don't understand how to test my repositories. I want to be sure that I really saved object with all of it parameters into database, and when I execute my SQL statement I really received what I am supposed to. But, I cannot put "" in method of unittest case because it will be created multiple times (tests of the same testcase are runned in parallel). So, as long as I create 2 methods in the same class which needs to work on the same table, it won't work (name clash of tables) Same, I cannot put , because, now the table is created once, but since tests are runned in parallel, there is nothing which prevents from inserting the same object multiple times into my table, which breakes the unicity constraint of some field. Same, I cannot in every method, because I need to globally ""SET search_path TO ..."" for a given Database, so every method runned in parallel will be affected. The only way I see is to create to for each test, and with unique name, and establish a invidual connection to each database.. This looks extreeeemly wasteful. Is there a better way? Also, I cannot use SQLite in memory because I need to test PostgreSQL.","""CREATE TABLE test_table setUp ""CREATE TABLE test_table"" setUpModule ""CREATE SCHEMA some_random_schema_name"" ""CREATE DATABASE""",-6,13,0,0,
191,50293656,50293917,6637,Why does NumPy's random function seemingly display a pattern in its generated values?,4,<python><numpy><random><python-imaging-library>,13,"<p>I was playing around with NumPy and Pillow and came across an interesting result that apparently showcases a pattern in NumPy <code>random.random()</code> results.</p>

<p><a href=""https://i.stack.imgur.com/zeJPU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zeJPU.png"" alt=""Image One""></a> <a href=""https://i.stack.imgur.com/1aZua.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1aZua.png"" alt=""Image Two""></a> <a href=""https://i.stack.imgur.com/YOVsV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YOVsV.png"" alt=""Image Three""></a> <a href=""https://i.stack.imgur.com/u18BH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/u18BH.png"" alt=""Image Four""></a></p>

<p>Here a sample of the full code for generating and saving 100 of these images (with seed 0), the above are the first four images generated by this code.</p>

<pre><code>import numpy as np
from PIL import Image

np.random.seed(0)
img_arrays = np.random.random((100, 256, 256, 3)) * 255
for i, img_array in enumerate(img_arrays):
    img = Image.fromarray(img_array, ""RGB"")
    img.save(""{}.png"".format(i))
</code></pre>

<p>The above are four different images created using <code>PIL.Image.fromarray()</code> on four different NumPy arrays created using <code>numpy.random.random((256, 256, 3)) * 255</code> to generate a 256 by 256 grid of RGB values in four different Python instances (the same thing also happens in the same instance).</p>

<p>I noticed that this only happens (in my limited testing) when the width and height of the image is a power of two, I am not sure how to interpret that.</p>

<p>Although it may be hard to see due to browser anti-aliasing (you can download the images and view them in image viewers with no anti-aliasing), there are clear purple-brown columns of pixels every 8th column starting from the 3rd column of every image. To make sure, I tested this on 100 different images and they all followed this pattern.</p>

<p>What is going on here? I am guessing that patterns like this are the reason that people always say to use cryptographically secure random number generators when true randomness is required, but is there a concrete explanation behind why this is happening in particular?</p>
",5695018,2150,11-05-2018 13:40,11-05-2018 13:53,0,2150,31,0,19,100,"{'badge_counts': {'bronze': 31, 'silver': 19, 'gold': 0}, 'account_id': 7493248, 'is_employee': False, 'last_modified_date': 1647997410, 'last_access_date': 1708922943, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2150, 'creation_date': 1450439353, 'user_type': 'registered', 'user_id': 5695018, 'accept_rate': 100, 'location': 'San Francisco, California, USA', 'website_url': 'https://ziyadedher.com', 'link': 'https://stackoverflow.com/users/5695018/ziyad-edher', 'profile_image': 'https://i.stack.imgur.com/cFqOq.jpg?s=256&g=1', 'display_name': 'Ziyad Edher'}","I was playing around with NumPy and Pillow and came across an interesting result that apparently showcases a pattern in NumPy results. Here a sample of the full code for generating and saving 100 of these images (with seed 0), the above are the first four images generated by this code. The above are four different images created using on four different NumPy arrays created using to generate a 256 by 256 grid of RGB values in four different Python instances (the same thing also happens in the same instance). I noticed that this only happens (in my limited testing) when the width and height of the image is a power of two, I am not sure how to interpret that. Although it may be hard to see due to browser anti-aliasing (you can download the images and view them in image viewers with no anti-aliasing), there are clear purple-brown columns of pixels every 8th column starting from the 3rd column of every image. To make sure, I tested this on 100 different images and they all followed this pattern. What is going on here? I am guessing that patterns like this are the reason that people always say to use cryptographically secure random number generators when true randomness is required, but is there a concrete explanation behind why this is happening in particular?","random.random() import numpy as np
from PIL import Image

np.random.seed(0)
img_arrays = np.random.random((100, 256, 256, 3)) * 255
for i, img_array in enumerate(img_arrays):
    img = Image.fromarray(img_array, ""RGB"")
    img.save(""{}.png"".format(i))
 PIL.Image.fromarray() numpy.random.random((256, 256, 3)) * 255",4,23,4,4,
192,48480972,48481955,8652,Hide password field in GET but not POST in Django REST Framework where depth=1 in serializer,5,<python><django><rest><django-rest-framework><django-serializer>,12,"<p>I have 2 models : User &amp; UserSummary. UserSummary has a foreign key to User. I just noticed that if I set <code>depth= 1</code> within <code>UserSummarySerializer</code>, the password field is included in the output. It's hashed, but it would still be best to exclude this field.</p>

<p>To hide the password field, I've just set the user field explicitly in the serializer, just like this :</p>

<pre><code>class UserSerializer(serializers.ModelSerializer):
    """"""A serializer for our user profile objects.""""""

    class Meta:
        model = models.User
       extra_kwargs = {'password': {'write_only': True}}
        exclude = ('groups', 'last_login', 'is_superuser', 'user_permissions', 'created_at')

    def create(self, validated_data):
        """"""Create and return a new user.""""""

        user = models.User(
            email = validated_data['email'],
            firstname = validated_data['firstname'],
            lastname = validated_data['lastname'],
            mobile = validated_data['mobile']
        )

        user.set_password(validated_data['password'])
        user.save()

        return user


class UserSummarySerializer(serializers.ModelSerializer):
    user = UserSerializer()

    class Meta:
        model = models.UserSummary
        fields = '__all__'
        depth = 1
</code></pre>

<p>The downside of this way of doing is that, the field password is not available anymore on the POST request when creating a new user.</p>

<p>How could I hide the <code>password</code> field on the GET request of UserSummary but display it in the POST request of User ?</p>
",5703539,1557,27-01-2018 21:56,28-01-2018 00:23,1,1567,53,6,28,63,"{'badge_counts': {'bronze': 53, 'silver': 28, 'gold': 6}, 'account_id': 7507451, 'is_employee': False, 'last_modified_date': 1668808500, 'last_access_date': 1709720148, 'reputation_change_year': 9, 'reputation_change_quarter': 9, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1567, 'creation_date': 1450704306, 'user_type': 'registered', 'user_id': 5703539, 'accept_rate': 63, 'website_url': '', 'link': 'https://stackoverflow.com/users/5703539/kabrice', 'profile_image': 'https://i.stack.imgur.com/LNcVF.png?s=256&g=1', 'display_name': 'kabrice'}","I have 2 models : User &amp; UserSummary. UserSummary has a foreign key to User. I just noticed that if I set within , the password field is included in the output. It's hashed, but it would still be best to exclude this field. To hide the password field, I've just set the user field explicitly in the serializer, just like this : The downside of this way of doing is that, the field password is not available anymore on the POST request when creating a new user. How could I hide the field on the GET request of UserSummary but display it in the POST request of User ?","depth= 1 UserSummarySerializer class UserSerializer(serializers.ModelSerializer):
    """"""A serializer for our user profile objects.""""""

    class Meta:
        model = models.User
       extra_kwargs = {'password': {'write_only': True}}
        exclude = ('groups', 'last_login', 'is_superuser', 'user_permissions', 'created_at')

    def create(self, validated_data):
        """"""Create and return a new user.""""""

        user = models.User(
            email = validated_data['email'],
            firstname = validated_data['firstname'],
            lastname = validated_data['lastname'],
            mobile = validated_data['mobile']
        )

        user.set_password(validated_data['password'])
        user.save()

        return user


class UserSummarySerializer(serializers.ModelSerializer):
    user = UserSerializer()

    class Meta:
        model = models.UserSummary
        fields = '__all__'
        depth = 1
 password",27,40,0,0,
193,49537057,49537831,1825,df.groupby(...).agg(set) produces different result compared to df.groupby(...).agg(lambda x: set(x)),2,<python><pandas><pandas-groupby>,18,"<p>Answering <a href=""https://stackoverflow.com/questions/49535966/what-is-the-pythonic-way-of-collapsing-values-into-a-set-for-multiple-columns-pe"">this question</a> it turned out that <code>df.groupby(...).agg(set)</code> and <code>df.groupby(...).agg(lambda x: set(x))</code> are producing different results.</p>

<p>Data:</p>

<pre><code>df = pd.DataFrame({
       'user_id': [1, 2, 3, 4, 1, 2, 3], 
       'class_type': ['Krav Maga', 'Yoga', 'Ju-jitsu', 'Krav Maga', 
                      'Ju-jitsu','Krav Maga', 'Karate'], 
       'instructor': ['Bob', 'Alice','Bob', 'Alice','Alice', 'Alice','Bob']})
</code></pre>

<p>Demo:</p>

<pre><code>In [36]: df.groupby('user_id').agg(lambda x: set(x))
Out[36]:
                    class_type    instructor
user_id
1        {Krav Maga, Ju-jitsu}  {Alice, Bob}
2            {Yoga, Krav Maga}       {Alice}
3           {Ju-jitsu, Karate}         {Bob}
4                  {Krav Maga}       {Alice}

In [37]: df.groupby('user_id').agg(set)
Out[37]:
                                class_type                         instructor
user_id
1        {user_id, class_type, instructor}  {user_id, class_type, instructor}
2        {user_id, class_type, instructor}  {user_id, class_type, instructor}
3        {user_id, class_type, instructor}  {user_id, class_type, instructor}
4        {user_id, class_type, instructor}  {user_id, class_type, instructor}
</code></pre>

<p>I would expect the same behaviour here - do you know what I am missing?</p>
",5741205,209081,28-03-2018 14:21,28-03-2018 14:54,0,209321,425,36,394,100,"{'badge_counts': {'bronze': 425, 'silver': 394, 'gold': 36}, 'account_id': 7565400, 'is_employee': False, 'last_modified_date': 1709035684, 'last_access_date': 1711179424, 'reputation_change_year': 1390, 'reputation_change_quarter': 1390, 'reputation_change_month': 378, 'reputation_change_week': 100, 'reputation_change_day': 0, 'reputation': 209321, 'creation_date': 1451847573, 'user_type': 'registered', 'user_id': 5741205, 'accept_rate': 100, 'location': 'Munich, Germany', 'website_url': 'https://www.linkedin.com/in/maxuzunov/', 'link': 'https://stackoverflow.com/users/5741205/maxu-stand-with-ukraine', 'profile_image': 'https://i.stack.imgur.com/SG6BE.png?s=256&g=1', 'display_name': 'MaxU - stand with Ukraine'}",Answering this question it turned out that and are producing different results. Data: Demo: I would expect the same behaviour here - do you know what I am missing?,"df.groupby(...).agg(set) df.groupby(...).agg(lambda x: set(x)) df = pd.DataFrame({
       'user_id': [1, 2, 3, 4, 1, 2, 3], 
       'class_type': ['Krav Maga', 'Yoga', 'Ju-jitsu', 'Krav Maga', 
                      'Ju-jitsu','Krav Maga', 'Karate'], 
       'instructor': ['Bob', 'Alice','Bob', 'Alice','Alice', 'Alice','Bob']})
 In [36]: df.groupby('user_id').agg(lambda x: set(x))
Out[36]:
                    class_type    instructor
user_id
1        {Krav Maga, Ju-jitsu}  {Alice, Bob}
2            {Yoga, Krav Maga}       {Alice}
3           {Ju-jitsu, Karate}         {Bob}
4                  {Krav Maga}       {Alice}

In [37]: df.groupby('user_id').agg(set)
Out[37]:
                                class_type                         instructor
user_id
1        {user_id, class_type, instructor}  {user_id, class_type, instructor}
2        {user_id, class_type, instructor}  {user_id, class_type, instructor}
3        {user_id, class_type, instructor}  {user_id, class_type, instructor}
4        {user_id, class_type, instructor}  {user_id, class_type, instructor}
",18,33,0,1,
194,50298011,51628412,86969,How to remove Python 3.6 completely from Ubuntu 18.04,2,<python><ubuntu><anaconda><ubuntu-18.04>,16,"<p>Both Python 2.7 and 3.6 are installed by default in Ubuntu 18. But I wish to use the Anaconda Python with conda package manager. To avoid any conflicts I wish to completely remove the default Python 3.6. Are there any way to do that? Please help.</p>
",5766835,622,11-05-2018 18:12,01-08-2018 08:13,82,622,18,2,8,0,"{'badge_counts': {'bronze': 18, 'silver': 8, 'gold': 2}, 'account_id': 7602937, 'is_employee': False, 'last_modified_date': 1663983600, 'last_access_date': 1679200193, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 622, 'creation_date': 1452346059, 'user_type': 'registered', 'user_id': 5766835, 'accept_rate': 0, 'website_url': '', 'link': 'https://stackoverflow.com/users/5766835/sandip-nath', 'profile_image': 'https://lh5.googleusercontent.com/-feNqXUeNSxs/AAAAAAAAAAI/AAAAAAAAAAA/V70GVJsBsck/photo.jpg?sz=256', 'display_name': 'Sandip Nath'}",Both Python 2.7 and 3.6 are installed by default in Ubuntu 18. But I wish to use the Anaconda Python with conda package manager. To avoid any conflicts I wish to completely remove the default Python 3.6. Are there any way to do that? Please help.,,0,1,0,0,
195,50158152,50158228,4783,Subtracting columns based on key column in pandas dataframe,3,<python><python-3.x><pandas><dataframe>,13,"<p>I have two dataframes looking like</p>

<p>df1:</p>

<pre><code>   ID    A   B   C   D 
0 'ID1' 0.5 2.1 3.5 6.6
1 'ID2' 1.2 5.5 4.3 2.2
2 'ID1' 0.7 1.2 5.6 6.0 
3 'ID3' 1.1 7.2 10. 3.2
</code></pre>

<p>df2:</p>

<pre><code>   ID    A   B   C   D 
0 'ID1' 1.0 2.0 3.3 4.4
1 'ID2' 1.5 5.0 4.0 2.2
2 'ID3' 0.6 1.2 5.9 6.2 
3 'ID4' 1.1 7.2 8.5 3.0
</code></pre>

<p>df1 can have multiple entries with the same <code>ID</code> whereas each <code>ID</code> occurs only once in df2. Also not all <code>ID</code> in df2 are necessarily present in df1. I can't solve this by using <code>set_index()</code> as multiple rows in df1 can have the same <code>ID</code>, and that the <code>ID</code> in df1 and df2 are not aligned.</p>

<p>I want to create a new dataframe where I subtract the values in <code>df2[['A','B','C','D']]</code> from <code>df1[['A','B','C','D']]</code> based on matching the ID. </p>

<p>The resulting dataframe would look like:</p>

<p>df_new:</p>

<pre><code>   ID     A    B   C   D 
0 'ID1' -0.5  0.1 0.2 2.2
1 'ID2' -0.3  0.5 0.3 0.0
2 'ID1' -0.3 -0.8 2.3 1.6
3 'ID3'  0.5  6.0 1.5 0.2
</code></pre>

<p>I know how to do this with a loop, but since I'm dealing with huge data quantities this is not practical at all. What is the best way of approaching this with Pandas?</p>
",5774969,482,03-05-2018 14:59,03-05-2018 15:02,0,482,16,1,6,100,"{'badge_counts': {'bronze': 16, 'silver': 6, 'gold': 1}, 'account_id': 7615351, 'is_employee': False, 'last_modified_date': 1646222452, 'last_access_date': 1690100021, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 482, 'creation_date': 1452534874, 'user_type': 'registered', 'user_id': 5774969, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/5774969/astroat', 'profile_image': 'https://www.gravatar.com/avatar/c9c1f2412aa0e899163e69d7f3f605fd?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'AstroAT'}","I have two dataframes looking like df1: df2: df1 can have multiple entries with the same whereas each occurs only once in df2. Also not all in df2 are necessarily present in df1. I can't solve this by using as multiple rows in df1 can have the same , and that the in df1 and df2 are not aligned. I want to create a new dataframe where I subtract the values in from based on matching the ID. The resulting dataframe would look like: df_new: I know how to do this with a loop, but since I'm dealing with huge data quantities this is not practical at all. What is the best way of approaching this with Pandas?","   ID    A   B   C   D 
0 'ID1' 0.5 2.1 3.5 6.6
1 'ID2' 1.2 5.5 4.3 2.2
2 'ID1' 0.7 1.2 5.6 6.0 
3 'ID3' 1.1 7.2 10. 3.2
    ID    A   B   C   D 
0 'ID1' 1.0 2.0 3.3 4.4
1 'ID2' 1.5 5.0 4.0 2.2
2 'ID3' 0.6 1.2 5.9 6.2 
3 'ID4' 1.1 7.2 8.5 3.0
 ID ID ID set_index() ID ID df2[['A','B','C','D']] df1[['A','B','C','D']]    ID     A    B   C   D 
0 'ID1' -0.5  0.1 0.2 2.2
1 'ID2' -0.3  0.5 0.3 0.0
2 'ID1' -0.3 -0.8 2.3 1.6
3 'ID3'  0.5  6.0 1.5 0.2
",4,36,0,0,
196,49295311,49299921,49448,what is the difference between Flatten() and GlobalAveragePooling2D() in keras,5,<python><tensorflow><keras><deep-learning><keras-layer>,47,"<p>I want to pass the output of ConvLSTM and Conv2D to a Dense Layer in Keras, what is the difference between using global average pooling and flatten
Both is working in my case.</p>



<pre class=""lang-python prettyprint-override""><code>model.add(ConvLSTM2D(filters=256,kernel_size=(3,3)))
model.add(Flatten())
# or model.add(GlobalAveragePooling2D())
model.add(Dense(256,activation='relu'))
</code></pre>
",5797699,1836,15-03-2018 09:11,15-03-2018 12:49,0,1846,26,1,17,100,"{'badge_counts': {'bronze': 26, 'silver': 17, 'gold': 1}, 'account_id': 7647683, 'is_employee': False, 'last_modified_date': 1663579201, 'last_access_date': 1611322214, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1846, 'creation_date': 1452926584, 'user_type': 'registered', 'user_id': 5797699, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/5797699/user239457', 'profile_image': 'https://graph.facebook.com/450581345136674/picture?type=large', 'display_name': 'user239457'}","I want to pass the output of ConvLSTM and Conv2D to a Dense Layer in Keras, what is the difference between using global average pooling and flatten Both is working in my case.","model.add(ConvLSTM2D(filters=256,kernel_size=(3,3)))
model.add(Flatten())
# or model.add(GlobalAveragePooling2D())
model.add(Dense(256,activation='relu'))
",3,10,0,0,
197,49579684,49579995,51383,What is the difference between Dataset.from_tensors and Dataset.from_tensor_slices?,5,<python><tensorflow><tensorflow-datasets>,95,"<p>I have a dataset represented as a NumPy matrix of shape <code>(num_features, num_examples)</code> and I wish to convert it to TensorFlow type <code>tf.Dataset</code>.</p>

<p>I am struggling trying to understand the difference between these two methods: <code>Dataset.from_tensors</code> and <code>Dataset.from_tensor_slices</code>. What is the right one and why?</p>

<p>TensorFlow documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer"">link</a>) says that both method accept a nested structure of tensor although when using <code>from_tensor_slices</code> the tensor should have same size in the 0-th dimension. </p>
",5291026,1533,30-03-2018 18:40,30-03-2018 19:05,0,1533,11,1,12,,"{'badge_counts': {'bronze': 11, 'silver': 12, 'gold': 1}, 'account_id': 6885982, 'is_employee': False, 'last_modified_date': 1573679451, 'last_access_date': 1588288951, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1533, 'creation_date': 1441171155, 'user_type': 'registered', 'user_id': 5291026, 'location': 'Boston, MA', 'website_url': '', 'link': 'https://stackoverflow.com/users/5291026/llewlyn', 'profile_image': 'https://i.stack.imgur.com/j4Wo6.jpg?s=256&g=1', 'display_name': 'Llewlyn'}",I have a dataset represented as a NumPy matrix of shape and I wish to convert it to TensorFlow type . I am struggling trying to understand the difference between these two methods: and . What is the right one and why? TensorFlow documentation (link) says that both method accept a nested structure of tensor although when using the tensor should have same size in the 0-th dimension.,"(num_features, num_examples) tf.Dataset Dataset.from_tensors Dataset.from_tensor_slices from_tensor_slices",-5,5,0,1,
198,48136978,48137140,17802,How to use Feature2D (such as SimpleBlobDetector) correctly? (Python + OpenCV),1,<python><opencv>,17,"<p>I'm trying to run blob detection using some simple code:</p>

<pre><code>img = cv2.imread(args[""image""])
height, width, channels = img.shape

params = cv2.SimpleBlobDetector_Params()

params.filterByColor = True
params.blobColor = 0

blob_detector = cv2.SimpleBlobDetector(params)
keypoints = blob_detector.detect(img)
</code></pre>

<p>However I keep getting the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""test2.py"", line 37, in &lt;module&gt;
    keypoints = blob_detector.detect(img)
TypeError: Incorrect type of self (must be 'Feature2D' or its derivative)
</code></pre>

<p>Does anyone know what might be wrong?</p>
",5349476,2702,07-01-2018 12:15,07-01-2018 12:33,0,2702,57,7,27,87,"{'badge_counts': {'bronze': 57, 'silver': 27, 'gold': 7}, 'account_id': 6565646, 'is_employee': False, 'last_modified_date': 1709949300, 'last_access_date': 1710913150, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2702, 'creation_date': 1442561318, 'user_type': 'registered', 'user_id': 5349476, 'accept_rate': 87, 'website_url': '', 'link': 'https://stackoverflow.com/users/5349476/john-m', 'profile_image': 'https://www.gravatar.com/avatar/86ef15eeeb67cad92224eeb11cc41b80?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'John M.'}",I'm trying to run blob detection using some simple code: However I keep getting the following error: Does anyone know what might be wrong?,"img = cv2.imread(args[""image""])
height, width, channels = img.shape

params = cv2.SimpleBlobDetector_Params()

params.filterByColor = True
params.blobColor = 0

blob_detector = cv2.SimpleBlobDetector(params)
keypoints = blob_detector.detect(img)
 Traceback (most recent call last):
  File ""test2.py"", line 37, in &lt;module&gt;
    keypoints = blob_detector.detect(img)
TypeError: Incorrect type of self (must be 'Feature2D' or its derivative)
",12,23,0,0,
199,48712154,48731794,47234,PyInstaller WARNING: lib not found,9,<python><pyinstaller>,19,"<p>I know this type of question has been asked here before, but I did not find a working solution to it. I have a python file which I want to convert to an exe on Windows 10. I ran <code>pyinstaller --onefile --noconsole myfile.py</code>, and it outputted <em>a lot</em> of warnings:</p>

<pre><code>C:\Users\conne\Desktop\Python &gt;pyinstaller --onefile --noconsole normal.py
277 INFO: PyInstaller: 3.3.1
277 INFO: Python: 3.6.2
278 INFO: Platform: Windows-10-10.0.15063-SP0
279 INFO: wrote C:\Users\conne\Desktop\Python\normal.spec
280 INFO: UPX is not available.
283 INFO: Extending PYTHONPATH with paths
['C:\\Users\\conne\\Desktop\\python',
 'C:\\Users\\conne\\Desktop\\python']
283 INFO: checking Analysis
284 INFO: Building Analysis because out00-Analysis.toc is non existent
284 INFO: Initializing module dependency graph...
286 INFO: Initializing module graph hooks...
289 INFO: Analyzing base_library.zip ...
5055 INFO: running Analysis out00-Analysis.toc
5058 INFO: Adding Microsoft.Windows.Common-Controls to dependent assemblies of final executable
  required by c:\users\conne\appdata\local\programs\python\python36\python.exe
5145 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5225 WARNING: lib not found: api-ms-win-crt-math-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5298 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5371 WARNING: lib not found: api-ms-win-crt-locale-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5442 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5526 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
5598 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
5669 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
5753 WARNING: lib not found: api-ms-win-crt-convert-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
5840 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
6061 WARNING: lib not found: api-ms-win-crt-time-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6134 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6208 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6282 WARNING: lib not found: api-ms-win-crt-conio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6354 WARNING: lib not found: api-ms-win-crt-environment-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6430 WARNING: lib not found: api-ms-win-crt-math-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6503 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6576 WARNING: lib not found: api-ms-win-crt-convert-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6651 WARNING: lib not found: api-ms-win-crt-locale-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6725 WARNING: lib not found: api-ms-win-crt-process-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6798 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6879 WARNING: lib not found: api-ms-win-crt-filesystem-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6882 INFO: Caching module hooks...
6892 INFO: Analyzing C:\Users\conne\Desktop\python meterpreter\normal.py
6898 INFO: Loading module hooks...
6899 INFO: Loading module hook ""hook-encodings.py""...
7055 INFO: Loading module hook ""hook-pydoc.py""...
7057 INFO: Loading module hook ""hook-xml.py""...
7351 INFO: Looking for ctypes DLLs
7351 INFO: Analyzing run-time hooks ...
7360 INFO: Looking for dynamic libraries
7439 WARNING: lib not found: api-ms-win-crt-time-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7515 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7589 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7660 WARNING: lib not found: api-ms-win-crt-conio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7731 WARNING: lib not found: api-ms-win-crt-environment-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7813 WARNING: lib not found: api-ms-win-crt-utility-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7895 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7968 WARNING: lib not found: api-ms-win-crt-convert-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
8051 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
8133 WARNING: lib not found: api-ms-win-crt-filesystem-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
8213 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\unicodedata.pyd
8286 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\unicodedata.pyd
8361 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\unicodedata.pyd
8440 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\pyexpat.pyd
8512 WARNING: lib not found: api-ms-win-crt-environment-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\pyexpat.pyd
8585 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\pyexpat.pyd
8658 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\pyexpat.pyd
8741 WARNING: lib not found: api-ms-win-crt-time-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
8815 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
8887 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
8958 WARNING: lib not found: api-ms-win-crt-conio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9032 WARNING: lib not found: api-ms-win-crt-environment-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9116 WARNING: lib not found: api-ms-win-crt-utility-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9194 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9271 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9344 WARNING: lib not found: api-ms-win-crt-convert-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9428 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9501 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9580 WARNING: lib not found: api-ms-win-crt-math-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9651 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9723 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9801 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_lzma.pyd
9874 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_lzma.pyd
9959 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_socket.pyd
10030 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_socket.pyd
10115 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\select.pyd
10123 INFO: Looking for eggs
10123 INFO: Using Python library c:\users\conne\appdata\local\programs\python\python36\python36.dll
10124 INFO: Found binding redirects:
[]
10128 INFO: Warnings written to C:\Users\conne\Desktop\python\build\normal\warnnormal.txt
10196 INFO: Graph cross-reference written to C:\Users\conne\Desktop\python\build\normal\xref-normal.html
10212 INFO: checking PYZ
10212 INFO: Building PYZ because out00-PYZ.toc is non existent
10213 INFO: Building PYZ (ZlibArchive) C:\Users\conne\Desktop\python\build\normal\out00-PYZ.pyz
10948 INFO: Building PYZ (ZlibArchive) C:\Users\conne\Desktop\python\build\normal\out00-PYZ.pyz completed successfully.
10958 INFO: checking PKG
10958 INFO: Building PKG because out00-PKG.toc is non existent
10959 INFO: Building PKG (CArchive) out00-PKG.pkg
12884 INFO: Building PKG (CArchive) out00-PKG.pkg completed successfully.
12887 INFO: Bootloader c:\users\conne\appdata\local\programs\python\python36\lib\site-packages\PyInstaller\bootloader\Windows-64bit\runw.exe
12887 INFO: checking EXE
12888 INFO: Building EXE because out00-EXE.toc is non existent
12888 INFO: Building EXE from out00-EXE.toc
12916 INFO: Appending archive to EXE C:\Users\conne\Desktop\python meterpreter\dist\normal.exe
13011 INFO: Building EXE from out00-EXE.toc completed successfully.
</code></pre>

<p>As you can see the exe is created successfully, but it doesn't work as it should when running it.</p>

<p>EDIT:
I ran it with debug enabled and apparently it can't import ctypes, it throws a name error. It only does this when running with pyinstaller, when running as a .py file it works fine.</p>
",5822306,740,09-02-2018 18:39,11-02-2018 13:06,2,740,28,4,11,33,"{'badge_counts': {'bronze': 28, 'silver': 11, 'gold': 4}, 'account_id': 7683842, 'is_employee': False, 'last_modified_date': 1691875501, 'last_access_date': 1710814264, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 740, 'creation_date': 1453394054, 'user_type': 'registered', 'user_id': 5822306, 'accept_rate': 33, 'link': 'https://stackoverflow.com/users/5822306/conner-dassen', 'profile_image': 'https://www.gravatar.com/avatar/691b1d9ea8bb10d55a4da8b76b9d7de2?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Conner Dassen'}","I know this type of question has been asked here before, but I did not find a working solution to it. I have a python file which I want to convert to an exe on Windows 10. I ran , and it outputted a lot of warnings: As you can see the exe is created successfully, but it doesn't work as it should when running it. EDIT: I ran it with debug enabled and apparently it can't import ctypes, it throws a name error. It only does this when running with pyinstaller, when running as a .py file it works fine.","pyinstaller --onefile --noconsole myfile.py C:\Users\conne\Desktop\Python &gt;pyinstaller --onefile --noconsole normal.py
277 INFO: PyInstaller: 3.3.1
277 INFO: Python: 3.6.2
278 INFO: Platform: Windows-10-10.0.15063-SP0
279 INFO: wrote C:\Users\conne\Desktop\Python\normal.spec
280 INFO: UPX is not available.
283 INFO: Extending PYTHONPATH with paths
['C:\\Users\\conne\\Desktop\\python',
 'C:\\Users\\conne\\Desktop\\python']
283 INFO: checking Analysis
284 INFO: Building Analysis because out00-Analysis.toc is non existent
284 INFO: Initializing module dependency graph...
286 INFO: Initializing module graph hooks...
289 INFO: Analyzing base_library.zip ...
5055 INFO: running Analysis out00-Analysis.toc
5058 INFO: Adding Microsoft.Windows.Common-Controls to dependent assemblies of final executable
  required by c:\users\conne\appdata\local\programs\python\python36\python.exe
5145 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5225 WARNING: lib not found: api-ms-win-crt-math-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5298 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5371 WARNING: lib not found: api-ms-win-crt-locale-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5442 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python.exe
5526 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
5598 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
5669 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
5753 WARNING: lib not found: api-ms-win-crt-convert-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
5840 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\VCRUNTIME140.dll
6061 WARNING: lib not found: api-ms-win-crt-time-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6134 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6208 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6282 WARNING: lib not found: api-ms-win-crt-conio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6354 WARNING: lib not found: api-ms-win-crt-environment-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6430 WARNING: lib not found: api-ms-win-crt-math-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6503 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6576 WARNING: lib not found: api-ms-win-crt-convert-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6651 WARNING: lib not found: api-ms-win-crt-locale-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6725 WARNING: lib not found: api-ms-win-crt-process-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6798 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6879 WARNING: lib not found: api-ms-win-crt-filesystem-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\python36.dll
6882 INFO: Caching module hooks...
6892 INFO: Analyzing C:\Users\conne\Desktop\python meterpreter\normal.py
6898 INFO: Loading module hooks...
6899 INFO: Loading module hook ""hook-encodings.py""...
7055 INFO: Loading module hook ""hook-pydoc.py""...
7057 INFO: Loading module hook ""hook-xml.py""...
7351 INFO: Looking for ctypes DLLs
7351 INFO: Analyzing run-time hooks ...
7360 INFO: Looking for dynamic libraries
7439 WARNING: lib not found: api-ms-win-crt-time-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7515 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7589 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7660 WARNING: lib not found: api-ms-win-crt-conio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7731 WARNING: lib not found: api-ms-win-crt-environment-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7813 WARNING: lib not found: api-ms-win-crt-utility-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7895 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
7968 WARNING: lib not found: api-ms-win-crt-convert-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
8051 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
8133 WARNING: lib not found: api-ms-win-crt-filesystem-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_ssl.pyd
8213 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\unicodedata.pyd
8286 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\unicodedata.pyd
8361 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\unicodedata.pyd
8440 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\pyexpat.pyd
8512 WARNING: lib not found: api-ms-win-crt-environment-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\pyexpat.pyd
8585 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\pyexpat.pyd
8658 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\pyexpat.pyd
8741 WARNING: lib not found: api-ms-win-crt-time-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
8815 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
8887 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
8958 WARNING: lib not found: api-ms-win-crt-conio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9032 WARNING: lib not found: api-ms-win-crt-environment-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9116 WARNING: lib not found: api-ms-win-crt-utility-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9194 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9271 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9344 WARNING: lib not found: api-ms-win-crt-convert-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_hashlib.pyd
9428 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9501 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9580 WARNING: lib not found: api-ms-win-crt-math-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9651 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9723 WARNING: lib not found: api-ms-win-crt-stdio-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_bz2.pyd
9801 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_lzma.pyd
9874 WARNING: lib not found: api-ms-win-crt-heap-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_lzma.pyd
9959 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_socket.pyd
10030 WARNING: lib not found: api-ms-win-crt-string-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\_socket.pyd
10115 WARNING: lib not found: api-ms-win-crt-runtime-l1-1-0.dll dependency of c:\users\conne\appdata\local\programs\python\python36\DLLs\select.pyd
10123 INFO: Looking for eggs
10123 INFO: Using Python library c:\users\conne\appdata\local\programs\python\python36\python36.dll
10124 INFO: Found binding redirects:
[]
10128 INFO: Warnings written to C:\Users\conne\Desktop\python\build\normal\warnnormal.txt
10196 INFO: Graph cross-reference written to C:\Users\conne\Desktop\python\build\normal\xref-normal.html
10212 INFO: checking PYZ
10212 INFO: Building PYZ because out00-PYZ.toc is non existent
10213 INFO: Building PYZ (ZlibArchive) C:\Users\conne\Desktop\python\build\normal\out00-PYZ.pyz
10948 INFO: Building PYZ (ZlibArchive) C:\Users\conne\Desktop\python\build\normal\out00-PYZ.pyz completed successfully.
10958 INFO: checking PKG
10958 INFO: Building PKG because out00-PKG.toc is non existent
10959 INFO: Building PKG (CArchive) out00-PKG.pkg
12884 INFO: Building PKG (CArchive) out00-PKG.pkg completed successfully.
12887 INFO: Bootloader c:\users\conne\appdata\local\programs\python\python36\lib\site-packages\PyInstaller\bootloader\Windows-64bit\runw.exe
12887 INFO: checking EXE
12888 INFO: Building EXE because out00-EXE.toc is non existent
12888 INFO: Building EXE from out00-EXE.toc
12916 INFO: Appending archive to EXE C:\Users\conne\Desktop\python meterpreter\dist\normal.exe
13011 INFO: Building EXE from out00-EXE.toc completed successfully.
",102,112,0,0,
200,49806586,50824512,4048,Twine upload TypeError: expected string or bytes-like object,2,<python><twine>,19,"<p>Has anybody got an error like this when you try to upload your package ?</p>

<pre><code>   $ twine upload dist/*
   Uploading distributions to https://upload.pypi.org/legacy/
   Enter your username: MyUsername
   Enter your password: ********
   TypeError: expected string or bytes-like object
</code></pre>

<p>Edit: Got the same error again but this time to fix it I upgraded twine and it started working again.</p>
",5428459,199,12-04-2018 22:06,12-06-2018 19:29,61,199,7,0,2,,"{'badge_counts': {'bronze': 7, 'silver': 2, 'gold': 0}, 'account_id': 7094274, 'is_employee': False, 'last_modified_date': 1573679393, 'last_access_date': 1646222316, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 199, 'creation_date': 1444407174, 'user_type': 'registered', 'user_id': 5428459, 'link': 'https://stackoverflow.com/users/5428459/bruno-lopes', 'profile_image': 'https://www.gravatar.com/avatar/40b87bbb799e083c85b914bbd9318488?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Bruno Lopes'}",Has anybody got an error like this when you try to upload your package ? Edit: Got the same error again but this time to fix it I upgraded twine and it started working again.,"   $ twine upload dist/*
   Uploading distributions to https://upload.pypi.org/legacy/
   Enter your username: MyUsername
   Enter your password: ********
   TypeError: expected string or bytes-like object
",4,10,0,0,
201,49197386,49197435,33301,"pandas Categorical error: ""Cannot setitem on a Categorical with a new category, set the categories first""",1,<python><pandas><categorical-data>,12,"<p>I have the following df data frame in pandas:</p>

<pre><code>    weekday  venta_total_cy
0   Viernes    5.430211e+09
1     Lunes    3.425554e+09
2     Sabado    6.833202e+09
3   Domingo    6.566466e+09
4    Jueves    2.748710e+09
5    Martes    3.328418e+09
6  Miercoles    3.136277e+09
</code></pre>

<p>What I want to do is to order the data frame by the following days' order:</p>

<pre><code>weekday
Lunes
Martes
Miercoles
Jueves
Viernes
Sabado
Domingo
</code></pre>

<p>To do so, I used the following code:</p>

<pre><code>df['weekday'] = pd.Categorical(df[['weekday']], categories=[""Lunes"", ""Martes"", ""Miercoles"", ""Jueves"", ""Viernes"", ""Sabado"", ""Domingo""])
</code></pre>

<p>When I run the code, I get this error:</p>

<pre><code>ValueError: Cannot setitem on a Categorical with a new category, set the categories first
</code></pre>

<p>I have not found enough documentation to resolve this. Can you help me? Thanks!</p>
",5909014,605,09-03-2018 15:47,09-03-2018 15:51,0,605,25,2,10,71,"{'badge_counts': {'bronze': 25, 'silver': 10, 'gold': 2}, 'account_id': 7813550, 'is_employee': False, 'last_modified_date': 1584767403, 'last_access_date': 1690557590, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 605, 'creation_date': 1455117000, 'user_type': 'registered', 'user_id': 5909014, 'accept_rate': 71, 'website_url': '', 'link': 'https://stackoverflow.com/users/5909014/gabriela-m', 'profile_image': 'https://www.gravatar.com/avatar/4fbaed491d9ffb977cf3fc460dfab09e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Gabriela M'}","I have the following df data frame in pandas: What I want to do is to order the data frame by the following days' order: To do so, I used the following code: When I run the code, I get this error: I have not found enough documentation to resolve this. Can you help me? Thanks!","    weekday  venta_total_cy
0   Viernes    5.430211e+09
1     Lunes    3.425554e+09
2     Sabado    6.833202e+09
3   Domingo    6.566466e+09
4    Jueves    2.748710e+09
5    Martes    3.328418e+09
6  Miercoles    3.136277e+09
 weekday
Lunes
Martes
Miercoles
Jueves
Viernes
Sabado
Domingo
 df['weekday'] = pd.Categorical(df[['weekday']], categories=[""Lunes"", ""Martes"", ""Miercoles"", ""Jueves"", ""Viernes"", ""Sabado"", ""Domingo""])
 ValueError: Cannot setitem on a Categorical with a new category, set the categories first
",14,35,0,0,
202,49009870,49009871,425,What are the differences between bool() and operator.truth()?,1,<python><python-3.x><python-2.7><performance><boolean>,16,"<p><a href=""https://docs.python.org/3/library/functions.html#bool"" rel=""noreferrer""><code>bool()</code></a> and <a href=""https://docs.python.org/3/library/operator.html#operator.truth"" rel=""noreferrer""><code>operator.truth()</code></a> both test whether a value is <em>truthy</em> or <em>falsy</em> and they seem rather similar from the docs, it even says in the <code>truth()</code> docs that:</p>

<blockquote>
  <p>This is equivalent to using the bool constructor.</p>
</blockquote>

<p>However, <code>truth()</code> is over twice as fast as <code>bool()</code> from a simple test (Python 3.6 timings shown, but 2.7 is similar):</p>

<pre><code>from timeit import timeit
print(timeit('bool(1)', number=10000000))
# 2.180289956042543
print(timeit('truth(1)', setup='from operator import truth', number=10000000))
# 0.7202018899843097
</code></pre>

<p>So what are the differences? Should I use <code>truth()</code> instead of <code>bool()</code>?</p>

<p>This Q&amp;A arose after extensive comments and discussion with <a href=""https://stackoverflow.com/users/364696/shadowranger"">ShadowRanger</a> under <a href=""https://stackoverflow.com/questions/48909056/creating-a-list-within-a-list-in-python/"">this question</a>.</p>
",6260170,39879,27-02-2018 13:30,27-02-2018 13:30,0,39959,120,14,88,92,"{'badge_counts': {'bronze': 120, 'silver': 88, 'gold': 14}, 'account_id': 8332072, 'is_employee': False, 'last_modified_date': 1703340600, 'last_access_date': 1709677190, 'reputation_change_year': 370, 'reputation_change_quarter': 370, 'reputation_change_month': 120, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 39959, 'creation_date': 1461742221, 'user_type': 'registered', 'user_id': 6260170, 'accept_rate': 92, 'location': 'Cambridge, UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/6260170/chris-rands', 'profile_image': 'https://www.gravatar.com/avatar/9bf6decd8e2acafb6d371c2a1a6f7e1b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Chris_Rands'}","and both test whether a value is truthy or falsy and they seem rather similar from the docs, it even says in the docs that: This is equivalent to using the bool constructor. However, is over twice as fast as from a simple test (Python 3.6 timings shown, but 2.7 is similar): So what are the differences? Should I use instead of ? This Q&amp;A arose after extensive comments and discussion with ShadowRanger under this question.","bool() operator.truth() truth() truth() bool() from timeit import timeit
print(timeit('bool(1)', number=10000000))
# 2.180289956042543
print(timeit('truth(1)', setup='from operator import truth', number=10000000))
# 0.7202018899843097
 truth() bool()",-3,18,0,4,
203,48239242,48239653,5255,Why is Random Forest with a single tree much better than a Decision Tree classifier?,1,<python><machine-learning><scikit-learn><random-forest><decision-tree>,21,"<p>I apply the
decision tree classifier and the random forest classifier to my data with the following code:</p>

<pre class=""lang-python prettyprint-override""><code>def decision_tree(train_X, train_Y, test_X, test_Y):

    clf = tree.DecisionTreeClassifier()
    clf.fit(train_X, train_Y)

    return clf.score(test_X, test_Y)


def random_forest(train_X, train_Y, test_X, test_Y):
    clf = RandomForestClassifier(n_estimators=1)
    clf = clf.fit(X, Y)

    return clf.score(test_X, test_Y)
</code></pre>
<p>Why the result are so much better for the random forest classifier (for 100 runs, with randomly sampling 2/3 of data for the training and 1/3 for the test)?</p>
<pre class=""lang-python prettyprint-override""><code>100%|███████████████████████████████████████| 100/100 [00:01&lt;00:00, 73.59it/s]
Algorithm: Decision Tree
  Min     : 0.3883495145631068
  Max     : 0.6476190476190476
  Mean    : 0.4861783113770316
  Median  : 0.48868030937802126
  Stdev   : 0.047158171852401135
  Variance: 0.0022238931724605985
100%|███████████████████████████████████████| 100/100 [00:01&lt;00:00, 85.38it/s]
Algorithm: Random Forest
  Min     : 0.6846846846846847
  Max     : 0.8653846153846154
  Mean    : 0.7894823428836184
  Median  : 0.7906101571063208
  Stdev   : 0.03231671150915106
  Variance: 0.0010443698427656967
</code></pre>
<p>The random forest estimators with one estimator isn't just a decision tree?
Have i done something wrong or misunderstood the concept?</p>
",6300710,1213,13-01-2018 11:04,13-01-2018 11:59,0,1213,16,0,10,50,"{'badge_counts': {'bronze': 16, 'silver': 10, 'gold': 0}, 'account_id': 8392388, 'is_employee': False, 'last_modified_date': 1661361007, 'last_access_date': 1548925863, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1213, 'creation_date': 1462544296, 'user_type': 'registered', 'user_id': 6300710, 'accept_rate': 50, 'website_url': '', 'link': 'https://stackoverflow.com/users/6300710/hallow-me', 'profile_image': 'https://www.gravatar.com/avatar/395501e3d8013ddef7a4f9ada3650766?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'hallow_me'}","I apply the decision tree classifier and the random forest classifier to my data with the following code: Why the result are so much better for the random forest classifier (for 100 runs, with randomly sampling 2/3 of data for the training and 1/3 for the test)? The random forest estimators with one estimator isn't just a decision tree? Have i done something wrong or misunderstood the concept?","def decision_tree(train_X, train_Y, test_X, test_Y):

    clf = tree.DecisionTreeClassifier()
    clf.fit(train_X, train_Y)

    return clf.score(test_X, test_Y)


def random_forest(train_X, train_Y, test_X, test_Y):
    clf = RandomForestClassifier(n_estimators=1)
    clf = clf.fit(X, Y)

    return clf.score(test_X, test_Y)
 100%|███████████████████████████████████████| 100/100 [00:01&lt;00:00, 73.59it/s]
Algorithm: Decision Tree
  Min     : 0.3883495145631068
  Max     : 0.6476190476190476
  Mean    : 0.4861783113770316
  Median  : 0.48868030937802126
  Stdev   : 0.047158171852401135
  Variance: 0.0022238931724605985
100%|███████████████████████████████████████| 100/100 [00:01&lt;00:00, 85.38it/s]
Algorithm: Random Forest
  Min     : 0.6846846846846847
  Max     : 0.8653846153846154
  Mean    : 0.7894823428836184
  Median  : 0.7906101571063208
  Stdev   : 0.03231671150915106
  Variance: 0.0010443698427656967
",27,37,0,0,
204,48355319,48355586,21571,"Cmd Windows ""python"" command works, but ""python3"" doesn't although my python version is 3.6",2,<python><windows><cmd><windows-10><anaconda>,14,"<p>So I have followed a couple of posts here such as <a href=""https://stackoverflow.com/questions/17953124/python-is-not-recognized-as-an-internal-or-external-command"">this</a>.</p>

<p>So I have installed python 3.6 with Anaconda. Then I went into the PATH and I inserted the path to Python.  </p>

<p>So now when I type into the cmd ""python"" I get the response</p>

<pre><code>Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt;
</code></pre>

<p>However if I type in ""python3"" I get the usual problem </p>

<pre><code>'python3' is not recognized as an internal or external command,
operable program or batch file.
</code></pre>

<p>Why is that?</p>
",6435921,3461,20-01-2018 11:03,20-01-2018 11:28,0,3469,77,9,34,81,"{'badge_counts': {'bronze': 77, 'silver': 34, 'gold': 9}, 'account_id': 7888593, 'is_employee': False, 'last_modified_date': 1711023600, 'last_access_date': 1711054318, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 50, 'reputation_change_week': -2, 'reputation_change_day': 0, 'reputation': 3469, 'creation_date': 1465311013, 'user_type': 'registered', 'user_id': 6435921, 'accept_rate': 81, 'location': 'Bristol, UK', 'link': 'https://stackoverflow.com/users/6435921/euler-salter', 'profile_image': 'https://www.gravatar.com/avatar/743c389e4c05d0573d91f201d9e1f9d6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Euler_Salter'}","So I have followed a couple of posts here such as this. So I have installed python 3.6 with Anaconda. Then I went into the PATH and I inserted the path to Python. So now when I type into the cmd ""python"" I get the response However if I type in ""python3"" I get the usual problem Why is that?","Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt;
 'python3' is not recognized as an internal or external command,
operable program or batch file.
",3,18,0,1,
205,50279402,50279412,1291,Compute co-occurrence matrix by counting values in cells,4,<python><pandas><dataframe>,14,"<p>I have a dataframe like this </p>

<pre><code>df = pd.DataFrame({'a' : [1,1,0,0], 'b': [0,1,1,0], 'c': [0,0,1,1]})
</code></pre>

<p>I want to get </p>

<pre><code>  a b c
a 2 1 0
b 1 2 1
c 0 1 2
</code></pre>

<p>where a,b,c are column names, and I get the values counting '1' in all columns when the filter is '1' in another column.
For ample, when df.a == 1, we count a = 2, b =1, c = 0 etc </p>

<p>I made a loop to solve</p>

<pre><code>matrix = []
for name, values in df.iteritems():
    matrix.append(pd.DataFrame( df.groupby(name, as_index=False).apply(lambda x: x[x == 1].count())).values.tolist()[1])
pd.DataFrame(matrix)
</code></pre>

<p>But I think that there is a simpler solution, isn't it?</p>
",5998425,4543,10-05-2018 18:34,10-05-2018 18:34,0,4543,81,16,48,94,"{'badge_counts': {'bronze': 81, 'silver': 48, 'gold': 16}, 'account_id': 7945862, 'is_employee': False, 'last_modified_date': 1676606451, 'last_access_date': 1708728250, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4543, 'creation_date': 1456760458, 'user_type': 'registered', 'user_id': 5998425, 'accept_rate': 94, 'website_url': 'https://edwvb.blogspot.ru/', 'link': 'https://stackoverflow.com/users/5998425/edward', 'profile_image': 'https://i.stack.imgur.com/6CkM9.jpg?s=256&g=1', 'display_name': 'Edward'}","I have a dataframe like this I want to get where a,b,c are column names, and I get the values counting '1' in all columns when the filter is '1' in another column. For ample, when df.a == 1, we count a = 2, b =1, c = 0 etc I made a loop to solve But I think that there is a simpler solution, isn't it?","df = pd.DataFrame({'a' : [1,1,0,0], 'b': [0,1,1,0], 'c': [0,0,1,1]})
   a b c
a 2 1 0
b 1 2 1
c 0 1 2
 matrix = []
for name, values in df.iteritems():
    matrix.append(pd.DataFrame( df.groupby(name, as_index=False).apply(lambda x: x[x == 1].count())).values.tolist()[1])
pd.DataFrame(matrix)
",6,25,0,0,
206,48421142,48421303,18733,Fastest way to generate a random-like unique string with random length in Python 3,6,<python><string><python-3.x><random>,31,"<p>I know how to create random string, like:</p>

<pre><code>''.join(secrets.choice(string.ascii_uppercase + string.digits) for _ in range(N))
</code></pre>

<p>However, there should be no duplicates so what I am currently just checking if the key already exists in a list, like shown in the following code:</p>

<pre><code>import secrets
import string
import numpy as np


amount_of_keys = 40000

keys = []

for i in range(0,amount_of_keys):
    N = np.random.randint(12,20)
    n_key = ''.join(secrets.choice(string.ascii_uppercase + string.digits) for _ in range(N))
    if not n_key in keys:
        keys.append(n_key)
</code></pre>

<p>Which is okay for a small amount of keys like <code>40000</code>, however the problem does not scale well the more keys there are. So I am wondering if there is a faster way to get to the result for even more keys, like <code>999999</code></p>
",6786718,3623,24-01-2018 11:14,24-01-2018 11:22,0,3623,96,8,47,89,"{'badge_counts': {'bronze': 96, 'silver': 47, 'gold': 8}, 'account_id': 9121419, 'is_employee': False, 'last_modified_date': 1672188000, 'last_access_date': 1670427567, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3623, 'creation_date': 1472804924, 'user_type': 'registered', 'user_id': 6786718, 'accept_rate': 89, 'location': 'Mannheim, Deutschland', 'website_url': '', 'link': 'https://stackoverflow.com/users/6786718/kev1n91', 'profile_image': 'https://i.stack.imgur.com/l8z29.png?s=256&g=1', 'display_name': 'Kev1n91'}","I know how to create random string, like: However, there should be no duplicates so what I am currently just checking if the key already exists in a list, like shown in the following code: Which is okay for a small amount of keys like , however the problem does not scale well the more keys there are. So I am wondering if there is a faster way to get to the result for even more keys, like","''.join(secrets.choice(string.ascii_uppercase + string.digits) for _ in range(N))
 import secrets
import string
import numpy as np


amount_of_keys = 40000

keys = []

for i in range(0,amount_of_keys):
    N = np.random.randint(12,20)
    n_key = ''.join(secrets.choice(string.ascii_uppercase + string.digits) for _ in range(N))
    if not n_key in keys:
        keys.append(n_key)
 40000 999999",11,24,0,0,
207,48692500,48692740,16573,fit-transform on training data and transform on test data,1,<python><scikit-learn>,15,"<p>I am having trouble understanding how exactly <code>transform()</code> and <code>fit_transform()</code> are working together.</p>

<p>I call <code>fit_transform()</code> on my training data set and <code>transform()</code> on my test set afterwards.</p>

<p>However if I call <code>fit_transform()</code> on the test set I get bad results.</p>

<p>Can anybody give me an explanation how and why this occurs?</p>
",6442993,197,08-02-2018 18:32,08-02-2018 18:49,0,197,8,1,1,100,"{'badge_counts': {'bronze': 8, 'silver': 1, 'gold': 1}, 'account_id': 7951307, 'is_employee': False, 'last_modified_date': 1643884200, 'last_access_date': 1689597902, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 197, 'creation_date': 1465432671, 'user_type': 'registered', 'user_id': 6442993, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/6442993/b4shyou', 'profile_image': 'https://graph.facebook.com/1083899141671125/picture?type=large', 'display_name': 'b4shyou'}",I am having trouble understanding how exactly and are working together. I call on my training data set and on my test set afterwards. However if I call on the test set I get bad results. Can anybody give me an explanation how and why this occurs?,transform() fit_transform() fit_transform() transform() fit_transform(),-5,7,0,0,
208,48173798,48175227,15904,additional row colors in seaborn cluster map,3,<python><matplotlib><seaborn>,14,"<p>I am currently generating clustermaps in seaborn and labeling the row colors as below.</p>

<pre><code>matrix = pd.DataFrame(np.random.random_integers(0,1, size=(50,4)))
labels = np.random.random_integers(0,5, size=50)

lut = dict(zip(set(labels), sns.hls_palette(len(set(labels)), l=0.5, s=0.8)))
row_colors = pd.DataFrame(labels)[0].map(lut)

g=sns.clustermap(matrix, col_cluster=False, linewidths=0.1, cmap='coolwarm', row_colors=row_colors)
plt.show()
</code></pre>

<p>I have a second annotation column similar to the labels data I would also like to add to the plot. The seaborn API doesn't support adding a second <code>row_colors</code> column, which is fine, but I am struggling in finding a workaround using matplotlib to add this annotation column to the clustering.</p>

<p>If I cannot use seaborn to do this and have to generate all of this manually using matplotlib that would be fine, I just can't figure that out either.</p>

<p>Thanks for your help!</p>

<p><img src=""https://i.stack.imgur.com/1ingf.png"" alt=""clustermap example plot""></p>
",6008998,490,09-01-2018 17:36,09-01-2018 19:17,0,490,14,2,4,,"{'badge_counts': {'bronze': 14, 'silver': 4, 'gold': 2}, 'account_id': 7961220, 'is_employee': False, 'last_modified_date': 1636633456, 'last_access_date': 1702070669, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 490, 'creation_date': 1456936498, 'user_type': 'registered', 'user_id': 6008998, 'location': 'Houston, TX, United States', 'website_url': 'https://www.canthonyscott.com', 'link': 'https://stackoverflow.com/users/6008998/anthony', 'profile_image': 'https://i.stack.imgur.com/ntVLR.jpg?s=256&g=1', 'display_name': 'Anthony'}","I am currently generating clustermaps in seaborn and labeling the row colors as below. I have a second annotation column similar to the labels data I would also like to add to the plot. The seaborn API doesn't support adding a second column, which is fine, but I am struggling in finding a workaround using matplotlib to add this annotation column to the clustering. If I cannot use seaborn to do this and have to generate all of this manually using matplotlib that would be fine, I just can't figure that out either. Thanks for your help!","matrix = pd.DataFrame(np.random.random_integers(0,1, size=(50,4)))
labels = np.random.random_integers(0,5, size=50)

lut = dict(zip(set(labels), sns.hls_palette(len(set(labels)), l=0.5, s=0.8)))
row_colors = pd.DataFrame(labels)[0].map(lut)

g=sns.clustermap(matrix, col_cluster=False, linewidths=0.1, cmap='coolwarm', row_colors=row_colors)
plt.show()
 row_colors",6,19,1,0,
209,48707117,48707182,31631,Count of elements in lists within pandas data frame,4,<python><pandas>,22,"<p>I need to get the frequency of each element in a list when the list is in a pandas data frame columns </p>

<p>In data:</p>

<pre><code>din=pd.DataFrame({'x':[['a','b','c'],['a','e','d', 'c']]})`

              x
0     [a, b, c]
1  [a, e, d, c]
</code></pre>

<p>Desired Output:</p>

<pre><code>   f  x
0  2  a
1  1  b
2  2  c
3  1  d
4  1  e
</code></pre>

<p>I can expand the list into rows and then perform a group by but this data could be large ( million plus records ) and was wondering if there is a more efficient/direct way.</p>

<p>Thanks</p>
",6039925,1094,09-02-2018 13:36,09-02-2018 13:39,0,1094,19,1,8,,"{'badge_counts': {'bronze': 19, 'silver': 8, 'gold': 1}, 'account_id': 8006865, 'is_employee': False, 'last_modified_date': 1607614499, 'last_access_date': 1706111606, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1094, 'creation_date': 1457533376, 'user_type': 'registered', 'user_id': 6039925, 'location': 'Delhi, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/6039925/gaurav-taneja', 'profile_image': 'https://lh3.googleusercontent.com/-1Db7N7ozoOc/AAAAAAAAAAI/AAAAAAAABEw/wRest7_Wsfs/photo.jpg?sz=256', 'display_name': 'Gaurav Taneja'}",I need to get the frequency of each element in a list when the list is in a pandas data frame columns In data: Desired Output: I can expand the list into rows and then perform a group by but this data could be large ( million plus records ) and was wondering if there is a more efficient/direct way. Thanks,"din=pd.DataFrame({'x':[['a','b','c'],['a','e','d', 'c']]})`

              x
0     [a, b, c]
1  [a, e, d, c]
    f  x
0  2  a
1  1  b
2  2  c
3  1  d
4  1  e
",9,24,0,0,
210,48363057,48363074,20741,How to pass a list by reference?,4,<python><pass-by-reference>,11,"<p>I'm trying to implement a function 'add' that combines the list <code>L1</code> with <code>L2</code> into <code>L3</code>:</p>

<pre><code>def add(L1,L2,L3):
    L3 = L1 + L2

L3 = []
add([1],[0],L3)
print L3
</code></pre>

<p>The code above produces an empty list as a result instead of <code>[1,0]</code> - This means that <code>L3</code> wasn't passed by reference.</p>

<p>How to pass <code>L3</code> by reference?</p>
",6039980,3306,21-01-2018 02:17,21-01-2018 02:21,0,3316,57,8,33,96,"{'badge_counts': {'bronze': 57, 'silver': 33, 'gold': 8}, 'account_id': 6919698, 'is_employee': False, 'last_modified_date': 1695919200, 'last_access_date': 1631978228, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 40, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3316, 'creation_date': 1457534029, 'user_type': 'registered', 'user_id': 6039980, 'accept_rate': 96, 'website_url': '', 'link': 'https://stackoverflow.com/users/6039980/user6039980', 'profile_image': 'https://www.gravatar.com/avatar/6f33a16acf6ad4a1e0d884b567fe7246?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user6039980'}",I'm trying to implement a function 'add' that combines the list with into : The code above produces an empty list as a result instead of - This means that wasn't passed by reference. How to pass by reference?,"L1 L2 L3 def add(L1,L2,L3):
    L3 = L1 + L2

L3 = []
add([1],[0],L3)
print L3
 [1,0] L3 L3",-1,13,0,0,
211,48135994,48136002,16507,Doing the opposite of pivot in pandas Python,1,<python><python-3.x><pandas>,22,"<p>I'm trying to do the opposite of pivot in pandas. Wondering whether someone can provide some assistance? Trying to transform the data from the below</p>

<pre><code>|Date|A|B|C|D|E |
|----|-|-|-|-|--|
|2005|1|2|3|4|50|
|2006|6|7|8|9|10|
</code></pre>

<p>to </p>

<pre><code>|Date|X|Val|
|----|-|---|
|2005|A|1  |
|2005|B|2  |
|2005|C|3  |
etc 
|2006|A|6  |
etc
|2006|E|10 |
</code></pre>

<p>Thank you very much</p>
",5862295,477,07-01-2018 09:47,07-01-2018 09:48,0,487,15,3,6,47,"{'badge_counts': {'bronze': 15, 'silver': 6, 'gold': 3}, 'account_id': 7743545, 'is_employee': False, 'last_modified_date': 1662169800, 'last_access_date': 1675732947, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 487, 'creation_date': 1454189465, 'user_type': 'registered', 'user_id': 5862295, 'accept_rate': 47, 'link': 'https://stackoverflow.com/users/5862295/glencloncurry', 'profile_image': 'https://www.gravatar.com/avatar/07cfc6214bd68618aaaf5b4fb52041df?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'GlenCloncurry'}",I'm trying to do the opposite of pivot in pandas. Wondering whether someone can provide some assistance? Trying to transform the data from the below to Thank you very much,"|Date|A|B|C|D|E |
|----|-|-|-|-|--|
|2005|1|2|3|4|50|
|2006|6|7|8|9|10|
 |Date|X|Val|
|----|-|---|
|2005|A|1  |
|2005|B|2  |
|2005|C|3  |
etc 
|2006|A|6  |
etc
|2006|E|10 |
",11,22,0,0,
212,48716282,48774911,2710,Display more attributes in the decision tree,1,<python><scikit-learn><decision-tree><pygraphviz>,13,"<p>I am currently viewing the decision tree using the following code.  Is there a way that we can export some calculated fields as output too?</p>

<p>For example, is it possible to display the sum of an input attribute at each node, i.e. sum of feature 1 from 'X' data array in the leafs of the tree.</p>

<pre><code>from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:]  
y = iris.target
#%%
from sklearn.tree import DecisionTreeClassifier
alg=DecisionTreeClassifier( max_depth=5,min_samples_leaf=2, max_leaf_nodes = 10)
alg.fit(X,y)

#%%
## View tree
import graphviz
from sklearn import tree
dot_data = tree.export_graphviz(alg,out_file=None, node_ids = True, proportion = True, class_names = True, filled = True, rounded = True)
graph = graphviz.Source(dot_data)
graph
</code></pre>

<p><a href=""https://i.stack.imgur.com/rKDsy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rKDsy.png"" alt=""enter image description here""></a></p>
",6192694,609,10-02-2018 01:17,13-02-2018 20:16,3,609,25,1,8,62,"{'badge_counts': {'bronze': 25, 'silver': 8, 'gold': 1}, 'account_id': 8232398, 'is_employee': False, 'last_modified_date': 1653099300, 'last_access_date': 1680595896, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 609, 'creation_date': 1460455513, 'user_type': 'registered', 'user_id': 6192694, 'accept_rate': 62, 'location': 'Hartford, CT, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/6192694/praveen-gupta-sanka', 'profile_image': 'https://lh4.googleusercontent.com/-IWFUXERsQf8/AAAAAAAAAAI/AAAAAAAAAYk/Bp6x-oOc6pg/photo.jpg?sz=256', 'display_name': 'Praveen Gupta Sanka'}","I am currently viewing the decision tree using the following code. Is there a way that we can export some calculated fields as output too? For example, is it possible to display the sum of an input attribute at each node, i.e. sum of feature 1 from 'X' data array in the leafs of the tree.","from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:]  
y = iris.target
#%%
from sklearn.tree import DecisionTreeClassifier
alg=DecisionTreeClassifier( max_depth=5,min_samples_leaf=2, max_leaf_nodes = 10)
alg.fit(X,y)

#%%
## View tree
import graphviz
from sklearn import tree
dot_data = tree.export_graphviz(alg,out_file=None, node_ids = True, proportion = True, class_names = True, filled = True, rounded = True)
graph = graphviz.Source(dot_data)
graph
",16,24,1,1,
213,49166768,49168623,14455,Setting SECURE_HSTS_SECONDS can irreversibly break your site?,1,<python><django>,41,"<p>I'm wanting to implement <code>SECURE_HSTS_SECONDS</code> to my Django settings for extra security - however the warning from the Django docs is making me abit scared so I want some clarification. Here is what is says:</p>

<p><strong>SECURE_HSTS_SECONDS</strong></p>

<blockquote>
  <p>Default: 0</p>
  
  <p>If set to a non-zero integer value, the SecurityMiddleware sets the HTTP Strict Transport Security header on all responses that do not
  already have it.</p>
  
  <p>Warning:
  Setting this incorrectly can irreversibly (for some time) break your site. Read the HTTP Strict Transport Security documentation first.</p>
</blockquote>

<p>What has to happen for it to ""break my site""? I read the <code>HTTP Strict Transport Security documentation</code> first and it didn't make it any clearer.</p>
",6733153,8627,08-03-2018 06:29,08-03-2018 08:30,0,8647,222,29,113,52,"{'badge_counts': {'bronze': 222, 'silver': 113, 'gold': 29}, 'account_id': 9037970, 'is_employee': False, 'last_modified_date': 1675476600, 'last_access_date': 1710982332, 'reputation_change_year': 260, 'reputation_change_quarter': 260, 'reputation_change_month': 40, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 8647, 'creation_date': 1471577112, 'user_type': 'registered', 'user_id': 6733153, 'accept_rate': 52, 'link': 'https://stackoverflow.com/users/6733153/zorgan', 'profile_image': 'https://www.gravatar.com/avatar/60dd54fc1c49693c1c961f0d3c31f500?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Zorgan'}","I'm wanting to implement to my Django settings for extra security - however the warning from the Django docs is making me abit scared so I want some clarification. Here is what is says: SECURE_HSTS_SECONDS Default: 0 If set to a non-zero integer value, the SecurityMiddleware sets the HTTP Strict Transport Security header on all responses that do not already have it. Warning: Setting this incorrectly can irreversibly (for some time) break your site. Read the HTTP Strict Transport Security documentation first. What has to happen for it to ""break my site""? I read the first and it didn't make it any clearer.",SECURE_HSTS_SECONDS HTTP Strict Transport Security documentation,-2,15,0,0,
214,49887717,49932353,16747,Why would I get a memory error with fast_executemany on a tiny df?,4,<python><sql-server><pandas><sqlalchemy><pyodbc>,13,"<p>I was looking for ways to speed up pushing a dataframe to sql server and stumbled upon an approach <a href=""https://stackoverflow.com/a/48861231/7144068"">here.</a> This approach blew me away in terms of speed. Using normal <code>to_sql</code> took almost 2 hours and this script was done in 12.54 seconds to push a 100k row X 100 column df.</p>

<p>So after testing the code below with a sample df, I attempted to use a df that had many different datatypes (int, string, floats, Booleans). However, I was sad to see a memory error. So I started reducing the size of my df to to see what the limitations were. I noticed that if my df had any strings then I wasn't able to load to sql server. I am having trouble isolating the issue further. The script below is taken from the question in the link, however, I added a tiny df with strings. Any suggestions on how to rectify this issue would be great!</p>

<pre><code>import pandas as pd
import numpy as np
import time
from sqlalchemy import create_engine, event
from urllib.parse import quote_plus
import pyodbc

conn =  ""DRIVER={SQL Server};SERVER=SERVER_IP;DATABASE=DB_NAME;UID=USER_ID;PWD=PWD""
quoted = quote_plus(conn)
new_con = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)
engine = create_engine(new_con)


@event.listens_for(engine, 'before_cursor_execute')
def receive_before_cursor_execute(conn, cursor, statement, params, context, executemany):
    print(""FUNC call"")
    if executemany:
        cursor.fast_executemany = True


table_name = 'fast_executemany_test'
df1 = pd.DataFrame({'col1':['tyrefdg','ertyreg','efdgfdg'],
                   'col2':['tydfggfdgrefdg','erdfgfdgfdgfdgtyreg','edfgfdgdfgdffdgfdg']
                   })



s = time.time()
df1.to_sql(table_name, engine, if_exists = 'replace', chunksize = None)
print(time.time() - s)
</code></pre>
",6205382,2199,17-04-2018 21:24,20-04-2018 01:03,3,2199,52,2,30,,"{'badge_counts': {'bronze': 52, 'silver': 30, 'gold': 2}, 'account_id': 8250583, 'is_employee': False, 'last_modified_date': 1671422617, 'last_access_date': 1711128692, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2199, 'creation_date': 1460653795, 'user_type': 'registered', 'user_id': 6205382, 'website_url': '', 'link': 'https://stackoverflow.com/users/6205382/candlewax', 'profile_image': 'https://www.gravatar.com/avatar/a919cb162a5ef02154de995db426dabf?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'CandleWax'}","I was looking for ways to speed up pushing a dataframe to sql server and stumbled upon an approach here. This approach blew me away in terms of speed. Using normal took almost 2 hours and this script was done in 12.54 seconds to push a 100k row X 100 column df. So after testing the code below with a sample df, I attempted to use a df that had many different datatypes (int, string, floats, Booleans). However, I was sad to see a memory error. So I started reducing the size of my df to to see what the limitations were. I noticed that if my df had any strings then I wasn't able to load to sql server. I am having trouble isolating the issue further. The script below is taken from the question in the link, however, I added a tiny df with strings. Any suggestions on how to rectify this issue would be great!","to_sql import pandas as pd
import numpy as np
import time
from sqlalchemy import create_engine, event
from urllib.parse import quote_plus
import pyodbc

conn =  ""DRIVER={SQL Server};SERVER=SERVER_IP;DATABASE=DB_NAME;UID=USER_ID;PWD=PWD""
quoted = quote_plus(conn)
new_con = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)
engine = create_engine(new_con)


@event.listens_for(engine, 'before_cursor_execute')
def receive_before_cursor_execute(conn, cursor, statement, params, context, executemany):
    print(""FUNC call"")
    if executemany:
        cursor.fast_executemany = True


table_name = 'fast_executemany_test'
df1 = pd.DataFrame({'col1':['tyrefdg','ertyreg','efdgfdg'],
                   'col2':['tydfggfdgrefdg','erdfgfdgfdgfdgtyreg','edfgfdgdfgdffdgfdg']
                   })



s = time.time()
df1.to_sql(table_name, engine, if_exists = 'replace', chunksize = None)
print(time.time() - s)
",28,35,0,1,
215,49545599,49545635,45207,How to turn a Pandas column into array and transpose it?,3,<python><arrays><pandas><numpy><transpose>,12,"<p>I have a Pandas dataframe called 'training_set' that resembles the screenshot below:</p>
<p><a href=""https://i.stack.imgur.com/neApT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/neApT.png"" alt=""enter image description here"" /></a></p>
<p>I try to turn the 'label' column into array and transpose it. I tried doing <code>Y_train=np.asarray(training_set['label'])</code> but what I got is a horizontal array that resembles the screenshot below, which is not what I want.</p>
<p><a href=""https://i.stack.imgur.com/3hMv9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3hMv9.png"" alt=""enter image description here"" /></a></p>
<p>I want the array to display vertically just like the screenshot below (The screenshot has 2 variables per row. My desired output should only contain 1 variable, the 'label', per row.)</p>
<p><a href=""https://i.stack.imgur.com/9Tau6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9Tau6.png"" alt=""enter image description here"" /></a></p>
<p>Any suggestion or help would be greatly appreciated!</p>
",6228056,867,28-03-2018 23:09,28-03-2018 23:12,0,867,33,4,12,86,"{'badge_counts': {'bronze': 33, 'silver': 12, 'gold': 4}, 'account_id': 8284531, 'is_employee': False, 'last_modified_date': 1672344600, 'last_access_date': 1711151334, 'reputation_change_year': -1, 'reputation_change_quarter': -1, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 867, 'creation_date': 1461123852, 'user_type': 'registered', 'user_id': 6228056, 'accept_rate': 86, 'link': 'https://stackoverflow.com/users/6228056/stanleyrr', 'profile_image': 'https://www.gravatar.com/avatar/9dc73fe1df80c53969f2af544e536dd8?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Stanleyrr'}","I have a Pandas dataframe called 'training_set' that resembles the screenshot below: I try to turn the 'label' column into array and transpose it. I tried doing but what I got is a horizontal array that resembles the screenshot below, which is not what I want. I want the array to display vertically just like the screenshot below (The screenshot has 2 variables per row. My desired output should only contain 1 variable, the 'label', per row.) Any suggestion or help would be greatly appreciated!",Y_train=np.asarray(training_set['label']),-1,7,3,3,
216,48364250,48364639,67011,Write to /tmp directory in aws lambda with python,1,<python><amazon-web-services><aws-lambda><errno><tmp>,25,"<p><strong>Goal</strong></p>

<p>I'm trying to write a zip file to the /tmp folder in a python aws lambda, so I can extract manipulate before zipping, and placing it in s3 bucket.</p>

<p><strong>Problem</strong></p>

<p>Os Errno30 Read Only FileSystem</p>

<p>This code was tested locally on my computer to make sure the file would write to my working directory before I uploaded it to aws. This is the code i'm trying to use.</p>

<pre><code>file = downloadFile() #This is api call that returns binary zip object
newFile = open('/tmp/myZip.zip','wb')
newFile.write(file)
extractAll('/tmp/myZip.zip')
</code></pre>

<p>here is the code that is trying to extract the zip file</p>

<pre><code>def extractAll(self,source):
        with zipfile.ZipFile(source, 'r') as archive:
            archive.extractall()
</code></pre>

<p>here is the trace</p>

<pre><code>[Errno 30] Read-only file system: '/var/task/test-deploy': OSError
Traceback (most recent call last):
File ""/var/task/web.py"", line 31, in web
performAction(bb, eventBody)
File ""/var/task/src/api/web.py"", line 42, in performAction
zipHelper.extractAll('/tmp/myZip.zip')
File ""/var/task/src/shared/utils/zipfilehelper.py"", line 24, in extractAll
archive.extractall()
File ""/var/lang/lib/python3.6/zipfile.py"", line 1491, in extractall
self.extract(zipinfo, path, pwd)
File ""/var/lang/lib/python3.6/zipfile.py"", line 1479, in extract
return self._extract_member(member, path, pwd)
File ""/var/lang/lib/python3.6/zipfile.py"", line 1538, in _extract_member
os.mkdir(targetpath)
OSError: [Errno 30] Read-only file system: '/var/task/test-deploy'
</code></pre>
",6257547,558,21-01-2018 06:25,21-01-2018 07:36,0,568,14,1,7,,"{'badge_counts': {'bronze': 14, 'silver': 7, 'gold': 1}, 'account_id': 8328117, 'is_employee': False, 'last_modified_date': 1587473561, 'last_access_date': 1684322266, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 568, 'creation_date': 1461688952, 'user_type': 'registered', 'user_id': 6257547, 'website_url': '', 'link': 'https://stackoverflow.com/users/6257547/codyj110', 'profile_image': 'https://www.gravatar.com/avatar/f083b86fd8afe60dd762bd124790ec47?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Codyj110'}","Goal I'm trying to write a zip file to the /tmp folder in a python aws lambda, so I can extract manipulate before zipping, and placing it in s3 bucket. Problem Os Errno30 Read Only FileSystem This code was tested locally on my computer to make sure the file would write to my working directory before I uploaded it to aws. This is the code i'm trying to use. here is the code that is trying to extract the zip file here is the trace","file = downloadFile() #This is api call that returns binary zip object
newFile = open('/tmp/myZip.zip','wb')
newFile.write(file)
extractAll('/tmp/myZip.zip')
 def extractAll(self,source):
        with zipfile.ZipFile(source, 'r') as archive:
            archive.extractall()
 [Errno 30] Read-only file system: '/var/task/test-deploy': OSError
Traceback (most recent call last):
File ""/var/task/web.py"", line 31, in web
performAction(bb, eventBody)
File ""/var/task/src/api/web.py"", line 42, in performAction
zipHelper.extractAll('/tmp/myZip.zip')
File ""/var/task/src/shared/utils/zipfilehelper.py"", line 24, in extractAll
archive.extractall()
File ""/var/lang/lib/python3.6/zipfile.py"", line 1491, in extractall
self.extract(zipinfo, path, pwd)
File ""/var/lang/lib/python3.6/zipfile.py"", line 1479, in extract
return self._extract_member(member, path, pwd)
File ""/var/lang/lib/python3.6/zipfile.py"", line 1538, in _extract_member
os.mkdir(targetpath)
OSError: [Errno 30] Read-only file system: '/var/task/test-deploy'
",19,41,0,0,
217,48373685,48374003,27620,Keras ImageDataGenerator() how to get all labels from data,5,<python><keras>,12,"<p>I am using the ImageDataGenerator() in Keras and I would like to get the labels of my entire test data.</p>

<p>Currently I am using the following code to accomplish this task:</p>

<pre><code>test_batches = ImageDataGenerator().flow_from_directory(...)

test_labels = []

for i in range(0,3):
    test_labels.extend(np.array(test_batches[i][1]))
</code></pre>

<p>This code however only works because I know I have a total of 150 images and my batch size is defined to be 50.</p>

<p>Moreover using:</p>

<pre><code>imgs, labels = next(test_batches)
</code></pre>

<p>as suggested in similar posts on this topic only returns labels for one batch and not the entire dataset. As such I wonder if there is a more efficient way of doing this than the method I am using above.</p>
",6341510,4010,22-01-2018 01:45,22-01-2018 02:41,0,4010,74,8,33,77,"{'badge_counts': {'bronze': 74, 'silver': 33, 'gold': 8}, 'account_id': 8453370, 'is_employee': False, 'last_modified_date': 1710399300, 'last_access_date': 1671451170, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4010, 'creation_date': 1463412360, 'user_type': 'registered', 'user_id': 6341510, 'accept_rate': 77, 'location': 'Hamburg, Deutschland', 'link': 'https://stackoverflow.com/users/6341510/aarondt', 'profile_image': 'https://www.gravatar.com/avatar/10338d1d79c5f747bd7f45b9740b5acf?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'AaronDT'}",I am using the ImageDataGenerator() in Keras and I would like to get the labels of my entire test data. Currently I am using the following code to accomplish this task: This code however only works because I know I have a total of 150 images and my batch size is defined to be 50. Moreover using: as suggested in similar posts on this topic only returns labels for one batch and not the entire dataset. As such I wonder if there is a more efficient way of doing this than the method I am using above.,"test_batches = ImageDataGenerator().flow_from_directory(...)

test_labels = []

for i in range(0,3):
    test_labels.extend(np.array(test_batches[i][1]))
 imgs, labels = next(test_batches)
",5,20,0,0,
218,49153714,49153997,6228,"Why is the size of 2⁶³ 36 bytes, but 2⁶³-1 is only 24 bytes?",2,<python><python-2.7><cpython><python-internals>,51,"<p>Everything in Python is an object. So the size of an int in Python will be larger than usual.</p>

<pre><code>&gt;&gt;&gt; sys.getsizeof(int())
24
</code></pre>

<p>OK, but why does it take 12 more bytes for <code>2⁶³</code> compared too <code>2⁶³ - 1</code> and not just one?</p>

<pre><code>&gt;&gt;&gt; sys.getsizeof(2**63)
36
&gt;&gt;&gt; sys.getsizeof(2**62)
24
</code></pre>

<p>I get that <code>2⁶³</code> is a long and <code>2⁶³-1</code> an int, but why 12 bytes of difference?</p>

<p>No more intuitive, I tried some other things:</p>

<pre><code>&gt;&gt;&gt; a = 2**63
&gt;&gt;&gt; a -= 2**62
&gt;&gt;&gt; sys.getsizeof(a)
36
</code></pre>

<p><code>a</code> is still stored as a long even if it could be in an int now. So that's not surprising. But:</p>

<pre><code>&gt;&gt;&gt; a -= (2**63 - 1)
&gt;&gt;&gt; a = 2**63
&gt;&gt;&gt; a -= (2**63 - 1)
&gt;&gt;&gt; a
1L
&gt;&gt;&gt; sys.getsizeof(a)
28
</code></pre>

<p>A new size.</p>

<pre><code>&gt;&gt;&gt; a = 2**63
&gt;&gt;&gt; a -= 2**63
&gt;&gt;&gt; a
0L
&gt;&gt;&gt; sys.getsizeof(a)
24
</code></pre>

<p>Back to 24 bytes, but still with a long.</p>

<p>Last thing I got:</p>

<pre><code>&gt;&gt;&gt; sys.getsizeof(long())
24
</code></pre>

<p><strong>Question:</strong></p>

<p>How does the memory storage work in those scenarios?</p>

<p><strong>Sub-questions:</strong></p>

<p>Why is there a gap of 12 bytes to add what our intuition tells us is just 1 bit?</p>

<p>Why are <code>int()</code> and <code>long()</code> 24 bytes, but <code>long(1)</code> is already 28 bytes and <code>int(2⁶²)</code>?</p>

<p><em>NB: Python 3.X is working a bit differently, but not more intuitively. Here I focused on Python 2.7; I did not test on prior versions.</em></p>
",6345404,1548,07-03-2018 13:58,07-03-2018 14:13,0,1548,36,2,18,86,"{'badge_counts': {'bronze': 36, 'silver': 18, 'gold': 2}, 'account_id': 8459050, 'is_employee': False, 'last_modified_date': 1703316600, 'last_access_date': 1655819761, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1548, 'creation_date': 1463484190, 'user_type': 'registered', 'user_id': 6345404, 'accept_rate': 86, 'location': 'Paris, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/6345404/t-nel', 'profile_image': 'https://www.gravatar.com/avatar/70c11a3f84fafdce82780a5bc426454b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'T.Nel'}","Everything in Python is an object. So the size of an int in Python will be larger than usual. OK, but why does it take 12 more bytes for compared too and not just one? I get that is a long and an int, but why 12 bytes of difference? No more intuitive, I tried some other things: is still stored as a long even if it could be in an int now. So that's not surprising. But: A new size. Back to 24 bytes, but still with a long. Last thing I got: Question: How does the memory storage work in those scenarios? Sub-questions: Why is there a gap of 12 bytes to add what our intuition tells us is just 1 bit? Why are and 24 bytes, but is already 28 bytes and ? NB: Python 3.X is working a bit differently, but not more intuitively. Here I focused on Python 2.7; I did not test on prior versions.","&gt;&gt;&gt; sys.getsizeof(int())
24
 2⁶³ 2⁶³ - 1 &gt;&gt;&gt; sys.getsizeof(2**63)
36
&gt;&gt;&gt; sys.getsizeof(2**62)
24
 2⁶³ 2⁶³-1 &gt;&gt;&gt; a = 2**63
&gt;&gt;&gt; a -= 2**62
&gt;&gt;&gt; sys.getsizeof(a)
36
 a &gt;&gt;&gt; a -= (2**63 - 1)
&gt;&gt;&gt; a = 2**63
&gt;&gt;&gt; a -= (2**63 - 1)
&gt;&gt;&gt; a
1L
&gt;&gt;&gt; sys.getsizeof(a)
28
 &gt;&gt;&gt; a = 2**63
&gt;&gt;&gt; a -= 2**63
&gt;&gt;&gt; a
0L
&gt;&gt;&gt; sys.getsizeof(a)
24
 &gt;&gt;&gt; sys.getsizeof(long())
24
 int() long() long(1) int(2⁶²)",10,64,0,0,
219,49027872,49027927,18137,Convert pandas.DataFrame to list of dictionaries in Python,5,<python><json><pandas><dictionary><dataframe>,15,"<p>I have a dictionary which is converted from a dataframe as below :</p>

<pre><code>a = d.to_json(orient='index')
</code></pre>

<p>Dictionary  :</p>

<pre><code>{""0"":{""yr"":2017,""PKID"":""58306, 57011"",""Subject"":""ABC"",""ID"":""T001""},""1"":{""yr"":2018,""PKID"":""1234,54321"",""Subject"":""XYZ"",""ID"":""T002""}}
</code></pre>

<p>What I need is it be in a list, so essentially a list of dictionary.
So i just add a [] because that is the format to be used in the rest of the code.</p>

<pre><code>input_dict = [a]
</code></pre>

<p>input_dict  :</p>

<pre><code>['
{""0"":{""yr"":2017,""PKID"":""58306, 57011"",""Subject"":""ABC"",""ID"":""T001""},""1"":{""yr"":2018,""PKID"":""1234,54321"",""Subject"":""XYZ"",""ID"":""T002""}}
']
</code></pre>

<p>I need to get the single quotes removed just after the [ and just before the ]. Also, have the PKID values in form of list.</p>

<p>How can this be achieved ?</p>

<p>Expected Output :</p>

<pre><code>[ {""yr"":2017,""PKID"":[58306, 57011],""Subject"":""ABC"",""ID"":""T001""},""1"":{""yr"":2018,""PKID"":[1234,54321],""Subject"":""XYZ"",""ID"":""T002""} ]
</code></pre>

<p>NOTE : The PKID column has multiple integer values which have to come as a lift of integers. a string is not acceptable.
so we need like ""PKID"":[58306, 57011] and not ""PKID"":""[58306, 57011]""</p>
",6374424,451,28-02-2018 10:55,28-02-2018 10:58,0,451,22,1,4,27,"{'badge_counts': {'bronze': 22, 'silver': 4, 'gold': 1}, 'account_id': 8501667, 'is_employee': False, 'last_modified_date': 1620995700, 'last_access_date': 1670433222, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 451, 'creation_date': 1464070356, 'user_type': 'registered', 'user_id': 6374424, 'accept_rate': 27, 'location': 'Bangalore, Karnataka, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/6374424/shankar-pandey', 'profile_image': 'https://graph.facebook.com/10156948647540022/picture?type=large', 'display_name': 'Shankar Pandey'}","I have a dictionary which is converted from a dataframe as below : Dictionary : What I need is it be in a list, so essentially a list of dictionary. So i just add a [] because that is the format to be used in the rest of the code. input_dict : I need to get the single quotes removed just after the [ and just before the ]. Also, have the PKID values in form of list. How can this be achieved ? Expected Output : NOTE : The PKID column has multiple integer values which have to come as a lift of integers. a string is not acceptable. so we need like ""PKID"":[58306, 57011] and not ""PKID"":""[58306, 57011]""","a = d.to_json(orient='index')
 {""0"":{""yr"":2017,""PKID"":""58306, 57011"",""Subject"":""ABC"",""ID"":""T001""},""1"":{""yr"":2018,""PKID"":""1234,54321"",""Subject"":""XYZ"",""ID"":""T002""}}
 input_dict = [a]
 ['
{""0"":{""yr"":2017,""PKID"":""58306, 57011"",""Subject"":""ABC"",""ID"":""T001""},""1"":{""yr"":2018,""PKID"":""1234,54321"",""Subject"":""XYZ"",""ID"":""T002""}}
']
 [ {""yr"":2017,""PKID"":[58306, 57011],""Subject"":""ABC"",""ID"":""T001""},""1"":{""yr"":2018,""PKID"":[1234,54321],""Subject"":""XYZ"",""ID"":""T002""} ]
",2,34,0,0,
220,48975383,48975437,17579,"why to use "" | safe"" in jinja2 Python",2,<python><flask><jinja2>,11,"<p>I am following a Flask tutorial where he is using "" | safe "" in jinja2 template. Why do we need this pipe symbol and safe?</p>

<p>without using safe it prints all html tags.</p>

<p>By using <code>| safe</code>, it shows proper formatting.  Why does it work this way?</p>

<p>Below is the jinja2 code:</p>

<pre><code>{% extends ""layout.html"" %}

{% block body %}
    &lt;h1&gt;{{article.title}}&lt;/h1&gt;
    &lt;small&gt;Written by {{article.author}} on {{article.create_date}}&lt;/small&gt;
    &lt;hr&gt;
    &lt;div&gt;
        {{article.body | safe}}
    &lt;/div&gt;
{% endblock %}
</code></pre>
",6401229,321,25-02-2018 15:52,25-02-2018 15:58,0,321,14,1,3,,"{'badge_counts': {'bronze': 14, 'silver': 3, 'gold': 1}, 'account_id': 8541144, 'is_employee': False, 'last_modified_date': 1639461346, 'last_access_date': 1697695955, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 321, 'creation_date': 1464624912, 'user_type': 'registered', 'user_id': 6401229, 'location': 'Pakistan', 'website_url': '', 'link': 'https://stackoverflow.com/users/6401229/muhammad-arslan-maqsood', 'profile_image': 'https://graph.facebook.com/1178440878847282/picture?type=large', 'display_name': 'Muhammad Arslan Maqsood'}","I am following a Flask tutorial where he is using "" | safe "" in jinja2 template. Why do we need this pipe symbol and safe? without using safe it prints all html tags. By using , it shows proper formatting. Why does it work this way? Below is the jinja2 code:","| safe {% extends ""layout.html"" %}

{% block body %}
    &lt;h1&gt;{{article.title}}&lt;/h1&gt;
    &lt;small&gt;Written by {{article.author}} on {{article.create_date}}&lt;/small&gt;
    &lt;hr&gt;
    &lt;div&gt;
        {{article.body | safe}}
    &lt;/div&gt;
{% endblock %}
",8,19,0,0,
221,49693560,49697288,7570,Removing the white border around an image when using matplotlib without saving the image,1,<python><matplotlib>,11,"<p>I have the following code:</p>

<pre><code>#load in image
image = cv2.imread('lenna.png')
title = ""foo""

ax = plt.axes([0,0,1,1])
ax.clear()
height, width = image.shape[:2]
ax.axis('off')
ax.set_title(title)

#some things plotted etc.
</code></pre>

<p>But then I need the figure as numpy array for further computation, so I am doing the following:</p>

<pre><code>masked_image = image
ax.imshow(masked_image.astype(np.uint8),interpolation=""nearest"")
ax.figure.canvas.draw()
w,h = ax.figure.get_size_inches()*ax.figure.get_dpi()
I = np.fromstring(ax.figure.canvas.tostring_rgb(),dtype=np.uint8).reshape(int(h),int(w),3)

#Has the white borders around it
Image.fromarray(I)
</code></pre>

<p>However,  <code>I</code>  still has now white borders around it, is there an easy way to remove the white borders without saving the figure?</p>

<p>The image I used is the following:</p>

<p><a href=""https://i.stack.imgur.com/nL4u3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nL4u3.png"" alt=""enter image description here""></a></p>

<p>which does not have any white borders around it</p>

<p>However after the code above, it looks like the following:
<a href=""https://i.stack.imgur.com/8rwB5.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8rwB5.png"" alt=""enter image description here""></a></p>

<p>Which now has white bars around it. </p>

<p>Other already posted solution to this are all relying on saving the image, which I do not want </p>
",6786718,3623,06-04-2018 13:07,06-04-2018 16:30,0,3623,96,8,47,89,"{'badge_counts': {'bronze': 96, 'silver': 47, 'gold': 8}, 'account_id': 9121419, 'is_employee': False, 'last_modified_date': 1672188000, 'last_access_date': 1670427567, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3623, 'creation_date': 1472804924, 'user_type': 'registered', 'user_id': 6786718, 'accept_rate': 89, 'location': 'Mannheim, Deutschland', 'website_url': '', 'link': 'https://stackoverflow.com/users/6786718/kev1n91', 'profile_image': 'https://i.stack.imgur.com/l8z29.png?s=256&g=1', 'display_name': 'Kev1n91'}","I have the following code: But then I need the figure as numpy array for further computation, so I am doing the following: However, still has now white borders around it, is there an easy way to remove the white borders without saving the figure? The image I used is the following: which does not have any white borders around it However after the code above, it looks like the following: Which now has white bars around it. Other already posted solution to this are all relying on saving the image, which I do not want","#load in image
image = cv2.imread('lenna.png')
title = ""foo""

ax = plt.axes([0,0,1,1])
ax.clear()
height, width = image.shape[:2]
ax.axis('off')
ax.set_title(title)

#some things plotted etc.
 masked_image = image
ax.imshow(masked_image.astype(np.uint8),interpolation=""nearest"")
ax.figure.canvas.draw()
w,h = ax.figure.get_size_inches()*ax.figure.get_dpi()
I = np.fromstring(ax.figure.canvas.tostring_rgb(),dtype=np.uint8).reshape(int(h),int(w),3)

#Has the white borders around it
Image.fromarray(I)
 I",16,41,2,2,
222,49556209,49557515,20078,python side_effect - mocking behavior of a method,1,<python><mocking><python-unittest><side-effects>,12,"<p>In the mock, I want a certain function to return a new value in the test. This is how i did it. </p>

<pre><code>Class MyClass:

      my_var = None  

      def foo(self, var1):
          return somevalue

      def bar(self):
          my_var = foo(1)

Class TestClass(unittest.TestCase):
      myClass = MyClass() 

      def _side_effect_foo(var1):
           if condition:
                return new_value

      @patch(""MyClass"", ""foo"", side_effect='_side_effect_foo')
      def test_foo(self):
           self.myClass.bar()
</code></pre>

<p>This gives me a error: </p>

<blockquote>
  <p>Can't pass kwargs to a mock we aren't creating.</p>
</blockquote>

<p>Am I using the correct format of side_effect?</p>

<p>Thanks for any help!</p>
",6797337,313,29-03-2018 12:33,29-03-2018 13:35,0,313,12,1,4,,"{'badge_counts': {'bronze': 12, 'silver': 4, 'gold': 1}, 'account_id': 9138591, 'is_employee': False, 'last_modified_date': 1682517122, 'last_access_date': 1554641038, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 313, 'creation_date': 1473098186, 'user_type': 'registered', 'user_id': 6797337, 'location': 'Netherlands', 'link': 'https://stackoverflow.com/users/6797337/jieke-wei', 'profile_image': 'https://graph.facebook.com/1791738637768832/picture?type=large', 'display_name': 'Jieke Wei'}","In the mock, I want a certain function to return a new value in the test. This is how i did it. This gives me a error: Can't pass kwargs to a mock we aren't creating. Am I using the correct format of side_effect? Thanks for any help!","Class MyClass:

      my_var = None  

      def foo(self, var1):
          return somevalue

      def bar(self):
          my_var = foo(1)

Class TestClass(unittest.TestCase):
      myClass = MyClass() 

      def _side_effect_foo(var1):
           if condition:
                return new_value

      @patch(""MyClass"", ""foo"", side_effect='_side_effect_foo')
      def test_foo(self):
           self.myClass.bar()
",19,33,0,0,
223,48184442,48185247,4260,Printing without parentheses varying error message using Python 3,4,<python><python-3.x><syntax-error><python-internals>,29,"<p>When I try to use <code>print</code> without parentheses on a simple name in Python 3.4 I get:</p>

<pre><code>&gt;&gt;&gt; print max
Traceback (most recent call last):
  ...
  File ""&lt;interactive input&gt;"", line 1
    print max
            ^
SyntaxError: Missing parentheses in call to 'print'
</code></pre>

<p>Ok, now I get it, I just forgot to port my Python 2 code.</p>

<p>But now when I try to print the result of a function:</p>

<pre><code>&gt;&gt;&gt; print max([1,2])
Traceback (most recent call last):
    ...
    print max([1,2])
            ^
SyntaxError: invalid syntax
</code></pre>

<p>Or:</p>

<pre><code>print max.__call__(23)
        ^
SyntaxError: invalid syntax
</code></pre>

<p>(Note that the cursor is pointing to the character before the first dot in that case.)</p>

<p>The message is different (and slightly misleading, since the marker is below the <code>max</code> function).</p>

<p>Why isn't Python able to detect the problem earlier?</p>

<p>Note: This question was inspired by the confusion around this question: <em><a href=""https://stackoverflow.com/questions/48184208/pandas-read-csv-syntax-error"">Pandas read.csv syntax error</a></em>, where a few Python experts missed the real issue because of the misleading error message.</p>
",6451573,138384,10-01-2018 09:45,10-01-2018 10:27,0,138542,227,23,163,90,"{'badge_counts': {'bronze': 227, 'silver': 163, 'gold': 23}, 'account_id': 8614021, 'is_employee': False, 'last_modified_date': 1711089000, 'last_access_date': 1711178945, 'reputation_change_year': 646, 'reputation_change_quarter': 646, 'reputation_change_month': 199, 'reputation_change_week': 77, 'reputation_change_day': -4, 'reputation': 138542, 'creation_date': 1465586393, 'user_type': 'moderator', 'user_id': 6451573, 'accept_rate': 90, 'location': 'Toulouse, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/6451573/jean-fran%c3%a7ois-fabre', 'profile_image': 'https://i.stack.imgur.com/Z5c6A.jpg?s=256&g=1', 'display_name': 'Jean-Fran&#231;ois Fabre'}","When I try to use without parentheses on a simple name in Python 3.4 I get: Ok, now I get it, I just forgot to port my Python 2 code. But now when I try to print the result of a function: Or: (Note that the cursor is pointing to the character before the first dot in that case.) The message is different (and slightly misleading, since the marker is below the function). Why isn't Python able to detect the problem earlier? Note: This question was inspired by the confusion around this question: Pandas read.csv syntax error, where a few Python experts missed the real issue because of the misleading error message.","print &gt;&gt;&gt; print max
Traceback (most recent call last):
  ...
  File ""&lt;interactive input&gt;"", line 1
    print max
            ^
SyntaxError: Missing parentheses in call to 'print'
 &gt;&gt;&gt; print max([1,2])
Traceback (most recent call last):
    ...
    print max([1,2])
            ^
SyntaxError: invalid syntax
 print max.__call__(23)
        ^
SyntaxError: invalid syntax
 max",11,37,0,1,
224,49814258,49838010,30582,statsmodel AttributeError: module 'scipy.stats' has no attribute 'chisqprob',3,<python><scipy><statsmodels>,16,"<p>I'm running the code below with statsmodel 0.8.0 which i believe is the latest. </p>

<pre><code>import statsmodels.api as sm
est = sm.Logit(y_train, x_train)
result = est.fit()
print(result.summary())
</code></pre>

<p>This is giving me an error saying:</p>

<p>AttributeError: module 'scipy.stats' has no attribute 'chisqprob'.</p>

<p>I dont seem to be able to find anything on stackoverflow or elsewhere to resolve this. Any help much appreciated.</p>
",6468053,1348,13-04-2018 09:58,15-04-2018 02:44,2,1358,35,3,19,93,"{'badge_counts': {'bronze': 35, 'silver': 19, 'gold': 3}, 'account_id': 8638808, 'is_employee': False, 'last_modified_date': 1705108202, 'last_access_date': 1709365069, 'reputation_change_year': 52, 'reputation_change_quarter': 52, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1358, 'creation_date': 1465974418, 'user_type': 'registered', 'user_id': 6468053, 'accept_rate': 93, 'location': 'UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/6468053/a-rob4', 'profile_image': 'https://www.gravatar.com/avatar/57af1b5a8eb8f17ccf47d515fdda24de?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'A Rob4'}",I'm running the code below with statsmodel 0.8.0 which i believe is the latest. This is giving me an error saying: AttributeError: module 'scipy.stats' has no attribute 'chisqprob'. I dont seem to be able to find anything on stackoverflow or elsewhere to resolve this. Any help much appreciated.,"import statsmodels.api as sm
est = sm.Logit(y_train, x_train)
result = est.fit()
print(result.summary())
",3,13,0,0,
225,50149085,50149522,75402,Python Airflow - Return result from PythonOperator,1,<python><airflow>,27,"<p>I have written a DAG with multiple PythonOperators </p>

<pre><code>task1 = af_op.PythonOperator(task_id='Data_Extraction_Environment',
                          provide_context=True,
                          python_callable=Task1, dag=dag1)

def Task1(**kwargs):
    return(kwargs['dag_run'].conf.get('file'))
</code></pre>

<p>From PythonOperator i am calling ""Task1"" method. That method is returning a value,that value i need to pass to the next PythonOperator.How can i get the value from the ""task1"" variable or How can i get the value which is returned from Task1 method?</p>

<p>updated :</p>

<pre><code>    def Task1(**kwargs):
          file_name = kwargs['dag_run'].conf.get[file]
          task_instance = kwargs['task_instance']
          task_instance.xcom_push(key='file', value=file_name) 
          return file_name

  t1 = PythonOperator(task_id = 'Task1',provide_context=True,python_callable=Task1,dag=dag)

  t2 =   BashOperator(
      task_id='Moving_bucket', 
      bash_command='python /home/raw.py {{ task_instance.xcom_pull(task_ids='Task1',key='file') }} ',
      dag=dag,
    )

t2.set_upstream(t1)
</code></pre>
",6490241,897,03-05-2018 07:18,03-05-2018 07:43,0,907,24,3,14,15,"{'badge_counts': {'bronze': 24, 'silver': 14, 'gold': 3}, 'account_id': 8671438, 'is_employee': False, 'last_modified_date': 1590827400, 'last_access_date': 1632978660, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 907, 'creation_date': 1466440948, 'user_type': 'registered', 'user_id': 6490241, 'accept_rate': 15, 'location': 'Bangalore, Karnataka, India', 'link': 'https://stackoverflow.com/users/6490241/teja', 'profile_image': 'https://www.gravatar.com/avatar/5369c0d971f09f2ff612e4b3e3aceb3e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Teja'}","I have written a DAG with multiple PythonOperators From PythonOperator i am calling ""Task1"" method. That method is returning a value,that value i need to pass to the next PythonOperator.How can i get the value from the ""task1"" variable or How can i get the value which is returned from Task1 method? updated :","task1 = af_op.PythonOperator(task_id='Data_Extraction_Environment',
                          provide_context=True,
                          python_callable=Task1, dag=dag1)

def Task1(**kwargs):
    return(kwargs['dag_run'].conf.get('file'))
     def Task1(**kwargs):
          file_name = kwargs['dag_run'].conf.get[file]
          task_instance = kwargs['task_instance']
          task_instance.xcom_push(key='file', value=file_name) 
          return file_name

  t1 = PythonOperator(task_id = 'Task1',provide_context=True,python_callable=Task1,dag=dag)

  t2 =   BashOperator(
      task_id='Moving_bucket', 
      bash_command='python /home/raw.py {{ task_instance.xcom_pull(task_ids='Task1',key='file') }} ',
      dag=dag,
    )

t2.set_upstream(t1)
",19,30,0,0,
226,49275868,49276459,18502,How to filter JSON Array in Django JSONField,4,<python><django><postgresql><django-models>,18,"<p>i am getting crazy with filtering a (postgres) JSONField in Django 2.0.3.
The json is stored as an array. E.g.</p>

<pre><code>tasks = [{""task"":""test"",""level"":""10""},{""task"":""test 123"",""level"":""20""}]
</code></pre>

<p>What i've tried:</p>

<pre><code>myModel.objects.filter(""tasks__task__contains""=""test"")
myModel.objects.filter(""tasks_task__0__contains""=""test"")
myModel.objects.filter(""tasks__0__task__contains""=""test"")
myModel.objects.filter(""tasks_task__0_x__contains""=""test"")
myModel.objects.filter(""tasks__0_x__task__contains""=""test"")
</code></pre>

<p>What goes wrong?
What i want to do is a icontains - but as i already read there is not support for icontains on jsonfields in Django right now...</p>
",6490893,922,14-03-2018 10:54,14-03-2018 11:21,0,922,28,1,10,20,"{'badge_counts': {'bronze': 28, 'silver': 10, 'gold': 1}, 'account_id': 8672410, 'is_employee': False, 'last_modified_date': 1709950200, 'last_access_date': 1711047642, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 922, 'creation_date': 1466451975, 'user_type': 'registered', 'user_id': 6490893, 'accept_rate': 20, 'location': 'Freiburg im Breisgau, Deutschland', 'website_url': '', 'link': 'https://stackoverflow.com/users/6490893/nrhode', 'profile_image': 'https://i.stack.imgur.com/ZVEtF.jpg?s=256&g=1', 'display_name': 'nrhode'}",i am getting crazy with filtering a (postgres) JSONField in Django 2.0.3. The json is stored as an array. E.g. What i've tried: What goes wrong? What i want to do is a icontains - but as i already read there is not support for icontains on jsonfields in Django right now...,"tasks = [{""task"":""test"",""level"":""10""},{""task"":""test 123"",""level"":""20""}]
 myModel.objects.filter(""tasks__task__contains""=""test"")
myModel.objects.filter(""tasks_task__0__contains""=""test"")
myModel.objects.filter(""tasks__0__task__contains""=""test"")
myModel.objects.filter(""tasks_task__0_x__contains""=""test"")
myModel.objects.filter(""tasks__0_x__task__contains""=""test"")
",4,17,0,0,
227,48780324,48781606,190872,Flask - Bad Request The browser (or proxy) sent a request that this server could not understand,1,<python><mongodb><flask><pymongo>,35,"<p>I'm trying to do a file upload &amp; enter data task into my MongoDB using flask
but I had this error when I filled my form &amp; upload the image:</p>

<blockquote>
  <p><strong>Bad Request</strong>
  The browser (or proxy) sent a request that this server could not understand.</p>
</blockquote>

<p>my HTML code</p>

<pre><code>    &lt;form class=""form-check form-control"" method=""post"" enctype=""multipart/form-data"" action=""{{ url_for('index') }}""&gt;      
          &lt;label&gt;Full Name*&lt;/label&gt;&lt;/td&gt;
          &lt;input name=""u_name"" type=""text"" class=""text-info my-input"" required=""required"" /&gt;
          &lt;label&gt;Email*&lt;/label&gt;
          &lt;input name=""u_email"" type=""email"" class=""text-info my-input"" required=""required"" /&gt;
          &lt;label&gt;Password*&lt;/label&gt;
          &lt;input name=""u_pass"" type=""password"" class=""text-info my-input"" required=""required"" /&gt;
          &lt;label&gt;Your Image*&lt;/label&gt;
          &lt;input name=""u_img"" type=""file"" class=""text-info"" required=""required"" /&gt;&lt;/td&gt;
          &lt;input name=""btn_submit"" type=""submit"" class=""btn-info"" /&gt;
    &lt;/form&gt;
</code></pre>

<p>&amp; my python code:</p>

<pre><code>from flask import Flask, render_template, request, url_for
from flask_pymongo import PyMongo
import os
app = Flask(__name__)
app.config['MONGO_DBNAME'] = 'flask_assignment'
app.config['MONGO_URI'] = 'mongodb://&lt;user&gt;:&lt;pass&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;'
mongo = PyMongo(app)
app_root = os.path.dirname(os.path.abspath(__file__))


@app.route('/', methods=['GET', 'POST'])
def index():
    target = os.path.join(app_root, 'static/img/')
    if not os.path.isdir(target):
        os.mkdir(target)
    if request.method == 'POST':
        name = request.form['u_name']
        password = request.form['u_pass']
        email = request.form['u_email']
        file_name = ''
        for file in request.form['u_img']:
            file_name = file.filename
            destination = '/'.join([target, file_name])
            file.save(destination)
        mongo.db.employee_entry.insert({'name': name, 'password': password, 'email': email, 'img_name': file_name})
        return render_template('index.html')
    else:
        return render_template('index.html')

app.run(debug=True)
</code></pre>
",6495610,629,14-02-2018 05:37,14-02-2018 07:15,0,629,21,4,14,78,"{'badge_counts': {'bronze': 21, 'silver': 14, 'gold': 4}, 'account_id': 8679111, 'is_employee': False, 'last_modified_date': 1693997096, 'last_access_date': 1703105724, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 629, 'creation_date': 1466532669, 'user_type': 'registered', 'user_id': 6495610, 'accept_rate': 78, 'location': 'Pakistan', 'website_url': '', 'link': 'https://stackoverflow.com/users/6495610/zohaib', 'profile_image': 'https://i.stack.imgur.com/ynvrn.png?s=256&g=1', 'display_name': 'Zohaib'}",I'm trying to do a file upload &amp; enter data task into my MongoDB using flask but I had this error when I filled my form &amp; upload the image: Bad Request The browser (or proxy) sent a request that this server could not understand. my HTML code &amp; my python code:,"    &lt;form class=""form-check form-control"" method=""post"" enctype=""multipart/form-data"" action=""{{ url_for('index') }}""&gt;      
          &lt;label&gt;Full Name*&lt;/label&gt;&lt;/td&gt;
          &lt;input name=""u_name"" type=""text"" class=""text-info my-input"" required=""required"" /&gt;
          &lt;label&gt;Email*&lt;/label&gt;
          &lt;input name=""u_email"" type=""email"" class=""text-info my-input"" required=""required"" /&gt;
          &lt;label&gt;Password*&lt;/label&gt;
          &lt;input name=""u_pass"" type=""password"" class=""text-info my-input"" required=""required"" /&gt;
          &lt;label&gt;Your Image*&lt;/label&gt;
          &lt;input name=""u_img"" type=""file"" class=""text-info"" required=""required"" /&gt;&lt;/td&gt;
          &lt;input name=""btn_submit"" type=""submit"" class=""btn-info"" /&gt;
    &lt;/form&gt;
 from flask import Flask, render_template, request, url_for
from flask_pymongo import PyMongo
import os
app = Flask(__name__)
app.config['MONGO_DBNAME'] = 'flask_assignment'
app.config['MONGO_URI'] = 'mongodb://&lt;user&gt;:&lt;pass&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;'
mongo = PyMongo(app)
app_root = os.path.dirname(os.path.abspath(__file__))


@app.route('/', methods=['GET', 'POST'])
def index():
    target = os.path.join(app_root, 'static/img/')
    if not os.path.isdir(target):
        os.mkdir(target)
    if request.method == 'POST':
        name = request.form['u_name']
        password = request.form['u_pass']
        email = request.form['u_email']
        file_name = ''
        for file in request.form['u_img']:
            file_name = file.filename
            destination = '/'.join([target, file_name])
            file.save(destination)
        mongo.db.employee_entry.insert({'name': name, 'password': password, 'email': email, 'img_name': file_name})
        return render_template('index.html')
    else:
        return render_template('index.html')

app.run(debug=True)
",39,56,0,0,
228,49670832,49679189,18485,Keras lstm with masking layer for variable-length inputs,1,<python><keras><lstm><masking>,18,"<p>I know this is a subject with a lot of questions but I couldn't find any solution to my problem.</p>

<p>I am training a LSTM network on variable-length inputs using a masking layer but it seems that it doesn't have any effect.</p>

<p>Input shape (100, 362, 24) with 362 being the maximum sequence lenght, 24 the number of features and 100 the number of samples (divided 75 train / 25 valid).</p>

<p>Output shape (100, 362, 1) transformed later to (100, 362 - N, 1).</p>

<p>Here is the code for my network:</p>

<pre><code>from keras import Sequential
from keras.layers import Embedding, Masking, LSTM, Lambda
import keras.backend as K


#                          O O O
#   example for N:3        | | |
#                    O O O O O O
#                    | | | | | | 
#                    O O O O O O

N = 5
y= y[:,N:,:]

x_train = x[:75]
x_test = x[75:]
y_train = y[:75]
y_test = y[75:]

model = Sequential()
model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
model.add(LSTM(128, return_sequences=True))
model.add(LSTM(64, return_sequences=True))
model.add(LSTM(1, return_sequences=True))
model.add(Lambda(lambda x: x[:, N:, :]))

model.compile('adam', 'mae')

print(model.summary())
history = model.fit(x_train, y_train, 
                    epochs=3, 
                    batch_size=15, 
                    validation_data=[x_test, y_test])
</code></pre>

<p>my data is padded at the end. example:</p>

<pre><code>&gt;&gt; x_test[10,350]
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
   0., 0., 0., 0., 0., 0., 0.], dtype=float32)
</code></pre>

<p>The problem is that the mask layer seems to have no effect. I can see it with the loss value being printed during training which is equal to the one without mask I calculate after:</p>

<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 362, 24)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 362, 128)          78336     
_________________________________________________________________
lstm_2 (LSTM)                (None, 362, 64)           49408     
_________________________________________________________________
lstm_3 (LSTM)                (None, 362, 1)            264       
_________________________________________________________________
lambda_1 (Lambda)            (None, 357, 1)            0         
=================================================================
Total params: 128,008
Trainable params: 128,008
Non-trainable params: 0
_________________________________________________________________
None
Train on 75 samples, validate on 25 samples
Epoch 1/3
75/75 [==============================] - 8s 113ms/step - loss: 0.1711 - val_loss: 0.1814
Epoch 2/3
75/75 [==============================] - 5s 64ms/step - loss: 0.1591 - val_loss: 0.1307
Epoch 3/3
75/75 [==============================] - 5s 63ms/step - loss: 0.1057 - val_loss: 0.1034

&gt;&gt; from sklearn.metrics import mean_absolute_error
&gt;&gt; out = model.predict(x_test, batch_size=1)
&gt;&gt; print('wo mask', mean_absolute_error(y_test.ravel(), out.ravel()))
&gt;&gt; print('w mask', mean_absolute_error(y_test[~(x_test[:,N:] == 0).all(axis=2)].ravel(), out[~(x_test[:,N:] == 0).all(axis=2)].ravel()))
wo mask 0.10343371
w mask 0.16236152
</code></pre>

<p>Futhermore, if I use nan value for the masked output values, I can see the nan being propagated during training (loss equals nan).</p>

<p>What am I missing to make the masking layer work as expected?</p>
",6077035,1054,05-04-2018 11:05,05-04-2018 18:32,0,1054,13,1,6,,"{'badge_counts': {'bronze': 13, 'silver': 6, 'gold': 1}, 'account_id': 8061675, 'is_employee': False, 'last_modified_date': 1607614498, 'last_access_date': 1710346517, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1054, 'creation_date': 1458218202, 'user_type': 'registered', 'user_id': 6077035, 'link': 'https://stackoverflow.com/users/6077035/florian-mutel', 'profile_image': 'https://www.gravatar.com/avatar/ae42a1fe0b9db9523cce0ba7ced792b6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Florian Mutel'}","I know this is a subject with a lot of questions but I couldn't find any solution to my problem. I am training a LSTM network on variable-length inputs using a masking layer but it seems that it doesn't have any effect. Input shape (100, 362, 24) with 362 being the maximum sequence lenght, 24 the number of features and 100 the number of samples (divided 75 train / 25 valid). Output shape (100, 362, 1) transformed later to (100, 362 - N, 1). Here is the code for my network: my data is padded at the end. example: The problem is that the mask layer seems to have no effect. I can see it with the loss value being printed during training which is equal to the one without mask I calculate after: Futhermore, if I use nan value for the masked output values, I can see the nan being propagated during training (loss equals nan). What am I missing to make the masking layer work as expected?","from keras import Sequential
from keras.layers import Embedding, Masking, LSTM, Lambda
import keras.backend as K


#                          O O O
#   example for N:3        | | |
#                    O O O O O O
#                    | | | | | | 
#                    O O O O O O

N = 5
y= y[:,N:,:]

x_train = x[:75]
x_test = x[75:]
y_train = y[:75]
y_test = y[75:]

model = Sequential()
model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
model.add(LSTM(128, return_sequences=True))
model.add(LSTM(64, return_sequences=True))
model.add(LSTM(1, return_sequences=True))
model.add(Lambda(lambda x: x[:, N:, :]))

model.compile('adam', 'mae')

print(model.summary())
history = model.fit(x_train, y_train, 
                    epochs=3, 
                    batch_size=15, 
                    validation_data=[x_test, y_test])
 &gt;&gt; x_test[10,350]
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
   0., 0., 0., 0., 0., 0., 0.], dtype=float32)
 Layer (type)                 Output Shape              Param #   
=================================================================
masking_1 (Masking)          (None, 362, 24)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 362, 128)          78336     
_________________________________________________________________
lstm_2 (LSTM)                (None, 362, 64)           49408     
_________________________________________________________________
lstm_3 (LSTM)                (None, 362, 1)            264       
_________________________________________________________________
lambda_1 (Lambda)            (None, 357, 1)            0         
=================================================================
Total params: 128,008
Trainable params: 128,008
Non-trainable params: 0
_________________________________________________________________
None
Train on 75 samples, validate on 25 samples
Epoch 1/3
75/75 [==============================] - 8s 113ms/step - loss: 0.1711 - val_loss: 0.1814
Epoch 2/3
75/75 [==============================] - 5s 64ms/step - loss: 0.1591 - val_loss: 0.1307
Epoch 3/3
75/75 [==============================] - 5s 63ms/step - loss: 0.1057 - val_loss: 0.1034

&gt;&gt; from sklearn.metrics import mean_absolute_error
&gt;&gt; out = model.predict(x_test, batch_size=1)
&gt;&gt; print('wo mask', mean_absolute_error(y_test.ravel(), out.ravel()))
&gt;&gt; print('w mask', mean_absolute_error(y_test[~(x_test[:,N:] == 0).all(axis=2)].ravel(), out[~(x_test[:,N:] == 0).all(axis=2)].ravel()))
wo mask 0.10343371
w mask 0.16236152
",64,90,0,0,
229,50092954,50093247,46970,import opencv vs import cv2,2,<python><macos><opencv><pip>,16,"<p>I recently installed opencv-python-3.4.0.12 via pip install on Mac OS. When I run the Python interpreter, <code>import cv2</code> works fine whereas <code>import opencv</code> raises ModuleNotFoundError. However, when I run the Python3 Interpreter, <code>import opencv</code> works fine whereas <code>import cv2</code> raises ModuleNotFoundError. </p>

<p>What is the reason behind this difference and on a related note, should I use <code>import opencv</code> or <code>import cv2</code>? Does <code>cv2</code> refer to OpenCV version 2?</p>
",6497930,474,30-04-2018 01:36,30-04-2018 02:27,0,474,16,2,5,,"{'badge_counts': {'bronze': 16, 'silver': 5, 'gold': 2}, 'account_id': 8682521, 'is_employee': False, 'last_modified_date': 1609639136, 'last_access_date': 1600869391, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 474, 'creation_date': 1466581623, 'user_type': 'registered', 'user_id': 6497930, 'website_url': '', 'link': 'https://stackoverflow.com/users/6497930/stan25', 'profile_image': 'https://www.gravatar.com/avatar/808d258219ac65cfbf819ac8f8ae330d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'stan25'}","I recently installed opencv-python-3.4.0.12 via pip install on Mac OS. When I run the Python interpreter, works fine whereas raises ModuleNotFoundError. However, when I run the Python3 Interpreter, works fine whereas raises ModuleNotFoundError. What is the reason behind this difference and on a related note, should I use or ? Does refer to OpenCV version 2?",import cv2 import opencv import opencv import cv2 import opencv import cv2 cv2,-7,3,0,0,
230,48138632,48138759,52368,"In Python, what is `sys.maxsize`?",2,<python><memory><max-size>,46,"<p>I assumed that this number ( <code>2^63 - 1</code> ) was the maximum value python could handle, or store as a variable. But these commands seem to be working fine:</p>
<pre><code>&gt;&gt;&gt; sys.maxsize
9223372036854775807
&gt;&gt;&gt; a=sys.maxsize + 1
&gt;&gt;&gt; a 
9223372036854775808
</code></pre>
<p>So is there any significance at all? Can Python handle arbitrarily large numbers, if computation resoruces permitt?</p>
<p>Note, here's the print-out of my version is:</p>
<pre><code>&gt;&gt;&gt; sys.version
3.5.2 |Anaconda custom (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]'
</code></pre>
",6092721,955,07-01-2018 15:27,07-01-2018 15:41,0,955,25,1,10,8,"{'badge_counts': {'bronze': 25, 'silver': 10, 'gold': 1}, 'account_id': 8084838, 'is_employee': False, 'last_modified_date': 1654914600, 'last_access_date': 1659285786, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 955, 'creation_date': 1458552504, 'user_type': 'registered', 'user_id': 6092721, 'accept_rate': 8, 'link': 'https://stackoverflow.com/users/6092721/christopher-turnbull', 'profile_image': 'https://i.stack.imgur.com/rGp52.jpg?s=256&g=1', 'display_name': 'Christopher Turnbull'}","I assumed that this number ( ) was the maximum value python could handle, or store as a variable. But these commands seem to be working fine: So is there any significance at all? Can Python handle arbitrarily large numbers, if computation resoruces permitt? Note, here's the print-out of my version is:","2^63 - 1 &gt;&gt;&gt; sys.maxsize
9223372036854775807
&gt;&gt;&gt; a=sys.maxsize + 1
&gt;&gt;&gt; a 
9223372036854775808
 &gt;&gt;&gt; sys.version
3.5.2 |Anaconda custom (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]'
",4,12,0,0,
231,49831163,50357371,23952,Compile Python 3.6 script to standalone exe with Nuitka on Windows 10,4,<python><windows-10><exe><python-3.6><nuitka>,15,"<p><b>Note:</b><br>
Before marking this question as duplicate, please verify that the other question answers the topic for this setup:</p>

<ul>
<li>OS: Windows 10, 64-bit</li>
<li>Python version: 3.6 or higher</li>
<li>Python Compiler: Nuitka, development version 0.5.30rc5</li>
<li>MSVC compiler: Visual Studio 2017 Community, vcvars64.bat</li>
</ul>

<p>&nbsp;<br></p>

<h2>1. How I build my exe</h2>

<p>I'll first explain how I build my executable. Suppose I have a folder with a simple python script that I want to build:</p>

<p><a href=""https://i.stack.imgur.com/xRSni.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xRSni.png"" alt=""enter image description here""></a></p>

<p>The <code>buildscript.py</code> looks like this:</p>

<pre><code>#####################################################
#               NUITKA BUILD SCRIPT                 #
#####################################################
# Author: Matic Kukovec
# Date: April 2018

import os
import platform


NUITKA = ""C:/Python36/Scripts/nuitka3-script.py""  # Path where my nuitka3-script.py is
CWD = os.getcwd().replace(""\\"", ""/"")
MSVC = ""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Auxiliary/Build/vcvars64.bat""
PYTHON_VERSION = ""3.6""
PYTHON_EXE_PATH= ""C:/Python36/python.exe""
NUMBER_OF_CORES_FOR_COMPILATION = 1 # 1 is the safest choice, but you can try more

# Generate command
command = '""{}"" amd64 &amp;'.format(MSVC)
command += ""{} "".format(PYTHON_EXE_PATH)
command += ""{} "".format(NUITKA)
command += ""--python-version={} "".format(PYTHON_VERSION)
command += ""--output-dir={}/output "".format(CWD)
command += ""--verbose ""
command += ""--jobs={} "".format(NUMBER_OF_CORES_FOR_COMPILATION)
command += ""--show-scons ""
# command += ""--windows-disable-console ""
# command += ""--icon={}/myicon.ico "".format(CWD)
command += ""--standalone ""
# command += ""--run ""
command += ""{}/cubeimporter.py "".format(CWD)
os.system(command)

print(""END"")
</code></pre>

<p>&nbsp;<br></p>

<h2>2. Result of the build</h2>

<p>After the build finishes, the folder looks like this (see picture below). As you can see, there are plenty of other files sitting next to the executable. I can see <code>.dll</code> and <code>.pyd</code> files.</p>

<p><a href=""https://i.stack.imgur.com/KYc3j.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KYc3j.png"" alt=""enter image description here""></a></p>

<p>&nbsp;<br></p>

<h2>3. Desired result</h2>

<p>I wish I could build just a standalone executable. No dll- or other files needed. When I put the executable on a thumb drive and stick it into another computer (running Windows 10, 64-bit), it should just work. Even if there is no Python installed on that computer.</p>

<p>Is this possible with Nuitka?<br>
If no, is it possible with another Python compiler?<br>
Please provide all the steps needed, one-by-one :-)</p>
",6178507,8519,14-04-2018 11:56,15-05-2018 18:54,31,8529,145,20,84,92,"{'badge_counts': {'bronze': 145, 'silver': 84, 'gold': 20}, 'account_id': 8210034, 'is_employee': False, 'last_modified_date': 1706742302, 'last_access_date': 1710922140, 'reputation_change_year': 170, 'reputation_change_quarter': 170, 'reputation_change_month': 50, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 8529, 'creation_date': 1460138984, 'user_type': 'registered', 'user_id': 6178507, 'accept_rate': 92, 'location': 'Leuven', 'website_url': 'https://chipdesk.com', 'link': 'https://stackoverflow.com/users/6178507/k-mulier', 'profile_image': 'https://i.stack.imgur.com/5VkoH.jpg?s=256&g=1', 'display_name': 'K.Mulier'}","Note: Before marking this question as duplicate, please verify that the other question answers the topic for this setup: OS: Windows 10, 64-bit Python version: 3.6 or higher Python Compiler: Nuitka, development version 0.5.30rc5 MSVC compiler: Visual Studio 2017 Community, vcvars64.bat &nbsp; 1. How I build my exe I'll first explain how I build my executable. Suppose I have a folder with a simple python script that I want to build: The looks like this: &nbsp; 2. Result of the build After the build finishes, the folder looks like this (see picture below). As you can see, there are plenty of other files sitting next to the executable. I can see and files. &nbsp; 3. Desired result I wish I could build just a standalone executable. No dll- or other files needed. When I put the executable on a thumb drive and stick it into another computer (running Windows 10, 64-bit), it should just work. Even if there is no Python installed on that computer. Is this possible with Nuitka? If no, is it possible with another Python compiler? Please provide all the steps needed, one-by-one :-)","buildscript.py #####################################################
#               NUITKA BUILD SCRIPT                 #
#####################################################
# Author: Matic Kukovec
# Date: April 2018

import os
import platform


NUITKA = ""C:/Python36/Scripts/nuitka3-script.py""  # Path where my nuitka3-script.py is
CWD = os.getcwd().replace(""\\"", ""/"")
MSVC = ""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Auxiliary/Build/vcvars64.bat""
PYTHON_VERSION = ""3.6""
PYTHON_EXE_PATH= ""C:/Python36/python.exe""
NUMBER_OF_CORES_FOR_COMPILATION = 1 # 1 is the safest choice, but you can try more

# Generate command
command = '""{}"" amd64 &amp;'.format(MSVC)
command += ""{} "".format(PYTHON_EXE_PATH)
command += ""{} "".format(NUITKA)
command += ""--python-version={} "".format(PYTHON_VERSION)
command += ""--output-dir={}/output "".format(CWD)
command += ""--verbose ""
command += ""--jobs={} "".format(NUMBER_OF_CORES_FOR_COMPILATION)
command += ""--show-scons ""
# command += ""--windows-disable-console ""
# command += ""--icon={}/myicon.ico "".format(CWD)
command += ""--standalone ""
# command += ""--run ""
command += ""{}/cubeimporter.py "".format(CWD)
os.system(command)

print(""END"")
 .dll .pyd",30,73,2,2,
232,49536332,49536386,1459,How to sum a column grouped by other columns in a list?,6,<python><list><pandas><dataframe><pandas-groupby>,15,"<p>I have a list as follows.</p>

<pre><code>[['Andrew', '1', '9'], ['Peter', '1', '10'], ['Andrew', '1', '8'], ['Peter', '1', '11'], ['Sam', '4', '9'], ['Andrew', '2', '2']]
</code></pre>

<p>I would like sum up the last column grouped by the other columns.The result is like this</p>

<pre><code>[['Andrew', '1', '17'], ['Peter', '1', '21'], ['Sam', '4', '9'], ['Andrew', '2', '2']]
</code></pre>

<p>which is still a list.</p>

<p>In real practice, I would always like to sum up the last column grouped by many other columns. Is there a way I can do this in Python? Much appreciated.</p>
",6181207,317,28-03-2018 13:48,28-03-2018 13:51,0,317,8,0,1,,"{'badge_counts': {'bronze': 8, 'silver': 1, 'gold': 0}, 'account_id': 7328537, 'is_employee': False, 'last_modified_date': 1581120324, 'last_access_date': 1696429233, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 317, 'creation_date': 1460210784, 'user_type': 'registered', 'user_id': 6181207, 'link': 'https://stackoverflow.com/users/6181207/deepleeqe', 'profile_image': 'https://www.gravatar.com/avatar/922647be643fe6b93d879b2aa6050bac?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Deepleeqe'}","I have a list as follows. I would like sum up the last column grouped by the other columns.The result is like this which is still a list. In real practice, I would always like to sum up the last column grouped by many other columns. Is there a way I can do this in Python? Much appreciated.","[['Andrew', '1', '9'], ['Peter', '1', '10'], ['Andrew', '1', '8'], ['Peter', '1', '11'], ['Sam', '4', '9'], ['Andrew', '2', '2']]
 [['Andrew', '1', '17'], ['Peter', '1', '21'], ['Sam', '4', '9'], ['Andrew', '2', '2']]
",0,13,0,0,
233,48716184,48746165,6609,How can I download and skip VGG weights that have no counterpart with my CNN in Keras?,2,<python><machine-learning><deep-learning><keras><pytorch>,11,"<p>I would like to follow the Convolutional Neural Net (CNN) approach <a href=""https://github.com/nshaud/DeepNetsForEO/blob/master/SegNet_PyTorch_v2.ipynb"" rel=""noreferrer"">here</a>. However, this code in github uses Pytorch, whereas I am using Keras. </p>

<p>I want to reproduce boxes 6,7 and 8 where pre-trained weights from VGG-16 on ImageNet is downloaded and is used to make the CNN converge faster.</p>

<p>In particular, there is a portion (box 8) where weights are downloaded and skipped from VGG-16 that have no counterpart in <code>SegNet</code> (the CNN model). In my work, I am using a CNN model called <code>U-Net</code> instead of <code>Segnet</code>. The <code>U-Net</code> Keras code that I am using can be found <a href=""https://github.com/SUZhaoyu/keras-semantic-segmentation/blob/develop/src/semseg/models/unet.py"" rel=""noreferrer"">here</a>.</p>

<p>I am new to Keras and would appreciate any insight in Keras code on how I can go about downloading and skipping the VGG weights that have no counterpart with my <code>U-Net</code>model.</p>
",6185403,911,10-02-2018 01:01,12-02-2018 12:07,2,911,39,4,23,97,"{'badge_counts': {'bronze': 39, 'silver': 23, 'gold': 4}, 'account_id': 8220803, 'is_employee': False, 'last_modified_date': 1705108800, 'last_access_date': 1674169513, 'reputation_change_year': 42, 'reputation_change_quarter': 42, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 911, 'creation_date': 1460319999, 'user_type': 'registered', 'user_id': 6185403, 'accept_rate': 97, 'website_url': '', 'link': 'https://stackoverflow.com/users/6185403/user121', 'profile_image': 'https://www.gravatar.com/avatar/d98be8bd20590a104f21139f482c3cb5?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user121'}","I would like to follow the Convolutional Neural Net (CNN) approach here. However, this code in github uses Pytorch, whereas I am using Keras. I want to reproduce boxes 6,7 and 8 where pre-trained weights from VGG-16 on ImageNet is downloaded and is used to make the CNN converge faster. In particular, there is a portion (box 8) where weights are downloaded and skipped from VGG-16 that have no counterpart in (the CNN model). In my work, I am using a CNN model called instead of . The Keras code that I am using can be found here. I am new to Keras and would appreciate any insight in Keras code on how I can go about downloading and skipping the VGG weights that have no counterpart with my model.",SegNet U-Net Segnet U-Net U-Net,-5,7,0,2,
234,49775502,49775808,18909,WebDriverWait not working as expected,1,<python><selenium><web-scraping><webdriverwait><expected-condition>,18,"<p>I am working with selenium to scrape some data.</p>

<p>There is button on the page that I am clicking say ""custom_cols"". This button opens up a window for me where I can select my columns. </p>

<p>This new window sometimes takes some time to open (around 5 seconds). So to handle this I have used </p>

<pre><code>WebDriverWait 
</code></pre>

<p>with delay as 20 seconds. But some times it fails to select find elements on new window, even if the element is visible. This happens only once in ten times for rest of time it works properly.</p>

<p>I have used same function(WebDriverWait) on other places also and it is works as expected. I mean it waits till the elements gets visible and then clicks it at the moment it finds it.</p>

<p>My question is why elements on new window is not visible even though I am waiting for element to get visible. To add here I have tried to increase delay time but still I get that error once in a while.</p>

<p>My code is here </p>

<pre><code>def wait_for_elem_xpath(self, delay = None, xpath = """"):
    if delay is None:
        delay = self.delay

    try:
        myElem = WebDriverWait(self.browser, delay).until(EC.presence_of_element_located((By.XPATH , xpath)))
    except TimeoutException:
        print (""xpath: Loading took too much time!"")
    return myElem
select_all_performance = '//*[@id=""mks""]/body/div[7]/div[2]/div/div/div/div/div[2]/div/div[2]/div[2]/div/div[1]/div[1]/section/header/div'
self.wait_for_elem_xpath(xpath = select_all_performance).click()
</code></pre>
",6770735,1221,11-04-2018 12:43,11-04-2018 12:58,0,1221,12,1,11,,"{'badge_counts': {'bronze': 12, 'silver': 11, 'gold': 1}, 'account_id': 9097179, 'is_employee': False, 'last_modified_date': 1573678949, 'last_access_date': 1606271649, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1221, 'creation_date': 1472482740, 'user_type': 'registered', 'user_id': 6770735, 'location': 'Gurgaon, Haryana, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/6770735/rao-sahab', 'profile_image': 'https://i.stack.imgur.com/ycSX2.png?s=256&g=1', 'display_name': 'Rao Sahab'}","I am working with selenium to scrape some data. There is button on the page that I am clicking say ""custom_cols"". This button opens up a window for me where I can select my columns. This new window sometimes takes some time to open (around 5 seconds). So to handle this I have used with delay as 20 seconds. But some times it fails to select find elements on new window, even if the element is visible. This happens only once in ten times for rest of time it works properly. I have used same function(WebDriverWait) on other places also and it is works as expected. I mean it waits till the elements gets visible and then clicks it at the moment it finds it. My question is why elements on new window is not visible even though I am waiting for element to get visible. To add here I have tried to increase delay time but still I get that error once in a while. My code is here","WebDriverWait 
 def wait_for_elem_xpath(self, delay = None, xpath = """"):
    if delay is None:
        delay = self.delay

    try:
        myElem = WebDriverWait(self.browser, delay).until(EC.presence_of_element_located((By.XPATH , xpath)))
    except TimeoutException:
        print (""xpath: Loading took too much time!"")
    return myElem
select_all_performance = '//*[@id=""mks""]/body/div[7]/div[2]/div/div/div/div/div[2]/div/div[2]/div[2]/div/div[1]/div[1]/section/header/div'
self.wait_for_elem_xpath(xpath = select_all_performance).click()
",10,29,0,0,
235,50134468,50134667,10261,Convert boolean numpy array to pillow image,2,<python><numpy><image-processing><python-imaging-library><scikit-image>,11,"<p>I'm currently working with image processing in python using the scikit-image library. I'm trying to make a binary image using sauvola thresholding with the following code:</p>

<pre><code>from PIL import Image
import numpy
from skimage.color import rgb2gray
from skimage.filters import threshold_sauvola

im = Image.open(""test.jpg"")
pix = numpy.array(im)
img = rgb2gray(pix)

window_size = 25
thresh_sauvola = threshold_sauvola(img, window_size=window_size)
binary_sauvola = img &gt; thresh_sauvola
</code></pre>

<p>Which gives the following result:
<a href=""https://i.stack.imgur.com/Pfpjl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Pfpjl.png"" alt=""enter image description here""></a></p>

<p>the output is a numpy array with data type of this image is a bool </p>

<pre><code>[[ True  True  True ...  True  True  True]
 [ True  True  True ...  True  True  True]
 [ True  True  True ...  True  True  True]
 ...
 [ True  True  True ...  True  True  True]
 [ True  True  True ...  True  True  True]
 [ True  True  True ...  True  True  True]]
</code></pre>

<p>The problem is that I need to convert this array back to a PIL image using the following line of code:</p>

<pre><code>image = Image.fromarray(binary_sauvola)
</code></pre>

<p>which makes the image look like this:</p>

<p><a href=""https://i.stack.imgur.com/NMzZo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NMzZo.png"" alt=""enter image description here""></a></p>

<p>I also tried to change the data type from bool to uint8 but then I'll get the following exception:</p>

<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'mask'
</code></pre>

<p>So far I haven't found a solution to get a PIL image which looks like the result of the thresholding.</p>
",6778685,345,02-05-2018 12:16,02-05-2018 12:27,0,345,19,2,4,86,"{'badge_counts': {'bronze': 19, 'silver': 4, 'gold': 2}, 'account_id': 9109386, 'is_employee': False, 'last_modified_date': 1573678947, 'last_access_date': 1589955340, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 345, 'creation_date': 1472645488, 'user_type': 'registered', 'user_id': 6778685, 'accept_rate': 86, 'website_url': '', 'link': 'https://stackoverflow.com/users/6778685/r-hagens', 'profile_image': 'https://www.gravatar.com/avatar/b567bc90910c8510c30ba247988000f4?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'R.hagens'}",I'm currently working with image processing in python using the scikit-image library. I'm trying to make a binary image using sauvola thresholding with the following code: Which gives the following result: the output is a numpy array with data type of this image is a bool The problem is that I need to convert this array back to a PIL image using the following line of code: which makes the image look like this: I also tried to change the data type from bool to uint8 but then I'll get the following exception: So far I haven't found a solution to get a PIL image which looks like the result of the thresholding.,"from PIL import Image
import numpy
from skimage.color import rgb2gray
from skimage.filters import threshold_sauvola

im = Image.open(""test.jpg"")
pix = numpy.array(im)
img = rgb2gray(pix)

window_size = 25
thresh_sauvola = threshold_sauvola(img, window_size=window_size)
binary_sauvola = img &gt; thresh_sauvola
 [[ True  True  True ...  True  True  True]
 [ True  True  True ...  True  True  True]
 [ True  True  True ...  True  True  True]
 ...
 [ True  True  True ...  True  True  True]
 [ True  True  True ...  True  True  True]
 [ True  True  True ...  True  True  True]]
 image = Image.fromarray(binary_sauvola)
 AttributeError: 'numpy.ndarray' object has no attribute 'mask'
",17,45,2,2,
236,49951768,49951820,25192,replace index values in pandas dataframe with values from list,3,<python><list><pandas><loops><dataframe>,16,"<p>I have a dataframe and 2 lists.</p>

<p>the 1st list gives a set of index values from the dataframe I want to replace</p>

<p>the 2nd list gives the values I want to use</p>

<p>I don't want to touch any of the other values</p>

<p>Here is the dataframe:</p>

<pre><code>df =  pd.DataFrame.from_dict({u'Afghanistan': 6532.0,
 u'Albania': 662.0,
 u'Andorra': 2.0,
 u'Angola': 2219.0,
 u'Antigua and Barbuda': 0.0,
 u'Argentina': 6.0,
 u'Armenia': 15.0,
 u'Australia': 108.0,
 u'Azerbaijan': 210.0,
 u'Bahamas': 0.0,
 u'Bahrain': 6.0,
 u'Bangladesh': 5098.0,
 u'Barbados': 0.0,
 u'Belarus': 21.0,
 u'Belize': 0.0,
 u'Benin': 4244.0,
 u'Bhutan': 418.0,
 u'Bolivia (Plurinational State of)': 122.0,
 u'Bosnia and Herzegovina': 43.0,
 u'Botswana': 2672.0,
 u'Brazil': 36.0,
 u'Brunei Darussalam': 42.0,
 u'Bulgaria': 46.0,
 u'Burkina Faso': 6074.0,
 u'Burundi': 18363.0,
 u'Cabo Verde': 2.0,
 u'Cambodia': 12237.0,
 u'Cameroon': 14629.0,
 u'Canada': 206.0,
 u'Central African Republic': 3207.0,
 u'Chad': 3546.0,
 u'Chile': 0.0,
 u'China': 71093.0,
 u'Colombia': 1.0,
 u'Congo': 1678.0,
 u'Cook Islands': 2.0,
 u'Costa Rica': 0.0,
 u'Croatia': 9.0,
 u'Cuba': 0.0,
 u'Cyprus': 0.0,
 u'Czechia': 9.0,
 u""C\xf4te d'Ivoire"": 5729.0,
 u'Democratic Republic of the Congo': 8282.0,
 u'Denmark': 14.0,
 u'Djibouti': 183.0,
 u'Dominica': 0.0,
 u'Dominican Republic': 253.0,
 u'Ecuador': 0.0,
 u'Egypt': 2633.0,
 u'El Salvador': 0.0,
 u'Eritrea': 789.0,
 u'Estonia': 9.0,
 u'Ethiopia': 1660.0,
 u'France': 10000.0,
 u'Gabon': 15.0,
 u'Gambia': 336.0,
 u'Georgia': 50.0,
 u'Ghana': 23068.0,
 u'Greece': 56.0,
 u'Grenada': 0.0,
 u'Guatemala': 0.0,
 u'Guinea': 11294.0,
 u'Guyana': 0.0,
 u'Haiti': 992.0,
 u'Honduras': 0.0,
 u'Hungary': 1.0,
 u'Iceland': 0.0,
 u'India': 38835.0,
 u'Indonesia': 3344.0,
 u'Iran (Islamic Republic of)': 11874.0,
 u'Iraq': 726.0,
 u'Israel': 36.0,
 u'Italy': 1457.0,
 u'Jamaica': 0.0,
 u'Japan': 22497.0,
 u'Jordan': 32.0,
 u'Kazakhstan': 245.0,
 u'Kenya': 21002.0,
 u'Kiribati': 0.0,
 u'Kuwait': 6.0,
 u'Kyrgyzstan': 16.0,
 u""Lao People's Democratic Republic"": 332.0,
 u'Latvia': 0.0,
 u'Lebanon': 5.0,
 u'Lesotho': 660.0,
 u'Liberia': 5977.0,
 u'Lithuania': 19.0,
 u'Luxembourg': 0.0,
 u'Madagascar': 35256.0,
 u'Malawi': 304.0,
 u'Malaysia': 6187.0,
 u'Maldives': 20.0,
 u'Mali': 1578.0,
 u'Malta': 2.0,
 u'Marshall Islands': 0.0,
 u'Mauritius': 0.0,
 u'Mexico': 30.0,
 u'Micronesia (Federated States of)': 0.0,
 u'Mongolia': 925.0,
 u'Morocco': 7368.0,
 u'Mozambique': 7375.0,
 u'Myanmar': 845.0,
 u'Namibia': 469.0,
 u'Nauru': 0.0,
 u'Nepal': 9397.0,
 u'Netherlands': 1019.0,
 u'New Zealand': 65.0,
 u'Nicaragua': 0.0,
 u'Niger': 21319.0,
 u'Nigeria': 212183.0,
 u'Niue': 0.0,
 u'Norway': 0.0,
 u'Oman': 15.0,
 u'Pakistan': 2064.0,
 u'Palau': 0.0,
 u'Panama': 0.0,
 u'Papua New Guinea': 7135.0,
 u'Paraguay': 0.0,
 u'Peru': 1.0,
 u'Philippines': 7120.0,
 u'Poland': 77.0,
 u'Portugal': 45.0,
 u'Qatar': 46.0,
 u'Republic of Korea': 32647.0,
 u'Republic of Moldova': 687.0,
 u'Romania': 35.0,
 u'Russian Federation': 4800.0,
 u'Rwanda': 2095.0,
 u'Saint Kitts and Nevis': 0.0,
 u'Saint Lucia': 0.0,
 u'Saint Vincent and the Grenadines': 0.0,
 u'San Marino': 1.0,
 u'Sao Tome and Principe': 0.0,
 u'Senegal': 5839.0,
 u'Serbia': 38.0,
 u'Sierra Leone': 3575.0,
 u'Singapore': 141.0,
 u'Slovakia': 0.0,
 u'Somalia': 3965.0,
 u'South Africa': 1459.0,
 u'Spain': 152.0,
 u'Sri Lanka': 16527.0,
 u'Sudan': 2875.0,
 u'Suriname': 0.0,
 u'Swaziland': 10.0,
 u'Sweden': 59.0,
 u'Syrian Arab Republic': 146.0,
 u'Tajikistan': 192.0,
 u'Thailand': 4074.0,
 u'The former Yugoslav republic of Macedonia': 36.0,
 u'Togo': 3578.0,
 u'Tonga': 0.0,
 u'Trinidad and Tobago': 0.0,
 u'Tunisia': 47.0,
 u'Turkey': 16244.0,
 u'Turkmenistan': 113.0,
 u'Uganda': 42554.0,
 u'Ukraine': 817.0,
 u'United Arab Emirates': 69.0,
 u'United Kingdom of Great Britain and Northern Ireland': 104.0,
 u'United Republic of Tanzania': 14649.0,
 u'United States of America': 85.0,
 u'Uruguay': 0.0,
 u'Uzbekistan': 80.0,
 u'Vanuatu': 9.0,
 u'Venezuela (Bolivarian Republic of)': 22.0,
 u'Viet Nam': 16512.0,
 u'Zambia': 30930.0,
 u'Zimbabwe': 1483.0}, orient = 'index')
</code></pre>

<p>Here is the 1st list:</p>

<pre><code>list1 = [u'Bolivia (Plurinational State of)', u'Brunei Darussalam', u'Cabo Verde', u'China',
    u'Congo', u'Cook Islands', u'Czechia', u""C\xf4te d'Ivoire"", 
    u""Democratic People's Republic of Korea"", u'France', u'Iran (Islamic Republic of)', 
    u""Lao People's Democratic Republic"", u'Micronesia (Federated States of)', u'Niue', 
    u'Republic of Korea', u'Republic of Moldova', u'Russian Federation', u'Sao Tome and Principe', 
    u'Serbia', u'Somalia', u'Syrian Arab Republic', u'The former Yugoslav republic of Macedonia', 
    u'United Kingdom of Great Britain and Northern Ireland', u'United Republic of Tanzania', 
    u'United States of America', u'Venezuela (Bolivarian Republic of)', u'Viet Nam']
</code></pre>

<p>Here is the 2nd list</p>

<pre><code>list2 = [u'Bolivia', u'Brunei', u'Cape Verde', u'China[1]', u'Democratic Republic of the Congo', 
    u'Cook Islands (NZ)', u'Czech Republic', u'Ivory Coast', u'North Korea', u'France[2]', 
    u'Iran', u'Laos', u'Federated States of Micronesia', u'Niue (NZ)', u'South Korea', 
    u'Moldova[3]', u'Russia', u'S\xe3o Tom\xe9 and Pr\xedncipe', u'Serbia[5]', 
    u'Somalia[6]', u'Syria', u'Macedonia', u'United Kingdom', u'Tanzania', 
    u'United States', u'Venezuela', u'Vietnam']
</code></pre>

<p>This is clearly the sort of thing python excels at - and I suspect a simple for loop will do it but I can't quite wrap my head around the logic (yet)</p>

<p>Any help gratefully appreciated!</p>
",7240180,1107,21-04-2018 02:18,21-04-2018 02:27,0,1107,28,2,13,89,"{'badge_counts': {'bronze': 28, 'silver': 13, 'gold': 2}, 'account_id': 9766966, 'is_employee': False, 'last_modified_date': 1607614459, 'last_access_date': 1710734362, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1107, 'creation_date': 1480666731, 'user_type': 'registered', 'user_id': 7240180, 'accept_rate': 89, 'location': 'Auckland, New Zealand', 'website_url': '', 'link': 'https://stackoverflow.com/users/7240180/kiltannen', 'profile_image': 'https://i.stack.imgur.com/dX8NQ.jpg?s=256&g=1', 'display_name': 'kiltannen'}",I have a dataframe and 2 lists. the 1st list gives a set of index values from the dataframe I want to replace the 2nd list gives the values I want to use I don't want to touch any of the other values Here is the dataframe: Here is the 1st list: Here is the 2nd list This is clearly the sort of thing python excels at - and I suspect a simple for loop will do it but I can't quite wrap my head around the logic (yet) Any help gratefully appreciated!,"df =  pd.DataFrame.from_dict({u'Afghanistan': 6532.0,
 u'Albania': 662.0,
 u'Andorra': 2.0,
 u'Angola': 2219.0,
 u'Antigua and Barbuda': 0.0,
 u'Argentina': 6.0,
 u'Armenia': 15.0,
 u'Australia': 108.0,
 u'Azerbaijan': 210.0,
 u'Bahamas': 0.0,
 u'Bahrain': 6.0,
 u'Bangladesh': 5098.0,
 u'Barbados': 0.0,
 u'Belarus': 21.0,
 u'Belize': 0.0,
 u'Benin': 4244.0,
 u'Bhutan': 418.0,
 u'Bolivia (Plurinational State of)': 122.0,
 u'Bosnia and Herzegovina': 43.0,
 u'Botswana': 2672.0,
 u'Brazil': 36.0,
 u'Brunei Darussalam': 42.0,
 u'Bulgaria': 46.0,
 u'Burkina Faso': 6074.0,
 u'Burundi': 18363.0,
 u'Cabo Verde': 2.0,
 u'Cambodia': 12237.0,
 u'Cameroon': 14629.0,
 u'Canada': 206.0,
 u'Central African Republic': 3207.0,
 u'Chad': 3546.0,
 u'Chile': 0.0,
 u'China': 71093.0,
 u'Colombia': 1.0,
 u'Congo': 1678.0,
 u'Cook Islands': 2.0,
 u'Costa Rica': 0.0,
 u'Croatia': 9.0,
 u'Cuba': 0.0,
 u'Cyprus': 0.0,
 u'Czechia': 9.0,
 u""C\xf4te d'Ivoire"": 5729.0,
 u'Democratic Republic of the Congo': 8282.0,
 u'Denmark': 14.0,
 u'Djibouti': 183.0,
 u'Dominica': 0.0,
 u'Dominican Republic': 253.0,
 u'Ecuador': 0.0,
 u'Egypt': 2633.0,
 u'El Salvador': 0.0,
 u'Eritrea': 789.0,
 u'Estonia': 9.0,
 u'Ethiopia': 1660.0,
 u'France': 10000.0,
 u'Gabon': 15.0,
 u'Gambia': 336.0,
 u'Georgia': 50.0,
 u'Ghana': 23068.0,
 u'Greece': 56.0,
 u'Grenada': 0.0,
 u'Guatemala': 0.0,
 u'Guinea': 11294.0,
 u'Guyana': 0.0,
 u'Haiti': 992.0,
 u'Honduras': 0.0,
 u'Hungary': 1.0,
 u'Iceland': 0.0,
 u'India': 38835.0,
 u'Indonesia': 3344.0,
 u'Iran (Islamic Republic of)': 11874.0,
 u'Iraq': 726.0,
 u'Israel': 36.0,
 u'Italy': 1457.0,
 u'Jamaica': 0.0,
 u'Japan': 22497.0,
 u'Jordan': 32.0,
 u'Kazakhstan': 245.0,
 u'Kenya': 21002.0,
 u'Kiribati': 0.0,
 u'Kuwait': 6.0,
 u'Kyrgyzstan': 16.0,
 u""Lao People's Democratic Republic"": 332.0,
 u'Latvia': 0.0,
 u'Lebanon': 5.0,
 u'Lesotho': 660.0,
 u'Liberia': 5977.0,
 u'Lithuania': 19.0,
 u'Luxembourg': 0.0,
 u'Madagascar': 35256.0,
 u'Malawi': 304.0,
 u'Malaysia': 6187.0,
 u'Maldives': 20.0,
 u'Mali': 1578.0,
 u'Malta': 2.0,
 u'Marshall Islands': 0.0,
 u'Mauritius': 0.0,
 u'Mexico': 30.0,
 u'Micronesia (Federated States of)': 0.0,
 u'Mongolia': 925.0,
 u'Morocco': 7368.0,
 u'Mozambique': 7375.0,
 u'Myanmar': 845.0,
 u'Namibia': 469.0,
 u'Nauru': 0.0,
 u'Nepal': 9397.0,
 u'Netherlands': 1019.0,
 u'New Zealand': 65.0,
 u'Nicaragua': 0.0,
 u'Niger': 21319.0,
 u'Nigeria': 212183.0,
 u'Niue': 0.0,
 u'Norway': 0.0,
 u'Oman': 15.0,
 u'Pakistan': 2064.0,
 u'Palau': 0.0,
 u'Panama': 0.0,
 u'Papua New Guinea': 7135.0,
 u'Paraguay': 0.0,
 u'Peru': 1.0,
 u'Philippines': 7120.0,
 u'Poland': 77.0,
 u'Portugal': 45.0,
 u'Qatar': 46.0,
 u'Republic of Korea': 32647.0,
 u'Republic of Moldova': 687.0,
 u'Romania': 35.0,
 u'Russian Federation': 4800.0,
 u'Rwanda': 2095.0,
 u'Saint Kitts and Nevis': 0.0,
 u'Saint Lucia': 0.0,
 u'Saint Vincent and the Grenadines': 0.0,
 u'San Marino': 1.0,
 u'Sao Tome and Principe': 0.0,
 u'Senegal': 5839.0,
 u'Serbia': 38.0,
 u'Sierra Leone': 3575.0,
 u'Singapore': 141.0,
 u'Slovakia': 0.0,
 u'Somalia': 3965.0,
 u'South Africa': 1459.0,
 u'Spain': 152.0,
 u'Sri Lanka': 16527.0,
 u'Sudan': 2875.0,
 u'Suriname': 0.0,
 u'Swaziland': 10.0,
 u'Sweden': 59.0,
 u'Syrian Arab Republic': 146.0,
 u'Tajikistan': 192.0,
 u'Thailand': 4074.0,
 u'The former Yugoslav republic of Macedonia': 36.0,
 u'Togo': 3578.0,
 u'Tonga': 0.0,
 u'Trinidad and Tobago': 0.0,
 u'Tunisia': 47.0,
 u'Turkey': 16244.0,
 u'Turkmenistan': 113.0,
 u'Uganda': 42554.0,
 u'Ukraine': 817.0,
 u'United Arab Emirates': 69.0,
 u'United Kingdom of Great Britain and Northern Ireland': 104.0,
 u'United Republic of Tanzania': 14649.0,
 u'United States of America': 85.0,
 u'Uruguay': 0.0,
 u'Uzbekistan': 80.0,
 u'Vanuatu': 9.0,
 u'Venezuela (Bolivarian Republic of)': 22.0,
 u'Viet Nam': 16512.0,
 u'Zambia': 30930.0,
 u'Zimbabwe': 1483.0}, orient = 'index')
 list1 = [u'Bolivia (Plurinational State of)', u'Brunei Darussalam', u'Cabo Verde', u'China',
    u'Congo', u'Cook Islands', u'Czechia', u""C\xf4te d'Ivoire"", 
    u""Democratic People's Republic of Korea"", u'France', u'Iran (Islamic Republic of)', 
    u""Lao People's Democratic Republic"", u'Micronesia (Federated States of)', u'Niue', 
    u'Republic of Korea', u'Republic of Moldova', u'Russian Federation', u'Sao Tome and Principe', 
    u'Serbia', u'Somalia', u'Syrian Arab Republic', u'The former Yugoslav republic of Macedonia', 
    u'United Kingdom of Great Britain and Northern Ireland', u'United Republic of Tanzania', 
    u'United States of America', u'Venezuela (Bolivarian Republic of)', u'Viet Nam']
 list2 = [u'Bolivia', u'Brunei', u'Cape Verde', u'China[1]', u'Democratic Republic of the Congo', 
    u'Cook Islands (NZ)', u'Czech Republic', u'Ivory Coast', u'North Korea', u'France[2]', 
    u'Iran', u'Laos', u'Federated States of Micronesia', u'Niue (NZ)', u'South Korea', 
    u'Moldova[3]', u'Russia', u'S\xe3o Tom\xe9 and Pr\xedncipe', u'Serbia[5]', 
    u'Somalia[6]', u'Syria', u'Macedonia', u'United Kingdom', u'Tanzania', 
    u'United States', u'Venezuela', u'Vietnam']
",180,206,0,0,
237,49035156,49035810,43016,"PyTorch - How to use ""toPILImage"" correctly",3,<python><pytorch>,11,"<p>I would like to know, whether I used <a href=""http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.ToPILImage"" rel=""noreferrer"">toPILImage</a> from torchvision correctly. I want to use it, to see how the images look after initial image transformations are applied to the dataset.</p>

<p>When I use it like in the code below, the image that comes up has weird colors like <a href=""https://i.stack.imgur.com/G9IMe.png"" rel=""noreferrer"">this one</a>. The original image is a regular RGB image.</p>

<p>This is my code:</p>

<pre><code>import os
import torch
from PIL import Image, ImageFont, ImageDraw
import torch.utils.data as data
import torchvision
from torchvision import transforms    
import matplotlib.pyplot as plt

# Image transformations
normalize = transforms.Normalize(
    mean=[0.485, 0.456, 0.406],
    std=[0.229, 0.224, 0.225]
    )
transform_img = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    normalize ])

train_data = torchvision.datasets.ImageFolder(
    root='./train_cl/',
    transform=transform_img
    )
test_data = torchvision.datasets.ImageFolder(
    root='./test_named_cl/',
    transform=transform_img                                             
    )

train_data_loader = data.DataLoader(train_data,
    batch_size=4,
    shuffle=True,
    num_workers=4) #num_workers=args.nThreads)

test_data_loader = data.DataLoader(test_data,
    batch_size=32,
    shuffle=False,
    num_workers=4)        

# Open Image from dataset:
to_pil_image = transforms.ToPILImage()
my_img, _ = train_data[248]
results = to_pil_image(my_img)
results.show()
</code></pre>

<p>Edit:</p>

<p>I had to use .data on the Torch Variable to get the tensor. 
Also I needed to rescale the numpy array before transposing. I found a working solution <a href=""https://stackoverflow.com/questions/47318871/valueerror-floating-point-image-rgb-values-must-be-in-the-0-1-range-while-usi"">here</a>, but it doesn't always work well. How can I do this better?</p>

<pre><code>for i, data in enumerate(train_data_loader, 0):
    img, labels = data
    img = Variable(img)
    break

image = img.data.cpu().numpy()[0]

# This worked for rescaling:
image = (1/(2*2.25)) * image + 0.5

# Both of these didn't work:
# image /= (image.max()/255.0)
# image *= (255.0/image.max())

image = np.transpose(image, (1,2,0))
plt.imshow(image)
plt.show() 
</code></pre>
",6799476,917,28-02-2018 17:16,28-02-2018 17:58,0,917,22,3,9,,"{'badge_counts': {'bronze': 22, 'silver': 9, 'gold': 3}, 'account_id': 9056890, 'is_employee': False, 'last_modified_date': 1626873600, 'last_access_date': 1650896251, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 917, 'creation_date': 1473152292, 'user_type': 'registered', 'user_id': 6799476, 'location': 'Rostock, Germany', 'link': 'https://stackoverflow.com/users/6799476/kett', 'profile_image': 'https://www.gravatar.com/avatar/c2df7a44ab86f372bbf7c9fb7c124a56?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'kett'}","I would like to know, whether I used toPILImage from torchvision correctly. I want to use it, to see how the images look after initial image transformations are applied to the dataset. When I use it like in the code below, the image that comes up has weird colors like this one. The original image is a regular RGB image. This is my code: Edit: I had to use .data on the Torch Variable to get the tensor. Also I needed to rescale the numpy array before transposing. I found a working solution here, but it doesn't always work well. How can I do this better?","import os
import torch
from PIL import Image, ImageFont, ImageDraw
import torch.utils.data as data
import torchvision
from torchvision import transforms    
import matplotlib.pyplot as plt

# Image transformations
normalize = transforms.Normalize(
    mean=[0.485, 0.456, 0.406],
    std=[0.229, 0.224, 0.225]
    )
transform_img = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    normalize ])

train_data = torchvision.datasets.ImageFolder(
    root='./train_cl/',
    transform=transform_img
    )
test_data = torchvision.datasets.ImageFolder(
    root='./test_named_cl/',
    transform=transform_img                                             
    )

train_data_loader = data.DataLoader(train_data,
    batch_size=4,
    shuffle=True,
    num_workers=4) #num_workers=args.nThreads)

test_data_loader = data.DataLoader(test_data,
    batch_size=32,
    shuffle=False,
    num_workers=4)        

# Open Image from dataset:
to_pil_image = transforms.ToPILImage()
my_img, _ = train_data[248]
results = to_pil_image(my_img)
results.show()
 for i, data in enumerate(train_data_loader, 0):
    img, labels = data
    img = Variable(img)
    break

image = img.data.cpu().numpy()[0]

# This worked for rescaling:
image = (1/(2*2.25)) * image + 0.5

# Both of these didn't work:
# image /= (image.max()/255.0)
# image *= (255.0/image.max())

image = np.transpose(image, (1,2,0))
plt.imshow(image)
plt.show() 
",58,74,0,3,
238,50258960,50259157,61824,How to apply LabelEncoder for a specific column in Pandas dataframe,3,<python><python-3.x><machine-learning><scikit-learn><label-encoding>,33,"<p>I have a dataset loaded by dataframe where the class label needs to be encoded using <code>LabelEncoder</code> from scikit-learn. The column <code>label</code> is the class label column which has the following classes:</p>

<pre><code>[‘Standing’, ‘Walking’, ‘Running’, ‘null’]
</code></pre>

<p>To perform label encoding, I tried the following but it does not work. How can I fix it? </p>

<pre><code>from sklearn import preprocessing
import pandas as pd

df = pd.read_csv('dataset.csv', sep=',') 
df.apply(preprocessing.LabelEncoder().fit_transform(df['label']))
</code></pre>
",6810148,1457,09-05-2018 17:26,09-05-2018 17:39,0,1457,30,2,20,57,"{'badge_counts': {'bronze': 30, 'silver': 20, 'gold': 2}, 'account_id': 9157712, 'is_employee': False, 'last_modified_date': 1573678938, 'last_access_date': 1561657306, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1457, 'creation_date': 1473351246, 'user_type': 'registered', 'user_id': 6810148, 'accept_rate': 57, 'website_url': '', 'link': 'https://stackoverflow.com/users/6810148/kristofer', 'profile_image': 'https://www.gravatar.com/avatar/568574ba3f304fc587c8d1c3bff08e12?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Kristofer'}","I have a dataset loaded by dataframe where the class label needs to be encoded using from scikit-learn. The column is the class label column which has the following classes: To perform label encoding, I tried the following but it does not work. How can I fix it?","LabelEncoder label [‘Standing’, ‘Walking’, ‘Running’, ‘null’]
 from sklearn import preprocessing
import pandas as pd

df = pd.read_csv('dataset.csv', sep=',') 
df.apply(preprocessing.LabelEncoder().fit_transform(df['label']))
",2,13,0,0,
239,48729220,48729252,1594,"Set Python dict items recursively, when given a compound key 'foo.bar.baz'",1,<python><dictionary><recursion>,12,"<p>I'd like to achieve the following:</p>

<pre><code>foodict['foo.bar.baz'] = 'foo'
{
   'foo': {
      'bar': {
            'baz': 'foo'
         }
       }
   }
}
</code></pre>

<p>...creating keys recursively.</p>

<p>After scratching my head for a while, I've come up with this:</p>

<pre class=""lang-py prettyprint-override""><code>class Config(dict):
    def __init__(self, *args, **kwargs):
        self.super = super(Config, self)
        self.update(*args, **kwargs)

    def __setitem__(self, keys, value):
        keys   = keys.split('.')
        keys.reverse()

        config = Config()

        for i, k in enumerate(keys):
            if i == 0:
                config  = Config(**{ k: value })
            else:
                config  = Config(**{ k: config })

        self.super.update(config)
</code></pre>
",6818563,3369,11-02-2018 07:10,11-02-2018 07:17,0,3387,16,1,12,,"{'badge_counts': {'bronze': 16, 'silver': 12, 'gold': 1}, 'account_id': 9082534, 'is_employee': False, 'last_modified_date': 1656728704, 'last_access_date': 1669134505, 'reputation_change_year': 38, 'reputation_change_quarter': 38, 'reputation_change_month': 18, 'reputation_change_week': -2, 'reputation_change_day': 0, 'reputation': 3387, 'creation_date': 1473577521, 'user_type': 'registered', 'user_id': 6818563, 'link': 'https://stackoverflow.com/users/6818563/achilles-gasper-rasquinha', 'profile_image': 'https://lh3.googleusercontent.com/-xMLVm64vMCI/AAAAAAAAAAI/AAAAAAAAABg/LyREBW7YL4I/photo.jpg?sz=256', 'display_name': 'Achilles Gasper Rasquinha'}","I'd like to achieve the following: ...creating keys recursively. After scratching my head for a while, I've come up with this:","foodict['foo.bar.baz'] = 'foo'
{
   'foo': {
      'bar': {
            'baz': 'foo'
         }
       }
   }
}
 class Config(dict):
    def __init__(self, *args, **kwargs):
        self.super = super(Config, self)
        self.update(*args, **kwargs)

    def __setitem__(self, keys, value):
        keys   = keys.split('.')
        keys.reverse()

        config = Config()

        for i, k in enumerate(keys):
            if i == 0:
                config  = Config(**{ k: value })
            else:
                config  = Config(**{ k: config })

        self.super.update(config)
",25,36,0,0,
240,48417071,48417985,2286,"Purpose of __name__ in TypeVar, NewType",1,<python><python-3.6><type-hinting>,11,"<p>In the <code>typing</code> module, both <code>TypeVar</code> and <code>NewType</code> require as a first positional argument, a string to be used as the created object's <code>__name__</code> attribute. What is the purpose of <code>__name__</code> here?</p>

<p>Considering that this is a compulsory argument, I would expect it to be something essential. In PEP-484 where type hinting was introduced, the argument is normally set as a string of the variable name assigned to the object:</p>

<pre><code>T = TypeVar('T', int, float, complex)
</code></pre>

<p>But, I can't really tell how this end up being used in <code>typing.py</code> in <code>CPython</code>. Replacing the string with indeed any other string does not appear to break anything in my tests. </p>
",6573647,1570,24-01-2018 07:33,24-01-2018 08:33,0,1570,18,0,12,,"{'badge_counts': {'bronze': 18, 'silver': 12, 'gold': 0}, 'account_id': 2632276, 'is_employee': False, 'last_modified_date': 1607614479, 'last_access_date': 1686849827, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1570, 'creation_date': 1468226479, 'user_type': 'registered', 'user_id': 6573647, 'link': 'https://stackoverflow.com/users/6573647/melvin', 'profile_image': 'https://www.gravatar.com/avatar/8eadd5c2a7a266feb22c128a41275248?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Melvin'}","In the module, both and require as a first positional argument, a string to be used as the created object's attribute. What is the purpose of here? Considering that this is a compulsory argument, I would expect it to be something essential. In PEP-484 where type hinting was introduced, the argument is normally set as a string of the variable name assigned to the object: But, I can't really tell how this end up being used in in . Replacing the string with indeed any other string does not appear to break anything in my tests.","typing TypeVar NewType __name__ __name__ T = TypeVar('T', int, float, complex)
 typing.py CPython",-7,8,0,0,
241,49046931,49063063,6072,How can I use DateAxisItem of PyQtGraph?,1,<python><pyqtgraph>,11,"<p>I'm using PyQtGraph '0.9.8+gd627e39' on Python 3.6.2(32bit) and Windows 10.</p>

<p>My goal is to plot real time data with an X-axis that shows datetime.</p>

<pre><code>Time                                                Value
datetime.datetime(2018, 3, 1, 9, 36, 50, 136415)    10
datetime.datetime(2018, 3, 1, 9, 36, 51, 330912)    9
datetime.datetime(2018, 3, 1, 9, 36, 51, 382815)    12
datetime.datetime(2018, 3, 1, 9, 36, 52, 928818)    11
...
</code></pre>

<p>I looked up related issues such as 
<a href=""https://gist.github.com/friendzis/4e98ebe2cf29c0c2c232"" rel=""noreferrer"">https://gist.github.com/friendzis/4e98ebe2cf29c0c2c232</a>, <a href=""https://stackoverflow.com/questions/28296049/pyqtgraph-plotting-time-series"">pyqtgraph, plotting time series</a>, but it's still hard for me to grasp how to use <code>DateAxisItem</code></p>

<p>I tried to make a simple code using the module, </p>

<pre><code>import numpy as np
import pyqtgraph as pg
from pyqtgraph.Qt import QtCore, QtGui
from datetime import datetime
from time import time

t1 = datetime.now()
t2 = datetime.now()

list_x = [ t1, t2 ]
list_y = [ 0, 1 ]

date_axis = pg.graphicsItems.DateAxisItem.DateAxisItem(orientation = 'bottom')
graph = pg.PlotWidget(axisItems = {'bottom': date_axis})

graph.plot(x=list_x, y=list_y, pen=None, symbol='o')
graph.show()
</code></pre>

<p>but it shows an error message and doesn't show its X-axis at all.</p>

<pre><code>Traceback (most recent call last):
File ""&lt;tmp 10&gt;"", line 19, in &lt;module&gt;
    graph.plot(x=list_x, y=list_y, pen=None, symbol='o')
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\PlotItem\PlotItem.py"", line 636, in plot
    item = PlotDataItem(*args, **kargs)
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\PlotDataItem.py"", line 177, in __init__
    self.setData(*args, **kargs)
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\PlotDataItem.py"", line 461, in setData
    self.updateItems()
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\PlotDataItem.py"", line 493, in updateItems
    self.scatter.setData(x=x, y=y, **scatterArgs)
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\ScatterPlotItem.py"", line 308, in setData
    self.addPoints(*args, **kargs)
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\ScatterPlotItem.py"", line 388, in addPoints
    newData['x'] = kargs['x']
TypeError: float() argument must be a string or a number, not 'datetime.datetime'
</code></pre>

<p>Is it because <code>DateAxisItem</code> doesn't support datetime? It would be great if I could understand the module by looking its code, but unfortunately, my skills are not good.</p>

<p>I'd appreciate it if anyone could show me how to use the module with some simple data.</p>
",7250111,1956,01-03-2018 09:51,02-03-2018 04:56,1,1956,49,4,27,77,"{'badge_counts': {'bronze': 49, 'silver': 27, 'gold': 4}, 'account_id': 9782775, 'is_employee': False, 'last_modified_date': 1703297100, 'last_access_date': 1695919777, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1956, 'creation_date': 1480906779, 'user_type': 'registered', 'user_id': 7250111, 'accept_rate': 77, 'link': 'https://stackoverflow.com/users/7250111/maynull', 'profile_image': 'https://www.gravatar.com/avatar/aedfa3224862956c2b3c1f40f80e228f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'maynull'}","I'm using PyQtGraph '0.9.8+gd627e39' on Python 3.6.2(32bit) and Windows 10. My goal is to plot real time data with an X-axis that shows datetime. I looked up related issues such as https://gist.github.com/friendzis/4e98ebe2cf29c0c2c232, pyqtgraph, plotting time series, but it's still hard for me to grasp how to use I tried to make a simple code using the module, but it shows an error message and doesn't show its X-axis at all. Is it because doesn't support datetime? It would be great if I could understand the module by looking its code, but unfortunately, my skills are not good. I'd appreciate it if anyone could show me how to use the module with some simple data.","Time                                                Value
datetime.datetime(2018, 3, 1, 9, 36, 50, 136415)    10
datetime.datetime(2018, 3, 1, 9, 36, 51, 330912)    9
datetime.datetime(2018, 3, 1, 9, 36, 51, 382815)    12
datetime.datetime(2018, 3, 1, 9, 36, 52, 928818)    11
...
 DateAxisItem import numpy as np
import pyqtgraph as pg
from pyqtgraph.Qt import QtCore, QtGui
from datetime import datetime
from time import time

t1 = datetime.now()
t2 = datetime.now()

list_x = [ t1, t2 ]
list_y = [ 0, 1 ]

date_axis = pg.graphicsItems.DateAxisItem.DateAxisItem(orientation = 'bottom')
graph = pg.PlotWidget(axisItems = {'bottom': date_axis})

graph.plot(x=list_x, y=list_y, pen=None, symbol='o')
graph.show()
 Traceback (most recent call last):
File ""&lt;tmp 10&gt;"", line 19, in &lt;module&gt;
    graph.plot(x=list_x, y=list_y, pen=None, symbol='o')
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\PlotItem\PlotItem.py"", line 636, in plot
    item = PlotDataItem(*args, **kargs)
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\PlotDataItem.py"", line 177, in __init__
    self.setData(*args, **kargs)
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\PlotDataItem.py"", line 461, in setData
    self.updateItems()
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\PlotDataItem.py"", line 493, in updateItems
    self.scatter.setData(x=x, y=y, **scatterArgs)
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\ScatterPlotItem.py"", line 308, in setData
    self.addPoints(*args, **kargs)
File ""d:\python36-32\lib\site-packages\pyqtgraph\graphicsItems\ScatterPlotItem.py"", line 388, in addPoints
    newData['x'] = kargs['x']
TypeError: float() argument must be a string or a number, not 'datetime.datetime'
 DateAxisItem",34,59,0,2,
242,48371856,48371870,101079,count the number of occurrences of a certain value in a dictionary in python?,5,<python><python-3.x><dictionary><counting>,38,"<p>If I have got something like this:</p>

<pre><code>D = {'a': 97, 'c': 0 , 'b':0,'e': 94, 'r': 97 , 'g':0}
</code></pre>

<p>If I want for example to count the number of occurrences for the ""0"" as a value without having to iterate the whole list, is that even possible and how?</p>
",7310034,1282,21-01-2018 21:23,21-01-2018 21:25,0,1282,27,2,14,100,"{'badge_counts': {'bronze': 27, 'silver': 14, 'gold': 2}, 'account_id': 9870033, 'is_employee': False, 'last_modified_date': 1679427601, 'last_access_date': 1635002013, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1282, 'creation_date': 1481973880, 'user_type': 'registered', 'user_id': 7310034, 'accept_rate': 100, 'location': 'Egypt', 'website_url': '', 'link': 'https://stackoverflow.com/users/7310034/rowanx', 'profile_image': 'https://i.stack.imgur.com/IcIEG.jpg?s=256&g=1', 'display_name': 'RowanX'}","If I have got something like this: If I want for example to count the number of occurrences for the ""0"" as a value without having to iterate the whole list, is that even possible and how?","D = {'a': 97, 'c': 0 , 'b':0,'e': 94, 'r': 97 , 'g':0}
",0,6,0,0,
243,48348176,48348359,17123,Convert data types of a Pandas dataframe to match another,2,<python><pandas>,22,"<p>I have two data frames with the same column names but different data types:</p>

<p><code>df1.dtypes</code></p>

<pre><code>order                                                                                                             
int64
x                                                                                                                                
int64
y                                                                                                                     
int64
</code></pre>

<p><code>df2.dtypes</code></p>

<pre><code>order                                                                                                            
object
x                                                                                                                    
object
y                                                                                                                    
object
</code></pre>

<p>The dataframes are much larger than this, so I would like to capture the names/dtypes of df1 and convert df2 to match.</p>
",6938793,439,19-01-2018 19:24,19-01-2018 19:37,0,439,12,3,6,50,"{'badge_counts': {'bronze': 12, 'silver': 6, 'gold': 3}, 'account_id': 9348197, 'is_employee': False, 'last_modified_date': 1651109353, 'last_access_date': 1664399085, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 439, 'creation_date': 1475865702, 'user_type': 'registered', 'user_id': 6938793, 'accept_rate': 50, 'link': 'https://stackoverflow.com/users/6938793/cody', 'profile_image': 'https://www.gravatar.com/avatar/f0d7f334a7c1e830e5c6488475c380da?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Cody'}","I have two data frames with the same column names but different data types: The dataframes are much larger than this, so I would like to capture the names/dtypes of df1 and convert df2 to match.","df1.dtypes order                                                                                                             
int64
x                                                                                                                                
int64
y                                                                                                                     
int64
 df2.dtypes order                                                                                                            
object
x                                                                                                                    
object
y                                                                                                                    
object
",8,23,0,0,
244,48812198,49700271,22460,calculate Exponential Moving Average with pandas,2,<python><pandas>,11,"<p>I try to calculate ema with pandas but the result is not good.
I try 2 techniques to calculate :</p>

<p>The first technique is the panda's function <code>ewn</code>:</p>

<pre><code>window = 100
c = 2 / float(window + 1)
df['100ema'] = df['close'].ewm(com=c).mean()
</code></pre>

<p>But the last result of this function gives. <code>2695.4</code> but the real result is <code>2656.2</code></p>

<p>The second technique is</p>

<pre><code>window = 100
c = 2 / float(window + 1)
df['100sma'] = df['close'].rolling(window).mean()
df['100ema'] = (c * df['close']) + ((1 - c) * df['100sma'])
</code></pre>

<p>The result is <code>2649.1</code> it's closer than first technique but is always not good</p>

<p>The sma function give the good result </p>

<p>** EDIT  **</p>

<p>The response is </p>

<pre><code>df['100ema'] = pd.Series.ewm(df['close'], span=window).mean()
</code></pre>
",6939208,468,15-02-2018 16:41,06-04-2018 20:05,50,468,27,1,9,42,"{'badge_counts': {'bronze': 27, 'silver': 9, 'gold': 1}, 'account_id': 9348795, 'is_employee': False, 'last_modified_date': 1671300410, 'last_access_date': 1670931963, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 468, 'creation_date': 1475873278, 'user_type': 'registered', 'user_id': 6939208, 'accept_rate': 42, 'location': 'Montr&#233;al, QC, Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/6939208/john', 'profile_image': 'https://www.gravatar.com/avatar/cfcf60778afd1af38a4e29c65047b75a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'john'}",I try to calculate ema with pandas but the result is not good. I try 2 techniques to calculate : The first technique is the panda's function : But the last result of this function gives. but the real result is The second technique is The result is it's closer than first technique but is always not good The sma function give the good result ** EDIT ** The response is,"ewn window = 100
c = 2 / float(window + 1)
df['100ema'] = df['close'].ewm(com=c).mean()
 2695.4 2656.2 window = 100
c = 2 / float(window + 1)
df['100sma'] = df['close'].rolling(window).mean()
df['100ema'] = (c * df['close']) + ((1 - c) * df['100sma'])
 2649.1 df['100ema'] = pd.Series.ewm(df['close'], span=window).mean()
",1,30,0,0,
245,48908044,50949266,27361,How to disable ssl verification for http.client.HTTPSConnection class in python?,1,<python><ssl><httpclient>,14,"<p>In the official doc of <em>HTTP PROTOCOL CLIENT</em></p>

<blockquote>
  <p>class http.client.HTTPSConnection(host, port=None, key_file=None,
  cert_file=None, [timeout, ]source_address=None, *, context=None,
  check_hostname=None)</p>
  
  <p>A subclass of HTTPConnection that uses SSL for communication with
  secure servers. Default port is 443. If context is specified, it must
  be a ssl.SSLContext instance describing the various SSL options.</p>
</blockquote>

<p>Is there any option to disable ssl verification like python requests library as <code>verify=fasle</code></p>

<p>For some reasons I can't use <em>HTTPConnection</em> class which would be a straight forward solution. I have to use <em>HTTPSConnection</em> and work with</p>

<p><code>HTTPConnection.putrequest()</code></p>

<p>to send request without the ssl verification.</p>
",7010082,810,21-02-2018 14:12,20-06-2018 13:20,119,820,25,1,11,68,"{'badge_counts': {'bronze': 25, 'silver': 11, 'gold': 1}, 'collectives': [{'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 9424745, 'is_employee': False, 'last_modified_date': 1607614466, 'last_access_date': 1709265542, 'reputation_change_year': 8, 'reputation_change_quarter': 8, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 820, 'creation_date': 1476303061, 'user_type': 'registered', 'user_id': 7010082, 'accept_rate': 68, 'website_url': '', 'link': 'https://stackoverflow.com/users/7010082/s-rakin', 'profile_image': 'https://i.stack.imgur.com/tjq6Y.jpg?s=256&g=1', 'display_name': 'S.Rakin'}","In the official doc of HTTP PROTOCOL CLIENT class http.client.HTTPSConnection(host, port=None, key_file=None, cert_file=None, [timeout, ]source_address=None, *, context=None, check_hostname=None) A subclass of HTTPConnection that uses SSL for communication with secure servers. Default port is 443. If context is specified, it must be a ssl.SSLContext instance describing the various SSL options. Is there any option to disable ssl verification like python requests library as For some reasons I can't use HTTPConnection class which would be a straight forward solution. I have to use HTTPSConnection and work with to send request without the ssl verification.",verify=fasle HTTPConnection.putrequest(),-2,19,0,0,
246,49370769,49370890,28416,"Python __init__ and classmethod, do they have to have the same number of args?",5,<python><oop><constructor><arguments><class-method>,15,"<p>So classmethods can be used as an alternative &quot;constructor&quot; in python, they are bound to the class and not to an instance, pretty clear so far. But my question is, if it is mandatory to have the same number of arguments in the returned instance of the class method as in the <code>__init__</code>. More exactly :</p>
<pre class=""lang-py prettyprint-override""><code>    class MyClass(object):
      def __init__(self,name):
         self.name=name

      @classmethod
      def alternative_init(cls,name,surname):
          return cls(name,surname)
</code></pre>
<p>And if I try to create an instance <code>Myclass(&quot;alex&quot;)</code> works fine, but if I try to create an instance <code>Myclass.alternative_init(&quot;alex&quot;,&quot;james&quot;)</code> I have a <code>TypeError</code> , because I pass to many arguments, and init take only 2 . Am I missing something?</p>
",7013263,4098,19-03-2018 19:19,19-03-2018 19:27,0,4108,42,6,27,10,"{'badge_counts': {'bronze': 42, 'silver': 27, 'gold': 6}, 'collectives': [{'collective': {'tags': ['azure-sdk-ruby', 'azure-lab-services', 'azure-devtest-labs', 'azure-stream-analytics', 'azureadgraph-deprecation', 'azure-configuration', 'azure-cosmosdb-cassandra-api', 'azure-eventgrid', 'azure-functions-proxies', 'azure-dns', 'azureclicredential', 'azure-ad-msal', 'sql-azure-federations', 'azure-log-analytics', 'azure-git-deployment', 'azure-queues', 'azure-auto-ml', 'azure-acr', 'azure-nsg', 'azure-integration-account', 'azure-managed-app', 'azure-sql-reporting', 'azure-public-ip', 'azure-sdk', 'adal', 'azure-service-plan', 'azure-appservice', 'azure-blob-trigger', 'azure-keyvault', 'azure-ddos', 'azure-ilb', 'azure-logic-app-standard', 'azure-functions', 'azure-iot-hub-device-update', 'azure-web-roles', 'azure-cloud-shell', 'azure-android-sdk', 'azure-acs', 'azure-alerts', 'azure-web-app-service', 'azure-batch', 'adal.js', 'azure-xplat-cli', 'azure-functions-runtime', 'azure-databricks', 'azure-sql', 'azure-site-recovery', 'azure-rbac', 'azure-analysis-services', 'azure-service-principal', 'azure-sentinel', 'azure-diagnostics', 'azure-databoxfamily', 'azure-media-player', 'azure-appfabric', 'azure-compute-emulator', 'azure-ai-translator', 'azure-traffic-manager', 'azure-sql-database', 'azure-clouddrive', 'azure-billing', 'azure-affinity-group', 'azure-blueprints', 'azure-text-translation', 'azure-caching', 'azure-ai', 'azure', 'azure-sas', 'azure-sphere', 'azure-promptflow', 'azure-free-services', 'azure-servicebus-topics', 'azure-iot-hub', 'azureshell', 'azure-sql-edge', 'azure-app-api', 'azure-security-center', 'azure-static-website-routing', 'azure-ad-powershell-v2', 'azure-ad-role', 'azure-spatial-anchors', 'azure-iot-central', 'azure-adf', 'azure-dsvm', 'azure-active-directory', 'azure-update-management-center', 'azure-service-fabric', 'azure-web-app-for-containers', 'azure-secrets', 'azure-workflow-automation', 'azure-mobile-engagement', 'azure-defender', 'azure-ad-graph-api', 'kitchen-azurerm', 'azure-china', 'azure-log-analytics-workspace', 'azure-data-factory', 'azure-batch-account', 'azure-adal-deprecation', 'microsoft-entra-id', 'azure-calculator', 'azure-remote-rendering', 'azure-image-builder', 'azure-bot-service', 'azure-anomaly-detector', 'azure-virtual-network', 'spring-cloud-azure', 'microsoft-entra-internet-access', 'microsoft-custom-vision', 'azure-database-postgresql', 'azure-maps', 'azure-hdinsight', 'azure-feature-manager', 'azure-ad-b2c-custom-policy', 'azure-private-dns', 'azure-management-portal', 'azure-cosmosdb-gremlinapi', 'azure-function-async', 'django-pyodbc-azure', 'azure-signalr', 'azure-billing-api', 'azure-scheduler', 'azure-anomaly-detection', 'azure-app-service-envrmnt', 'azure-front-door', 'azure-elasticpool', 'azure-role-environment', 'azure-vm-templates', 'azure-redis-cache', 'azure-waf', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-tablequery', 'azure-blob-storage', 'azure-postgresql', 'azure-servicebusrelay', 'azure-security', 'azure-automation', 'azure-quantum', 'azure-web-pubsub', 'azure-blockchain-service', 'azure-notificationhub', 'azure-load-testing', 'azure-object-anchors', 'azure-media-services', 'azure-dashboard', 'azure-managed-disk', 'azure-metrics-advisor', 'azure-emulator', 'azure-mobile-services', 'azure-static-web-app-routing', 'azure-webjobssdk', 'azure-zulu', 'azure-iot-suite', 'azure-app-service-plans', 'azure-application-proxy', 'azure-identity', 'azure-data-lake-gen2', 'azure-availability-set', 'azure-function-app-proxy', 'azure-container-service', 'azure-mcd', 'azure-analytics', 'azure-debugger', 'azure-gov', 'azure-vm', 'azure-iot-dps', 'defaultazurecredential', 'azure-triggers', 'azure-purview', 'azure-service-fabric-mesh', 'azure-notebooks', 'azure-autoscaling-block', 'azure-python-sdk', 'azure-node-sdk', 'azure-aks', 'azure-cosmosdb-mongoapi', 'azuremlsdk', 'azure-oauth', 'rebus-azureservicebus', 'azure-file-copy', 'azure-dev-spaces', 'azure-monitor', 'azure-sql-managed-instance', 'azure-database-mysql', 'azure-cosmosdb-tables', 'azure-relay', 'fhir-server-for-azure', 'azure-packaging', 'azurekinect', 'azure-webjobs', 'azure-java-tools', 'azure-api-management', 'azure-sdk-.net', 'azure-arc', 'azure-functions-core-tools', 'azure-cloud-services', 'azure-video-indexer', 'azure-worker-roles', 'azure-servicebus-queues', 'azure-powershell', 'sql-server-azure', 'pulumi-azure', 'azure-rest-api', 'azure-compliance-policy', 'azurerm-app-service', 'azure-regions', 'azure-java-sdk', 'azure-private-dns-zone', 'azure-function-queue', 'azure-data-share', 'azure-functions-docker', 'azure-advisor', 'azure-deployment', 'azure-managed-database', 'azure-mapping-data-flow', 'azure-application-roles', 'azure-deployment-slots', 'azure-cosmosdb-changefeed', 'azure-webhooks', 'azure-communication-services', 'azure-synapse-link', 'azure-storage-account', 'azure-storage-explorer', 'passport-azure-ad', 'azure-durable-functions', 'azure-search-.net-sdk', 'azure-language-understanding', 'azure-iot-hub-device-management', 'azure-api-apps', 'azure-application-gateway', 'azure-pack', 'azure-disk', 'azure-resource-group', 'azure-sdk-for-java', 'azureservicebus', 'azure-storage-queues', 'azure-resource-graph', 'azure-storage-files', 'azure-bicep', 'azure-data-sync', 'azure-management', 'azure-rm', 'azure-spring-cloud', 'azure-performancecounters', 'azure-pipelines-release-pipeline', 'azure-management-api', 'azure-management-groups', 'azure-policy', 'azure-servicebus-subscriptions', 'azure-files', 'microsoft-entra-external-id', 'azure-industrial-iot', 'azure-load-balancer', 'azure-elastic-scale', 'azure-application-insights-profiler', 'azure-information-protection', 'azure-managed-grafana', 'azure-container-apps', 'azure-blockchain-workbench', 'azure-static-web-app', 'azure-sdk-for-go', 'azure-monitoring', 'azure-function-http', 'azure-runbook', 'azure-eventhub', 'azure-service-runtime', 'azure-ad-b2b', 'azure-ml-component', 'azure-webjobs-continuous', 'azure-static-website-hosting', 'azure-sdk-php', 'azure-private-link', 'azure-digital-twins', 'azure-availability-zones', 'azure-agent', 'azure-subscription', 'azure-data-catalog', 'azure-migrate', 'azure-linux', 'azure.data.tables', 'azure-vpn', 'azure-oms', 'azure-application-settings', 'azure-app-configuration', 'azure-form-recognizer', 'kql', 'azure-http-trigger', 'azure-backup-vault', 'azure-synapse', 'azure-table-storage', 'azure-custom-providers', 'azure-iot-sdk', 'azure-container-registry', 'azure-authentication', 'spark-bash-azure-databricks', 'azure-data-studio', 'azure-sdk-js', 'azure-machine-learning-service', 'azure-rm-template', 'azure-custom-domain', 'azure-bastion', 'azure-sdk-go', 'azure-cli2', 'azure-ad-b2c', 'azure-cosmosdb-sqlapi', 'sitecore-azure', 'azure-application-insights', 'azure-cosmosdb-emulator', 'azure-timeseries-insights', 'azure-service-hooks', 'azure-fluent-api', 'azure-monitor-workbooks', 'azure-web-app-firewall', 'azure-cost-calculation', 'azure-cosmosdb-mongovcore', 'azure-connect', 'azureml-python-sdk', 'azure-ad-v2', 'azure-storage', 'azure-iot-edge', 'azure-cdn', 'microsoft-entra-private-access', 'azure-in-role-cache', 'azure-hybrid-connections', 'azure-data-explorer', 'azure-ad-domain-services', 'azure-speech', 'azure-store', 'azure-webjobs-triggered', 'azure-function-app', 'azureportal', 'azure-stack', 'sql-azure-alerts', 'azure-virtual-machine', 'terraform-provider-azure', 'azure-webapps', 'azure-sdk-for-ruby', 'azure-vm-scale-set', 'azure-rtos', 'azure-hub', 'azure-cli', 'azure-qna-maker', 'azure-marketplace', 'azure-logic-apps', 'azure-cosmosdb', 'azure-app-registration', 'azure-application-registration', 'azure-managed-identity', 'azure-cognitive-search', 'azure-ml-pipelines', 'azure-cognitive-services', 'azure-mysql-database', 'azure-sdk-python', 'azure-container-instances', 'azure-ase', 'azure-spring-boot', 'azure-storage-emulator', 'azure-sql-server', 'azure-data-lake', 'azure-ad-verifiable-credentials'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}, {'collective': {'tags': ['google-cloud-ml', 'firebase-hosting', 'nativescript-firebase', 'dialogflow-cx', 'firebase-admin', 'google-prediction', 'google-cloud-data-fusion', 'looker-studio', 'firebase-cloud-messaging', 'google-cloud-transcoder', 'google-cloud-dataproc', 'google-cloud-automl-nl', 'firebase-console', 'google-app-engine-deploy', 'google-cloud-dataflow', 'firebase-polymer', 'google-cloud-trace', 'google-cloud-source-repos', 'google-fusion-tables', 'firebase-crash-reporting', 'firebase-tools', 'google-cloud-asset-inventory', 'gcloud', 'google-cloud-python', 'google-cloud-iot', 'google-cloud-metrics', 'firebase-storage', 'google-cloud-firestore', 'firebase-dynamic-links', 'firebase-extensions', 'firebase-predictions', 'google-cloud-pubsublite', 'google-cloud-cpp', 'google-cloud-automl', 'google-cloud-language', 'firebase-cli', 'google-cloud-platform', 'google-cloud-vertex-ai', 'google-cloud-nl', 'firebase-mlkit', 'google-migrate-for-compute-engine', 'firebase-assistant', 'google-cloud-dataprep', 'firebase-queue', 'firebase-security', 'firebase-database', 'react-native-firebase', 'google-cloud-functions', 'google-cloud-scheduler', 'google-container-optimized-os', 'google-cloud-php-client', 'google-container-builder', 'google-cloud-monitoring', 'google-app-engine-python', 'google-app-engine-php', 'google-cloud-data-transfer', 'google-cloud-registry', 'google-cloud-stackdriver', 'firebase-remote-config', 'google-cloud-datastore', 'google-cloud-instances', 'cloud-document-ai', 'google-cloud-run', 'google-cloud-datalab', 'google-cloud-composer', 'firebaseui', 'firebase-job-dispatcher', 'google-cloud-url-maps', 'google-cloud-visualstudio', 'google-cloud-kms', 'google-cloud-dns', 'google-cloud-identity', 'firebase-app-check', 'google-cloud-error-reporting', 'google-cloud-print-privet', 'google-cloud-workstations', 'google-anthos', 'rest-firebase', 'firebase-notifications', 'google-cloud-pubsub', 'firebase-app-indexing', 'apigee-baas', 'google-cloud-armor', 'firebase-authentication', 'firebase-test-lab', 'google-cloud-code', 'google-app-engine-patch', 'google-cloud-test-lab', 'google-bigquery', 'firebase-analytics', 'bigtable', 'stackdriver', 'maven-jib', 'dialogflow-es', 'firebase-util', 'firebasesimplelogin', 'firebase-realtime-database', 'google-app-engine', 'google-cloud-node', 'redux-saga-firebase', 'google-cloud-print', 'google-cloud-profiler', 'google-cloud-billing', 'google-kubernetes-engine', 'firebase-admob', 'google-cloud-tpu', 'google-cloud-launcher', 'google-cloud-translate', 'google-cloud-proxy', 'apigee', 'firebase', 'google-cloud-robotics', 'google-cloud-load-balancer', 'google-cloud-vision', 'google-cloud-vpn', 'vertex-ai-search', 'google-cloud-tasks', 'google-container-registry', 'google-compute-engine', 'google-cloud-save', 'google-cloud-dataproc-metastore', 'google-cloud-iam', 'google-cloud-sql', 'google-cloud-instance-template', 'google-cloud-logging', 'google-cloud-sdk', 'google-cloud-messaging', 'google-cloud-storage-r', 'google-cloud-api-gateway', 'google-cloud-ai-platform-pipelines', 'google-app-engine-golang', 'firebase-ab-testing', 'google-cloud-intellij', 'google-cloud-storage', 'google-cloud-marketplace', 'firebase-performance', 'google-cloud-internal-load-balancer', 'google-cloud-webrisk', 'google-cloud-console', 'google-cloud-dlp', 'google-cloud-shell-editor', 'google-cloud-speech', 'google-app-engine-launch', 'looker', 'google-cloud-ops-agent', 'google-cloud-networking', 'google-cloud-repository', 'google-cloud-talent-solution', 'google-cloud-endpoints-v2', 'recaptcha-enterprise', 'google-app-engine-go', 'google-cloud-endpoints', 'google-cloud-powershell', 'google-cloud-spanner-emulator', 'firebase-in-app-messaging', 'google-cloud-router', 'google-cloud-debugger', 'google-cloud-cdn', 'react-redux-firebase', 'google-cloud-http-load-balancer', 'google-cloud-identity-aware-proxy', 'google-cloud-tools', 'google-cloud-search', 'google-cloud-deploy', 'google-cloud-filestore', 'google-translate', 'google-container-os', 'google-cloud-recommendation', 'google-cloud-spanner', 'google-cloud-build', 'google-cloud-ml-engine', 'google-cloud-ai', 'google-cloud-shell', 'cordova-plugin-firebasex', 'firebase-machine-learning', 'firebase-app-distribution', 'google-cloud-bigtable', 'google-cloud-interconnect', 'google-cloud-memorystore', 'dialogflow-es-fulfillment', 'google-cloud-resource-manager', 'google-analytics-firebase', 'google-cloud-healthcare', 'jib', 'google-cloud-network-load-balancer', 'firebase-invites', 'google-dataflow'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}, {'collective': {'tags': ['amazon-macie', 'amazon-managed-blockchain', 'amazon-kinesis-video-streams', 'aws-sdk-net', 'aws-sdk-cpp', 'aws-sdk-nodejs', 'aws-vpn', 'amazon-workmail', 'aws-codecommit', 'aws-sdk-js-v3', 'amazon-rekognition', 'aws-serverless', 'aws-iot-sitewise', 'amazon-connect', 'amazon-workspaces', 'amazon-efs', 'amazon-elastic-beanstalk', 'aws-glue', 'aws-datasync', 'aws-xray', 'aws-sdk-ios', 'amazon-sumerian', 'amazon-kinesis-firehose', 'aws-acm', 'amazon-qldb', 'aws-iot-analytics', 'aws-codestar', 'amazon-ses', 'amazon-opensearch', 'amazon-kendra', 'aws-iot', 'aws-storage-gateway', 'aws-sdk-rust', 'aws-sdk-comprehend', 'alexa-account-linking', 'aws-elemental', 'aws-ssm', 'aws-sam-cli', 'amazon-ebs', 'amazon-timestream', 'aws-auto-scaling', 'aws-certificate-manager', 'alexa-interaction-model', 'aws-mediastore', 'aws-cli', 'amazon-location-service', 'aws-amplify', 'aws-copilot-cli', 'alexa-presentation-language', 'amazon-sagemaker', 'amazon-kinesis', 'aws-mobilehub', 'amazon-ecs', 'amazon-swf', 'aws-media-convert', 'aws-codeguru', 'aws-mediaconnect', 'aws-media-live', 'amazon-forecast', 'aws-codepipeline', 'aws-sdk-ruby', 'amazon-sqs', 'aws-cdk', 'amazon-athena', 'aws-direct-connect', 'aws-batch', 'amazon-lex', 'amazon-sns', 'amazon-ec2-spot-market', 'amazon-eks', 'aws-sam', 'aws-msk', 'aws-sso', 'aws-sdk-go', 'aws-snowball', 'alexa-smart-home-skill', 'aws-transfer-family', 'amazon-textract', 'amazon-inspector', 'aws-reserved-instances', 'aws-resource-group', 'aws-app-config', 'aws-cloudmap', 'amazon-ivs', 'aws-iot-core', 'aws-deeplens', 'amazon-web-services', 'amazon-kinesis-analytics', 'aws-application-load-balancer', 'amazon-guardduty', 'amazon-kms', 'aws-iam-identity-center', 'amazon-imagebuilder', 'aws-iot-greengrass', 'aws-device-farm', 'amazon-elastic-transcoder', 'amazon-rds', 'amazon-cloudwatchlogs', 'aws-fis', 'aws-global-accelerator', 'amazon-transcribe', 'aws-sdk-java-2.0', 'amazon-route53', 'aws-elb', 'amazon-cloudfront', 'amazon-cloudtrail', 'aws-mediatailor', 'amazon-redshift-spectrum', 'alexa-sdk-nodejs', 'alexa-sdk-python', 'amazon-keyspaces', 'aws-codebuild', 'aws-codecatalyst', 'aws-cloudshell', 'aws-nlb', 'aws-billing', 'aws-directory-services', 'amazon-quicksight', 'aws-appstream', 'aws-pinpoint', 'amazon-gamelift', 'amazon-s3', 'amazon-sagemaker-compilers', 'amazon-cloudwatch', 'alexa-skills-kit', 'aws-dms', 'aws-data-exchange', 'amazon-elasticsearch', 'aws-sct', 'aws-lambda-powertools', 'aws-event-bridge', 'aws-app-mesh', 'amazon-simpledb', 'alexa-smapi', 'amazon-dynamodb-dax', 'aws-iot-events', 'aws-appsync', 'aws-lambda-edge', 'amazon-cloudsearch', 'aws-control-tower', 'amazon-ecr', 'amazon-elasticache', 'amazon-workdocs', 'aws-sdk-go-v2', 'amazon-aurora', 'amazon-memory-db', 'amazon-lightsail', 'aws-step-functions', 'aws-sdk-java', 'aws-opsworks', 'aws-api-gateway', 'amazon-emr', 'amazon-cloudhsm', 'aws-sdk', 'aws-code-deploy', 'aws-lambda', 'amazon-redshift', 'elastic-ip', 'aws-elastictranscoder', 'amazon-fsx', 'amazon-iam', 'aws-codeartifact', 'aws-sdk-js', 'amazon-translate', 'aws-graviton', 'aws-security-hub', 'alexa-flash-briefing-skill', 'aws-private-link', 'aws-cloud9', 'amazon-waf', 'amazon-data-pipeline', 'aws-sdk-android', 'amazon-personalize', 'amazon-polly', 'aws-databrew', 'aws-secrets-manager', 'aws-backup', 'amazon-cognito', 'amazon-dynamodb', 'amazon-neptune', 'aws-chatbot', 'amazon-mq', 'amazon-ec2', 'amazon-vpc', 'aws-copilot', 'aws-fargate', 'aws-lake-formation', 'aws-cloudformation', 'aws-organizations', 'amazon-app-runner', 'amazon-ami', 'aws-config', 'aws-security-group', 'amazon-appflow', 'amazon-s3-select', 'aws-documentdb', 'aws-parameter-store', 'amazon-elb', 'amazon-bedrock', 'aws-mediapackage', 'amazon-glacier', 'aws-sdk-mock', 'amazon-honeycode', 'amazon-comprehend', 'aws-service-catalog'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 4181863, 'is_employee': False, 'last_modified_date': 1694667601, 'last_access_date': 1711051850, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4108, 'creation_date': 1476357067, 'user_type': 'registered', 'user_id': 7013263, 'accept_rate': 10, 'location': 'Bucharest, Romania', 'website_url': 'https://medium.com/@dejanualex', 'link': 'https://stackoverflow.com/users/7013263/dejanualex', 'profile_image': 'https://i.stack.imgur.com/41kW6.jpg?s=256&g=1', 'display_name': 'dejanualex'}","So classmethods can be used as an alternative &quot;constructor&quot; in python, they are bound to the class and not to an instance, pretty clear so far. But my question is, if it is mandatory to have the same number of arguments in the returned instance of the class method as in the . More exactly : And if I try to create an instance works fine, but if I try to create an instance I have a , because I pass to many arguments, and init take only 2 . Am I missing something?","__init__     class MyClass(object):
      def __init__(self,name):
         self.name=name

      @classmethod
      def alternative_init(cls,name,surname):
          return cls(name,surname)
 Myclass(&quot;alex&quot;) Myclass.alternative_init(&quot;alex&quot;,&quot;james&quot;) TypeError",2,10,0,0,
247,50121477,50122617,41905,Tensorflow no module named official,12,<python><tensorflow>,19,"<p>I am trying to use the nets from the official mnist directory of tensorflows model repository. On my windows system I receive this error:</p>

<pre><code>C:\Users\ry\Desktop\NNTesting\models\official\mnist&gt;mnist_test.py
Traceback (most recent call last):
  File ""C:\Users\ry\Desktop\NNTesting\models\official\mnist\mnist_test.py"",line 24, in &lt;module&gt;
    from official.mnist import mnist
ModuleNotFoundError: No module named 'official'
</code></pre>

<p>I have followed <a href=""https://github.com/tensorflow/models/tree/master/official#running-the-models"" rel=""noreferrer"">their official directions</a> and set my python path using</p>

<pre><code>set PYTHONPATH=""PYTHONPATH:""%cd%""
</code></pre>

<p>and can confirm that</p>

<pre><code>PYTHONPATH=""$PYTHONPATH:C:\Users\ry\Desktop\NNTesting\models""
</code></pre>

<p>and I have also installed the dependencies successfully. Does anyone have experience using these models on a windows system and can help me with this pathing issue? I'm not sure what I have done incorrectly here.</p>

<p>Thanks</p>
",7175088,361,01-05-2018 18:05,01-05-2018 19:29,0,361,9,1,2,,"{'badge_counts': {'bronze': 9, 'silver': 2, 'gold': 1}, 'account_id': 9669719, 'is_employee': False, 'last_modified_date': 1653864398, 'last_access_date': 1545673414, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 361, 'creation_date': 1479414867, 'user_type': 'registered', 'user_id': 7175088, 'website_url': '', 'link': 'https://stackoverflow.com/users/7175088/ryan-brady', 'profile_image': 'https://i.stack.imgur.com/VinrW.png?s=256&g=1', 'display_name': 'Ryan Brady'}",I am trying to use the nets from the official mnist directory of tensorflows model repository. On my windows system I receive this error: I have followed their official directions and set my python path using and can confirm that and I have also installed the dependencies successfully. Does anyone have experience using these models on a windows system and can help me with this pathing issue? I'm not sure what I have done incorrectly here. Thanks,"C:\Users\ry\Desktop\NNTesting\models\official\mnist&gt;mnist_test.py
Traceback (most recent call last):
  File ""C:\Users\ry\Desktop\NNTesting\models\official\mnist\mnist_test.py"",line 24, in &lt;module&gt;
    from official.mnist import mnist
ModuleNotFoundError: No module named 'official'
 set PYTHONPATH=""PYTHONPATH:""%cd%""
 PYTHONPATH=""$PYTHONPATH:C:\Users\ry\Desktop\NNTesting\models""
",4,22,0,1,
248,48864631,48868811,2680,What are the differences between a cpdef and a cdef wrapped in a def?,2,<python><cython>,11,"<p>In the Cython docs there is an <a href=""http://docs.cython.org/en/latest/src/userguide/early_binding_for_speed.html"" rel=""noreferrer"">example</a> where they give two ways of writing a C/Python hybrid method. An explicit one with a cdef for fast C access and a wrapper def for access from Python:</p>

<pre><code>cdef class Rectangle:
    cdef int x0, y0
    cdef int x1, y1
    def __init__(self, int x0, int y0, int x1, int y1):
        self.x0 = x0; self.y0 = y0; self.x1 = x1; self.y1 = y1
    cdef int _area(self):
        cdef int area
        area = (self.x1 - self.x0) * (self.y1 - self.y0)
        if area &lt; 0:
            area = -area
        return area
    def area(self):
        return self._area()
</code></pre>

<p>And one using cpdef:</p>

<pre><code>cdef class Rectangle:
    cdef int x0, y0
    cdef int x1, y1
    def __init__(self, int x0, int y0, int x1, int y1):
        self.x0 = x0; self.y0 = y0; self.x1 = x1; self.y1 = y1
    cpdef int area(self):
        cdef int area
        area = (self.x1 - self.x0) * (self.y1 - self.y0)
        if area &lt; 0:
            area = -area
        return area
</code></pre>

<p>I was wondering what the differences are in practical terms.</p>

<p>For example, is either method faster/slower when called from C/Python?</p>

<p>Also, when subclassing/overriding does cpdef offer anything that the other method lacks?</p>
",7207392,52505,19-02-2018 10:57,19-02-2018 14:59,0,52553,101,3,58,73,"{'badge_counts': {'bronze': 101, 'silver': 58, 'gold': 3}, 'account_id': 9717573, 'is_employee': False, 'last_modified_date': 1700341801, 'last_access_date': 1632814038, 'reputation_change_year': 258, 'reputation_change_quarter': 258, 'reputation_change_month': 78, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 52553, 'creation_date': 1480030740, 'user_type': 'registered', 'user_id': 7207392, 'accept_rate': 73, 'link': 'https://stackoverflow.com/users/7207392/paul-panzer', 'profile_image': 'https://www.gravatar.com/avatar/a761f6d6ab16a24e4e9395ec8aa65d23?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Paul Panzer'}","In the Cython docs there is an example where they give two ways of writing a C/Python hybrid method. An explicit one with a cdef for fast C access and a wrapper def for access from Python: And one using cpdef: I was wondering what the differences are in practical terms. For example, is either method faster/slower when called from C/Python? Also, when subclassing/overriding does cpdef offer anything that the other method lacks?","cdef class Rectangle:
    cdef int x0, y0
    cdef int x1, y1
    def __init__(self, int x0, int y0, int x1, int y1):
        self.x0 = x0; self.y0 = y0; self.x1 = x1; self.y1 = y1
    cdef int _area(self):
        cdef int area
        area = (self.x1 - self.x0) * (self.y1 - self.y0)
        if area &lt; 0:
            area = -area
        return area
    def area(self):
        return self._area()
 cdef class Rectangle:
    cdef int x0, y0
    cdef int x1, y1
    def __init__(self, int x0, int y0, int x1, int y1):
        self.x0 = x0; self.y0 = y0; self.x1 = x1; self.y1 = y1
    cpdef int area(self):
        cdef int area
        area = (self.x1 - self.x0) * (self.y1 - self.y0)
        if area &lt; 0:
            area = -area
        return area
",22,37,0,1,
249,49318402,49530874,94283,Read/Write single file in DataBricks,2,<python><pyspark><databricks>,17,"<p>I have a file which contains a list of names stored in a simple text file. Each row contains one name. Now I need to pro grammatically append a new name to this file based on a users input.
For the input itself I use DataBricks widgets - this is working just fine and I have the new name stored in a string object.
Now I need to append this name to my file.</p>

<p>the file is mounted in the DataBricks File System (DBFS) under /mnt/blob/myNames.txt</p>

<p>when trying to read the file like this:</p>

<pre><code>f = open(""/mnt/blob/myNames.txt"", ""r"") 
print f
</code></pre>

<p>it returns an error ""No such file or directory""</p>

<p>So I tried to wrap my new name into a dataframe and append it to the existing file but this also did not work as dataframe.write.save is designed to write into folders</p>

<p>what would be the most simple python could that I could use to append this new name to my file?</p>
",7220250,738,16-03-2018 10:25,28-03-2018 09:29,12,738,24,1,9,60,"{'badge_counts': {'bronze': 24, 'silver': 9, 'gold': 1}, 'account_id': 9737458, 'is_employee': False, 'last_modified_date': 1688712600, 'last_access_date': 1697822619, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 738, 'creation_date': 1480337452, 'user_type': 'registered', 'user_id': 7220250, 'accept_rate': 60, 'location': 'Austria', 'website_url': 'http://blog.gbrueckl.at', 'link': 'https://stackoverflow.com/users/7220250/gerhard-brueckl', 'profile_image': 'https://i.stack.imgur.com/0X9He.jpg?s=256&g=1', 'display_name': 'Gerhard Brueckl'}","I have a file which contains a list of names stored in a simple text file. Each row contains one name. Now I need to pro grammatically append a new name to this file based on a users input. For the input itself I use DataBricks widgets - this is working just fine and I have the new name stored in a string object. Now I need to append this name to my file. the file is mounted in the DataBricks File System (DBFS) under /mnt/blob/myNames.txt when trying to read the file like this: it returns an error ""No such file or directory"" So I tried to wrap my new name into a dataframe and append it to the existing file but this also did not work as dataframe.write.save is designed to write into folders what would be the most simple python could that I could use to append this new name to my file?","f = open(""/mnt/blob/myNames.txt"", ""r"") 
print f
",1,17,0,0,
250,48082264,48082385,46209,Implement Parallel for loops in Python,2,<python><python-3.x><python-2.7><parallel-processing>,15,"<p>I have a Python program which looks like this:</p>

<pre><code>total_error = []
for i in range(24):
    error = some_function_call(parameters1, parameters2)
    total_error += error
</code></pre>

<p>The function 'some_function_call' takes a lot of time and I can't find an easy way to reduce time complexity of the function. Is there a way to still reduce the execution time while performing parallel tasks and later adding them up in total_error.
I tried using pool and joblib but could not successfully use either.</p>
",7841612,822,03-01-2018 17:28,03-01-2018 17:37,0,822,24,1,9,20,"{'badge_counts': {'bronze': 24, 'silver': 9, 'gold': 1}, 'account_id': 10650463, 'is_employee': False, 'last_modified_date': 1685020504, 'last_access_date': 1711168992, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 822, 'creation_date': 1491765611, 'user_type': 'registered', 'user_id': 7841612, 'accept_rate': 20, 'location': 'Austin, TX', 'website_url': '', 'link': 'https://stackoverflow.com/users/7841612/thechargedneutron', 'profile_image': 'https://www.gravatar.com/avatar/30277a7fe167be3361c05051b5db004f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'thechargedneutron'}",I have a Python program which looks like this: The function 'some_function_call' takes a lot of time and I can't find an easy way to reduce time complexity of the function. Is there a way to still reduce the execution time while performing parallel tasks and later adding them up in total_error. I tried using pool and joblib but could not successfully use either.,"total_error = []
for i in range(24):
    error = some_function_call(parameters1, parameters2)
    total_error += error
",3,10,0,0,
251,49123439,49123627,15063,python how to run process in detached mode,2,<python><multiprocessing>,15,"<p>here is a example:</p>

<pre><code>from multiprocessing import Process
import time


def func():
    print('sub process is running')
    time.sleep(5)
    print('sub process finished')


if __name__ == '__main__':
    p = Process(target=func)
    p.start()
    print('done')
</code></pre>

<p>what I expect is that the main process will terminate right after it start a subprocess. But after printing out 'done', the terminal is still waiting....Is there any way to do this so that the main process will exit right after printing out 'done', instead of waiting for subprocess?  I'm confused here because I'm not calling <code>p.join()</code></p>
",6861219,3011,06-03-2018 04:34,06-03-2018 04:55,0,3011,67,5,33,88,"{'badge_counts': {'bronze': 67, 'silver': 33, 'gold': 5}, 'account_id': 9236879, 'is_employee': False, 'last_modified_date': 1703318700, 'last_access_date': 1685502497, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3011, 'creation_date': 1474501022, 'user_type': 'registered', 'user_id': 6861219, 'accept_rate': 88, 'link': 'https://stackoverflow.com/users/6861219/ziqi-liu', 'profile_image': 'https://graph.facebook.com/141586299631822/picture?type=large', 'display_name': 'Ziqi Liu'}","here is a example: what I expect is that the main process will terminate right after it start a subprocess. But after printing out 'done', the terminal is still waiting....Is there any way to do this so that the main process will exit right after printing out 'done', instead of waiting for subprocess? I'm confused here because I'm not calling","from multiprocessing import Process
import time


def func():
    print('sub process is running')
    time.sleep(5)
    print('sub process finished')


if __name__ == '__main__':
    p = Process(target=func)
    p.start()
    print('done')
 p.join()",12,19,0,0,
252,48313387,48314533,42112,ValueError: n_splits=10 cannot be greater than the number of members in each class,1,<python><scikit-learn><cross-validation>,19,"<p>I am trying to run the following code:</p>

<pre><code>from sklearn.model_selection import StratifiedKFold 
X = [""hey"", ""join now"", ""hello"", ""join today"", ""join us now"", ""not today"", ""join this trial"", "" hey hey"", "" no"", ""hola"", ""bye"", ""join today"", ""no"",""join join""]
y = [""n"", ""r"", ""n"", ""r"", ""r"", ""n"", ""n"", ""n"", ""n"", ""r"", ""n"", ""n"", ""n"", ""r""]

skf = StratifiedKFold(n_splits=10)

for train, test in skf.split(X,y):  
    print(""%s %s"" % (train,test))
</code></pre>

<p>But I get the following error:</p>

<pre><code>ValueError: n_splits=10 cannot be greater than the number of members in each class.
</code></pre>

<p>I have looked here <a href=""https://stackoverflow.com/questions/43179429/scikit-learn-error-the-least-populated-class-in-y-has-only-1-member"">scikit-learn error: The least populated class in y has only 1 member</a> but I'm still not really sure what is wrong with my code. </p>

<p>My lists both have lengths of 14 <code>print(len(X))</code> <code>print(len(y))</code>. </p>

<p>Part of my confusion is that I am not sure what a <code>members</code> is defined as and what a <code>class</code> is in this context.</p>

<p><strong>Questions:</strong> How do I fix the error? What is a member? What is a class? (in this context)  </p>
",6598999,753,18-01-2018 03:30,18-01-2018 05:42,0,763,23,2,11,91,"{'badge_counts': {'bronze': 23, 'silver': 11, 'gold': 2}, 'account_id': 8833927, 'is_employee': False, 'last_modified_date': 1636496400, 'last_access_date': 1624402722, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 763, 'creation_date': 1468724901, 'user_type': 'registered', 'user_id': 6598999, 'accept_rate': 91, 'website_url': '', 'link': 'https://stackoverflow.com/users/6598999/sfc', 'profile_image': 'https://www.gravatar.com/avatar/1e16a155de78a68c71b3017fa4ec5dee?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'SFC'}",I am trying to run the following code: But I get the following error: I have looked here scikit-learn error: The least populated class in y has only 1 member but I'm still not really sure what is wrong with my code. My lists both have lengths of 14 . Part of my confusion is that I am not sure what a is defined as and what a is in this context. Questions: How do I fix the error? What is a member? What is a class? (in this context),"from sklearn.model_selection import StratifiedKFold 
X = [""hey"", ""join now"", ""hello"", ""join today"", ""join us now"", ""not today"", ""join this trial"", "" hey hey"", "" no"", ""hola"", ""bye"", ""join today"", ""no"",""join join""]
y = [""n"", ""r"", ""n"", ""r"", ""r"", ""n"", ""n"", ""n"", ""n"", ""r"", ""n"", ""n"", ""n"", ""r""]

skf = StratifiedKFold(n_splits=10)

for train, test in skf.split(X,y):  
    print(""%s %s"" % (train,test))
 ValueError: n_splits=10 cannot be greater than the number of members in each class.
 print(len(X)) print(len(y)) members class",3,24,0,1,
253,49819908,49867775,23521,VSCode debugger attach to local process,2,<python><visual-studio-code><attach-to-process>,20,"<p>One of the great features of PyCharm is that it allows its debugger to attach to python processes running locally (and outside of the IDE). </p>

<p>As I am trying to move to VSCode to work in Python, I am struggling to configure <strong>launch.json</strong> to simulate PyCharm's attach to local process feature.</p>

<pre><code>{
    ""name"": ""Python: Attach"",
    ""type"": ""python"",
    ""request"": ""attach"",
    ""localRoot"": ""${workspaceFolder}"",
    ""remoteRoot"": ""${workspaceFolder}"",
    ""port"": 8001,
    ""secret"": ""my_secret"",
    ""host"": ""localhost""
},
</code></pre>

<p>This configuration is created by default when I select <strong>Python: attach</strong> option for debugger but I am convinced this is for remote debugging (with port and all), and most Google search results just talk about remote debugging for Python with VSCode.</p>

<p>Anyone had success in attaching a local debugger or two to multiple python processes running locally?</p>
",6632550,508,13-04-2018 14:58,16-04-2018 23:47,3,508,20,1,7,92,"{'badge_counts': {'bronze': 20, 'silver': 7, 'gold': 1}, 'account_id': 8883355, 'is_employee': False, 'last_modified_date': 1625950801, 'last_access_date': 1699036644, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 508, 'creation_date': 1469393130, 'user_type': 'registered', 'user_id': 6632550, 'accept_rate': 92, 'website_url': '', 'link': 'https://stackoverflow.com/users/6632550/jun-park', 'profile_image': 'https://i.stack.imgur.com/ZH3zD.jpg?s=256&g=1', 'display_name': 'Jun Park'}","One of the great features of PyCharm is that it allows its debugger to attach to python processes running locally (and outside of the IDE). As I am trying to move to VSCode to work in Python, I am struggling to configure launch.json to simulate PyCharm's attach to local process feature. This configuration is created by default when I select Python: attach option for debugger but I am convinced this is for remote debugging (with port and all), and most Google search results just talk about remote debugging for Python with VSCode. Anyone had success in attaching a local debugger or two to multiple python processes running locally?","{
    ""name"": ""Python: Attach"",
    ""type"": ""python"",
    ""request"": ""attach"",
    ""localRoot"": ""${workspaceFolder}"",
    ""remoteRoot"": ""${workspaceFolder}"",
    ""port"": 8001,
    ""secret"": ""my_secret"",
    ""host"": ""localhost""
},
",9,19,0,0,
254,49602671,49602709,26144,Make One-Row Dataframe,3,<python><pandas><dataframe><naming>,11,"<p>I'm unable to construct a dataframe from 3 individual numbers. I want to do this in order for a function to return the dataframe, which I then append to other existing results.</p>

<p>Desired result is a dataframe with columns named ""a"", ""b"" and ""C"", each containing the value of a, b, and c.</p>

<p>Try one:</p>

<pre><code>a=1
b=2
c=3
dat=pd.DataFrame([a,b,c], columns=list('abc')) #fails with size error
</code></pre>

<p>Try two:</p>

<pre><code>dat=pd.DataFrame()
dat['a']=pd.np.nan
dat['b']=pd.np.nan
dat['c']=pd.np.nan

dat['c']=c # no numbers are added to the column; still has 0 rows
</code></pre>

<p>What am I missing here?</p>

<p>Desired result is:</p>

<pre><code>    a  | b  | c
   -------------
    1  | 2  | 3
</code></pre>
",6710738,1147,01-04-2018 21:12,01-04-2018 21:17,0,1157,24,3,13,71,"{'badge_counts': {'bronze': 24, 'silver': 13, 'gold': 3}, 'account_id': 9002716, 'is_employee': False, 'last_modified_date': 1648510802, 'last_access_date': 1683851582, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1157, 'creation_date': 1471030937, 'user_type': 'registered', 'user_id': 6710738, 'accept_rate': 71, 'location': 'Anchorage, AK, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/6710738/ehb', 'profile_image': 'https://www.gravatar.com/avatar/1b7447ec381de023c80d298da16060cb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'EHB'}","I'm unable to construct a dataframe from 3 individual numbers. I want to do this in order for a function to return the dataframe, which I then append to other existing results. Desired result is a dataframe with columns named ""a"", ""b"" and ""C"", each containing the value of a, b, and c. Try one: Try two: What am I missing here? Desired result is:","a=1
b=2
c=3
dat=pd.DataFrame([a,b,c], columns=list('abc')) #fails with size error
 dat=pd.DataFrame()
dat['a']=pd.np.nan
dat['b']=pd.np.nan
dat['c']=pd.np.nan

dat['c']=c # no numbers are added to the column; still has 0 rows
     a  | b  | c
   -------------
    1  | 2  | 3
",10,30,0,0,
255,50108907,50109274,19546,Flask request.files is empty,1,<python><flask>,16,"<p>much like <a href=""https://stackoverflow.com/questions/25589649/why-request-files-is-empty"">this question</a>, I'm trying to follow the <a href=""http://flask.pocoo.org/docs/0.12/patterns/fileuploads/"" rel=""noreferrer"">simple Flask tutorial</a> for file upload to a flask server. In my specific case, I'm trying to upload an XML file.</p>

<p>The (simplified) HTML I'm using is:</p>

<pre><code>&lt;form action="""" method=""post"" enctype=""multipart/form-data""&gt;
    &lt;input type=""file""&gt;
    &lt;input type=""submit"" value=""Let's go!""&gt;
&lt;/form&gt;
</code></pre>

<p>The request is correctly handled by a <code>if request.method == 'POST':</code> block, so I put in some print statements to troubleshoot:</p>

<pre><code>print('request.method', request.method)
print('request.args', request.args)
print('request.form', request.form)
print('request.files', request.files)
</code></pre>

<p>and the result was the following:</p>

<pre><code>request.method POST
request.args ImmutableMultiDict([])
request.form ImmutableMultiDict([])
request.files ImmutableMultiDict([])
</code></pre>

<p>What am I doing wrong? I can provide more complete source code if needed.</p>
",6894799,878,30-04-2018 22:12,30-04-2018 22:58,0,878,17,1,10,,"{'badge_counts': {'bronze': 17, 'silver': 10, 'gold': 1}, 'account_id': 9284454, 'is_employee': False, 'last_modified_date': 1656679093, 'last_access_date': 1674068148, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 878, 'creation_date': 1475077003, 'user_type': 'registered', 'user_id': 6894799, 'website_url': 'http://swag.lgbt', 'link': 'https://stackoverflow.com/users/6894799/cass', 'profile_image': 'https://www.gravatar.com/avatar/08a06003dcb5d4f16dc507059bf1787c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'cass'}","much like this question, I'm trying to follow the simple Flask tutorial for file upload to a flask server. In my specific case, I'm trying to upload an XML file. The (simplified) HTML I'm using is: The request is correctly handled by a block, so I put in some print statements to troubleshoot: and the result was the following: What am I doing wrong? I can provide more complete source code if needed.","&lt;form action="""" method=""post"" enctype=""multipart/form-data""&gt;
    &lt;input type=""file""&gt;
    &lt;input type=""submit"" value=""Let's go!""&gt;
&lt;/form&gt;
 if request.method == 'POST': print('request.method', request.method)
print('request.args', request.args)
print('request.form', request.form)
print('request.files', request.files)
 request.method POST
request.args ImmutableMultiDict([])
request.form ImmutableMultiDict([])
request.files ImmutableMultiDict([])
",8,27,0,2,
256,48728307,48818096,4481,Inferring date format versus passing a parser,3,<python><pandas><datetime><python-dateutil>,13,"<p>Pandas internals question: I've been surprised to find a few times that explicitly passing a callable to <code>date_parser</code> within <code>pandas.read_csv</code> results in <em>much</em> slower read time than simply using <code>infer_datetime_format=True</code>.</p>

<p>Why is this?  Will timing differences between these two options be date-format-specific, or what other factors will influence their relative timing?</p>

<p>In the below case, <code>infer_datetime_format=True</code> takes one-tenth the time of passing a date parser with a specified format.  I would have naively assumed the latter would be faster because it's explicit.</p>

<p>The docs do note, </p>

<blockquote>
  <p>[if True,] pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x.</p>
</blockquote>

<p>but there's not much detail given and I was unable to work my way fully through the source.</p>

<p>Setup:</p>

<pre><code>from io import StringIO

import numpy as np
import pandas as pd

np.random.seed(444)
dates = pd.date_range('1980', '2018')
df = pd.DataFrame(np.random.randint(0, 100, (len(dates), 2)),
                  index=dates).add_prefix('col').reset_index()

# Something reproducible to be read back in
buf = StringIO()
df.to_string(buf=buf, index=False)

def read_test(**kwargs):
    # Not ideal for .seek() to eat up runtime, but alleviate
    # this with more loops than needed in timing below
    buf.seek(0)
    return pd.read_csv(buf, sep='\s+', parse_dates=['index'], **kwargs)

# dateutil.parser.parser called in this case, according to docs
%timeit -r 7 -n 100 read_test()
18.1 ms ± 217 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

%timeit -r 7 -n 100 read_test(infer_datetime_format=True)
19.8 ms ± 516 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# Doesn't change with native Python datetime.strptime either
%timeit -r 7 -n 100 read_test(date_parser=lambda dt: pd.datetime.strptime(dt, '%Y-%m-%d'))
187 ms ± 4.05 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>

<p>I'm interested in knowing a bit about what is going on internally with <code>infer</code> to give it this advantage.  My old understanding was that there was already some type of inference going on in the first place because <code>dateutil.parser.parser</code> is used if neither is passed.</p>

<hr>

<p><strong>Update</strong>: did some digging on this but haven't been able to answer the question.  </p>

<p><code>read_csv()</code> calls a <a href=""https://github.com/pandas-dev/pandas/blob/v0.22.0/pandas/io/parsers.py#L3009"" rel=""noreferrer"">helper function</a> which in turn calls <a href=""https://github.com/pandas-dev/pandas/blob/master/pandas/core/tools/datetimes.py#L106"" rel=""noreferrer""><code>pd.core.tools.datetimes.to_datetime()</code></a>.  That function (accessible as just <code>pd.to_datetime()</code>) has both an <code>infer_datetime_format</code> and a <code>format</code> argument.</p>

<p>However, in this case, the relative timings are very different and don't reflect the above:</p>

<pre><code>s = pd.Series(['3/11/2000', '3/12/2000', '3/13/2000']*1000)

%timeit pd.to_datetime(s,infer_datetime_format=True)
19.8 ms ± 1.54 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit pd.to_datetime(s,infer_datetime_format=False)
1.01 s ± 65.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# This was taking the longest with i/o functions,
# now it's behaving ""as expected""
%timeit pd.to_datetime(s,format='%m/%d/%Y')
19 ms ± 373 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
",7954504,39578,11-02-2018 04:34,15-02-2018 23:46,4,39618,244,34,154,99,"{'badge_counts': {'bronze': 244, 'silver': 154, 'gold': 34}, 'account_id': 10815338, 'is_employee': False, 'last_modified_date': 1709973300, 'last_access_date': 1705676794, 'reputation_change_year': 301, 'reputation_change_quarter': 301, 'reputation_change_month': 81, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 39618, 'creation_date': 1493765210, 'user_type': 'registered', 'user_id': 7954504, 'accept_rate': 99, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/7954504/brad-solomon', 'profile_image': 'https://i.stack.imgur.com/6POkt.jpg?s=256&g=1', 'display_name': 'Brad Solomon'}","Pandas internals question: I've been surprised to find a few times that explicitly passing a callable to within results in much slower read time than simply using . Why is this? Will timing differences between these two options be date-format-specific, or what other factors will influence their relative timing? In the below case, takes one-tenth the time of passing a date parser with a specified format. I would have naively assumed the latter would be faster because it's explicit. The docs do note, [if True,] pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the parsing speed by 5-10x. but there's not much detail given and I was unable to work my way fully through the source. Setup: I'm interested in knowing a bit about what is going on internally with to give it this advantage. My old understanding was that there was already some type of inference going on in the first place because is used if neither is passed. Update: did some digging on this but haven't been able to answer the question. calls a helper function which in turn calls . That function (accessible as just ) has both an and a argument. However, in this case, the relative timings are very different and don't reflect the above:","date_parser pandas.read_csv infer_datetime_format=True infer_datetime_format=True from io import StringIO

import numpy as np
import pandas as pd

np.random.seed(444)
dates = pd.date_range('1980', '2018')
df = pd.DataFrame(np.random.randint(0, 100, (len(dates), 2)),
                  index=dates).add_prefix('col').reset_index()

# Something reproducible to be read back in
buf = StringIO()
df.to_string(buf=buf, index=False)

def read_test(**kwargs):
    # Not ideal for .seek() to eat up runtime, but alleviate
    # this with more loops than needed in timing below
    buf.seek(0)
    return pd.read_csv(buf, sep='\s+', parse_dates=['index'], **kwargs)

# dateutil.parser.parser called in this case, according to docs
%timeit -r 7 -n 100 read_test()
18.1 ms ± 217 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

%timeit -r 7 -n 100 read_test(infer_datetime_format=True)
19.8 ms ± 516 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# Doesn't change with native Python datetime.strptime either
%timeit -r 7 -n 100 read_test(date_parser=lambda dt: pd.datetime.strptime(dt, '%Y-%m-%d'))
187 ms ± 4.05 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
 infer dateutil.parser.parser read_csv() pd.core.tools.datetimes.to_datetime() pd.to_datetime() infer_datetime_format format s = pd.Series(['3/11/2000', '3/12/2000', '3/13/2000']*1000)

%timeit pd.to_datetime(s,infer_datetime_format=True)
19.8 ms ± 1.54 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit pd.to_datetime(s,infer_datetime_format=False)
1.01 s ± 65.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# This was taking the longest with i/o functions,
# now it's behaving ""as expected""
%timeit pd.to_datetime(s,format='%m/%d/%Y')
19 ms ± 373 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
",29,71,0,2,
257,48466959,48467116,40044,Query for list of attribute instead of tuples in SQLAlchemy,4,<python><sqlalchemy>,57,"<p>I'm querying for the ids of a model, and get a list of <code>(int,)</code> tuples back instead of a list of ids. Is there a way to query for the attribute directly?</p>

<pre><code>result = session.query(MyModel.id).all()
</code></pre>

<p>I realize it's possible to do </p>

<pre><code>results = [r for (r,) in results]
</code></pre>

<p>Is it possible for the query to return that form directly, instead of having to process it myself?</p>
",7684175,1076,26-01-2018 17:57,26-01-2018 18:06,0,1076,27,1,12,64,"{'badge_counts': {'bronze': 27, 'silver': 12, 'gold': 1}, 'account_id': 10421688, 'is_employee': False, 'last_modified_date': 1655343301, 'last_access_date': 1711008591, 'reputation_change_year': 28, 'reputation_change_quarter': 28, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1076, 'creation_date': 1489060541, 'user_type': 'registered', 'user_id': 7684175, 'accept_rate': 64, 'link': 'https://stackoverflow.com/users/7684175/papek24', 'profile_image': 'https://www.gravatar.com/avatar/30fa300e15ac4deeb75080e79119abcc?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'PapeK24'}","I'm querying for the ids of a model, and get a list of tuples back instead of a list of ids. Is there a way to query for the attribute directly? I realize it's possible to do Is it possible for the query to return that form directly, instead of having to process it myself?","(int,) result = session.query(MyModel.id).all()
 results = [r for (r,) in results]
",-1,11,0,0,
258,49906237,49906297,98326,How to find button with Selenium by its text inside (Python)?,4,<python><selenium><selenium-webdriver>,45,"<p>I have the following three buttons that I can't figure out how to grab the text that is inside of them (e.g Outliers). I tried <code>browser.find_element_by_link_text(""Outliers"").click()</code>, but got ""Unable to locate element"" error. How can I do it?</p>

<p><a href=""https://i.stack.imgur.com/e1ulw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/e1ulw.png"" alt=""enter image description here""></a></p>
",7692921,2845,18-04-2018 18:03,18-04-2018 18:07,0,2865,48,5,25,63,"{'badge_counts': {'bronze': 48, 'silver': 25, 'gold': 5}, 'account_id': 10434010, 'is_employee': False, 'last_modified_date': 1696038601, 'last_access_date': 1710971740, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2865, 'creation_date': 1489190599, 'user_type': 'registered', 'user_id': 7692921, 'accept_rate': 63, 'location': 'California, United States', 'link': 'https://stackoverflow.com/users/7692921/sprogissd', 'profile_image': 'https://www.gravatar.com/avatar/492075e8f2b83948c3671abcc2a2f0d3?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'sprogissd'}","I have the following three buttons that I can't figure out how to grab the text that is inside of them (e.g Outliers). I tried , but got ""Unable to locate element"" error. How can I do it?","browser.find_element_by_link_text(""Outliers"").click()",-1,3,1,1,
259,49752808,49752872,23595,What's the vertical line in PyCharm?,4,<python><pycharm>,23,"<p>In PyCharm, there is a vertical line as highlighted on the screenshot.</p>

<p>When I copy a long line of code into the editor, it wraps automatically if longer than this line.</p>

<p><a href=""https://i.stack.imgur.com/Np5PU.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Np5PU.jpg"" alt=""screenshot""></a></p>

<p>What is this line and what's its purpose?</p>
",7693832,6409,10-04-2018 11:45,10-04-2018 11:49,0,6461,124,20,71,66,"{'badge_counts': {'bronze': 124, 'silver': 71, 'gold': 20}, 'account_id': 10435346, 'is_employee': False, 'last_modified_date': 1711011615, 'last_access_date': 1711013034, 'reputation_change_year': 152, 'reputation_change_quarter': 152, 'reputation_change_month': 82, 'reputation_change_week': 22, 'reputation_change_day': 0, 'reputation': 6461, 'creation_date': 1489216876, 'user_type': 'registered', 'user_id': 7693832, 'accept_rate': 66, 'website_url': '', 'link': 'https://stackoverflow.com/users/7693832/user7693832', 'profile_image': 'https://i.stack.imgur.com/i0kVl.png?s=256&g=1', 'display_name': 'user7693832'}","In PyCharm, there is a vertical line as highlighted on the screenshot. When I copy a long line of code into the editor, it wraps automatically if longer than this line. What is this line and what's its purpose?",,0,7,1,1,
260,48363425,48363511,11130,Cython: How to print without GIL,2,<python><cython><cythonize>,18,"<p>How should I use <code>print</code> in a Cython function with no gil? For example:</p>

<pre><code>from libc.math cimport log, fabs
cpdef double f(double a, double b) nogil:
    cdef double c = log( fabs(a - b) )
    print c
    return c
</code></pre>

<p>gives this error when compiling:</p>

<pre><code>Error compiling Cython file:
...
    print c
    ^
------------------------------------------------------------

Python print statement not allowed without gil
...
</code></pre>

<p>I know how to use C libraries instead of their python equivalent (<code>math</code> library for example here) but I couldn't find a similar way for <code>print</code>. </p>
",7912237,904,21-01-2018 03:31,21-01-2018 03:52,0,904,25,2,10,93,"{'badge_counts': {'bronze': 25, 'silver': 10, 'gold': 2}, 'account_id': 10753703, 'is_employee': False, 'last_modified_date': 1607614440, 'last_access_date': 1668030815, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 904, 'creation_date': 1493018356, 'user_type': 'registered', 'user_id': 7912237, 'accept_rate': 93, 'location': 'Melbourne VIC, Australia', 'website_url': '', 'link': 'https://stackoverflow.com/users/7912237/behzad-jamali', 'profile_image': 'https://i.stack.imgur.com/PmxCe.jpg?s=256&g=1', 'display_name': 'Behzad Jamali'}",How should I use in a Cython function with no gil? For example: gives this error when compiling: I know how to use C libraries instead of their python equivalent ( library for example here) but I couldn't find a similar way for .,"print from libc.math cimport log, fabs
cpdef double f(double a, double b) nogil:
    cdef double c = log( fabs(a - b) )
    print c
    return c
 Error compiling Cython file:
...
    print c
    ^
------------------------------------------------------------

Python print statement not allowed without gil
...
 math print",8,22,0,0,
261,48390601,48398977,20531,Explicitly specifying test/train sets in GridSearchCV,3,<python><scikit-learn><grid-search>,19,"<p>I have a question about the <code>cv</code> parameter of sklearn's <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""noreferrer""><code>GridSearchCV</code></a>.</p>

<p>I'm working with data that has a time component to it, so I don't think random shuffling within KFold cross-validation seems sensible.</p>

<p>Instead, I want to explicitly specify cutoffs for training, validation, and test data within a <code>GridSearchCV</code>.  Can I do this?</p>

<p>To better illuminate the question, here's how I would to that manually.</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge
np.random.seed(444)

index = pd.date_range('2014', periods=60, freq='M')
X, y = make_regression(n_samples=60, n_features=3, random_state=444, noise=90.)
X = pd.DataFrame(X, index=index, columns=list('abc'))
y = pd.Series(y, index=index, name='y')

# Train on the first 30 samples, validate on the next 10, test on
#     the final 10.
X_train, X_val, X_test = np.array_split(X, [35, 50])
y_train, y_val, y_test = np.array_split(y, [35, 50])

param_grid = {'alpha': np.linspace(0, 1, 11)}
model = None
best_param_ = None
best_score_ = -np.inf

# Manual implementation
for alpha in param_grid['alpha']:
    ridge = Ridge(random_state=444, alpha=alpha).fit(X_train, y_train)
    score = ridge.score(X_val, y_val)
    if score &gt; best_score_:
        best_score_ = score
        best_param_ = alpha
        model = ridge

print('Optimal alpha parameter: {:0.2f}'.format(best_param_))
print('Best score (on validation data): {:0.2f}'.format(best_score_))
print('Test set score: {:.2f}'.format(model.score(X_test, y_test)))
# Optimal alpha parameter: 1.00
# Best score (on validation data): 0.64
# Test set score: 0.22
</code></pre>

<p>The process here is:</p>

<ul>
<li>For both X and Y, I want a training set, validation set, and testing set.  The training set is the first 35 samples in the time series.  The validation set is the next 15 samples.  The test set is the final 10.</li>
<li>The train and validation sets are use to determine the optimal <code>alpha</code> parameter within Ridge regression.  Here I test <code>alpha</code>s of (0.0, 0.1, ..., 0.9, 1.0).</li>
<li>The test set is held out for the ""actual"" testing as unseen data.</li>
</ul>

<p>Anyways ... It seems like I'm looking to do something like this, but am not sure what to pass to <code>cv</code> here:</p>

<pre><code>from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(Ridge(random_state=444), param_grid, cv= ???)
grid_search.fit(...?)
</code></pre>

<p>The docs, which I'm having trouble interpreting, specify:</p>

<blockquote>
  <p><code>cv</code> : int, cross-validation generator or an iterable, optional</p>
  
  <p>Determines the cross-validation splitting strategy. Possible inputs
  for cv are:</p>
  
  <ul>
  <li>None, to use the default 3-fold cross validation, </li>
  <li>integer, to specify the number of folds in a (Stratified)KFold, </li>
  <li>An object to be used as a cross-validation generator. </li>
  <li>An iterable yielding train, test splits.</li>
  </ul>
  
  <p>For integer/None inputs, if the estimator is a classifier and y is
  either binary or multiclass, StratifiedKFold is used. In all other
  cases, KFold is used.</p>
</blockquote>
",7954504,39578,22-01-2018 21:32,23-01-2018 10:06,1,39618,244,34,154,99,"{'badge_counts': {'bronze': 244, 'silver': 154, 'gold': 34}, 'account_id': 10815338, 'is_employee': False, 'last_modified_date': 1709973300, 'last_access_date': 1705676794, 'reputation_change_year': 301, 'reputation_change_quarter': 301, 'reputation_change_month': 81, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 39618, 'creation_date': 1493765210, 'user_type': 'registered', 'user_id': 7954504, 'accept_rate': 99, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/7954504/brad-solomon', 'profile_image': 'https://i.stack.imgur.com/6POkt.jpg?s=256&g=1', 'display_name': 'Brad Solomon'}","I have a question about the parameter of sklearn's . I'm working with data that has a time component to it, so I don't think random shuffling within KFold cross-validation seems sensible. Instead, I want to explicitly specify cutoffs for training, validation, and test data within a . Can I do this? To better illuminate the question, here's how I would to that manually. The process here is: For both X and Y, I want a training set, validation set, and testing set. The training set is the first 35 samples in the time series. The validation set is the next 15 samples. The test set is the final 10. The train and validation sets are use to determine the optimal parameter within Ridge regression. Here I test s of (0.0, 0.1, ..., 0.9, 1.0). The test set is held out for the ""actual"" testing as unseen data. Anyways ... It seems like I'm looking to do something like this, but am not sure what to pass to here: The docs, which I'm having trouble interpreting, specify: : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.","cv GridSearchCV GridSearchCV import numpy as np
import pandas as pd
from sklearn.linear_model import Ridge
np.random.seed(444)

index = pd.date_range('2014', periods=60, freq='M')
X, y = make_regression(n_samples=60, n_features=3, random_state=444, noise=90.)
X = pd.DataFrame(X, index=index, columns=list('abc'))
y = pd.Series(y, index=index, name='y')

# Train on the first 30 samples, validate on the next 10, test on
#     the final 10.
X_train, X_val, X_test = np.array_split(X, [35, 50])
y_train, y_val, y_test = np.array_split(y, [35, 50])

param_grid = {'alpha': np.linspace(0, 1, 11)}
model = None
best_param_ = None
best_score_ = -np.inf

# Manual implementation
for alpha in param_grid['alpha']:
    ridge = Ridge(random_state=444, alpha=alpha).fit(X_train, y_train)
    score = ridge.score(X_val, y_val)
    if score &gt; best_score_:
        best_score_ = score
        best_param_ = alpha
        model = ridge

print('Optimal alpha parameter: {:0.2f}'.format(best_param_))
print('Best score (on validation data): {:0.2f}'.format(best_score_))
print('Test set score: {:.2f}'.format(model.score(X_test, y_test)))
# Optimal alpha parameter: 1.00
# Best score (on validation data): 0.64
# Test set score: 0.22
 alpha alpha cv from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(Ridge(random_state=444), param_grid, cv= ???)
grid_search.fit(...?)
 cv",29,79,0,1,
262,49928463,49928542,100685,Python Pandas update a dataframe value from another dataframe,10,<python><pandas><dataframe>,72,"<p>I have two dataframes in python. I want to update rows in first dataframe using  matching values from another dataframe. Second dataframe serves as an override. </p>

<p>Here is an example with same data and code:  </p>

<p>DataFrame 1 : </p>

<p><a href=""https://i.stack.imgur.com/QPJs2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QPJs2.png"" alt=""enter image description here""></a></p>

<p>DataFrame 2: </p>

<p><a href=""https://i.stack.imgur.com/Xbpmh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xbpmh.png"" alt=""enter image description here""></a></p>

<p>I want to update update dataframe 1 based on matching code and name. In this example Dataframe 1 should be updated as below: </p>

<p><a href=""https://i.stack.imgur.com/JA3Fl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JA3Fl.png"" alt=""enter image description here""></a></p>

<p>Note : Row with Code =2 and Name= Company2 is updated with value 1000 (coming from Dataframe 2) </p>

<pre><code>import pandas as pd

data1 = {
         'Code': [1, 2, 3],
         'Name': ['Company1', 'Company2', 'Company3'],
         'Value': [200, 300, 400],

    }
df1 = pd.DataFrame(data1, columns= ['Code','Name','Value'])

data2 = {
         'Code': [2],
         'Name': ['Company2'],
         'Value': [1000],
    }

df2 = pd.DataFrame(data2, columns= ['Code','Name','Value'])
</code></pre>

<p>Any pointers or hints? </p>
",7369926,2590,19-04-2018 19:05,19-04-2018 19:11,0,2590,66,8,40,97,"{'badge_counts': {'bronze': 66, 'silver': 40, 'gold': 8}, 'account_id': 9960399, 'is_employee': False, 'last_modified_date': 1662776100, 'last_access_date': 1708536987, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2590, 'creation_date': 1483454468, 'user_type': 'registered', 'user_id': 7369926, 'accept_rate': 97, 'location': 'New York, NY, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/7369926/progsky', 'profile_image': 'https://www.gravatar.com/avatar/669a028e7911a2e5bf2771dc6d597cad?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ProgSky'}",I have two dataframes in python. I want to update rows in first dataframe using matching values from another dataframe. Second dataframe serves as an override. Here is an example with same data and code: DataFrame 1 : DataFrame 2: I want to update update dataframe 1 based on matching code and name. In this example Dataframe 1 should be updated as below: Note : Row with Code =2 and Name= Company2 is updated with value 1000 (coming from Dataframe 2) Any pointers or hints?,"import pandas as pd

data1 = {
         'Code': [1, 2, 3],
         'Name': ['Company1', 'Company2', 'Company3'],
         'Value': [200, 300, 400],

    }
df1 = pd.DataFrame(data1, columns= ['Code','Name','Value'])

data2 = {
         'Code': [2],
         'Name': ['Company2'],
         'Value': [1000],
    }

df2 = pd.DataFrame(data2, columns= ['Code','Name','Value'])
",16,38,3,3,
263,48796729,48796799,35270,How to add a year to a column of dates in pandas,5,<python><pandas><datetime>,23,"<p>I am attempting to add a year to a column of dates in a pandas dataframe, but when I use <code>pd.to_timedelta</code> I get additional hours &amp; minutes. I know I could take the updated time and truncate the hours, but I feel like there must be a way to add a year precisely. My attempt as follows:</p>

<pre><code>import pandas as pd
dates = pd.DataFrame({'date':['20170101','20170102','20170103']})
dates['date'] = pd.to_datetime(dates['date'], format='%Y%m%d')
dates['date2'] = dates['date'] +  pd.to_timedelta(1, unit='y')
dates
</code></pre>

<p>yields:</p>

<pre><code>Out[1]: 
    date        date2
0   2017-01-01  2018-01-01 05:49:12
1   2017-01-02  2018-01-02 05:49:12
2   2017-01-03  2018-01-03 05:49:12
</code></pre>

<p>How can I add a year without adding 05:49:12 HH:mm:ss?</p>
",7710904,1977,14-02-2018 21:43,14-02-2018 21:47,0,1977,20,2,12,,"{'badge_counts': {'bronze': 20, 'silver': 12, 'gold': 2}, 'account_id': 10459755, 'is_employee': False, 'last_modified_date': 1620302700, 'last_access_date': 1539747656, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1977, 'creation_date': 1489519464, 'user_type': 'registered', 'user_id': 7710904, 'location': 'San Francisco, CA, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/7710904/pdubbs', 'profile_image': 'https://www.gravatar.com/avatar/6bcbc9928f1c2afd97cc631e213c2f04?s=256&d=identicon&r=PG', 'display_name': 'Pdubbs'}","I am attempting to add a year to a column of dates in a pandas dataframe, but when I use I get additional hours &amp; minutes. I know I could take the updated time and truncate the hours, but I feel like there must be a way to add a year precisely. My attempt as follows: yields: How can I add a year without adding 05:49:12 HH:mm:ss?","pd.to_timedelta import pandas as pd
dates = pd.DataFrame({'date':['20170101','20170102','20170103']})
dates['date'] = pd.to_datetime(dates['date'], format='%Y%m%d')
dates['date2'] = dates['date'] +  pd.to_timedelta(1, unit='y')
dates
 Out[1]: 
    date        date2
0   2017-01-01  2018-01-01 05:49:12
1   2017-01-02  2018-01-02 05:49:12
2   2017-01-03  2018-01-03 05:49:12
",7,19,0,0,
264,48060354,48091770,31916,"ConfigurationError: Server at 127.0.0.1:27017 reports wire version 0, but this version of PyMongo requires at least 2 (MongoDB 2.6)",5,<python><mongodb><flask>,11,"<p>I am trying to build an application with mongoDB and Python Flask. While running the application, I am getting below error:</p>

<blockquote>
  <p>ConfigurationError: Server at 127.0.0.1:27017 reports wire version 0,
  but this version of PyMongo requires at least 2 (MongoDB 2.6).</p>
</blockquote>

<p>Can any one help me in this?</p>

<p>Thanks,
Balwinder</p>
",7414863,157,02-01-2018 11:43,04-01-2018 08:58,2,157,9,1,1,,"{'badge_counts': {'bronze': 9, 'silver': 1, 'gold': 1}, 'account_id': 10026959, 'is_employee': False, 'last_modified_date': 1586595900, 'last_access_date': 1626983453, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 157, 'creation_date': 1484311513, 'user_type': 'registered', 'user_id': 7414863, 'link': 'https://stackoverflow.com/users/7414863/balwinder-singh', 'profile_image': 'https://www.gravatar.com/avatar/e0264815016ceb57eb5c84cb1ccae36a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Balwinder Singh'}","I am trying to build an application with mongoDB and Python Flask. While running the application, I am getting below error: ConfigurationError: Server at 127.0.0.1:27017 reports wire version 0, but this version of PyMongo requires at least 2 (MongoDB 2.6). Can any one help me in this? Thanks, Balwinder",,0,11,0,0,
265,49596478,49597972,8874,LRU cache on disk for python,2,<python><caching><lru>,11,"<p>Am looking for on disk LRU cache package in Python.
Most of them are in memory cache.</p>

<p>Main reason is Database access is slow and
have limited RAM for in memory LRU.
However, large and fast SSD for LRU cache.</p>
",7416492,525,01-04-2018 09:03,01-04-2018 12:18,0,525,15,1,5,,"{'badge_counts': {'bronze': 15, 'silver': 5, 'gold': 1}, 'account_id': 7200503, 'is_employee': False, 'last_modified_date': 1694225100, 'last_access_date': 1693373111, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 525, 'creation_date': 1484334921, 'user_type': 'registered', 'user_id': 7416492, 'location': 'New York, NY, United States', 'link': 'https://stackoverflow.com/users/7416492/quantcode', 'profile_image': 'https://graph.facebook.com/977240852337490/picture?type=large', 'display_name': 'quantCode'}","Am looking for on disk LRU cache package in Python. Most of them are in memory cache. Main reason is Database access is slow and have limited RAM for in memory LRU. However, large and fast SSD for LRU cache.",,0,6,0,0,
266,49238725,49239313,23906,Chaining tests and passing an object from one test to another,2,<python><pytest>,35,"<p>I'm trying to pass the result of one test to another in pytest - or more specifically, reuse an object created by the first test in the second test. 
This is how I currently do it.</p>

<pre><code>@pytest.fixture(scope=""module"")
def result_holder:
    return []

def test_creation(result_holder):
    object = create_object()
    assert object.status == 'created' # test that creation works as expected
    result_holder.append(object.id) # I need this value for the next test

# ideally this test should only run if the previous test was successful
def test_deletion(result_holder):
    previous_id = result_holder.pop()
    object = get_object(previous_id) # here I retrieve the object created in the first test
    object.delete()
    assert object.status == 'deleted' # test for deletion
</code></pre>

<p>(before we go further, I'm aware of <a href=""https://stackoverflow.com/questions/37613883/py-test-passing-results-of-one-test-to-another"">py.test passing results of one test to another</a> - but the single answer on that question is off-topic, and the question itself is 2 years old)</p>

<p>Using fixtures like this doesn't feel super clean... And the behavior is not clear if the first test fails (although that can be remedied by testing for the content of the fixture, or using something like the incremental fixture in the pytest doc and the comments below). Is there a better/more canonical way to do this?</p>
",7857237,726,12-03-2018 15:20,12-03-2018 15:48,0,726,10,1,5,,"{'badge_counts': {'bronze': 10, 'silver': 5, 'gold': 1}, 'account_id': 9746478, 'is_employee': False, 'last_modified_date': 1573678683, 'last_access_date': 1690475778, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 726, 'creation_date': 1492008175, 'user_type': 'registered', 'user_id': 7857237, 'location': 'Paris, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/7857237/pills', 'profile_image': 'https://www.gravatar.com/avatar/2bc864272adee5529d4a89a5a9e548ad?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'pills'}","I'm trying to pass the result of one test to another in pytest - or more specifically, reuse an object created by the first test in the second test. This is how I currently do it. (before we go further, I'm aware of py.test passing results of one test to another - but the single answer on that question is off-topic, and the question itself is 2 years old) Using fixtures like this doesn't feel super clean... And the behavior is not clear if the first test fails (although that can be remedied by testing for the content of the fixture, or using something like the incremental fixture in the pytest doc and the comments below). Is there a better/more canonical way to do this?","@pytest.fixture(scope=""module"")
def result_holder:
    return []

def test_creation(result_holder):
    object = create_object()
    assert object.status == 'created' # test that creation works as expected
    result_holder.append(object.id) # I need this value for the next test

# ideally this test should only run if the previous test was successful
def test_deletion(result_holder):
    previous_id = result_holder.pop()
    object = get_object(previous_id) # here I retrieve the object created in the first test
    object.delete()
    assert object.status == 'deleted' # test for deletion
",14,23,0,1,
267,48303166,48303535,5353,Keras: TypeError: can't pickle _thread.lock objects with KerasClassifier,1,<python><tensorflow><neural-network><deep-learning><keras>,11,"<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset = pd.read_csv(""Churn_Modelling.csv"")
X = dataset.iloc[:,3:13].values
Y = dataset.iloc[:,13:].values

from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler

enc1=LabelEncoder()
enc2=LabelEncoder()
X[:,1] = enc1.fit_transform(X[:,1])
X[:,2] = enc2.fit_transform(X[:,2])

one = OneHotEncoder(categorical_features=[1])
X=one.fit_transform(X).toarray()

X = X[:,1:]

from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,Y,random_state=0,test_size=0.2)

scale = StandardScaler()
scale.fit_transform(Xtrain)
scale.transform(Xtest)

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score 
from keras.models import Sequential
from keras.layers import Dense

def func1():
    net = Sequential()
    net.add(Dense(input_dim=11,units=6,activation=""relu"",kernel_initializer='uniform'))
    net.add(Dense(units=6,activation=""relu"",kernel_initializer='uniform'))
    net.add(Dense(units=1,activation=""sigmoid"",kernel_initializer='uniform'))
    net.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')

    return net

classfier = KerasClassifier(build_fn=func1(),batch_size=10, epochs=100)
cross = cross_val_score(estimator=classfier, X=Xtrain, y=Ytrain, cv=10 , n_jobs=-1)
</code></pre>

<p>Throws the error:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-7-e80e82960eb9&gt;"", line 1, in &lt;module&gt;
    cross = cross_val_score(estimator=classfier, X=Xtrain, y=Ytrain, cv=10 , n_jobs=-1)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 342, in cross_val_score
    pre_dispatch=pre_dispatch)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 206, in cross_validate
    for train, test in cv.split(X, y, groups))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 620, in dispatch_one_batch
    tasks = BatchedCalls(itertools.islice(iterator, batch_size))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 127, in __init__
    self.items = list(iterator_slice)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 206, in &lt;genexpr&gt;
    for train, test in cv.split(X, y, groups))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\base.py"", line 62, in clone
    new_object_params[name] = clone(param, safe=False)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\base.py"", line 53, in clone
    return copy.deepcopy(estimator)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 169, in deepcopy
    rv = reductor(4)

TypeError: can't pickle _thread.lock objects
</code></pre>

<p>How do I solve this?</p>
",7449890,1508,17-01-2018 14:13,17-01-2018 14:31,0,1508,23,0,19,,"{'badge_counts': {'bronze': 23, 'silver': 19, 'gold': 0}, 'account_id': 7687607, 'is_employee': False, 'last_modified_date': 1669879500, 'last_access_date': 1710760477, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1508, 'creation_date': 1484994052, 'user_type': 'registered', 'user_id': 7449890, 'location': 'Chennai, Tamil Nadu, India', 'website_url': 'http://joishbosco.tk/', 'link': 'https://stackoverflow.com/users/7449890/joish', 'profile_image': 'https://www.gravatar.com/avatar/ab97ea9f463c9d79749e2682310e4222?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Joish'}",Throws the error: How do I solve this?,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset = pd.read_csv(""Churn_Modelling.csv"")
X = dataset.iloc[:,3:13].values
Y = dataset.iloc[:,13:].values

from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler

enc1=LabelEncoder()
enc2=LabelEncoder()
X[:,1] = enc1.fit_transform(X[:,1])
X[:,2] = enc2.fit_transform(X[:,2])

one = OneHotEncoder(categorical_features=[1])
X=one.fit_transform(X).toarray()

X = X[:,1:]

from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,Y,random_state=0,test_size=0.2)

scale = StandardScaler()
scale.fit_transform(Xtrain)
scale.transform(Xtest)

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score 
from keras.models import Sequential
from keras.layers import Dense

def func1():
    net = Sequential()
    net.add(Dense(input_dim=11,units=6,activation=""relu"",kernel_initializer='uniform'))
    net.add(Dense(units=6,activation=""relu"",kernel_initializer='uniform'))
    net.add(Dense(units=1,activation=""sigmoid"",kernel_initializer='uniform'))
    net.compile(optimizer='adam',metrics=['accuracy'],loss='binary_crossentropy')

    return net

classfier = KerasClassifier(build_fn=func1(),batch_size=10, epochs=100)
cross = cross_val_score(estimator=classfier, X=Xtrain, y=Ytrain, cv=10 , n_jobs=-1)
 Traceback (most recent call last):

  File ""&lt;ipython-input-7-e80e82960eb9&gt;"", line 1, in &lt;module&gt;
    cross = cross_val_score(estimator=classfier, X=Xtrain, y=Ytrain, cv=10 , n_jobs=-1)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 342, in cross_val_score
    pre_dispatch=pre_dispatch)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 206, in cross_validate
    for train, test in cv.split(X, y, groups))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 620, in dispatch_one_batch
    tasks = BatchedCalls(itertools.islice(iterator, batch_size))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 127, in __init__
    self.items = list(iterator_slice)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\model_selection\_validation.py"", line 206, in &lt;genexpr&gt;
    for train, test in cv.split(X, y, groups))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\base.py"", line 62, in clone
    new_object_params[name] = clone(param, safe=False)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\site-packages\sklearn\base.py"", line 53, in clone
    return copy.deepcopy(estimator)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\Joish\Anaconda3\envs\project\lib\copy.py"", line 169, in deepcopy
    rv = reductor(4)

TypeError: can't pickle _thread.lock objects
",158,167,0,0,
268,49138692,49146456,2178,What does it mean for an attribute name to end in an underscore?,1,<python><scikit-learn>,11,"<p>Similar question: <a href=""https://stackoverflow.com/questions/21075208/whats-the-advantage-of-a-trailing-underscore-in-python-naming"">What&#39;s the advantage of a trailing underscore in Python naming?</a>. This addresses advantages/disadvantages, whereas this addresses the reasoning behind doing it, both broadly and specifically to sklearn.</p>

<p>I am looking through the sklearn documentation, and I noticed that the sklearn.model_selection.GridSearchCV attributes all end in underscore. For example:</p>

<ul>
<li>cv_results_</li>
<li>best_params_</li>
<li>best_score_ </li>
</ul>

<p>Why is this? What does the underscore do? Please be as broad as possible in your answer (i.e. don't just refer to sklearn's GridSearchCV.</p>

<p>I'm assuming this isn't just an sklearn thing, and I have no idea what the appropriate tag is for this so I'm tagging sklearn. Please correct the tags (or me!).</p>
",7490953,962,06-03-2018 19:38,07-03-2018 07:34,1,982,21,1,12,100,"{'badge_counts': {'bronze': 21, 'silver': 12, 'gold': 1}, 'account_id': 10141279, 'is_employee': False, 'last_modified_date': 1686964200, 'last_access_date': 1690981083, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 982, 'creation_date': 1485795450, 'user_type': 'registered', 'user_id': 7490953, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/7490953/quanty', 'profile_image': 'https://www.gravatar.com/avatar/c88894039443fb3662f54a085a8b2407?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'quanty'}","Similar question: What&#39;s the advantage of a trailing underscore in Python naming?. This addresses advantages/disadvantages, whereas this addresses the reasoning behind doing it, both broadly and specifically to sklearn. I am looking through the sklearn documentation, and I noticed that the sklearn.model_selection.GridSearchCV attributes all end in underscore. For example: cv_results_ best_params_ best_score_ Why is this? What does the underscore do? Please be as broad as possible in your answer (i.e. don't just refer to sklearn's GridSearchCV. I'm assuming this isn't just an sklearn thing, and I have no idea what the appropriate tag is for this so I'm tagging sklearn. Please correct the tags (or me!).",,0,13,0,1,
269,48727337,48727387,27066,"Python, how to sort list of object?",2,<python><python-3.x><sorting><object>,11,"<p>I have a list of object that looks like this.</p>

<pre><code>hand = [ Card(10, 'H'), Card(2,'h'), Card(12,'h'), Card(13, 'h'), Card(14, 'h') ]
</code></pre>

<p>Card(10, 'H) here is not a tuple, but an object. I know how to sort this list if each item in the list was in a form of tuple, like this,</p>

<pre><code>hand = sorted(hand, key = lambda x: x[0])
</code></pre>

<p>but I have no idea how to sort a list of objects. I want to sort my list by the first input value, which is the number in Card()</p>

<p>How can I do this? </p>

<p>Edit: Here's the definition of Card().</p>

<pre><code>class Card(object):

    RANKS = (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)

    SUITS = ('C', 'D', 'H', 'S')

    def __init__(self, rank=12, suit='S'):

        if (rank in Card.RANKS):
            self.rank = rank
        else:
            self.rank = 12

        if (suit in Card.SUITS):
            self.suit = suit.upper()
        else:
            self.suit = 'S'

    def __str__(self):
        if (self.rank == 14):
            rank = 'A'
        elif (self.rank == 13):
            rank = 'K'
        elif (self.rank == 12):
            rank = 'Q'
        elif (self.rank == 11):
            rank = 'J'
        else:
            rank = str(self.rank)
        return rank + self.suit

    def __eq__(self, other):
        return (self.rank == other.rank)

    def __ne__(self, other):
        return (self.rank != other.rank)

    def __lt__(self, other):
        return (self.rank &lt; other.rank)

    def __le__(self, other):
        return (self.rank &lt;= other.rank)

    def __gt__(self, other):
        return (self.rank &gt; other.rank)

    def __ge__(self, other):
        return (self.rank &gt;= other.rank)
</code></pre>
",7552761,2580,11-02-2018 01:25,11-02-2018 01:33,0,2580,73,7,34,90,"{'badge_counts': {'bronze': 73, 'silver': 34, 'gold': 7}, 'account_id': 10232649, 'is_employee': False, 'last_modified_date': 1607614449, 'last_access_date': 1711134685, 'reputation_change_year': 58, 'reputation_change_quarter': 58, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2580, 'creation_date': 1486888310, 'user_type': 'registered', 'user_id': 7552761, 'accept_rate': 90, 'location': 'Austin, TX, United States', 'link': 'https://stackoverflow.com/users/7552761/eric-kim', 'profile_image': 'https://graph.facebook.com/1259153020839832/picture?type=large', 'display_name': 'Eric Kim'}","I have a list of object that looks like this. Card(10, 'H) here is not a tuple, but an object. I know how to sort this list if each item in the list was in a form of tuple, like this, but I have no idea how to sort a list of objects. I want to sort my list by the first input value, which is the number in Card() How can I do this? Edit: Here's the definition of Card().","hand = [ Card(10, 'H'), Card(2,'h'), Card(12,'h'), Card(13, 'h'), Card(14, 'h') ]
 hand = sorted(hand, key = lambda x: x[0])
 class Card(object):

    RANKS = (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)

    SUITS = ('C', 'D', 'H', 'S')

    def __init__(self, rank=12, suit='S'):

        if (rank in Card.RANKS):
            self.rank = rank
        else:
            self.rank = 12

        if (suit in Card.SUITS):
            self.suit = suit.upper()
        else:
            self.suit = 'S'

    def __str__(self):
        if (self.rank == 14):
            rank = 'A'
        elif (self.rank == 13):
            rank = 'K'
        elif (self.rank == 12):
            rank = 'Q'
        elif (self.rank == 11):
            rank = 'J'
        else:
            rank = str(self.rank)
        return rank + self.suit

    def __eq__(self, other):
        return (self.rank == other.rank)

    def __ne__(self, other):
        return (self.rank != other.rank)

    def __lt__(self, other):
        return (self.rank &lt; other.rank)

    def __le__(self, other):
        return (self.rank &lt;= other.rank)

    def __gt__(self, other):
        return (self.rank &gt; other.rank)

    def __ge__(self, other):
        return (self.rank &gt;= other.rank)
",47,65,0,0,
270,49188960,49189091,797884,How to show all columns' names on a large pandas dataframe?,22,<python><pandas><dataframe>,340,"<p>I have a dataframe that consist of hundreds of columns, and I need to see all column names.</p>

<p>What I did:</p>

<pre><code>In[37]:
data_all2.columns
</code></pre>

<p>The output is:</p>

<pre><code>Out[37]:
Index(['customer_id', 'incoming', 'outgoing', 'awan', 'bank', 'family', 'food',
       'government', 'internet', 'isipulsa',
       ...
       'overdue_3months_feature78', 'overdue_3months_feature79',
       'overdue_3months_feature80', 'overdue_3months_feature81',
       'overdue_3months_feature82', 'overdue_3months_feature83',
       'overdue_3months_feature84', 'overdue_3months_feature85',
       'overdue_3months_feature86', 'loan_overdue_3months_total_y'],
      dtype='object', length=102)
</code></pre>

<p>How do I show <em>all</em> columns, instead of a truncated list?</p>
",7585973,6761,09-03-2018 07:49,09-03-2018 07:58,0,6761,72,8,39,100,"{'badge_counts': {'bronze': 72, 'silver': 39, 'gold': 8}, 'account_id': 2371019, 'is_employee': False, 'last_modified_date': 1699667400, 'last_access_date': 1709286759, 'reputation_change_year': 190, 'reputation_change_quarter': 190, 'reputation_change_month': 40, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6761, 'creation_date': 1487439830, 'user_type': 'registered', 'user_id': 7585973, 'accept_rate': 100, 'location': 'Jakarta, Indonesia', 'website_url': 'http://nabihbawazir.com', 'link': 'https://stackoverflow.com/users/7585973/nabih-bawazir', 'profile_image': 'https://www.gravatar.com/avatar/1cf9f40e4f8e5e17076d55814a9c2ad9?s=256&d=identicon&r=PG', 'display_name': 'Nabih Bawazir'}","I have a dataframe that consist of hundreds of columns, and I need to see all column names. What I did: The output is: How do I show all columns, instead of a truncated list?","In[37]:
data_all2.columns
 Out[37]:
Index(['customer_id', 'incoming', 'outgoing', 'awan', 'bank', 'family', 'food',
       'government', 'internet', 'isipulsa',
       ...
       'overdue_3months_feature78', 'overdue_3months_feature79',
       'overdue_3months_feature80', 'overdue_3months_feature81',
       'overdue_3months_feature82', 'overdue_3months_feature83',
       'overdue_3months_feature84', 'overdue_3months_feature85',
       'overdue_3months_feature86', 'loan_overdue_3months_total_y'],
      dtype='object', length=102)
",10,23,0,0,
271,48909673,48909674,6013,How to annotate variadic parameters in Python using typing annotations?,2,<python><typing><pep>,13,"<p>How to annotate parameters of the variadic function?</p>

<p>Example:</p>

<pre><code>def foo(*args):  # Each arg expected to be of type T
    ...
</code></pre>

<p>Are there any typing annotations for that?</p>
",7598113,2911,21-02-2018 15:31,21-02-2018 15:31,0,2911,62,3,25,78,"{'badge_counts': {'bronze': 62, 'silver': 25, 'gold': 3}, 'account_id': 10177703, 'is_employee': False, 'last_modified_date': 1677845101, 'last_access_date': 1711029196, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 50, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2911, 'creation_date': 1487673514, 'user_type': 'registered', 'user_id': 7598113, 'accept_rate': 78, 'location': 'Middle East', 'website_url': '', 'link': 'https://stackoverflow.com/users/7598113/kuza', 'profile_image': 'https://www.gravatar.com/avatar/0bc516577db4123abe6f92b0fdd3cb81?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'kuza'}",How to annotate parameters of the variadic function? Example: Are there any typing annotations for that?,"def foo(*args):  # Each arg expected to be of type T
    ...
",1,9,0,0,
272,48640251,48640292,65487,How to peek front of deque without popping?,4,<python><collections><deque>,55,"<p>I want to check a condition against the front of a queue before deciding whether or not to pop. How can I achieve this in python with collections.deque?</p>

<pre><code>list(my_deque)[0]
</code></pre>

<p>seems ugly and poor for performance. </p>
",7598461,4439,06-02-2018 10:02,06-02-2018 10:04,0,4479,69,8,36,57,"{'badge_counts': {'bronze': 69, 'silver': 36, 'gold': 8}, 'account_id': 10299286, 'is_employee': False, 'last_modified_date': 1691328000, 'last_access_date': 1708697886, 'reputation_change_year': 160, 'reputation_change_quarter': 160, 'reputation_change_month': 50, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 4479, 'creation_date': 1487677368, 'user_type': 'registered', 'user_id': 7598461, 'accept_rate': 57, 'website_url': '', 'link': 'https://stackoverflow.com/users/7598461/jsstuball', 'profile_image': 'https://i.stack.imgur.com/XLSJS.jpg?s=256&g=1', 'display_name': 'jsstuball'}",I want to check a condition against the front of a queue before deciding whether or not to pop. How can I achieve this in python with collections.deque? seems ugly and poor for performance.,"list(my_deque)[0]
",0,6,0,0,
273,49774993,49775382,829,python function call with/without list comprehension,1,<python><python-3.x>,12,"<p>I have below two functions:</p>

<pre><code>def foo(n=50000):
    return sum(i*i for i in range(n))  # just called sum() directly without 

def bar(n=50000):
    return sum([i*i for i in range(n)])  # passed constructed list to sum()
</code></pre>

<p>I was hoping that <code>foo</code> will run faster then <code>bar</code> but I have checked in ipython with <code>%%timeit</code> that <code>foo</code> is taking slightly longer then <code>bar</code> </p>

<pre><code>In [2]: %%timeit
   ...: foo(50000)
   ...: 
100 loops, best of 3: 4.22 ms per loop

In [3]: %%timeit
   ...: bar(50000)
   ...: 
100 loops, best of 3: 3.45 ms per loop
In [4]: %%timeit
   ...: foo(10000000)
   ...: 
1 loops, best of 3: 1.02 s per loop

In [5]: %%timeit
   ...: bar(10000000)
   ...: 
1 loops, best of 3: 869 ms per loop
</code></pre>

<p>The difference increases as I increase value of n hence I tried to check function with <code>dis.dis(foo)</code> and <code>dis.dis(bar)</code> but it was identical.</p>

<p>So what would be the cause of such time difference between both methods?</p>
",7664524,4123,11-04-2018 12:20,11-04-2018 12:37,0,4133,45,4,25,40,"{'badge_counts': {'bronze': 45, 'silver': 25, 'gold': 4}, 'collectives': [{'collective': {'tags': ['bert-language-model', 'huggingface-transformers', 'topic-modeling', 'opennlp', 'tf-idf', 'gensim', 'stanford-nlp', 'spacy-3', 'named-entity-recognition', 'word-embedding', 'spacy', 'word2vec', 'nltk', 'sentiment-analysis', 'nlp-question-answering', 'nlp'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective focused on NLP (natural language processing), the transformation or extraction of useful information from natural language data.', 'link': '/collectives/nlp', 'name': 'NLP', 'slug': 'nlp'}, 'role': 'member'}, {'collective': {'tags': ['cicd', 'octopus-deploy', 'jenkins-plugins', 'bitbucket-pipelines', 'continuous-delivery', 'continuous-testing', 'github-actions', 'argocd', 'azure-pipelines', 'jenkins-groovy', 'jenkins', 'teamcity', 'hudson', 'jenkins-pipeline', 'continuous-deployment', 'gitlab-ci', 'gitlab-ci-runner', 'codemagic', 'tfsbuild', 'google-cloud-build', 'continuous-integration', 'circleci'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where developers focused on continuous integration, delivery, and deployment can find, share, and learn about simultaneous development.', 'link': '/collectives/ci-cd', 'name': 'CI/CD', 'slug': 'ci-cd'}, 'role': 'member'}, {'collective': {'tags': ['azure-sdk-ruby', 'azure-lab-services', 'azure-devtest-labs', 'azure-stream-analytics', 'azureadgraph-deprecation', 'azure-configuration', 'azure-cosmosdb-cassandra-api', 'azure-eventgrid', 'azure-functions-proxies', 'azure-dns', 'azureclicredential', 'azure-ad-msal', 'sql-azure-federations', 'azure-log-analytics', 'azure-git-deployment', 'azure-queues', 'azure-auto-ml', 'azure-acr', 'azure-nsg', 'azure-integration-account', 'azure-managed-app', 'azure-sql-reporting', 'azure-public-ip', 'azure-sdk', 'adal', 'azure-service-plan', 'azure-appservice', 'azure-blob-trigger', 'azure-keyvault', 'azure-ddos', 'azure-ilb', 'azure-logic-app-standard', 'azure-functions', 'azure-iot-hub-device-update', 'azure-web-roles', 'azure-cloud-shell', 'azure-android-sdk', 'azure-acs', 'azure-alerts', 'azure-web-app-service', 'azure-batch', 'adal.js', 'azure-xplat-cli', 'azure-functions-runtime', 'azure-databricks', 'azure-sql', 'azure-site-recovery', 'azure-rbac', 'azure-analysis-services', 'azure-service-principal', 'azure-sentinel', 'azure-diagnostics', 'azure-databoxfamily', 'azure-media-player', 'azure-appfabric', 'azure-compute-emulator', 'azure-ai-translator', 'azure-traffic-manager', 'azure-sql-database', 'azure-clouddrive', 'azure-billing', 'azure-affinity-group', 'azure-blueprints', 'azure-text-translation', 'azure-caching', 'azure-ai', 'azure', 'azure-sas', 'azure-sphere', 'azure-promptflow', 'azure-free-services', 'azure-servicebus-topics', 'azure-iot-hub', 'azureshell', 'azure-sql-edge', 'azure-app-api', 'azure-security-center', 'azure-static-website-routing', 'azure-ad-powershell-v2', 'azure-ad-role', 'azure-spatial-anchors', 'azure-iot-central', 'azure-adf', 'azure-dsvm', 'azure-active-directory', 'azure-update-management-center', 'azure-service-fabric', 'azure-web-app-for-containers', 'azure-secrets', 'azure-workflow-automation', 'azure-mobile-engagement', 'azure-defender', 'azure-ad-graph-api', 'kitchen-azurerm', 'azure-china', 'azure-log-analytics-workspace', 'azure-data-factory', 'azure-batch-account', 'azure-adal-deprecation', 'microsoft-entra-id', 'azure-calculator', 'azure-remote-rendering', 'azure-image-builder', 'azure-bot-service', 'azure-anomaly-detector', 'azure-virtual-network', 'spring-cloud-azure', 'microsoft-entra-internet-access', 'microsoft-custom-vision', 'azure-database-postgresql', 'azure-maps', 'azure-hdinsight', 'azure-feature-manager', 'azure-ad-b2c-custom-policy', 'azure-private-dns', 'azure-management-portal', 'azure-cosmosdb-gremlinapi', 'azure-function-async', 'django-pyodbc-azure', 'azure-signalr', 'azure-billing-api', 'azure-scheduler', 'azure-anomaly-detection', 'azure-app-service-envrmnt', 'azure-front-door', 'azure-elasticpool', 'azure-role-environment', 'azure-vm-templates', 'azure-redis-cache', 'azure-waf', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-tablequery', 'azure-blob-storage', 'azure-postgresql', 'azure-servicebusrelay', 'azure-security', 'azure-automation', 'azure-quantum', 'azure-web-pubsub', 'azure-blockchain-service', 'azure-notificationhub', 'azure-load-testing', 'azure-object-anchors', 'azure-media-services', 'azure-dashboard', 'azure-managed-disk', 'azure-metrics-advisor', 'azure-emulator', 'azure-mobile-services', 'azure-static-web-app-routing', 'azure-webjobssdk', 'azure-zulu', 'azure-iot-suite', 'azure-app-service-plans', 'azure-application-proxy', 'azure-identity', 'azure-data-lake-gen2', 'azure-availability-set', 'azure-function-app-proxy', 'azure-container-service', 'azure-mcd', 'azure-analytics', 'azure-debugger', 'azure-gov', 'azure-vm', 'azure-iot-dps', 'defaultazurecredential', 'azure-triggers', 'azure-purview', 'azure-service-fabric-mesh', 'azure-notebooks', 'azure-autoscaling-block', 'azure-python-sdk', 'azure-node-sdk', 'azure-aks', 'azure-cosmosdb-mongoapi', 'azuremlsdk', 'azure-oauth', 'rebus-azureservicebus', 'azure-file-copy', 'azure-dev-spaces', 'azure-monitor', 'azure-sql-managed-instance', 'azure-database-mysql', 'azure-cosmosdb-tables', 'azure-relay', 'fhir-server-for-azure', 'azure-packaging', 'azurekinect', 'azure-webjobs', 'azure-java-tools', 'azure-api-management', 'azure-sdk-.net', 'azure-arc', 'azure-functions-core-tools', 'azure-cloud-services', 'azure-video-indexer', 'azure-worker-roles', 'azure-servicebus-queues', 'azure-powershell', 'sql-server-azure', 'pulumi-azure', 'azure-rest-api', 'azure-compliance-policy', 'azurerm-app-service', 'azure-regions', 'azure-java-sdk', 'azure-private-dns-zone', 'azure-function-queue', 'azure-data-share', 'azure-functions-docker', 'azure-advisor', 'azure-deployment', 'azure-managed-database', 'azure-mapping-data-flow', 'azure-application-roles', 'azure-deployment-slots', 'azure-cosmosdb-changefeed', 'azure-webhooks', 'azure-communication-services', 'azure-synapse-link', 'azure-storage-account', 'azure-storage-explorer', 'passport-azure-ad', 'azure-durable-functions', 'azure-search-.net-sdk', 'azure-language-understanding', 'azure-iot-hub-device-management', 'azure-api-apps', 'azure-application-gateway', 'azure-pack', 'azure-disk', 'azure-resource-group', 'azure-sdk-for-java', 'azureservicebus', 'azure-storage-queues', 'azure-resource-graph', 'azure-storage-files', 'azure-bicep', 'azure-data-sync', 'azure-management', 'azure-rm', 'azure-spring-cloud', 'azure-performancecounters', 'azure-pipelines-release-pipeline', 'azure-management-api', 'azure-management-groups', 'azure-policy', 'azure-servicebus-subscriptions', 'azure-files', 'microsoft-entra-external-id', 'azure-industrial-iot', 'azure-load-balancer', 'azure-elastic-scale', 'azure-application-insights-profiler', 'azure-information-protection', 'azure-managed-grafana', 'azure-container-apps', 'azure-blockchain-workbench', 'azure-static-web-app', 'azure-sdk-for-go', 'azure-monitoring', 'azure-function-http', 'azure-runbook', 'azure-eventhub', 'azure-service-runtime', 'azure-ad-b2b', 'azure-ml-component', 'azure-webjobs-continuous', 'azure-static-website-hosting', 'azure-sdk-php', 'azure-private-link', 'azure-digital-twins', 'azure-availability-zones', 'azure-agent', 'azure-subscription', 'azure-data-catalog', 'azure-migrate', 'azure-linux', 'azure.data.tables', 'azure-vpn', 'azure-oms', 'azure-application-settings', 'azure-app-configuration', 'azure-form-recognizer', 'kql', 'azure-http-trigger', 'azure-backup-vault', 'azure-synapse', 'azure-table-storage', 'azure-custom-providers', 'azure-iot-sdk', 'azure-container-registry', 'azure-authentication', 'spark-bash-azure-databricks', 'azure-data-studio', 'azure-sdk-js', 'azure-machine-learning-service', 'azure-rm-template', 'azure-custom-domain', 'azure-bastion', 'azure-sdk-go', 'azure-cli2', 'azure-ad-b2c', 'azure-cosmosdb-sqlapi', 'sitecore-azure', 'azure-application-insights', 'azure-cosmosdb-emulator', 'azure-timeseries-insights', 'azure-service-hooks', 'azure-fluent-api', 'azure-monitor-workbooks', 'azure-web-app-firewall', 'azure-cost-calculation', 'azure-cosmosdb-mongovcore', 'azure-connect', 'azureml-python-sdk', 'azure-ad-v2', 'azure-storage', 'azure-iot-edge', 'azure-cdn', 'microsoft-entra-private-access', 'azure-in-role-cache', 'azure-hybrid-connections', 'azure-data-explorer', 'azure-ad-domain-services', 'azure-speech', 'azure-store', 'azure-webjobs-triggered', 'azure-function-app', 'azureportal', 'azure-stack', 'sql-azure-alerts', 'azure-virtual-machine', 'terraform-provider-azure', 'azure-webapps', 'azure-sdk-for-ruby', 'azure-vm-scale-set', 'azure-rtos', 'azure-hub', 'azure-cli', 'azure-qna-maker', 'azure-marketplace', 'azure-logic-apps', 'azure-cosmosdb', 'azure-app-registration', 'azure-application-registration', 'azure-managed-identity', 'azure-cognitive-search', 'azure-ml-pipelines', 'azure-cognitive-services', 'azure-mysql-database', 'azure-sdk-python', 'azure-container-instances', 'azure-ase', 'azure-spring-boot', 'azure-storage-emulator', 'azure-sql-server', 'azure-data-lake', 'azure-ad-verifiable-credentials'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}, {'collective': {'tags': ['amazon-macie', 'amazon-managed-blockchain', 'amazon-kinesis-video-streams', 'aws-sdk-net', 'aws-sdk-cpp', 'aws-sdk-nodejs', 'aws-vpn', 'amazon-workmail', 'aws-codecommit', 'aws-sdk-js-v3', 'amazon-rekognition', 'aws-serverless', 'aws-iot-sitewise', 'amazon-connect', 'amazon-workspaces', 'amazon-efs', 'amazon-elastic-beanstalk', 'aws-glue', 'aws-datasync', 'aws-xray', 'aws-sdk-ios', 'amazon-sumerian', 'amazon-kinesis-firehose', 'aws-acm', 'amazon-qldb', 'aws-iot-analytics', 'aws-codestar', 'amazon-ses', 'amazon-opensearch', 'amazon-kendra', 'aws-iot', 'aws-storage-gateway', 'aws-sdk-rust', 'aws-sdk-comprehend', 'alexa-account-linking', 'aws-elemental', 'aws-ssm', 'aws-sam-cli', 'amazon-ebs', 'amazon-timestream', 'aws-auto-scaling', 'aws-certificate-manager', 'alexa-interaction-model', 'aws-mediastore', 'aws-cli', 'amazon-location-service', 'aws-amplify', 'aws-copilot-cli', 'alexa-presentation-language', 'amazon-sagemaker', 'amazon-kinesis', 'aws-mobilehub', 'amazon-ecs', 'amazon-swf', 'aws-media-convert', 'aws-codeguru', 'aws-mediaconnect', 'aws-media-live', 'amazon-forecast', 'aws-codepipeline', 'aws-sdk-ruby', 'amazon-sqs', 'aws-cdk', 'amazon-athena', 'aws-direct-connect', 'aws-batch', 'amazon-lex', 'amazon-sns', 'amazon-ec2-spot-market', 'amazon-eks', 'aws-sam', 'aws-msk', 'aws-sso', 'aws-sdk-go', 'aws-snowball', 'alexa-smart-home-skill', 'aws-transfer-family', 'amazon-textract', 'amazon-inspector', 'aws-reserved-instances', 'aws-resource-group', 'aws-app-config', 'aws-cloudmap', 'amazon-ivs', 'aws-iot-core', 'aws-deeplens', 'amazon-web-services', 'amazon-kinesis-analytics', 'aws-application-load-balancer', 'amazon-guardduty', 'amazon-kms', 'aws-iam-identity-center', 'amazon-imagebuilder', 'aws-iot-greengrass', 'aws-device-farm', 'amazon-elastic-transcoder', 'amazon-rds', 'amazon-cloudwatchlogs', 'aws-fis', 'aws-global-accelerator', 'amazon-transcribe', 'aws-sdk-java-2.0', 'amazon-route53', 'aws-elb', 'amazon-cloudfront', 'amazon-cloudtrail', 'aws-mediatailor', 'amazon-redshift-spectrum', 'alexa-sdk-nodejs', 'alexa-sdk-python', 'amazon-keyspaces', 'aws-codebuild', 'aws-codecatalyst', 'aws-cloudshell', 'aws-nlb', 'aws-billing', 'aws-directory-services', 'amazon-quicksight', 'aws-appstream', 'aws-pinpoint', 'amazon-gamelift', 'amazon-s3', 'amazon-sagemaker-compilers', 'amazon-cloudwatch', 'alexa-skills-kit', 'aws-dms', 'aws-data-exchange', 'amazon-elasticsearch', 'aws-sct', 'aws-lambda-powertools', 'aws-event-bridge', 'aws-app-mesh', 'amazon-simpledb', 'alexa-smapi', 'amazon-dynamodb-dax', 'aws-iot-events', 'aws-appsync', 'aws-lambda-edge', 'amazon-cloudsearch', 'aws-control-tower', 'amazon-ecr', 'amazon-elasticache', 'amazon-workdocs', 'aws-sdk-go-v2', 'amazon-aurora', 'amazon-memory-db', 'amazon-lightsail', 'aws-step-functions', 'aws-sdk-java', 'aws-opsworks', 'aws-api-gateway', 'amazon-emr', 'amazon-cloudhsm', 'aws-sdk', 'aws-code-deploy', 'aws-lambda', 'amazon-redshift', 'elastic-ip', 'aws-elastictranscoder', 'amazon-fsx', 'amazon-iam', 'aws-codeartifact', 'aws-sdk-js', 'amazon-translate', 'aws-graviton', 'aws-security-hub', 'alexa-flash-briefing-skill', 'aws-private-link', 'aws-cloud9', 'amazon-waf', 'amazon-data-pipeline', 'aws-sdk-android', 'amazon-personalize', 'amazon-polly', 'aws-databrew', 'aws-secrets-manager', 'aws-backup', 'amazon-cognito', 'amazon-dynamodb', 'amazon-neptune', 'aws-chatbot', 'amazon-mq', 'amazon-ec2', 'amazon-vpc', 'aws-copilot', 'aws-fargate', 'aws-lake-formation', 'aws-cloudformation', 'aws-organizations', 'amazon-app-runner', 'amazon-ami', 'aws-config', 'aws-security-group', 'amazon-appflow', 'amazon-s3-select', 'aws-documentdb', 'aws-parameter-store', 'amazon-elb', 'amazon-bedrock', 'aws-mediapackage', 'amazon-glacier', 'aws-sdk-mock', 'amazon-honeycode', 'amazon-comprehend', 'aws-service-catalog'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 10394339, 'is_employee': False, 'last_modified_date': 1703326500, 'last_access_date': 1710825418, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 10, 'reputation': 4133, 'creation_date': 1488779274, 'user_type': 'registered', 'user_id': 7664524, 'accept_rate': 40, 'location': 'Bangalore, Karnataka', 'website_url': 'https://gahan9.github.io/', 'link': 'https://stackoverflow.com/users/7664524/gahan', 'profile_image': 'https://i.stack.imgur.com/UDnc1.jpg?s=256&g=1', 'display_name': 'Gahan'}",I have below two functions: I was hoping that will run faster then but I have checked in ipython with that is taking slightly longer then The difference increases as I increase value of n hence I tried to check function with and but it was identical. So what would be the cause of such time difference between both methods?,"def foo(n=50000):
    return sum(i*i for i in range(n))  # just called sum() directly without 

def bar(n=50000):
    return sum([i*i for i in range(n)])  # passed constructed list to sum()
 foo bar %%timeit foo bar In [2]: %%timeit
   ...: foo(50000)
   ...: 
100 loops, best of 3: 4.22 ms per loop

In [3]: %%timeit
   ...: bar(50000)
   ...: 
100 loops, best of 3: 3.45 ms per loop
In [4]: %%timeit
   ...: foo(10000000)
   ...: 
1 loops, best of 3: 1.02 s per loop

In [5]: %%timeit
   ...: bar(10000000)
   ...: 
1 loops, best of 3: 869 ms per loop
 dis.dis(foo) dis.dis(bar)",14,34,0,0,
274,50051210,50051280,15896,Avoiding Memory Issues For GroupBy on Large Pandas DataFrame,3,<python><pandas><dataframe><memory><dask>,15,"<p><strong>Update:</strong></p>

<p>The pandas df was created like this:</p>

<pre><code>df = pd.read_sql(query, engine)
encoded = pd.get_dummies(df, columns=['account'])
</code></pre>

<p>Creating a dask df from this df looks like this:</p>

<pre><code>df = dd.from_pandas(encoded, 50)
</code></pre>

<p>Performing the operation with dask results in no visible progress being made (checking with dask diagnostics):</p>

<pre><code>result = df.groupby('journal_entry').max().reset_index().compute()
</code></pre>

<p><strong>Original:</strong></p>

<p>I have a large pandas df with 2.7M rows and 4,000 columns. All but four of the columns are of dtype uint8. The uint8 columns only hold values of 1 or 0. I am attempting to perform this operation on the df:</p>

<pre><code>result = df.groupby('id').max().reset_index()
</code></pre>

<p>Predictably, this operation immediately returns a memory error. My initial thought is to chunk the df both horizontally and vertically. However, this creates a messy situation, since the <code>.max()</code> needs to be performed across all the uint8 columns, not just a pair of columns. In addition, it is still extremely slow to chunk the df like this. I have 32 GB of RAM on my machine.</p>

<p>What strategy could mitigate the memory issue?</p>
",7668467,2394,26-04-2018 20:18,26-04-2018 20:23,0,2394,78,2,31,71,"{'badge_counts': {'bronze': 78, 'silver': 31, 'gold': 2}, 'account_id': 10399759, 'is_employee': False, 'last_modified_date': 1651407900, 'last_access_date': 1683313131, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2394, 'creation_date': 1488828230, 'user_type': 'registered', 'user_id': 7668467, 'accept_rate': 71, 'website_url': '', 'link': 'https://stackoverflow.com/users/7668467/overflowingtheglass', 'profile_image': 'https://www.gravatar.com/avatar/0e908a767e2b80b8aae3d5964ee72185?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'OverflowingTheGlass'}","Update: The pandas df was created like this: Creating a dask df from this df looks like this: Performing the operation with dask results in no visible progress being made (checking with dask diagnostics): Original: I have a large pandas df with 2.7M rows and 4,000 columns. All but four of the columns are of dtype uint8. The uint8 columns only hold values of 1 or 0. I am attempting to perform this operation on the df: Predictably, this operation immediately returns a memory error. My initial thought is to chunk the df both horizontally and vertically. However, this creates a messy situation, since the needs to be performed across all the uint8 columns, not just a pair of columns. In addition, it is still extremely slow to chunk the df like this. I have 32 GB of RAM on my machine. What strategy could mitigate the memory issue?","df = pd.read_sql(query, engine)
encoded = pd.get_dummies(df, columns=['account'])
 df = dd.from_pandas(encoded, 50)
 result = df.groupby('journal_entry').max().reset_index().compute()
 result = df.groupby('id').max().reset_index()
 .max()",0,28,0,0,
275,48340463,48342103,9785,how to understand closed and label arguments in pandas resample method?,4,<python><pandas><dataframe><time-series>,21,"<p>Based on the pandas documentation from here: <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html"" rel=""noreferrer"">Docs</a></p>

<p>And the examples:</p>

<pre><code>&gt;&gt;&gt; index = pd.date_range('1/1/2000', periods=9, freq='T')
&gt;&gt;&gt; series = pd.Series(range(9), index=index)
&gt;&gt;&gt; series
2000-01-01 00:00:00    0
2000-01-01 00:01:00    1
2000-01-01 00:02:00    2
2000-01-01 00:03:00    3
2000-01-01 00:04:00    4
2000-01-01 00:05:00    5
2000-01-01 00:06:00    6
2000-01-01 00:07:00    7
2000-01-01 00:08:00    8
Freq: T, dtype: int64
</code></pre>

<p>After resampling:</p>

<pre><code>&gt;&gt;&gt; series.resample('3T', label='right', closed='right').sum()
2000-01-01 00:00:00     0
2000-01-01 00:03:00     6
2000-01-01 00:06:00    15
2000-01-01 00:09:00    15
</code></pre>

<p>In my thoughts, the bins should looks like these after resampling:</p>

<pre><code>=========bin 01=========
2000-01-01 00:00:00    0
2000-01-01 00:01:00    1
2000-01-01 00:02:00    2

=========bin 02=========
2000-01-01 00:03:00    3
2000-01-01 00:04:00    4
2000-01-01 00:05:00    5

=========bin 03=========
2000-01-01 00:06:00    6
2000-01-01 00:07:00    7
2000-01-01 00:08:00    8
</code></pre>

<p>Am I right on this step??</p>

<p>So after <code>.sum</code> I thought it should be like this:</p>

<pre><code>2000-01-01 00:02:00     3
2000-01-01 00:05:00    12
2000-01-01 00:08:00    21
</code></pre>

<p><strong>I just do not understand how it comes out:</strong></p>

<p><code>2000-01-01 00:00:00     0</code> </p>

<p>(because <code>label='right'</code>, 2000-01-01 00:00:00 cannot be any right edge of any bins in this case).</p>

<p><code>2000-01-01 00:09:00    15</code> </p>

<p>(the label 2000-01-01 00:09:00 even does not exists in the original Series.</p>
",7887590,470,19-01-2018 11:50,19-01-2018 13:21,0,470,12,0,3,,"{'badge_counts': {'bronze': 12, 'silver': 3, 'gold': 0}, 'account_id': 10717514, 'is_employee': False, 'last_modified_date': 1615943407, 'last_access_date': 1710685823, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 470, 'creation_date': 1492578275, 'user_type': 'registered', 'user_id': 7887590, 'website_url': '', 'link': 'https://stackoverflow.com/users/7887590/mingchau', 'profile_image': 'https://lh3.googleusercontent.com/-_TSxrNbPuUg/AAAAAAAAAAI/AAAAAAAAA3A/Vr37FmPopNA/photo.jpg?sz=256', 'display_name': 'mingchau'}","Based on the pandas documentation from here: Docs And the examples: After resampling: In my thoughts, the bins should looks like these after resampling: Am I right on this step?? So after I thought it should be like this: I just do not understand how it comes out: (because , 2000-01-01 00:00:00 cannot be any right edge of any bins in this case). (the label 2000-01-01 00:09:00 even does not exists in the original Series.","&gt;&gt;&gt; index = pd.date_range('1/1/2000', periods=9, freq='T')
&gt;&gt;&gt; series = pd.Series(range(9), index=index)
&gt;&gt;&gt; series
2000-01-01 00:00:00    0
2000-01-01 00:01:00    1
2000-01-01 00:02:00    2
2000-01-01 00:03:00    3
2000-01-01 00:04:00    4
2000-01-01 00:05:00    5
2000-01-01 00:06:00    6
2000-01-01 00:07:00    7
2000-01-01 00:08:00    8
Freq: T, dtype: int64
 &gt;&gt;&gt; series.resample('3T', label='right', closed='right').sum()
2000-01-01 00:00:00     0
2000-01-01 00:03:00     6
2000-01-01 00:06:00    15
2000-01-01 00:09:00    15
 =========bin 01=========
2000-01-01 00:00:00    0
2000-01-01 00:01:00    1
2000-01-01 00:02:00    2

=========bin 02=========
2000-01-01 00:03:00    3
2000-01-01 00:04:00    4
2000-01-01 00:05:00    5

=========bin 03=========
2000-01-01 00:06:00    6
2000-01-01 00:07:00    7
2000-01-01 00:08:00    8
 .sum 2000-01-01 00:02:00     3
2000-01-01 00:05:00    12
2000-01-01 00:08:00    21
 2000-01-01 00:00:00     0 label='right' 2000-01-01 00:09:00    15",27,64,0,1,
276,49467969,49524039,16291,Python Script using ExecuteStreamCommand,1,<python><apache-nifi>,13,"<p>After doing my best to find previous questions and examples relevant to this question, and still not finding the answers that I'm looking for I figured that I would submit a question myself.</p>

<p>ExecuteStreamCommand seems like the perfect processor for me due to the following reasons:</p>

<ul>
<li>I am able to execute any Python script and avoid Jython (in a similar fashion as ExecuteScript). Jython is not an option for me.</li>
<li>I can take in FlowFiles. This is necessary as my script is made to consume the output of a previous processor. Furthermore I like the idea of keeping the data under ""NiFi management"".</li>
<li>It writes an ""execution status"" which will be useful for routing.</li>
</ul>

<p>In a nutshell, what I'm trying to do with ExecuteStreamCommand is:</p>

<ul>
<li>Ingest the output of a previous processor (a Scrapy spider that outputs a text file with JSON lines to be exact)</li>
<li>Call a python script (e.g. <code>python3 my_script.py</code>)</li>
<li>Load the FlowFile that was ingested in my python script.</li>
<li>Select the content of the FlowFile.</li>
<li>Operate on the content of the FlowFile within python.</li>
<li>Output either an updated version of the original FlowFile or create a new one.</li>
<li>Continue with my NiFi flow with the updated/new FlowFile.</li>
</ul>

<p>For clarity's sake I currently don't understand:</p>

<ul>
<li>How to call the python script (from the ExecuteStreamCommand Processor)</li>
<li>How to load up the FlowFile from within Python</li>
<li>How to update or create a new FlowFile from within Python</li>
<li>How to output the updated FlowFile from Python back to NiFi.</li>
</ul>

<p>I have come across various examples for ExecuteScript, but unfortunately these don't exactly translate to the use of the ExecuteStreamCommand.</p>

<p>Thank you in advance. Any advice is appreciated.</p>
",7672370,336,24-03-2018 17:54,27-03-2018 23:46,3,336,16,1,3,100,"{'badge_counts': {'bronze': 16, 'silver': 3, 'gold': 1}, 'account_id': 10405235, 'is_employee': False, 'last_modified_date': 1593827704, 'last_access_date': 1700735360, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 336, 'creation_date': 1488890285, 'user_type': 'registered', 'user_id': 7672370, 'accept_rate': 100, 'location': 'Leiden, Netherlands', 'website_url': '', 'link': 'https://stackoverflow.com/users/7672370/vcovo', 'profile_image': 'https://i.stack.imgur.com/ZzGvH.jpg?s=256&g=1', 'display_name': 'vcovo'}","After doing my best to find previous questions and examples relevant to this question, and still not finding the answers that I'm looking for I figured that I would submit a question myself. ExecuteStreamCommand seems like the perfect processor for me due to the following reasons: I am able to execute any Python script and avoid Jython (in a similar fashion as ExecuteScript). Jython is not an option for me. I can take in FlowFiles. This is necessary as my script is made to consume the output of a previous processor. Furthermore I like the idea of keeping the data under ""NiFi management"". It writes an ""execution status"" which will be useful for routing. In a nutshell, what I'm trying to do with ExecuteStreamCommand is: Ingest the output of a previous processor (a Scrapy spider that outputs a text file with JSON lines to be exact) Call a python script (e.g. ) Load the FlowFile that was ingested in my python script. Select the content of the FlowFile. Operate on the content of the FlowFile within python. Output either an updated version of the original FlowFile or create a new one. Continue with my NiFi flow with the updated/new FlowFile. For clarity's sake I currently don't understand: How to call the python script (from the ExecuteStreamCommand Processor) How to load up the FlowFile from within Python How to update or create a new FlowFile from within Python How to output the updated FlowFile from Python back to NiFi. I have come across various examples for ExecuteScript, but unfortunately these don't exactly translate to the use of the ExecuteStreamCommand. Thank you in advance. Any advice is appreciated.",python3 my_script.py,-1,34,0,0,
277,48422762,49160705,53676,Is it possible to show `print` output as LaTeX in jupyter notebook?,4,<python><python-3.x><jupyter-notebook><latex><ipython>,46,"<p>I was writing a very simple script to count ellipsoid area and volume and some other things. I was presenting my output printing it out like this:</p>
<pre><code>print('Dims: {}x{}m\nArea: {}m^2\nVolume: {}m^3'.format(a, round(b,2), P, V))
</code></pre>
<p>What, of course, gave this output (with sample data):</p>
<pre><code>Dims: 13.49x2.25m
Area: 302.99m^2
Volume: 90.92m^3
</code></pre>
<p>As I wrote earlier, I am using jupyter notebook, so I can use <code>$</code> operators in <b>markdown</b> cells to create LaTeX formulas.</p>
<p>My question is, is it possible to generate output <strong>using Python code</strong> in a way that it will be understood as LaTeX formula and printed in such a way, that:</p>
<p><a href=""https://i.stack.imgur.com/f7qJu.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/f7qJu.gif"" alt=""Latex_example"" /></a></p>
<p>Thanks for all replies.</p>
",8016691,984,24-01-2018 12:40,07-03-2018 20:31,42,984,16,2,9,100,"{'badge_counts': {'bronze': 16, 'silver': 9, 'gold': 2}, 'account_id': 10905881, 'is_employee': False, 'last_modified_date': 1620495000, 'last_access_date': 1704013609, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 984, 'creation_date': 1494889851, 'user_type': 'registered', 'user_id': 8016691, 'accept_rate': 100, 'location': 'Warsaw, Poland', 'website_url': '', 'link': 'https://stackoverflow.com/users/8016691/sqoshu', 'profile_image': 'https://i.stack.imgur.com/DPgt1.png?s=256&g=1', 'display_name': 'Sqoshu'}","I was writing a very simple script to count ellipsoid area and volume and some other things. I was presenting my output printing it out like this: What, of course, gave this output (with sample data): As I wrote earlier, I am using jupyter notebook, so I can use operators in markdown cells to create LaTeX formulas. My question is, is it possible to generate output using Python code in a way that it will be understood as LaTeX formula and printed in such a way, that: Thanks for all replies.","print('Dims: {}x{}m\nArea: {}m^2\nVolume: {}m^3'.format(a, round(b,2), P, V))
 Dims: 13.49x2.25m
Area: 302.99m^2
Volume: 90.92m^3
 $",1,12,1,1,
278,49357417,49357642,49303,Why is numpy.linalg.pinv() preferred over numpy.linalg.inv() for creating inverse of a matrix in linear regression,3,<python><numpy><matrix><linear-algebra><linear-regression>,28,"<p>If we want to search for the optimal parameters theta for a linear regression model by using the normal equation with:</p>

<p><strong>theta = inv(X^T * X) * X^T * y</strong></p>

<p>one step is to calculate  inv(X^T*X). Therefore numpy provides <a href=""https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.inv.html"" rel=""noreferrer"">np.linalg.inv()</a> and <a href=""https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.pinv.html"" rel=""noreferrer"">np.linalg.pinv()</a></p>

<p>Though this leads to different results:</p>

<pre><code>X=np.matrix([[1,2104,5,1,45],[1,1416,3,2,40],[1,1534,3,2,30],[1,852,2,1,36]])
y=np.matrix([[460],[232],[315],[178]])

XT=X.T
XTX=XT@X

pinv=np.linalg.pinv(XTX)
theta_pinv=(pinv@XT)@y
print(theta_pinv)

[[188.40031946]
 [  0.3866255 ]
 [-56.13824955]
 [-92.9672536 ]
 [ -3.73781915]]

inv=np.linalg.inv(XTX)
theta_inv=(inv@XT)@y
print(theta_inv)

[[-648.7890625 ]
 [   0.79418945]
 [-110.09375   ]
 [ -74.0703125 ]
 [  -3.69091797]]
</code></pre>

<p>The first output, that is the output of pinv is the correct one and additionally recommended in the <a href=""https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.pinv.html"" rel=""noreferrer"">numpy.linalg.pinv()</a> docs. 
But why is this and where are the differences / Pros  / Cons between inv() and pinv().</p>
",8144295,3630,19-03-2018 07:04,19-03-2018 07:21,0,3630,56,6,30,94,"{'badge_counts': {'bronze': 56, 'silver': 30, 'gold': 6}, 'account_id': 11093159, 'is_employee': False, 'last_modified_date': 1635507300, 'last_access_date': 1641291282, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3630, 'creation_date': 1497179116, 'user_type': 'registered', 'user_id': 8144295, 'accept_rate': 94, 'website_url': '', 'link': 'https://stackoverflow.com/users/8144295/2obe', 'profile_image': 'https://i.stack.imgur.com/a2s3l.jpg?s=256&g=1', 'display_name': '2Obe'}","If we want to search for the optimal parameters theta for a linear regression model by using the normal equation with: theta = inv(X^T * X) * X^T * y one step is to calculate inv(X^T*X). Therefore numpy provides np.linalg.inv() and np.linalg.pinv() Though this leads to different results: The first output, that is the output of pinv is the correct one and additionally recommended in the numpy.linalg.pinv() docs. But why is this and where are the differences / Pros / Cons between inv() and pinv().","X=np.matrix([[1,2104,5,1,45],[1,1416,3,2,40],[1,1534,3,2,30],[1,852,2,1,36]])
y=np.matrix([[460],[232],[315],[178]])

XT=X.T
XTX=XT@X

pinv=np.linalg.pinv(XTX)
theta_pinv=(pinv@XT)@y
print(theta_pinv)

[[188.40031946]
 [  0.3866255 ]
 [-56.13824955]
 [-92.9672536 ]
 [ -3.73781915]]

inv=np.linalg.inv(XTX)
theta_inv=(inv@XT)@y
print(theta_inv)

[[-648.7890625 ]
 [   0.79418945]
 [-110.09375   ]
 [ -74.0703125 ]
 [  -3.69091797]]
",24,37,0,3,
279,48820586,48820766,17566,removing isolated vertices in networkx,3,<python><networkx>,20,"<p>The documentation says that isolated vertices in graph can be obtained using <code>networkx.isolates(*G*)</code>. It adds that the isolated vertices can be removed from a graph G using the code <code>*G*.remove_nodes_from(nx.isolates(*G*))</code>.</p>
<p><a href=""https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.isolate.isolates.html"" rel=""nofollow noreferrer"">https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.isolate.isolates.html</a></p>
<p><a href=""https://i.stack.imgur.com/IqrCi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IqrCi.png"" alt=""screenshot of documentation(url above)"" /></a></p>
<p>But I get the run time error &quot;dictionary changed size during iteration&quot; when I run the code.</p>
<p>Error report:-<br/>
<code>&gt;&gt;&gt; G.remove_nodes_from(nx.isolates(G)) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/home/iiitdm/anaconda2/lib/python2.7/site-packages/networkx/classes/graph.py&quot;, line 617, in remove_nodes_from for n in nodes: File &quot;/home/iiitdm/anaconda2/lib/python2.7/site-packages/networkx/algorithms/isolate.py&quot;, line 94, in &lt;genexpr&gt; return (n for n, d in G.degree() if d == 0) File &quot;/home/iiitdm/anaconda2/lib/python2.7/site-packages/networkx/classes/reportviews.py&quot;, line 443, in __iter__ for n in self._nodes: RuntimeError: dictionary changed size during iteration</code></p>
<p>It is understandable and was expected because (I think) the generator object created using the function <code>isolates()</code> changes with G and hence changing graph G while it is being 'iterated' should give a similar error. Then that line in the documentation must be wrong, isn't it? Am I completely off the mark? I am pretty new to python.</p>
<p>By the way, the object returned by <code>networkx.isolates()</code> is a generator object.</p>
",8653606,386,16-02-2018 05:28,16-02-2018 05:45,0,386,13,1,3,,"{'badge_counts': {'bronze': 13, 'silver': 3, 'gold': 1}, 'account_id': 3844520, 'is_employee': False, 'last_modified_date': 1635554700, 'last_access_date': 1698226124, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 386, 'creation_date': 1506061045, 'user_type': 'registered', 'user_id': 8653606, 'location': 'India', 'link': 'https://stackoverflow.com/users/8653606/cyriac-antony', 'profile_image': 'https://i.stack.imgur.com/QjWSm.jpg?s=256&g=1', 'display_name': 'Cyriac Antony'}","The documentation says that isolated vertices in graph can be obtained using . It adds that the isolated vertices can be removed from a graph G using the code . https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.isolate.isolates.html But I get the run time error &quot;dictionary changed size during iteration&quot; when I run the code. Error report:- It is understandable and was expected because (I think) the generator object created using the function changes with G and hence changing graph G while it is being 'iterated' should give a similar error. Then that line in the documentation must be wrong, isn't it? Am I completely off the mark? I am pretty new to python. By the way, the object returned by is a generator object.","networkx.isolates(*G*) *G*.remove_nodes_from(nx.isolates(*G*)) &gt;&gt;&gt; G.remove_nodes_from(nx.isolates(G)) Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/home/iiitdm/anaconda2/lib/python2.7/site-packages/networkx/classes/graph.py&quot;, line 617, in remove_nodes_from for n in nodes: File &quot;/home/iiitdm/anaconda2/lib/python2.7/site-packages/networkx/algorithms/isolate.py&quot;, line 94, in &lt;genexpr&gt; return (n for n, d in G.degree() if d == 0) File &quot;/home/iiitdm/anaconda2/lib/python2.7/site-packages/networkx/classes/reportviews.py&quot;, line 443, in __iter__ for n in self._nodes: RuntimeError: dictionary changed size during iteration isolates() networkx.isolates()",-5,8,1,2,
280,50037063,50076149,23333,Save grayscale video in OpenCV?,3,<python><opencv>,16,"<p>I am trying to save a video as <code>.avi</code> format but i keep getting the error <code>""could not demultiplex stream""</code>. I primarily want to save grayscale videos.</p>

<p>Is there any specific <code>codec</code> i need to use?</p>

<p>Right now i tried with <code>XVID, DIVX</code></p>

<pre><code>import imutils
import cv2
import numpy as np

interval = 30
outfilename = 'output.avi'
threshold=100.
fps = 10

cap = cv2.VideoCapture(""video.mp4"")

ret, frame = cap.read()
height, width, nchannels = frame.shape

fourcc = cv2.cv.CV_FOURCC(*'DIVX')
out = cv2.VideoWriter( outfilename,fourcc, fps, (width,height))

ret, frame = cap.read()
frame = imutils.resize(frame, width=500)
frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)

while(True):

  frame0 = frame

  ret, frame = cap.read()
  frame = imutils.resize(frame, width=500)
  frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)

  if not ret:
    deletedcount +=1
    break

  if np.sum( np.absolute(frame-frame0) )/np.size(frame) &gt; threshold:
    out.write(frame)
  else:
    print ""Deleted""

  cv2.imshow('Feed - Press ""q"" to exit',frame)

  key = cv2.waitKey(interval) &amp; 0xFF

  if key == ord('q'):
    print('received key q' )
    break

cap.release()
out.release()
print('Successfully completed')
</code></pre>
",8707637,1447,26-04-2018 07:19,28-04-2018 11:40,2,1447,49,4,22,43,"{'badge_counts': {'bronze': 49, 'silver': 22, 'gold': 4}, 'account_id': 11899152, 'is_employee': False, 'last_modified_date': 1642206900, 'last_access_date': 1710935750, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1447, 'creation_date': 1506936949, 'user_type': 'registered', 'user_id': 8707637, 'accept_rate': 43, 'location': 'Bangalore, Karnataka, India', 'website_url': 'https://nithinkashyapn.github.io/', 'link': 'https://stackoverflow.com/users/8707637/nithin', 'profile_image': 'https://www.gravatar.com/avatar/fe2b5ad84b2893da6f2aa9c5d5281516?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nithin'}",I am trying to save a video as format but i keep getting the error . I primarily want to save grayscale videos. Is there any specific i need to use? Right now i tried with,".avi ""could not demultiplex stream"" codec XVID, DIVX import imutils
import cv2
import numpy as np

interval = 30
outfilename = 'output.avi'
threshold=100.
fps = 10

cap = cv2.VideoCapture(""video.mp4"")

ret, frame = cap.read()
height, width, nchannels = frame.shape

fourcc = cv2.cv.CV_FOURCC(*'DIVX')
out = cv2.VideoWriter( outfilename,fourcc, fps, (width,height))

ret, frame = cap.read()
frame = imutils.resize(frame, width=500)
frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)

while(True):

  frame0 = frame

  ret, frame = cap.read()
  frame = imutils.resize(frame, width=500)
  frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)

  if not ret:
    deletedcount +=1
    break

  if np.sum( np.absolute(frame-frame0) )/np.size(frame) &gt; threshold:
    out.write(frame)
  else:
    print ""Deleted""

  cv2.imshow('Feed - Press ""q"" to exit',frame)

  key = cv2.waitKey(interval) &amp; 0xFF

  if key == ord('q'):
    print('received key q' )
    break

cap.release()
out.release()
print('Successfully completed')
",44,56,0,0,
281,49247506,49247599,11167,How to efficiently find the indices of matching elements in two lists,5,<python><algorithm><matching>,18,"<p>I am working on two large data sets, and my question is as follows.</p>

<p>Suppose I have two lists:</p>

<p><code>list1 = [A,B,C,D]</code></p>

<p><code>list2 = [B,D,A,G]</code></p>

<p>How can I efficiently find the matching index, using Python, other than O(n<sup>2</sup>) searching? The result should look like:</p>

<p><code>matching_index(list1,list2) -&gt; [(0,2),(1,0),(3,1)]</code> </p>
",8734796,305,13-03-2018 02:44,13-03-2018 02:57,0,305,9,1,3,,"{'badge_counts': {'bronze': 9, 'silver': 3, 'gold': 1}, 'account_id': 11936513, 'is_employee': False, 'last_modified_date': 1647455045, 'last_access_date': 1536526377, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 305, 'creation_date': 1507326686, 'user_type': 'registered', 'user_id': 8734796, 'location': 'New York', 'link': 'https://stackoverflow.com/users/8734796/haoran', 'profile_image': 'https://www.gravatar.com/avatar/de67a9a1d9660732d9ef590014e96046?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Haoran'}","I am working on two large data sets, and my question is as follows. Suppose I have two lists: How can I efficiently find the matching index, using Python, other than O(n2) searching? The result should look like:","list1 = [A,B,C,D] list2 = [B,D,A,G] matching_index(list1,list2) -&gt; [(0,2),(1,0),(3,1)]",-3,11,0,0,
282,49088443,49088473,23833,Search and replace dots and commas in pandas dataframe,3,<python><python-3.x><pandas>,13,"<p>This is my DataFrame:</p>
<pre><code>d = {'col1': ['sku 1.1', 'sku 1.2', 'sku 1.3'], 'col2': ['9.876.543,21', 654, '321,01']}
df = pd.DataFrame(data=d)
df

       col1           col2
0   sku 1.1   9.876.543,21
1   sku 1.2            654
2   sku 1.3         321,01
</code></pre>
<p>Data in col2 are numbers in local format, which I would like to convert into:</p>
<pre><code>      col2
9876543.21
       654
    321.01
</code></pre>
<p>I tried <code>df['col2'] = pd.to_numeric(df['col2'], downcast='float')</code>, which returns a ValueError: : Unable to parse string &quot;9.876.543,21&quot; at position 0.</p>
<p>I tried also <code>df = df.apply(lambda x: x.str.replace(',', '.'))</code>, which returns ValueError: could not convert string to float: '5.023.654.46'</p>
",8753634,331,03-03-2018 19:41,03-03-2018 19:43,0,331,13,1,4,,"{'badge_counts': {'bronze': 13, 'silver': 4, 'gold': 1}, 'account_id': 4987717, 'is_employee': False, 'last_modified_date': 1645529489, 'last_access_date': 1641200916, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 331, 'creation_date': 1507651530, 'user_type': 'registered', 'user_id': 8753634, 'location': 'Ljubljana, Slovenija', 'link': 'https://stackoverflow.com/users/8753634/andrej', 'profile_image': 'https://www.gravatar.com/avatar/ec59584bd338222273db57a0118ecc7f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'andrej'}","This is my DataFrame: Data in col2 are numbers in local format, which I would like to convert into: I tried , which returns a ValueError: : Unable to parse string &quot;9.876.543,21&quot; at position 0. I tried also , which returns ValueError: could not convert string to float: '5.023.654.46'","d = {'col1': ['sku 1.1', 'sku 1.2', 'sku 1.3'], 'col2': ['9.876.543,21', 654, '321,01']}
df = pd.DataFrame(data=d)
df

       col1           col2
0   sku 1.1   9.876.543,21
1   sku 1.2            654
2   sku 1.3         321,01
       col2
9876543.21
       654
    321.01
 df['col2'] = pd.to_numeric(df['col2'], downcast='float') df = df.apply(lambda x: x.str.replace(',', '.'))",8,18,0,0,
283,48366506,48366525,121975,Calculate new column as the mean of other columns in pandas,2,<python><pandas><dataframe>,55,"<p>I have a this data frame and I would like to calculate a new column as the mean of <code>salary_1</code>, <code>salary_2</code> and <code>salary_3</code>:</p>
<pre><code>df = pd.DataFrame({
    'salary_1': [230, 345, 222],
    'salary_2': [235, 375, 292],
    'salary_3': [210, 385, 260]
})
</code></pre>
<pre class=""lang-none prettyprint-override""><code>      salary_1     salary_2    salary_3
0        230           235        210
1        345           375        385
2        222           292        260
</code></pre>
<p>How can I do it in pandas in the most efficient way? Actually I have many more columns and I don't want to write this one by one.</p>
<p>Something like this:</p>
<pre class=""lang-none prettyprint-override""><code>      salary_1     salary_2    salary_3     salary_mean
0        230           235        210     (230+235+210)/3
1        345           375        385       ...
2        222           292        260       ...
</code></pre>
",8759371,1059,21-01-2018 11:58,21-01-2018 11:59,0,1059,15,2,12,100,"{'badge_counts': {'bronze': 15, 'silver': 12, 'gold': 2}, 'account_id': 11970827, 'is_employee': False, 'last_modified_date': 1573678502, 'last_access_date': 1669634280, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1059, 'creation_date': 1507729137, 'user_type': 'registered', 'user_id': 8759371, 'accept_rate': 100, 'location': 'Madrid, Espa&#241;a', 'link': 'https://stackoverflow.com/users/8759371/carmen-p%c3%a9rez-carrillo', 'profile_image': 'https://www.gravatar.com/avatar/81ea2c472a75462fa6b94bed012221cb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Carmen P&#233;rez Carrillo'}","I have a this data frame and I would like to calculate a new column as the mean of , and : How can I do it in pandas in the most efficient way? Actually I have many more columns and I don't want to write this one by one. Something like this:","salary_1 salary_2 salary_3 df = pd.DataFrame({
    'salary_1': [230, 345, 222],
    'salary_2': [235, 375, 292],
    'salary_3': [210, 385, 260]
})
       salary_1     salary_2    salary_3
0        230           235        210
1        345           375        385
2        222           292        260
       salary_1     salary_2    salary_3     salary_mean
0        230           235        210     (230+235+210)/3
1        345           375        385       ...
2        222           292        260       ...
",7,19,0,0,
284,50213761,50417291,67055,Changing visibility of a Dash Component by updating other Component,2,<python><plotly><dashboard><plotly-dash>,31,"<p>I need to hide some Components, for example by clicking on a checkbox (for example, a graph or a table). However, the documentation did not provide a suitable section for this purpose. Thanks in advance!</p>
",8767215,412,07-05-2018 12:01,18-05-2018 18:29,11,412,6,1,5,,"{'badge_counts': {'bronze': 6, 'silver': 5, 'gold': 1}, 'account_id': 11981427, 'is_employee': False, 'last_modified_date': 1624315158, 'last_access_date': 1702289772, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 412, 'creation_date': 1507833205, 'user_type': 'registered', 'user_id': 8767215, 'location': 'Екатеринбург, Свердловская область, Россия', 'link': 'https://stackoverflow.com/users/8767215/%d0%98%d0%b2%d0%b0%d0%bd-%d0%a1%d0%b8%d1%82%d0%bd%d0%b8%d0%ba%d0%be%d0%b2', 'profile_image': 'https://lh4.googleusercontent.com/-93TZwmUJivM/AAAAAAAAAAI/AAAAAAAAACI/tmjtMnsKwMk/photo.jpg?sz=256', 'display_name': 'Иван Ситников'}","I need to hide some Components, for example by clicking on a checkbox (for example, a graph or a table). However, the documentation did not provide a suitable section for this purpose. Thanks in advance!",,0,1,0,0,
285,49834380,49834968,13587,"K.gradients(loss, input_img)[0] return ""None"". (Keras CNN visualization with tensorflow backend)",3,<python><tensorflow><neural-network><deep-learning><keras>,12,"<p>I have CNN models trained using Keras with Tensorflow backend.
And I want to visualize my CNN filters with this tutorial: <a href=""https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html"" rel=""noreferrer"">https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html</a></p>

<pre><code>from keras import backend as K
from keras.models import load_model
import numpy as np

model = load_model('my_cnn_model.h5')
input_img = np.load('my_picture.npy')

# get the symbolic outputs of each ""key"" layer (we gave them unique names).
layer_dict = dict([(layer.name, layer) for layer in model.layers])

layer_name = 'block5_conv3'
filter_index = 0  # can be any integer from 0 to 511, as there are 512 filters in that layer

# build a loss function that maximizes the activation
# of the nth filter of the layer considered
layer_output = layer_dict[layer_name].output
loss = K.mean(layer_output[:, :, :, filter_index])

# compute the gradient of the input picture wrt this loss
grads = K.gradients(loss, input_img)[0]

# normalization trick: we normalize the gradient
grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)

# this function returns the loss and grads given the input picture
iterate = K.function([input_img], [loss, grads])
</code></pre>

<p>However, when the code execute to this line:<br>
<code>grads = K.gradients(loss, input_img)[0]</code><br>
I found it returns nothing but <code>None</code> object, so the program fail to progress after that.</p>

<p>I search for some solution. Some people say the<code>input_img</code> should be tensorflow's Tensor type:
<a href=""https://github.com/keras-team/keras/issues/5455"" rel=""noreferrer"">https://github.com/keras-team/keras/issues/5455</a></p>

<p>But when I tried to convert the img to Tensor, the problem is still exist.<br>
I tried the solution in the link above, but still fail.</p>

<p>There is also someone say that this problem exists because your CNN model is not differentiable.
<a href=""https://github.com/keras-team/keras/issues/8478"" rel=""noreferrer"">https://github.com/keras-team/keras/issues/8478</a></p>

<p>But my model use only the activate function of ReLU and Sigmoid(at output layer).
Is this problem really caused by nondifferentiable problem?</p>

<p>Can anyone help me? Thank you very much!</p>
",8815081,463,14-04-2018 17:43,14-04-2018 18:46,0,463,13,2,4,,"{'badge_counts': {'bronze': 13, 'silver': 4, 'gold': 2}, 'account_id': 12052800, 'is_employee': False, 'last_modified_date': 1607614418, 'last_access_date': 1707857155, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 463, 'creation_date': 1508685562, 'user_type': 'registered', 'user_id': 8815081, 'link': 'https://stackoverflow.com/users/8815081/jexus', 'profile_image': 'https://www.gravatar.com/avatar/049eb68339db2dade77dddc159135965?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jexus'}","I have CNN models trained using Keras with Tensorflow backend. And I want to visualize my CNN filters with this tutorial: https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html However, when the code execute to this line: I found it returns nothing but object, so the program fail to progress after that. I search for some solution. Some people say the should be tensorflow's Tensor type: https://github.com/keras-team/keras/issues/5455 But when I tried to convert the img to Tensor, the problem is still exist. I tried the solution in the link above, but still fail. There is also someone say that this problem exists because your CNN model is not differentiable. https://github.com/keras-team/keras/issues/8478 But my model use only the activate function of ReLU and Sigmoid(at output layer). Is this problem really caused by nondifferentiable problem? Can anyone help me? Thank you very much!","from keras import backend as K
from keras.models import load_model
import numpy as np

model = load_model('my_cnn_model.h5')
input_img = np.load('my_picture.npy')

# get the symbolic outputs of each ""key"" layer (we gave them unique names).
layer_dict = dict([(layer.name, layer) for layer in model.layers])

layer_name = 'block5_conv3'
filter_index = 0  # can be any integer from 0 to 511, as there are 512 filters in that layer

# build a loss function that maximizes the activation
# of the nth filter of the layer considered
layer_output = layer_dict[layer_name].output
loss = K.mean(layer_output[:, :, :, filter_index])

# compute the gradient of the input picture wrt this loss
grads = K.gradients(loss, input_img)[0]

# normalization trick: we normalize the gradient
grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)

# this function returns the loss and grads given the input picture
iterate = K.function([input_img], [loss, grads])
 grads = K.gradients(loss, input_img)[0] None input_img",22,48,0,3,
286,49782579,54286158,6205,Float16 slower than float32 in keras,2,<python><tensorflow><keras>,20,"<p>I'm testing out my new NVIDIA Titan V, which supports float16 operations.  I noticed that during training, float16 is much slower (~800 ms/step) than float32 (~500 ms/step).</p>

<p>To do float16 operations, I changed my keras.json file to:</p>

<pre><code>{
""backend"": ""tensorflow"",
""floatx"": ""float16"",
""image_data_format"": ""channels_last"",
""epsilon"": 1e-07
}
</code></pre>

<p>Why are the float16 operations so much slower?  Do I need to make modifications to my code and not just the keras.json file?</p>

<p>I am using CUDA 9.0, cuDNN 7.0, tensorflow 1.7.0, and keras 2.1.5 on Windows 10.
My python 3.5 code is below:</p>

<pre><code>img_width, img_height = 336, 224

train_data_dir = 'C:\\my_dir\\train'
test_data_dir = 'C:\\my_dir\\test'
batch_size=128

datagen = ImageDataGenerator(rescale=1./255,
    horizontal_flip=True,   # randomly flip the images 
    vertical_flip=True) 

train_generator = datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary')

test_generator = datagen.flow_from_directory(
    test_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary')

# Architecture of NN
model = Sequential()
model.add(Conv2D(32,(3, 3), input_shape=(img_height, img_width, 3),padding='same',kernel_initializer='lecun_normal'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32,(3, 3),padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64,(3, 3),padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64,(3, 3),padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(AveragePooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(1))
model.add(Activation('sigmoid'))

my_rmsprop = keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-04, decay=0.0)
model.compile(loss='binary_crossentropy',
          optimizer=my_rmsprop,
          metrics=['accuracy'])

# Training 
nb_epoch = 32
nb_train_samples = 512
nb_test_samples = 512

model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples/batch_size,
    epochs=nb_epoch,
    verbose=1,
    validation_data=test_generator,
    validation_steps=nb_test_samples/batch_size)

# Evaluating on the testing set
model.evaluate_generator(test_generator, nb_test_samples)
</code></pre>
",8819660,439,11-04-2018 18:56,21-01-2019 08:46,285,439,14,0,4,,"{'badge_counts': {'bronze': 14, 'silver': 4, 'gold': 0}, 'account_id': 12060441, 'is_employee': False, 'last_modified_date': 1573678493, 'last_access_date': 1706136700, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 439, 'creation_date': 1508766327, 'user_type': 'registered', 'user_id': 8819660, 'link': 'https://stackoverflow.com/users/8819660/mark', 'profile_image': 'https://www.gravatar.com/avatar/e47e680e00b73b58f236c747a5ce379b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Mark'}","I'm testing out my new NVIDIA Titan V, which supports float16 operations. I noticed that during training, float16 is much slower (~800 ms/step) than float32 (~500 ms/step). To do float16 operations, I changed my keras.json file to: Why are the float16 operations so much slower? Do I need to make modifications to my code and not just the keras.json file? I am using CUDA 9.0, cuDNN 7.0, tensorflow 1.7.0, and keras 2.1.5 on Windows 10. My python 3.5 code is below:","{
""backend"": ""tensorflow"",
""floatx"": ""float16"",
""image_data_format"": ""channels_last"",
""epsilon"": 1e-07
}
 img_width, img_height = 336, 224

train_data_dir = 'C:\\my_dir\\train'
test_data_dir = 'C:\\my_dir\\test'
batch_size=128

datagen = ImageDataGenerator(rescale=1./255,
    horizontal_flip=True,   # randomly flip the images 
    vertical_flip=True) 

train_generator = datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary')

test_generator = datagen.flow_from_directory(
    test_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary')

# Architecture of NN
model = Sequential()
model.add(Conv2D(32,(3, 3), input_shape=(img_height, img_width, 3),padding='same',kernel_initializer='lecun_normal'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32,(3, 3),padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64,(3, 3),padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64,(3, 3),padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(AveragePooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(1))
model.add(Activation('sigmoid'))

my_rmsprop = keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-04, decay=0.0)
model.compile(loss='binary_crossentropy',
          optimizer=my_rmsprop,
          metrics=['accuracy'])

# Training 
nb_epoch = 32
nb_train_samples = 512
nb_test_samples = 512

model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples/batch_size,
    epochs=nb_epoch,
    verbose=1,
    validation_data=test_generator,
    validation_steps=nb_test_samples/batch_size)

# Evaluating on the testing set
model.evaluate_generator(test_generator, nb_test_samples)
",69,83,0,0,
287,50202238,50212636,224561,Python (pip) - RequestsDependencyWarning: urllib3 (1.9.1) or chardet (2.3.0) doesn't match a supported version,21,<python><pip><python-requests><urllib3><chardet>,112,"<p>I found several pages about this issue but none of them solved my problem.</p>
<p>Even if I do a :</p>
<pre><code>pip show
</code></pre>
<p>I get :</p>
<pre><code>/usr/local/lib/python2.7/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.9.1) or chardet (2.3.0) doesn't match a supported version!
  RequestsDependencyWarning)
Traceback (most recent call last):
  File &quot;/usr/bin/pip&quot;, line 9, in &lt;module&gt;
    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 480, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2691, in load_entry_point
    return ep.load()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2322, in load
    return self.resolve()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2328, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File &quot;/usr/lib/python2.7/dist-packages/pip/__init__.py&quot;, line 74, in &lt;module&gt;
    from pip.vcs import git, mercurial, subversion, bazaar  # noqa
  File &quot;/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py&quot;, line 9, in &lt;module&gt;
    from pip.download import path_to_url
  File &quot;/usr/lib/python2.7/dist-packages/pip/download.py&quot;, line 22, in &lt;module&gt;
    import requests, six
  File &quot;/usr/local/lib/python2.7/dist-packages/requests/__init__.py&quot;, line 90, in &lt;module&gt;
    from urllib3.exceptions import DependencyWarning
ImportError: cannot import name DependencyWarning
</code></pre>
<p>What I did :</p>
<pre><code>pip install --upgrade chardet
</code></pre>
<p>but as explain up, it gaves me the same error.</p>
<p>so I did :</p>
<pre><code>sudo apt remove python-chardet
</code></pre>
<p>and unistalling all his dependecies.
After I reinstall it -&gt; the same :'(</p>
<p>I did the same for <code>python-pip</code>. After reinstalling it -&gt; the same. <br>
Here are the lines about <code>urllib3</code> and <code>chardet</code> versions needed :
extract of <code>/usr/local/lib/python2.7/dist-packages/requests/__init__.py</code> :</p>
<pre><code>    # Check urllib3 for compatibility.
    major, minor, patch = urllib3_version  # noqa: F811
    major, minor, patch = int(major), int(minor), int(patch)
    # urllib3 &gt;= 1.21.1, &lt;= 1.22
    assert major == 1
    assert minor &gt;= 21
    assert minor &lt;= 22

    # Check chardet for compatibility.
    major, minor, patch = chardet_version.split('.')[:3]
    major, minor, patch = int(major), int(minor), int(patch)
    # chardet &gt;= 3.0.2, &lt; 3.1.0
    assert major == 3
    assert minor &lt; 1
    assert patch &gt;= 2


# Check imported dependencies for compatibility.
try:
    check_compatibility(urllib3.__version__, chardet.__version__)
except (AssertionError, ValueError):
    warnings.warn(&quot;urllib3 ({0}) or chardet ({1}) doesn't match a supported &quot;
                  &quot;version!&quot;.format(urllib3.__version__, chardet.__version__),
                  RequestsDependencyWarning)
</code></pre>
<p>My versions are :</p>
<pre><code>ii  python-urllib3 1.9.1-3   all HTTP library with thread-safe connection pooling for Python 
ii  python-chardet  2.3.0-1  all universal character encoding detector for Python2
</code></pre>
<p>I don't have no more ideas...</p>
",8827756,1223,06-05-2018 16:54,07-05-2018 11:00,1,1243,9,2,8,,"{'badge_counts': {'bronze': 9, 'silver': 8, 'gold': 2}, 'account_id': 12073134, 'is_employee': False, 'last_modified_date': 1583555400, 'last_access_date': 1585582332, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1243, 'creation_date': 1508876201, 'user_type': 'registered', 'user_id': 8827756, 'link': 'https://stackoverflow.com/users/8827756/nux-o', 'profile_image': 'https://www.gravatar.com/avatar/0af4268dda2ab29dbe7d4b2434070831?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'NuX_o'}","I found several pages about this issue but none of them solved my problem. Even if I do a : I get : What I did : but as explain up, it gaves me the same error. so I did : and unistalling all his dependecies. After I reinstall it -&gt; the same :'( I did the same for . After reinstalling it -&gt; the same. Here are the lines about and versions needed : extract of : My versions are : I don't have no more ideas...","pip show
 /usr/local/lib/python2.7/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.9.1) or chardet (2.3.0) doesn't match a supported version!
  RequestsDependencyWarning)
Traceback (most recent call last):
  File &quot;/usr/bin/pip&quot;, line 9, in &lt;module&gt;
    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 480, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2691, in load_entry_point
    return ep.load()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2322, in load
    return self.resolve()
  File &quot;/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py&quot;, line 2328, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File &quot;/usr/lib/python2.7/dist-packages/pip/__init__.py&quot;, line 74, in &lt;module&gt;
    from pip.vcs import git, mercurial, subversion, bazaar  # noqa
  File &quot;/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py&quot;, line 9, in &lt;module&gt;
    from pip.download import path_to_url
  File &quot;/usr/lib/python2.7/dist-packages/pip/download.py&quot;, line 22, in &lt;module&gt;
    import requests, six
  File &quot;/usr/local/lib/python2.7/dist-packages/requests/__init__.py&quot;, line 90, in &lt;module&gt;
    from urllib3.exceptions import DependencyWarning
ImportError: cannot import name DependencyWarning
 pip install --upgrade chardet
 sudo apt remove python-chardet
 python-pip urllib3 chardet /usr/local/lib/python2.7/dist-packages/requests/__init__.py     # Check urllib3 for compatibility.
    major, minor, patch = urllib3_version  # noqa: F811
    major, minor, patch = int(major), int(minor), int(patch)
    # urllib3 &gt;= 1.21.1, &lt;= 1.22
    assert major == 1
    assert minor &gt;= 21
    assert minor &lt;= 22

    # Check chardet for compatibility.
    major, minor, patch = chardet_version.split('.')[:3]
    major, minor, patch = int(major), int(minor), int(patch)
    # chardet &gt;= 3.0.2, &lt; 3.1.0
    assert major == 3
    assert minor &lt; 1
    assert patch &gt;= 2


# Check imported dependencies for compatibility.
try:
    check_compatibility(urllib3.__version__, chardet.__version__)
except (AssertionError, ValueError):
    warnings.warn(&quot;urllib3 ({0}) or chardet ({1}) doesn't match a supported &quot;
                  &quot;version!&quot;.format(urllib3.__version__, chardet.__version__),
                  RequestsDependencyWarning)
 ii  python-urllib3 1.9.1-3   all HTTP library with thread-safe connection pooling for Python 
ii  python-chardet  2.3.0-1  all universal character encoding detector for Python2
",41,70,0,0,
288,48550201,48550825,15024,What does train_on_batch() do in keras model?,3,<python><tensorflow><machine-learning><keras><artificial-intelligence>,13,"<p>I saw a sample of code (too big to paste here) where the author used <code>model.train_on_batch(in, out)</code> instead of <code>model.fit(in, out)</code>. The official documentation of Keras says: </p>

<blockquote>
  <p>Single gradient update over one batch of samples.</p>
</blockquote>

<p>But I don't get it. Is it the same as <code>fit()</code>, but instead of doing many feed-forward and backprop steps, it does it once? Or am I wrong?</p>
",8278437,455,31-01-2018 19:41,31-01-2018 20:21,0,455,14,1,5,100,"{'badge_counts': {'bronze': 14, 'silver': 5, 'gold': 1}, 'account_id': 11288418, 'is_employee': False, 'last_modified_date': 1579273801, 'last_access_date': 1521893339, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 455, 'creation_date': 1499586209, 'user_type': 'registered', 'user_id': 8278437, 'accept_rate': 100, 'location': 'Tbilisi, Georgia', 'website_url': '', 'link': 'https://stackoverflow.com/users/8278437/cerushdope', 'profile_image': 'https://www.gravatar.com/avatar/2f596448297d8a54ace55987185ca2a0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'CerushDope'}","I saw a sample of code (too big to paste here) where the author used instead of . The official documentation of Keras says: Single gradient update over one batch of samples. But I don't get it. Is it the same as , but instead of doing many feed-forward and backprop steps, it does it once? Or am I wrong?","model.train_on_batch(in, out) model.fit(in, out) fit()",-3,7,0,0,
289,48199961,48200012,15174,How to interleave two lists of different length?,6,<python>,11,"<p>I want to write a function <code>twolists</code> which gives results like:</p>
<pre><code>outcome = twolists([ ], ['w', 'x', 'y', 'z'])
print(outcome)
['w', 'x', 'y', 'z']

outcome = twolists([0, 1], ['w', 'x'])
print(outcome)
[0, 'w', 1, 'x']
 
outcome = twolists([0, 1], ['w', 'x', 'y', 'z'])
print(outcome)
[0, 'w', 1, 'x', 'y', 'z']

outcome = twolists([0, 1, 2, 3], ['w', 'x'])
print(outcome)
[0, 'w', 1, 'x', 2, 3]
</code></pre>
<p>I have this so far:</p>
<pre><code>def twolists(list1, list2): # don't forget to return final_list
    alt_list = []
    a1 = len(list1)
    a2 = len(list2)
    
    for i in range(# ? ):
        # append one thing from list1 to alt_list - How?
        # append one thing from list2 to alt_list - How?
</code></pre>
<p>How can I complete the code?</p>
",8835507,131,11-01-2018 04:50,11-01-2018 04:57,0,131,7,1,1,75,"{'badge_counts': {'bronze': 7, 'silver': 1, 'gold': 1}, 'account_id': 12087439, 'is_employee': False, 'last_modified_date': 1573678490, 'last_access_date': 1529294744, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 131, 'creation_date': 1508995108, 'user_type': 'registered', 'user_id': 8835507, 'accept_rate': 75, 'link': 'https://stackoverflow.com/users/8835507/felixf', 'profile_image': 'https://www.gravatar.com/avatar/4266c556adba9fcf8420bc4b683fd38c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'FelixF'}",I want to write a function which gives results like: I have this so far: How can I complete the code?,"twolists outcome = twolists([ ], ['w', 'x', 'y', 'z'])
print(outcome)
['w', 'x', 'y', 'z']

outcome = twolists([0, 1], ['w', 'x'])
print(outcome)
[0, 'w', 1, 'x']
 
outcome = twolists([0, 1], ['w', 'x', 'y', 'z'])
print(outcome)
[0, 'w', 1, 'x', 'y', 'z']

outcome = twolists([0, 1, 2, 3], ['w', 'x'])
print(outcome)
[0, 'w', 1, 'x', 2, 3]
 def twolists(list1, list2): # don't forget to return final_list
    alt_list = []
    a1 = len(list1)
    a2 = len(list2)
    
    for i in range(# ? ):
        # append one thing from list1 to alt_list - How?
        # append one thing from list2 to alt_list - How?
",20,28,0,0,
290,49955489,50534419,378912,How to develop Android app completely using python?,4,<android><python>,179,"<p>I would like to develop a (rather simple) android app to be distributed via Play Store. I would like to do so completely in python. However, the online research hasn't quite enlightened me: most comments are either outdated (&gt;1 year old, and I feel there might be better integration of python since then) or they talk about <em>running python</em> in android (e.g. <a href=""https://stackoverflow.com/questions/40326178/how-can-i-write-an-android-app-in-python"">here</a>).</p>
<p>Therefore, I'm looking for information regarding the questions:</p>
<ul>
<li>is it feasible to develop an App completely in python - and what are the tools to do so? (Is e.g. Kivy recommendable?)</li>
<li>if so: what are the best software environments to implement this? (I unsuccessfully tried using Android Studio but couldn't figure out a way to run python code there.)</li>
</ul>
<p>I'm quite new to app development and would highly appreciate any leads of doing this in python rather than in Jave etc., which I don't know yet.</p>
",8839068,4020,21-04-2018 11:54,25-05-2018 17:42,34,4030,60,5,27,100,"{'badge_counts': {'bronze': 60, 'silver': 27, 'gold': 5}, 'account_id': 12094692, 'is_employee': False, 'last_modified_date': 1687251375, 'last_access_date': 1711027586, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4030, 'creation_date': 1509035278, 'user_type': 'registered', 'user_id': 8839068, 'accept_rate': 100, 'location': 'Berlin, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/8839068/ivo', 'profile_image': 'https://www.gravatar.com/avatar/febf1f986d293a479f5f224ce61eff82?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ivo'}","I would like to develop a (rather simple) android app to be distributed via Play Store. I would like to do so completely in python. However, the online research hasn't quite enlightened me: most comments are either outdated (&gt;1 year old, and I feel there might be better integration of python since then) or they talk about running python in android (e.g. here). Therefore, I'm looking for information regarding the questions: is it feasible to develop an App completely in python - and what are the tools to do so? (Is e.g. Kivy recommendable?) if so: what are the best software environments to implement this? (I unsuccessfully tried using Android Studio but couldn't figure out a way to run python code there.) I'm quite new to app development and would highly appreciate any leads of doing this in python rather than in Jave etc., which I don't know yet.",,0,7,0,1,
291,48703423,48718979,8876,Cython: Compile a Standalone Static Executable,1,<python><c++><c><cython><static-linking>,12,"<p>I'm trying to compile an executable (ELF file) that does not use a dynamic loader.
I used <code>Cython</code> to compile Python to C:</p>

<pre><code>cython3 -3 test.py --embed
</code></pre>

<p>Then</p>

<pre><code>gcc test.c -otest $(pkg-config --libs --cflags python3)
</code></pre>

<p>to compile C generated file.</p>

<p>Now I'd like to compile <code>test.c</code> in a static executable.
Usually I use <code>-static</code> flag, but here it return <code>collect2: error: ld returned 1 exit status</code>.</p>

<p>How can I do?</p>

<p><strong>EDIT 1:</strong> <em>Not the full messge becouse body is limited to 30000 chars</em></p>

<p><em>a long list of warnings and errors like these</em>:</p>

<pre><code>...
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x430): undefined reference to `XML_SetStartCdataSectionHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x458): undefined reference to `XML_SetEndCdataSectionHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x480): undefined reference to `XML_SetDefaultHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x4a8): undefined reference to `XML_SetDefaultHandlerExpand'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x4d0): undefined reference to `XML_SetNotStandaloneHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x4f8): undefined reference to `XML_SetExternalEntityRefHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x520): undefined reference to `XML_SetStartDoctypeDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x548): undefined reference to `XML_SetEndDoctypeDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x570): undefined reference to `XML_SetEntityDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x598): undefined reference to `XML_SetXmlDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x5c0): undefined reference to `XML_SetElementDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x5e8): undefined reference to `XML_SetAttlistDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x610): undefined reference to `XML_SetSkippedEntityHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x6c): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x9c): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0xbc): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x11b): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x153): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.text.unlikely+0x173): more undefined references to `XML_ErrorString' follow
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c62): undefined reference to `XML_GetErrorCode'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c6d): undefined reference to `XML_GetCurrentColumnNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c78): undefined reference to `XML_GetCurrentLineNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c83): undefined reference to `XML_Parse'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c8e): undefined reference to `XML_ParserCreate_MM'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c99): undefined reference to `XML_ParserFree'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1ca4): undefined reference to `XML_SetCharacterDataHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1caf): undefined reference to `XML_SetCommentHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cba): undefined reference to `XML_SetDefaultHandlerExpand'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cc5): undefined reference to `XML_SetElementHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cd0): undefined reference to `XML_SetNamespaceDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cdb): undefined reference to `XML_SetProcessingInstructionHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1ce6): undefined reference to `XML_SetUnknownEncodingHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cf1): undefined reference to `XML_SetUserData'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cfc): undefined reference to `XML_SetStartDoctypeDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1d07): undefined reference to `XML_SetEncoding'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x267b): undefined reference to `XML_GetCurrentLineNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x26d5): undefined reference to `XML_GetCurrentColumnNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x272a): undefined reference to `XML_GetCurrentByteIndex'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x27ee): undefined reference to `XML_GetCurrentColumnNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x281c): undefined reference to `XML_GetCurrentByteIndex'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x2853): undefined reference to `XML_GetCurrentLineNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x286d): undefined reference to `XML_GetErrorCode'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_xmlparser_Parse.cold.21':
(.text.unlikely+0x28f0): undefined reference to `XML_Parse'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `call_with_frame.isra.7.cold.24':
(.text.unlikely+0x2a9a): undefined reference to `XML_StopParser'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `get_parse_result.cold.27':
(.text.unlikely+0x2b0a): undefined reference to `XML_GetErrorCode'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `call_character_handler.cold.28':
(.text.unlikely+0x2b33): undefined reference to `XML_SetCharacterDataHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `call_character_handler.cold.28':
(.text.unlikely+0x2b95): undefined reference to `XML_SetCharacterDataHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `set_error.isra.8':
(.text.unlikely+0x2e1a): undefined reference to `XML_GetCurrentLineNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `set_error.isra.8':
(.text.unlikely+0x2e25): undefined reference to `XML_GetCurrentColumnNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `set_error.isra.8':
(.text.unlikely+0x2e30): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_xmlparser_UseForeignDTD':
(.text.unlikely+0x307f): undefined reference to `XML_UseForeignDTD'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_ErrorString':
(.text.unlikely+0x3b4d): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_xmlparser_GetBase':
(.text.unlikely+0x414e): undefined reference to `XML_GetBase'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_xmlparser_GetInputContext':
(.text.unlikely+0x449b): undefined reference to `XML_GetInputContext'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `flag_error':
(.text.unlikely+0x31e5): undefined reference to `XML_SetExternalEntityRefHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `my_ElementDeclHandler':
(.text.unlikely+0x3836): undefined reference to `XML_FreeContentModel'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(posixmodule.o): In function `os_openpty':
(.text.unlikely+0x1447): undefined reference to `openpty'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(posixmodule.o): In function `os_forkpty':
(.text.unlikely+0x1e6e): undefined reference to `forkpty'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(faulthandler.o): In function `faulthandler_thread':
(.text.unlikely+0x618): undefined reference to `pthread_sigmask'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info.part.4':
(.text+0x1662): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info_bases':
(.text+0x16e7): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info_table_bases':
(.text+0x17bb): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__deregister_frame_info_bases':
(.text+0x185e): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__deregister_frame_info_bases':
(.text+0x18e6): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':
(.text+0x19c6): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':
(.text+0x1a16): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':
(.text+0x1b00): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info.part.4':
(.text+0x1681): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info_bases':
(.text+0x1706): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info_table_bases':
(.text+0x17da): undefined reference to `pthread_mutex_unlock'
collect2: error: ld returned 1 exit status
</code></pre>

<p><strong>EDIT 2:</strong>
The exact gcc command line:</p>

<pre><code>gcc test.c -otest -static $(pkg-config --libs --cflags python3) -lc -lpthread -lexpat -lz -ldl -lutil -lm
</code></pre>

<p><strong>EDIT 3:</strong>
This is the result of <code>locate</code>:</p>

<pre><code>$ locate libexpat.a libc.a libm.a libz.a libpthread.a libdl.a libutil.a libpython3.5m.a | grep x86_64-
linux-gnu
/usr/lib/python3.5/config-3.5m-x86_64-linux-gnu/libpython3.5m.a
/usr/lib/x86_64-linux-gnu/libc.a
/usr/lib/x86_64-linux-gnu/libdl.a
/usr/lib/x86_64-linux-gnu/libexpat.a
/usr/lib/x86_64-linux-gnu/libm.a
/usr/lib/x86_64-linux-gnu/libpthread.a
/usr/lib/x86_64-linux-gnu/libpython3.5m.a
/usr/lib/x86_64-linux-gnu/libutil.a
/usr/lib/x86_64-linux-gnu/libz.a
</code></pre>

<p>So I should have all static libraries that I need.</p>

<p>This is the <em>new</em> error, a bit <em>shorter</em> than the first:</p>

<pre><code>$ gcc pregex.c -otest -static $(pkg-config --libs --cflags python3) -lc -lpthread -lexpat -lz -ldl -lutil -lm
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(posixmodule.o): In function `posix_getgrouplist':
(.text.unlikely+0x3aa6): warning: Using 'getgrouplist' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(posixmodule.o): In function `posix_initgroups':
(.text.unlikely+0x3cc1): warning: Using 'initgroups' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrall':
(.text+0x2f5): warning: Using 'getgrent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrgid':
(.text+0xe9): warning: Using 'getgrgid' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrnam':
(.text+0x266): warning: Using 'getgrnam' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrall':
(.text+0x2c9): warning: Using 'setgrent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrall.cold.7':
(.text.unlikely+0x15a): warning: Using 'endgrent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwall':
(.text+0x285): warning: Using 'getpwent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwnam':
(.text+0x1ee): warning: Using 'getpwnam' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwuid':
(.text+0x9f): warning: Using 'getpwuid' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwall':
(.text+0x259): warning: Using 'setpwent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwall.cold.6':
(.text.unlikely+0x14d): warning: Using 'endpwent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_getaddrinfo':
(.text+0x167b): warning: Using 'getaddrinfo' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(spwdmodule.o): In function `spwd_getspall':
(.text.unlikely+0x1a6): warning: Using 'getspent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(spwdmodule.o): In function `spwd_getspnam':
(.text.unlikely+0x27b): warning: Using 'getspnam' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(spwdmodule.o): In function `spwd_getspall':
(.text.unlikely+0x1a1): warning: Using 'setspent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(spwdmodule.o): In function `spwd_getspall':
(.text.unlikely+0x1f7): warning: Using 'endspent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_gethostbyaddr':
(.text+0x2d45): warning: Using 'gethostbyaddr_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_gethostbyname_ex':
(.text.unlikely+0x1ae4): warning: Using 'gethostbyname_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_getprotobyname':
(.text.unlikely+0x33ff): warning: Using 'getprotobyname' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_getservbyname':
(.text.unlikely+0x2263): warning: Using 'getservbyname' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_getservbyport':
(.text.unlikely+0x34c3): warning: Using 'getservbyport' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpthread.a(lowlevellock.o): In function `__lll_lock_wait_private':
/build/glibc-Cl5G7W/glibc-2.23/nptl/../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:78: multiple definition of `__lll_lock_wait_private'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libc.a(libc-lowlevellock.o):(.text+0x0): first defined here
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpthread.a(lowlevellock.o): In function `__lll_unlock_wake_private':
/build/glibc-Cl5G7W/glibc-2.23/nptl/../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:328: multiple definition of `__lll_unlock_wake_private'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libc.a(libc-lowlevellock.o):(.text+0x30): first defined here
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(dynload_shlib.o): In function `_PyImport_FindSharedFuncptr':
(.text+0x7c): warning: Using 'dlopen' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
collect2: error: ld returned 1 exit status
</code></pre>

<p><strong>EDIT 4:</strong></p>

<p>Ok, now I'm able to <strong>compile</strong> without errors with this command line:</p>

<pre><code>$ gcc -static test.c -otest $(pkg-config --libs --cflags python3) -lm -lutil -ldl -lz -lexpat -lpthread -lc
</code></pre>

<p><em>I don't know why</em> but with <code>-lc</code> at the end it compile!
The problem is that warnings are still in the output message:</p>

<p>Lots of warnings like this:</p>

<pre><code>/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(dynload_shlib.o): In function `_PyImport_FindSharedFuncptr':
(.text+0x7c): warning: Using 'dlopen' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
</code></pre>
",8860943,442,09-02-2018 10:10,10-02-2018 08:54,1,442,16,0,4,,"{'badge_counts': {'bronze': 16, 'silver': 4, 'gold': 0}, 'account_id': 12135311, 'is_employee': False, 'last_modified_date': 1646442300, 'last_access_date': 1689773635, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 442, 'creation_date': 1509436183, 'user_type': 'registered', 'user_id': 8860943, 'location': 'Italy', 'link': 'https://stackoverflow.com/users/8860943/ginoc', 'profile_image': 'https://www.gravatar.com/avatar/e7ae07d3e5bb65b266510e088b20ae3f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'GinoC'}","I'm trying to compile an executable (ELF file) that does not use a dynamic loader. I used to compile Python to C: Then to compile C generated file. Now I'd like to compile in a static executable. Usually I use flag, but here it return . How can I do? EDIT 1: Not the full messge becouse body is limited to 30000 chars a long list of warnings and errors like these: EDIT 2: The exact gcc command line: EDIT 3: This is the result of : So I should have all static libraries that I need. This is the new error, a bit shorter than the first: EDIT 4: Ok, now I'm able to compile without errors with this command line: I don't know why but with at the end it compile! The problem is that warnings are still in the output message: Lots of warnings like this:","Cython cython3 -3 test.py --embed
 gcc test.c -otest $(pkg-config --libs --cflags python3)
 test.c -static collect2: error: ld returned 1 exit status ...
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x430): undefined reference to `XML_SetStartCdataSectionHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x458): undefined reference to `XML_SetEndCdataSectionHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x480): undefined reference to `XML_SetDefaultHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x4a8): undefined reference to `XML_SetDefaultHandlerExpand'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x4d0): undefined reference to `XML_SetNotStandaloneHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x4f8): undefined reference to `XML_SetExternalEntityRefHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x520): undefined reference to `XML_SetStartDoctypeDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x548): undefined reference to `XML_SetEndDoctypeDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x570): undefined reference to `XML_SetEntityDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x598): undefined reference to `XML_SetXmlDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x5c0): undefined reference to `XML_SetElementDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x5e8): undefined reference to `XML_SetAttlistDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.data+0x610): undefined reference to `XML_SetSkippedEntityHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x6c): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x9c): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0xbc): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x11b): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x153): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o):(.text.unlikely+0x173): more undefined references to `XML_ErrorString' follow
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c62): undefined reference to `XML_GetErrorCode'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c6d): undefined reference to `XML_GetCurrentColumnNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c78): undefined reference to `XML_GetCurrentLineNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c83): undefined reference to `XML_Parse'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c8e): undefined reference to `XML_ParserCreate_MM'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1c99): undefined reference to `XML_ParserFree'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1ca4): undefined reference to `XML_SetCharacterDataHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1caf): undefined reference to `XML_SetCommentHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cba): undefined reference to `XML_SetDefaultHandlerExpand'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cc5): undefined reference to `XML_SetElementHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cd0): undefined reference to `XML_SetNamespaceDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cdb): undefined reference to `XML_SetProcessingInstructionHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1ce6): undefined reference to `XML_SetUnknownEncodingHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cf1): undefined reference to `XML_SetUserData'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1cfc): undefined reference to `XML_SetStartDoctypeDeclHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `PyInit_pyexpat.cold.17':
(.text.unlikely+0x1d07): undefined reference to `XML_SetEncoding'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x267b): undefined reference to `XML_GetCurrentLineNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x26d5): undefined reference to `XML_GetCurrentColumnNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x272a): undefined reference to `XML_GetCurrentByteIndex'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x27ee): undefined reference to `XML_GetCurrentColumnNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x281c): undefined reference to `XML_GetCurrentByteIndex'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x2853): undefined reference to `XML_GetCurrentLineNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `xmlparse_getattro.cold.20':
(.text.unlikely+0x286d): undefined reference to `XML_GetErrorCode'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_xmlparser_Parse.cold.21':
(.text.unlikely+0x28f0): undefined reference to `XML_Parse'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `call_with_frame.isra.7.cold.24':
(.text.unlikely+0x2a9a): undefined reference to `XML_StopParser'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `get_parse_result.cold.27':
(.text.unlikely+0x2b0a): undefined reference to `XML_GetErrorCode'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `call_character_handler.cold.28':
(.text.unlikely+0x2b33): undefined reference to `XML_SetCharacterDataHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `call_character_handler.cold.28':
(.text.unlikely+0x2b95): undefined reference to `XML_SetCharacterDataHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `set_error.isra.8':
(.text.unlikely+0x2e1a): undefined reference to `XML_GetCurrentLineNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `set_error.isra.8':
(.text.unlikely+0x2e25): undefined reference to `XML_GetCurrentColumnNumber'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `set_error.isra.8':
(.text.unlikely+0x2e30): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_xmlparser_UseForeignDTD':
(.text.unlikely+0x307f): undefined reference to `XML_UseForeignDTD'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_ErrorString':
(.text.unlikely+0x3b4d): undefined reference to `XML_ErrorString'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_xmlparser_GetBase':
(.text.unlikely+0x414e): undefined reference to `XML_GetBase'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `pyexpat_xmlparser_GetInputContext':
(.text.unlikely+0x449b): undefined reference to `XML_GetInputContext'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `flag_error':
(.text.unlikely+0x31e5): undefined reference to `XML_SetExternalEntityRefHandler'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pyexpat.o): In function `my_ElementDeclHandler':
(.text.unlikely+0x3836): undefined reference to `XML_FreeContentModel'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(posixmodule.o): In function `os_openpty':
(.text.unlikely+0x1447): undefined reference to `openpty'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(posixmodule.o): In function `os_forkpty':
(.text.unlikely+0x1e6e): undefined reference to `forkpty'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(faulthandler.o): In function `faulthandler_thread':
(.text.unlikely+0x618): undefined reference to `pthread_sigmask'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info.part.4':
(.text+0x1662): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info_bases':
(.text+0x16e7): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info_table_bases':
(.text+0x17bb): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__deregister_frame_info_bases':
(.text+0x185e): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__deregister_frame_info_bases':
(.text+0x18e6): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':
(.text+0x19c6): undefined reference to `pthread_mutex_lock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':
(.text+0x1a16): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':
(.text+0x1b00): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info.part.4':
(.text+0x1681): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info_bases':
(.text+0x1706): undefined reference to `pthread_mutex_unlock'
/usr/lib/gcc/x86_64-linux-gnu/5/libgcc_eh.a(unwind-dw2-fde-dip.o): In function `__register_frame_info_table_bases':
(.text+0x17da): undefined reference to `pthread_mutex_unlock'
collect2: error: ld returned 1 exit status
 gcc test.c -otest -static $(pkg-config --libs --cflags python3) -lc -lpthread -lexpat -lz -ldl -lutil -lm
 locate $ locate libexpat.a libc.a libm.a libz.a libpthread.a libdl.a libutil.a libpython3.5m.a | grep x86_64-
linux-gnu
/usr/lib/python3.5/config-3.5m-x86_64-linux-gnu/libpython3.5m.a
/usr/lib/x86_64-linux-gnu/libc.a
/usr/lib/x86_64-linux-gnu/libdl.a
/usr/lib/x86_64-linux-gnu/libexpat.a
/usr/lib/x86_64-linux-gnu/libm.a
/usr/lib/x86_64-linux-gnu/libpthread.a
/usr/lib/x86_64-linux-gnu/libpython3.5m.a
/usr/lib/x86_64-linux-gnu/libutil.a
/usr/lib/x86_64-linux-gnu/libz.a
 $ gcc pregex.c -otest -static $(pkg-config --libs --cflags python3) -lc -lpthread -lexpat -lz -ldl -lutil -lm
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(posixmodule.o): In function `posix_getgrouplist':
(.text.unlikely+0x3aa6): warning: Using 'getgrouplist' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(posixmodule.o): In function `posix_initgroups':
(.text.unlikely+0x3cc1): warning: Using 'initgroups' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrall':
(.text+0x2f5): warning: Using 'getgrent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrgid':
(.text+0xe9): warning: Using 'getgrgid' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrnam':
(.text+0x266): warning: Using 'getgrnam' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrall':
(.text+0x2c9): warning: Using 'setgrent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(grpmodule.o): In function `grp_getgrall.cold.7':
(.text.unlikely+0x15a): warning: Using 'endgrent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwall':
(.text+0x285): warning: Using 'getpwent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwnam':
(.text+0x1ee): warning: Using 'getpwnam' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwuid':
(.text+0x9f): warning: Using 'getpwuid' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwall':
(.text+0x259): warning: Using 'setpwent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(pwdmodule.o): In function `pwd_getpwall.cold.6':
(.text.unlikely+0x14d): warning: Using 'endpwent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_getaddrinfo':
(.text+0x167b): warning: Using 'getaddrinfo' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(spwdmodule.o): In function `spwd_getspall':
(.text.unlikely+0x1a6): warning: Using 'getspent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(spwdmodule.o): In function `spwd_getspnam':
(.text.unlikely+0x27b): warning: Using 'getspnam' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(spwdmodule.o): In function `spwd_getspall':
(.text.unlikely+0x1a1): warning: Using 'setspent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(spwdmodule.o): In function `spwd_getspall':
(.text.unlikely+0x1f7): warning: Using 'endspent' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_gethostbyaddr':
(.text+0x2d45): warning: Using 'gethostbyaddr_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_gethostbyname_ex':
(.text.unlikely+0x1ae4): warning: Using 'gethostbyname_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_getprotobyname':
(.text.unlikely+0x33ff): warning: Using 'getprotobyname' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_getservbyname':
(.text.unlikely+0x2263): warning: Using 'getservbyname' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(socketmodule.o): In function `socket_getservbyport':
(.text.unlikely+0x34c3): warning: Using 'getservbyport' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpthread.a(lowlevellock.o): In function `__lll_lock_wait_private':
/build/glibc-Cl5G7W/glibc-2.23/nptl/../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:78: multiple definition of `__lll_lock_wait_private'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libc.a(libc-lowlevellock.o):(.text+0x0): first defined here
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpthread.a(lowlevellock.o): In function `__lll_unlock_wake_private':
/build/glibc-Cl5G7W/glibc-2.23/nptl/../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:328: multiple definition of `__lll_unlock_wake_private'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libc.a(libc-lowlevellock.o):(.text+0x30): first defined here
/usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(dynload_shlib.o): In function `_PyImport_FindSharedFuncptr':
(.text+0x7c): warning: Using 'dlopen' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
collect2: error: ld returned 1 exit status
 $ gcc -static test.c -otest $(pkg-config --libs --cflags python3) -lm -lutil -ldl -lz -lexpat -lpthread -lc
 -lc /usr/lib/gcc/x86_64-linux-gnu/5/../../../x86_64-linux-gnu/libpython3.5m.a(dynload_shlib.o): In function `_PyImport_FindSharedFuncptr':
(.text+0x7c): warning: Using 'dlopen' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
",185,249,0,0,
292,48460057,48460190,10597,What does it mean that a tf.variable is trainable in TensorFlow,1,<python><tensorflow>,12,"<p>This question came to me when I read the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/train/global_step"" rel=""noreferrer"">global_step</a>.
Here it explicitly declares global_step is not trainable.</p>
<blockquote>
<p>global_step_tensor = tf.Variable(10, trainable=False, name='global_step')</p>
<p>sess = tf.Session()</p>
<p>print('global_step: %s' % tf.train.global_step(sess, global_step_tensor))</p>
</blockquote>
<p>From my understanding, trainable means that the value could be changed during sess.run(). I have tried to declare it both trainable and non-trainable and got the same results. So I didn't understand why we need to declare it not trainable.</p>
<p>I read the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/trainable_variables"" rel=""noreferrer"">trainable</a> but didn't quite get it.</p>
<p>So my question is:</p>
<ol>
<li>Can non-trainable variable value be changed during sess.run() and vice versa?</li>
<li>What is the point that make a variable not trainable?</li>
</ol>
",8871872,489,26-01-2018 11:00,26-01-2018 11:10,0,489,19,3,6,25,"{'badge_counts': {'bronze': 19, 'silver': 6, 'gold': 3}, 'account_id': 12029463, 'is_employee': False, 'last_modified_date': 1663749000, 'last_access_date': 1711038197, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 489, 'creation_date': 1509592498, 'user_type': 'registered', 'user_id': 8871872, 'accept_rate': 25, 'website_url': '', 'link': 'https://stackoverflow.com/users/8871872/jekyll-song', 'profile_image': 'https://lh5.googleusercontent.com/-o1o8H_ti9d4/AAAAAAAAAAI/AAAAAAAAAeo/slWNd1LeK8U/photo.jpg?sz=256', 'display_name': 'Jekyll SONG'}","This question came to me when I read the documentation of global_step. Here it explicitly declares global_step is not trainable. global_step_tensor = tf.Variable(10, trainable=False, name='global_step') sess = tf.Session() print('global_step: %s' % tf.train.global_step(sess, global_step_tensor)) From my understanding, trainable means that the value could be changed during sess.run(). I have tried to declare it both trainable and non-trainable and got the same results. So I didn't understand why we need to declare it not trainable. I read the documentation of trainable but didn't quite get it. So my question is: Can non-trainable variable value be changed during sess.run() and vice versa? What is the point that make a variable not trainable?",,0,14,0,2,
293,49579282,49579373,85160,Can't find module cPickle using Python 3.5 and Anaconda,2,<python><anaconda><pickle><versioning>,52,"<p>I am trying to use cPickle on a windows box, using Anaconda. I am using python 3.5. I am not using a virtualenv (though probably should be).</p>

<p>When I try to import cPickle I get <em>""ImportError: No module named 'cPickle'""</em></p>

<pre><code>Python 3.5.0 |Anaconda custom (64-bit)| (default, Dec  1 2015, 11:46:22) [MSC v.
1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import cPickle
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'cPickle'
</code></pre>

<p>My understanding is that cPickle comes built in with Python 3.5, so I can't understand why cPickle is not found. Any idea what has gone wrong/how I can clean things up/how to troubleshoot the issue.</p>
",8283463,857,30-03-2018 18:09,30-03-2018 18:16,0,857,12,1,8,,"{'badge_counts': {'bronze': 12, 'silver': 8, 'gold': 1}, 'account_id': 9448228, 'is_employee': False, 'last_modified_date': 1650039000, 'last_access_date': 1671598531, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 857, 'creation_date': 1499686848, 'user_type': 'registered', 'user_id': 8283463, 'link': 'https://stackoverflow.com/users/8283463/tom-walker', 'profile_image': 'https://www.gravatar.com/avatar/c7c85e7579fd9051ecc21ddb6a8a81f6?s=256&d=identicon&r=PG', 'display_name': 'Tom Walker'}","I am trying to use cPickle on a windows box, using Anaconda. I am using python 3.5. I am not using a virtualenv (though probably should be). When I try to import cPickle I get ""ImportError: No module named 'cPickle'"" My understanding is that cPickle comes built in with Python 3.5, so I can't understand why cPickle is not found. Any idea what has gone wrong/how I can clean things up/how to troubleshoot the issue.","Python 3.5.0 |Anaconda custom (64-bit)| (default, Dec  1 2015, 11:46:22) [MSC v.
1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import cPickle
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'cPickle'
",6,14,0,0,
294,48550653,48567255,39081,JOIN same table twice with aliases on SQLAlchemy,2,<python><sqlalchemy>,38,"<p>I am trying to port the following query to SQLAlchemy:</p>

<pre><code>SELECT u.username, GROUP_CONCAT(DISTINCT userS.name)
FROM Skills AS filterS 
INNER JOIN UserSkills AS ufs ON filterS.id = ufs.skill_id
INNER JOIN Users AS u ON ufs.user_id = u.id
INNER JOIN UserSkills AS us ON u.id = us.user_id
INNER JOIN Skills AS userS ON us.skill_id = userS.id
WHERE filterS.name IN ('C#', 'SQL')
GROUP BY u.id;
</code></pre>

<p>I don't understand how to achieve AS statement in SQLAlchemy. Here is what I currently have:</p>

<pre><code># User class has attribute skills, that points to class UserSkill
# UserSkill class has attribute skill, that points to class Skill
db.session.query(User.id, User.username, func.group_concat(Skill.name).label('skills')).\
   join(User.skills).\
   join(UserSkill.skill).filter(Skill.id.in_(skillIds)).\
   order_by(desc(func.count(Skill.id))).\
   group_by(User.id).all()
</code></pre>

<p>Please help.</p>
",8385666,2525,31-01-2018 20:10,01-02-2018 16:18,1,2525,36,2,23,,"{'badge_counts': {'bronze': 36, 'silver': 23, 'gold': 2}, 'account_id': 1353687, 'is_employee': False, 'last_modified_date': 1690537518, 'last_access_date': 1711120640, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2525, 'creation_date': 1363333191, 'user_type': 'registered', 'user_id': 8385666, 'location': 'Dublin, Ireland', 'website_url': 'http://andnik.com', 'link': 'https://stackoverflow.com/users/8385666/andnik', 'profile_image': 'https://i.stack.imgur.com/uFiRs.jpg?s=256&g=1', 'display_name': 'andnik'}",I am trying to port the following query to SQLAlchemy: I don't understand how to achieve AS statement in SQLAlchemy. Here is what I currently have: Please help.,"SELECT u.username, GROUP_CONCAT(DISTINCT userS.name)
FROM Skills AS filterS 
INNER JOIN UserSkills AS ufs ON filterS.id = ufs.skill_id
INNER JOIN Users AS u ON ufs.user_id = u.id
INNER JOIN UserSkills AS us ON u.id = us.user_id
INNER JOIN Skills AS userS ON us.skill_id = userS.id
WHERE filterS.name IN ('C#', 'SQL')
GROUP BY u.id;
 # User class has attribute skills, that points to class UserSkill
# UserSkill class has attribute skill, that points to class Skill
db.session.query(User.id, User.username, func.group_concat(Skill.name).label('skills')).\
   join(User.skills).\
   join(UserSkill.skill).filter(Skill.id.in_(skillIds)).\
   order_by(desc(func.count(Skill.id))).\
   group_by(User.id).all()
",13,24,0,0,
295,49369163,49369203,12616,Custom exceptions in unittests,1,<python><python-3.x><unit-testing><python-unittest><assertraises>,12,"<p>I have created my custom exceptions as such within <code>errors.py</code></p>

<pre><code>mapper = {
    'E101':
    'There is no data at all for these constraints',
    'E102':
    'There is no data for these constraints in this market, try changing market',
    'E103':
    'There is no data for these constraints during these dates, try changing dates',
}


class DataException(Exception):
    def __init__(self, code):
        super().__init__()
        self.msg = mapper[code]

    def __str__(self):
        return self.msg
</code></pre>

<p>Another function somewhere else in the code raises different instances of <code>DataException</code> if there is not enough data in a <code>pandas</code> dataframe. I want to use <code>unittest</code> to ensure that it returns the appropriate exception with its corresponding message.</p>

<p>Using a simple example, why does this not work:</p>

<pre><code>from .. import DataException
def foobar():
    raise DataException('E101')

import unittest
with unittest.TestCase.assertRaises(DataException):
    foobar()
</code></pre>

<p>As suggested here: <a href=""https://stackoverflow.com/questions/35490843/python-assertraises-on-user-defined-exceptions"">Python assertRaises on user-defined exceptions</a></p>

<p>I get this error:</p>

<pre><code>TypeError: assertRaises() missing 1 required positional argument: 'expected_exception'
</code></pre>

<p>Or alternatively:</p>

<pre><code>def foobar():
    raise DataException('E101')

import unittest
unittest.TestCase.assertRaises(DataException, foobar)
</code></pre>

<p>results in:</p>

<pre><code>TypeError: assertRaises() arg 1 must be an exception type or tuple of exception types
</code></pre>

<p>Why is it not recognizing <code>DataException</code> as an <code>Exception</code>? Why does the linked stackoverflow question answer work without supplying a second argument to <code>assertRaises</code>?</p>
",8397886,2417,19-03-2018 17:38,19-03-2018 17:40,0,2427,59,4,30,86,"{'badge_counts': {'bronze': 59, 'silver': 30, 'gold': 4}, 'account_id': 11457265, 'is_employee': False, 'last_modified_date': 1673299200, 'last_access_date': 1695143667, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2427, 'creation_date': 1501569221, 'user_type': 'registered', 'user_id': 8397886, 'accept_rate': 86, 'location': 'Indonesia', 'link': 'https://stackoverflow.com/users/8397886/ludo', 'profile_image': 'https://www.gravatar.com/avatar/1d3e2ddd936219816d334db27f00fea9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ludo'}","I have created my custom exceptions as such within Another function somewhere else in the code raises different instances of if there is not enough data in a dataframe. I want to use to ensure that it returns the appropriate exception with its corresponding message. Using a simple example, why does this not work: As suggested here: Python assertRaises on user-defined exceptions I get this error: Or alternatively: results in: Why is it not recognizing as an ? Why does the linked stackoverflow question answer work without supplying a second argument to ?","errors.py mapper = {
    'E101':
    'There is no data at all for these constraints',
    'E102':
    'There is no data for these constraints in this market, try changing market',
    'E103':
    'There is no data for these constraints during these dates, try changing dates',
}


class DataException(Exception):
    def __init__(self, code):
        super().__init__()
        self.msg = mapper[code]

    def __str__(self):
        return self.msg
 DataException pandas unittest from .. import DataException
def foobar():
    raise DataException('E101')

import unittest
with unittest.TestCase.assertRaises(DataException):
    foobar()
 TypeError: assertRaises() missing 1 required positional argument: 'expected_exception'
 def foobar():
    raise DataException('E101')

import unittest
unittest.TestCase.assertRaises(DataException, foobar)
 TypeError: assertRaises() arg 1 must be an exception type or tuple of exception types
 DataException Exception assertRaises",19,56,0,1,
296,48367128,48367185,24192,String to Bytes Python without change in encoding,2,<python><string><python-3.x><byte>,13,"<p>I have this issue and I can't figure out how to solve it. I have this string:</p>

<pre><code>data = '\xc4\xb7\x86\x17\xcd'
</code></pre>

<p>When I tried to encode it:</p>

<pre><code>data.encode()
</code></pre>

<p>I get this result:</p>

<pre><code>b'\xc3\x84\xc2\xb7\xc2\x86\x17\xc3\x8d'
</code></pre>

<p>I only want:</p>

<pre><code>b'\xc4\xb7\x86\x17\xcd'
</code></pre>

<p>Anyone knows the reason and how to fix this. The string is already stored in a variable, so I can't add the literal b in front of it.</p>
",8413985,337,21-01-2018 13:11,21-01-2018 13:16,0,337,12,1,5,,"{'badge_counts': {'bronze': 12, 'silver': 5, 'gold': 1}, 'account_id': 11479412, 'is_employee': False, 'last_modified_date': 1662997068, 'last_access_date': 1665580009, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 337, 'creation_date': 1501792377, 'user_type': 'registered', 'user_id': 8413985, 'website_url': '', 'link': 'https://stackoverflow.com/users/8413985/avan989', 'profile_image': 'https://www.gravatar.com/avatar/2b3cb8f57e2bfc7b546c421550a1ccba?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'avan989'}","I have this issue and I can't figure out how to solve it. I have this string: When I tried to encode it: I get this result: I only want: Anyone knows the reason and how to fix this. The string is already stored in a variable, so I can't add the literal b in front of it.","data = '\xc4\xb7\x86\x17\xcd'
 data.encode()
 b'\xc3\x84\xc2\xb7\xc2\x86\x17\xc3\x8d'
 b'\xc4\xb7\x86\x17\xcd'
",0,21,0,0,
297,48048279,48054575,41052,TypeError: Cannot read property '_uploadFiles' of undefined in google colaboratory,20,<python><google-colaboratory>,21,"<p>I am trying to write upload the file in Google Colaboratory and I'm going to write the code as below. </p>

<pre><code>from google.colab import files
uploaded = files.upload()
</code></pre>

<p>But I am getting the below error to run the code in browser.</p>

<blockquote>
  <p>MessageError: TypeError: Cannot read property '_uploadFiles' of undefined</p>
</blockquote>

<p>Please help me solve the issue.</p>
",9015771,233,01-01-2018 08:42,02-01-2018 01:20,1,233,5,1,2,,"{'badge_counts': {'bronze': 5, 'silver': 2, 'gold': 1}, 'account_id': 12361894, 'is_employee': False, 'last_modified_date': 1598282618, 'last_access_date': 1696623498, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 233, 'creation_date': 1511799698, 'user_type': 'registered', 'user_id': 9015771, 'location': 'Bangalore, Karnataka, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/9015771/mallikarjun-namagiri', 'profile_image': 'https://www.gravatar.com/avatar/b7bc7f6b17fab91a5d4c13218abae0bb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'MalliKarjun Namagiri'}",I am trying to write upload the file in Google Colaboratory and I'm going to write the code as below. But I am getting the below error to run the code in browser. MessageError: TypeError: Cannot read property '_uploadFiles' of undefined Please help me solve the issue.,"from google.colab import files
uploaded = files.upload()
",1,13,0,0,
298,50151189,50152869,16002,"Pytest ""run-around-tests"" fixture to run only once before all tests in a class",2,<python><selenium><automated-tests><pytest>,13,"<p>I am testing the User message function of a web solution using pytest + selenium. The tests will generate a test message to a test user, and then log in that user to verify that the message indeed is displaying for that user. </p>

<ul>
<li>I need to generate those messages through an internal API. </li>
<li>In order to be able to access this API, I first have to generate an AUTH token through a different API.</li>
</ul>

<p>So the test scenario is basically:</p>

<ol>
<li>At test startup, generate a new AUTH token through an API helper function.</li>
<li>Send a request to another API to set a new message (requires the AUTH token)</li>
<li>Send a request to yet another API to ""map"" this message to a specified user (requires the AUTH token)</li>
<li>Log in test user and verify that the new message is indeed displaying.</li>
</ol>

<p>My problem is that I want to avoid creating a new AUTH token every time every test within my test class is run - I want to create a new token once that all tests use within the same test run. </p>

<p>What is the smartest solution to generate one new access token when invoking all tests? </p>

<p>Right now I have come up with something like this, which will generate a new token every time any individual test is run:</p>

<pre><code>import pytest
import helpers.api_access_token_helper as token_helper
import helpers.user_message_generator_api_helper as message_helper
import helpers.login_helper as login_helper
import helpers.popup_helper as popup_helper

class TestStuff(object):

    @pytest.yield_fixture(autouse=True)
    def run_around_tests(self):
        yield token_helper.get_api_access_token()

    def test_one(self, run_around_tests):
        auth_token = run_around_tests
        message_helper.create_new_message(auth_token, some_message_data)
        message_helper.map_message_to_user(auth_token, user_one[""user_id""])
        login_helper.log_in_user(user_one[""user_name""], user_one[""user_password""])
        assert popup_helper.user_message_is_displaying(some_message_data[""title""])

    def test_two(self, run_around_tests):
        auth_token = run_around_tests
        message_helper.create_new_message(auth_token, some_other_message_data)
        message_helper.map_message_to_user(auth_token, user_two[""user_id""])
        login_helper.log_in_user(user_two[""user_name""], user_two[""user_password""])
        assert popup_helper.user_message_is_displaying(some_other_message_data[""title""])
</code></pre>

<p>I have laborated back and forth a bit with the ""run-around-tests"" fixture but havent been able to find a solution.</p>
",8164849,133,03-05-2018 09:12,03-05-2018 10:33,0,133,5,1,1,,"{'badge_counts': {'bronze': 5, 'silver': 1, 'gold': 1}, 'account_id': 11122349, 'is_employee': False, 'last_modified_date': 1573678617, 'last_access_date': 1529581619, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 133, 'creation_date': 1497513687, 'user_type': 'registered', 'user_id': 8164849, 'link': 'https://stackoverflow.com/users/8164849/nordin', 'profile_image': 'https://www.gravatar.com/avatar/9740f838eff57e386fb52707d4ede96a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nordin'}","I am testing the User message function of a web solution using pytest + selenium. The tests will generate a test message to a test user, and then log in that user to verify that the message indeed is displaying for that user. I need to generate those messages through an internal API. In order to be able to access this API, I first have to generate an AUTH token through a different API. So the test scenario is basically: At test startup, generate a new AUTH token through an API helper function. Send a request to another API to set a new message (requires the AUTH token) Send a request to yet another API to ""map"" this message to a specified user (requires the AUTH token) Log in test user and verify that the new message is indeed displaying. My problem is that I want to avoid creating a new AUTH token every time every test within my test class is run - I want to create a new token once that all tests use within the same test run. What is the smartest solution to generate one new access token when invoking all tests? Right now I have come up with something like this, which will generate a new token every time any individual test is run: I have laborated back and forth a bit with the ""run-around-tests"" fixture but havent been able to find a solution.","import pytest
import helpers.api_access_token_helper as token_helper
import helpers.user_message_generator_api_helper as message_helper
import helpers.login_helper as login_helper
import helpers.popup_helper as popup_helper

class TestStuff(object):

    @pytest.yield_fixture(autouse=True)
    def run_around_tests(self):
        yield token_helper.get_api_access_token()

    def test_one(self, run_around_tests):
        auth_token = run_around_tests
        message_helper.create_new_message(auth_token, some_message_data)
        message_helper.map_message_to_user(auth_token, user_one[""user_id""])
        login_helper.log_in_user(user_one[""user_name""], user_one[""user_password""])
        assert popup_helper.user_message_is_displaying(some_message_data[""title""])

    def test_two(self, run_around_tests):
        auth_token = run_around_tests
        message_helper.create_new_message(auth_token, some_other_message_data)
        message_helper.map_message_to_user(auth_token, user_two[""user_id""])
        login_helper.log_in_user(user_two[""user_name""], user_two[""user_password""])
        assert popup_helper.user_message_is_displaying(some_other_message_data[""title""])
",24,50,0,0,
299,48279061,48279267,113218,GCS - Read a text file from Google Cloud Storage directly into python,5,<python><google-cloud-platform><google-cloud-storage>,41,"<p>I feel kind of stupid right now. I have been reading numerous documentations and stackoverflow questions but I can't get it right.</p>

<p>I have a file on Google Cloud Storage. It is in a bucket 'test_bucket'. Inside this bucket there is a folder, 'temp_files_folder', which contains two files, one .txt file named 'test.txt' and one .csv file named 'test.csv'. The two files are simply because I try using both but the result is the same either way.</p>

<p>The content in the files is</p>

<pre><code>hej
san
</code></pre>

<p>and I am hoping to read it into python the same way I would do on a local with </p>

<pre><code>textfile = open(""/file_path/test.txt"", 'r')
times = textfile.read().splitlines()
textfile.close()
print(times)
</code></pre>

<p>which gives</p>

<pre><code>['hej', 'san']
</code></pre>

<p>I have tried using </p>

<pre><code>from google.cloud import storage

client = storage.Client()

bucket = client.get_bucket('test_bucket')

blob = bucket.get_blob('temp_files_folder/test.txt')

print(blob.download_as_string)
</code></pre>

<p>but it gives the output</p>

<pre><code>&lt;bound method Blob.download_as_string of &lt;Blob: test_bucket, temp_files_folder/test.txt&gt;&gt;
</code></pre>

<p>How can I get the actual string(s) in the file?</p>
",8170216,700,16-01-2018 10:25,16-01-2018 10:37,0,700,16,1,9,75,"{'badge_counts': {'bronze': 16, 'silver': 9, 'gold': 1}, 'account_id': 11129961, 'is_employee': False, 'last_modified_date': 1573678615, 'last_access_date': 1678461226, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 700, 'creation_date': 1497596780, 'user_type': 'registered', 'user_id': 8170216, 'accept_rate': 75, 'website_url': '', 'link': 'https://stackoverflow.com/users/8170216/digestivee', 'profile_image': 'https://www.gravatar.com/avatar/5644d029d783b174dbdb91f542ed57d6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'digestivee'}","I feel kind of stupid right now. I have been reading numerous documentations and stackoverflow questions but I can't get it right. I have a file on Google Cloud Storage. It is in a bucket 'test_bucket'. Inside this bucket there is a folder, 'temp_files_folder', which contains two files, one .txt file named 'test.txt' and one .csv file named 'test.csv'. The two files are simply because I try using both but the result is the same either way. The content in the files is and I am hoping to read it into python the same way I would do on a local with which gives I have tried using but it gives the output How can I get the actual string(s) in the file?","hej
san
 textfile = open(""/file_path/test.txt"", 'r')
times = textfile.read().splitlines()
textfile.close()
print(times)
 ['hej', 'san']
 from google.cloud import storage

client = storage.Client()

bucket = client.get_bucket('test_bucket')

blob = bucket.get_blob('temp_files_folder/test.txt')

print(blob.download_as_string)
 &lt;bound method Blob.download_as_string of &lt;Blob: test_bucket, temp_files_folder/test.txt&gt;&gt;
",12,42,0,0,
300,50047237,50051542,9452,how to preserve dtypes of dataframes when using to_csv?,2,<python><pandas>,12,"<p>To reduce memory costs, I specified dtypes of my pandas dataframe using <code>astype()</code>,like:</p>

<pre><code>df['A'] = df['A'].astype(int8)
</code></pre>

<p>then I use <code>to_csv()</code> to store it, but when I use <code>read_csv()</code> to read it again and check the <code>dtypes</code>, I found it still stored in <code>int64</code>.
How can I preserve the dtypes while saving it in local storages?</p>
",8556666,123,26-04-2018 15:50,26-04-2018 20:43,0,123,5,0,1,,"{'badge_counts': {'bronze': 5, 'silver': 1, 'gold': 0}, 'account_id': 11687344, 'is_employee': False, 'last_modified_date': 1652469900, 'last_access_date': 1543596351, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 123, 'creation_date': 1504495150, 'user_type': 'registered', 'user_id': 8556666, 'link': 'https://stackoverflow.com/users/8556666/feephy', 'profile_image': 'https://www.gravatar.com/avatar/daca421fd42954f54905ab173057eeb4?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Feephy'}","To reduce memory costs, I specified dtypes of my pandas dataframe using ,like: then I use to store it, but when I use to read it again and check the , I found it still stored in . How can I preserve the dtypes while saving it in local storages?","astype() df['A'] = df['A'].astype(int8)
 to_csv() read_csv() dtypes int64",-5,7,0,0,
301,49161120,49161189,452987,Set value of one Pandas column based on value in another column,10,<python><pandas><conditional-statements>,142,"<p>I need to set the value of one column based on the value of another in a Pandas dataframe. This is the logic:</p>
<pre><code>if df['c1'] == 'Value':
    df['c2'] = 10
else:
    df['c2'] = df['c3']
</code></pre>
<p>I am unable to get this to do what I want, which is to simply create a column with new values (or change the value of an existing column: either one works for me).</p>
<p>If I try to run the code above or if I write it as a function and use the apply method, I get the following:</p>
<p><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</code></p>
",8610662,1754,07-03-2018 21:01,07-03-2018 21:06,0,1764,22,2,12,,"{'badge_counts': {'bronze': 22, 'silver': 12, 'gold': 2}, 'account_id': 11766111, 'is_employee': False, 'last_modified_date': 1668649800, 'last_access_date': 1688963907, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1764, 'creation_date': 1505410678, 'user_type': 'registered', 'user_id': 8610662, 'website_url': '', 'link': 'https://stackoverflow.com/users/8610662/nlr', 'profile_image': 'https://i.stack.imgur.com/NW6qK.jpg?s=256&g=1', 'display_name': 'NLR'}","I need to set the value of one column based on the value of another in a Pandas dataframe. This is the logic: I am unable to get this to do what I want, which is to simply create a column with new values (or change the value of an existing column: either one works for me). If I try to run the code above or if I write it as a function and use the apply method, I get the following:","if df['c1'] == 'Value':
    df['c2'] = 10
else:
    df['c2'] = df['c3']
 ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",2,9,0,0,
302,48957094,48957122,29796,"How can i use ""leaky_relu"" as an activation in Tensorflow ""tf.layers.dense""?",5,<python><tensorflow>,12,"<p>Using Tensorflow 1.5, I am trying to add <code>leaky_relu</code> activation to the output of a dense layer while I am able to change the <code>alpha</code> of <code>leaky_relu</code> (check <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu"" rel=""noreferrer"">here</a>). I know I can do it as follows:</p>

<pre><code>output = tf.layers.dense(input, n_units)
output = tf.nn.leaky_relu(output, alpha=0.01)
</code></pre>

<p>I was wondering if there is a way to write this in one line as we can do for <code>relu</code>:</p>

<p><code>ouput = tf.layers.dense(input, n_units, activation=tf.nn.relu)</code></p>

<p>I tried the following but I get an error:</p>

<pre><code>output = tf.layers.dense(input, n_units, activation=tf.nn.leaky_relu(alpha=0.01))
TypeError: leaky_relu() missing 1 required positional argument: 'features'
</code></pre>

<p>Is there a way to do this?</p>
",8650470,177,23-02-2018 22:26,23-02-2018 22:30,0,177,11,1,2,,"{'badge_counts': {'bronze': 11, 'silver': 2, 'gold': 1}, 'account_id': 11821496, 'is_employee': False, 'last_modified_date': 1607614421, 'last_access_date': 1676890753, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 177, 'creation_date': 1506012635, 'user_type': 'registered', 'user_id': 8650470, 'link': 'https://stackoverflow.com/users/8650470/amin', 'profile_image': 'https://www.gravatar.com/avatar/2a04fe82881970d459f95e2b11b55432?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Amin'}","Using Tensorflow 1.5, I am trying to add activation to the output of a dense layer while I am able to change the of (check here). I know I can do it as follows: I was wondering if there is a way to write this in one line as we can do for : I tried the following but I get an error: Is there a way to do this?","leaky_relu alpha leaky_relu output = tf.layers.dense(input, n_units)
output = tf.nn.leaky_relu(output, alpha=0.01)
 relu ouput = tf.layers.dense(input, n_units, activation=tf.nn.relu) output = tf.layers.dense(input, n_units, activation=tf.nn.leaky_relu(alpha=0.01))
TypeError: leaky_relu() missing 1 required positional argument: 'features'
",-3,17,0,1,
303,48880934,49039320,6355,Performance decrease for huge amount of columns. Pyspark,2,<python><pandas><apache-spark><machine-learning><pyspark>,11,"<p>I met problem with processing of spark wide dataframe (about 9000 columns and sometimes more).
<br>Task:</p>

<ol>
<li>Create wide DF via groupBy and pivot. </li>
<li>Transform columns to vector and processing in to KMeans from pyspark.ml.</li>
</ol>

<p>So I made extensive frame and try to create vector with VectorAssembler, cached it and trained on it KMeans. 
<br>It took about 11 minutes for assembling and 2 minutes for KMeans for 7 different count of clusters on my pc in standalone mode for frame ~500x9000. Another side this processing in pandas (pivot df, and iterate 7 clusters) takes less one minute. 
<br><em>Obviously</em> I understand overhead and performance decreasing for standalone mode and caching and so on but it's really discourages me. 
<br>Could somebody explain how I can avoid this overhead? 
<br>How peoples work with wide DF instead of using vectorassembler and getting performance decreasing? 
<br>More formal question (for sof rules) sound like - <strong>How can I speed up this code?</strong></p>

<pre><code>%%time
tmp = (df_states.select('ObjectPath', 'User', 'PropertyFlagValue')
       .groupBy('User')
       .pivot('ObjectPath')
       .agg({'PropertyFlagValue':'max'})
       .fillna(0))
ignore = ['User']
assembler = VectorAssembler(
    inputCols=[x for x in tmp.columns if x not in ignore],
    outputCol='features')
Wall time: 36.7 s

print(tmp.count(), len(tmp.columns))
552, 9378

%%time
transformed = assembler.transform(tmp).select('User', 'features').cache()
Wall time: 10min 45s

%%time
lst_levels = []
for num in range(3, 14):
    kmeans = KMeans(k=num, maxIter=50)
    model = kmeans.fit(transformed)
    lst_levels.append(model.computeCost(transformed))
rs = [i-j for i,j in list(zip(lst_levels, lst_levels[1:]))]
for i, j in zip(rs, rs[1:]):
    if i - j &lt; j:
        print(rs.index(i))
        kmeans = KMeans(k=rs.index(i) + 3, maxIter=50)
        model = kmeans.fit(transformed)
        break
 Wall time: 1min 32s
</code></pre>

<p>Config:</p>

<pre><code>.config(""spark.sql.pivotMaxValues"", ""100000"") \
.config(""spark.sql.autoBroadcastJoinThreshold"", ""-1"") \
.config(""spark.sql.shuffle.partitions"", ""4"") \
.config(""spark.sql.inMemoryColumnarStorage.batchSize"", ""1000"") \
</code></pre>
",8730789,562,20-02-2018 08:39,28-02-2018 22:00,8,552,19,0,9,,"{'badge_counts': {'bronze': 19, 'silver': 9, 'gold': 0}, 'account_id': 11930889, 'is_employee': False, 'last_modified_date': 1711024200, 'last_access_date': 1711107586, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': -10, 'reputation_change_week': -10, 'reputation_change_day': 0, 'reputation': 552, 'creation_date': 1507274183, 'user_type': 'registered', 'user_id': 8730789, 'website_url': '', 'link': 'https://stackoverflow.com/users/8730789/anton-alekseev', 'profile_image': 'https://www.gravatar.com/avatar/8fb8489806825fcf92d28be03d9cb845?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Anton Alekseev'}","I met problem with processing of spark wide dataframe (about 9000 columns and sometimes more). Task: Create wide DF via groupBy and pivot. Transform columns to vector and processing in to KMeans from pyspark.ml. So I made extensive frame and try to create vector with VectorAssembler, cached it and trained on it KMeans. It took about 11 minutes for assembling and 2 minutes for KMeans for 7 different count of clusters on my pc in standalone mode for frame ~500x9000. Another side this processing in pandas (pivot df, and iterate 7 clusters) takes less one minute. Obviously I understand overhead and performance decreasing for standalone mode and caching and so on but it's really discourages me. Could somebody explain how I can avoid this overhead? How peoples work with wide DF instead of using vectorassembler and getting performance decreasing? More formal question (for sof rules) sound like - How can I speed up this code? Config:","%%time
tmp = (df_states.select('ObjectPath', 'User', 'PropertyFlagValue')
       .groupBy('User')
       .pivot('ObjectPath')
       .agg({'PropertyFlagValue':'max'})
       .fillna(0))
ignore = ['User']
assembler = VectorAssembler(
    inputCols=[x for x in tmp.columns if x not in ignore],
    outputCol='features')
Wall time: 36.7 s

print(tmp.count(), len(tmp.columns))
552, 9378

%%time
transformed = assembler.transform(tmp).select('User', 'features').cache()
Wall time: 10min 45s

%%time
lst_levels = []
for num in range(3, 14):
    kmeans = KMeans(k=num, maxIter=50)
    model = kmeans.fit(transformed)
    lst_levels.append(model.computeCost(transformed))
rs = [i-j for i,j in list(zip(lst_levels, lst_levels[1:]))]
for i, j in zip(rs, rs[1:]):
    if i - j &lt; j:
        print(rs.index(i))
        kmeans = KMeans(k=rs.index(i) + 3, maxIter=50)
        model = kmeans.fit(transformed)
        break
 Wall time: 1min 32s
 .config(""spark.sql.pivotMaxValues"", ""100000"") \
.config(""spark.sql.autoBroadcastJoinThreshold"", ""-1"") \
.config(""spark.sql.shuffle.partitions"", ""4"") \
.config(""spark.sql.inMemoryColumnarStorage.batchSize"", ""1000"") \
",35,57,0,0,
304,48193502,48193556,15296,"How can I access the nested data in this complex JSON, which includes another JSON document as one of the strings?",3,<python><json><list><dictionary>,19,"<p>I have some JSON data like:</p>
<pre><code>{
  &quot;status&quot;: &quot;200&quot;,
  &quot;msg&quot;: &quot;&quot;,
  &quot;data&quot;: {
    &quot;time&quot;: &quot;1515580011&quot;,
    &quot;video_info&quot;: [
      {
          &quot;announcement&quot;: &quot;{\&quot;announcement_id\&quot;:\&quot;6\&quot;,\&quot;name\&quot;:\&quot;INS\\u8d26\\u53f7\&quot;,\&quot;icon\&quot;:\&quot;http:\\\/\\\/liveme.cms.ksmobile.net\\\/live\\\/announcement\\\/2017-08-18_19:44:54\\\/ins.png\&quot;,\&quot;icon_new\&quot;:\&quot;http:\\\/\\\/liveme.cms.ksmobile.net\\\/live\\\/announcement\\\/2017-10-20_22:24:38\\\/4.png\&quot;,\&quot;videoid\&quot;:\&quot;15154610218328614178\&quot;,\&quot;content\&quot;:\&quot;FOLLOW ME PLEASE\&quot;,\&quot;x_coordinate\&quot;:\&quot;0.22\&quot;,\&quot;y_coordinate\&quot;:\&quot;0.23\&quot;}&quot;,
          &quot;announcement_shop&quot;: &quot;&quot;,
</code></pre>
<p>etc.</p>
<p>How do I grab the content <code>&quot;FOLLOW ME PLEASE&quot;</code>? I tried using</p>
<pre><code>replay_data = raw_replay_data['data']['video_info'][0]
announcement = replay_data['announcement']
</code></pre>
<p>But now <code>announcement</code> is a string representing more JSON data. I can't continue indexing <code>announcement['content']</code> results in <code>TypeError: string indices must be integers</code>.</p>
<p>How can I get the desired string in the &quot;right&quot; way, i.e. respecting the actual structure of the data?</p>
",8258990,615,10-01-2018 18:00,10-01-2018 18:03,0,615,21,1,9,91,"{'badge_counts': {'bronze': 21, 'silver': 9, 'gold': 1}, 'account_id': 11260507, 'is_employee': False, 'last_modified_date': 1690595400, 'last_access_date': 1676058441, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 615, 'creation_date': 1499255506, 'user_type': 'registered', 'user_id': 8258990, 'accept_rate': 91, 'link': 'https://stackoverflow.com/users/8258990/aquatic7', 'profile_image': 'https://www.gravatar.com/avatar/91c19f19808b16251bfd6c9c6c941e46?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'aquatic7'}","I have some JSON data like: etc. How do I grab the content ? I tried using But now is a string representing more JSON data. I can't continue indexing results in . How can I get the desired string in the &quot;right&quot; way, i.e. respecting the actual structure of the data?","{
  &quot;status&quot;: &quot;200&quot;,
  &quot;msg&quot;: &quot;&quot;,
  &quot;data&quot;: {
    &quot;time&quot;: &quot;1515580011&quot;,
    &quot;video_info&quot;: [
      {
          &quot;announcement&quot;: &quot;{\&quot;announcement_id\&quot;:\&quot;6\&quot;,\&quot;name\&quot;:\&quot;INS\\u8d26\\u53f7\&quot;,\&quot;icon\&quot;:\&quot;http:\\\/\\\/liveme.cms.ksmobile.net\\\/live\\\/announcement\\\/2017-08-18_19:44:54\\\/ins.png\&quot;,\&quot;icon_new\&quot;:\&quot;http:\\\/\\\/liveme.cms.ksmobile.net\\\/live\\\/announcement\\\/2017-10-20_22:24:38\\\/4.png\&quot;,\&quot;videoid\&quot;:\&quot;15154610218328614178\&quot;,\&quot;content\&quot;:\&quot;FOLLOW ME PLEASE\&quot;,\&quot;x_coordinate\&quot;:\&quot;0.22\&quot;,\&quot;y_coordinate\&quot;:\&quot;0.23\&quot;}&quot;,
          &quot;announcement_shop&quot;: &quot;&quot;,
 &quot;FOLLOW ME PLEASE&quot; replay_data = raw_replay_data['data']['video_info'][0]
announcement = replay_data['announcement']
 announcement announcement['content'] TypeError: string indices must be integers",5,18,0,0,
305,49774825,49775393,32739,How to use lightgbm.cv for regression?,1,<python><regression><cross-validation><lightgbm>,24,"<p>I want to do a cross validation for LightGBM model with <em>lgb.Dataset</em> and use <em>early_stopping_rounds</em>. The following approach works without a problem with XGBoost's <em>xgboost.cv</em>. I prefer not to use Scikit Learn's approach with GridSearchCV, because it doesn't support early stopping or lgb.Dataset.</p>
<pre><code>import lightgbm as lgb
from sklearn.metrics import mean_absolute_error
dftrainLGB = lgb.Dataset(data = dftrain, label = ytrain, feature_name = list(dftrain))

params = {'objective': 'regression'}
    
cv_results = lgb.cv(
        params,
        dftrainLGB,
        num_boost_round=100,
        nfold=3,
        metrics='mae',
        early_stopping_rounds=10
        )
</code></pre>
<p>The task is to do regression, but the following code throws an error:</p>
<pre><code>Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.
</code></pre>
<p>Does LightGBM support regression, or did I supply wrong parameters?</p>
",8873666,419,11-04-2018 12:12,11-04-2018 12:37,0,419,9,1,5,,"{'badge_counts': {'bronze': 9, 'silver': 5, 'gold': 1}, 'account_id': 12154318, 'is_employee': False, 'last_modified_date': 1573686054, 'last_access_date': 1650033661, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 419, 'creation_date': 1509614749, 'user_type': 'registered', 'user_id': 8873666, 'location': 'Lithuania', 'link': 'https://stackoverflow.com/users/8873666/marius', 'profile_image': 'https://www.gravatar.com/avatar/fe02e7df1223bd0569595bc5c47bb37f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Marius'}","I want to do a cross validation for LightGBM model with lgb.Dataset and use early_stopping_rounds. The following approach works without a problem with XGBoost's xgboost.cv. I prefer not to use Scikit Learn's approach with GridSearchCV, because it doesn't support early stopping or lgb.Dataset. The task is to do regression, but the following code throws an error: Does LightGBM support regression, or did I supply wrong parameters?","import lightgbm as lgb
from sklearn.metrics import mean_absolute_error
dftrainLGB = lgb.Dataset(data = dftrain, label = ytrain, feature_name = list(dftrain))

params = {'objective': 'regression'}
    
cv_results = lgb.cv(
        params,
        dftrainLGB,
        num_boost_round=100,
        nfold=3,
        metrics='mae',
        early_stopping_rounds=10
        )
 Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.
",13,20,0,0,
306,49243719,49244044,35006,How to access SparkContext from SparkSession instance?,2,<python><apache-spark><pyspark>,28,"<p>I am importing <code>SparkSession</code> as follows in PySpark:</p>

<pre><code>from pyspark.sql import SparkSession
</code></pre>

<p>Then I create <code>SparkSession</code>:</p>

<pre><code>spark = SparkSession.builder.appName(""test"").getOrCreate()
</code></pre>

<p>and try to access <code>SparkContext</code>:</p>

<pre><code>spark.SparkContext.broadcast(...)
</code></pre>

<p>However, I get an error that <code>SparkContext</code> does not exist. How can I access it in order to set <code>broadcast</code> variables?</p>
",8952023,3662,12-03-2018 20:14,12-03-2018 20:38,0,3662,87,14,51,84,"{'badge_counts': {'bronze': 87, 'silver': 51, 'gold': 14}, 'account_id': 12265336, 'is_employee': False, 'last_modified_date': 1608942000, 'last_access_date': 1541167937, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3662, 'creation_date': 1510840672, 'user_type': 'registered', 'user_id': 8952023, 'accept_rate': 84, 'link': 'https://stackoverflow.com/users/8952023/markus', 'profile_image': 'https://www.gravatar.com/avatar/b78ba3fc674615965907567e2e50b78b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Markus'}","I am importing as follows in PySpark: Then I create : and try to access : However, I get an error that does not exist. How can I access it in order to set variables?","SparkSession from pyspark.sql import SparkSession
 SparkSession spark = SparkSession.builder.appName(""test"").getOrCreate()
 SparkContext spark.SparkContext.broadcast(...)
 SparkContext broadcast",-5,16,0,0,
307,48613002,48613256,73560,sha-256 hashing in python,1,<python><hash><sha256>,48,"<p>I wanted to create a python program thats asks for an input then hashes(sha-256) the input then prints it. Does this already exist? How would I go about doing so.</p>
",8281499,696,04-02-2018 21:05,04-02-2018 21:38,0,706,14,2,8,,"{'badge_counts': {'bronze': 14, 'silver': 8, 'gold': 2}, 'account_id': 11293222, 'is_employee': False, 'last_modified_date': 1648507800, 'last_access_date': 1525319797, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 706, 'creation_date': 1499660723, 'user_type': 'registered', 'user_id': 8281499, 'link': 'https://stackoverflow.com/users/8281499/user9123', 'profile_image': 'https://lh4.googleusercontent.com/-y_yxSk38rCA/AAAAAAAAAAI/AAAAAAAAA9s/My81flNk8Lk/photo.jpg?sz=256', 'display_name': 'User9123'}",I wanted to create a python program thats asks for an input then hashes(sha-256) the input then prints it. Does this already exist? How would I go about doing so.,,0,1,0,0,
308,48709839,48709941,9800,StopIteration: generator_output = next(output_generator),2,<python><numpy><keras>,12,"<p>I have the following code which I rewrite to work on a large scale dataset. I am using Python generator to Fit the model on data yielded batch-by-batch.</p>

<pre><code>def subtract_mean_gen(x_source,y_source,avg_image,batch):
    batch_list_x=[]
    batch_list_y=[]
    for line,y in zip(x_source,y_source):
        x=line.astype('float32')
        x=x-avg_image
        batch_list_x.append(x)
        batch_list_y.append(y)
        if len(batch_list_x) == batch:
            yield (np.array(batch_list_x),np.array(batch_list_y))
            batch_list_x=[]
            batch_list_y=[] 

model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

val = subtract_mean_gen(X_test,Y_test,avg_image_test,batch_size)
model.fit_generator(subtract_mean_gen(X_train,Y_train,avg_image_train,batch_size), steps_per_epoch=X_train.shape[0]//batch_size,epochs=nb_epoch,validation_data = val,
                    validation_steps = X_test.shape[0]//batch_size)
</code></pre>

<p>I obtain the following error:</p>

<pre><code>239/249 [===========================&gt;..] - ETA: 60s - loss: 1.3318 - acc: 0.8330Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/utils/data_utils.py"", line 560, in data_generator_task
    generator_output = next(self._generator)
StopIteration

240/249 [===========================&gt;..] - ETA: 54s - loss: 1.3283 - acc: 0.8337Traceback (most recent call last):
  File ""cifa10-copy.py"", line 125, in &lt;module&gt;
    validation_steps = X_test.shape[0]//batch_size)
  File ""/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py"", line 87, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1809, in fit_generator
    generator_output = next(output_generator)
StopIteration
</code></pre>

<p>I looked into a similar question posted <a href=""https://stackoverflow.com/questions/46302911/what-raises-stopiteration-in-mine-keras-model-fit-generator"">here</a> however, I am not able to resolve the error why StopIteration is raised. </p>
",8967121,401,09-02-2018 16:10,09-02-2018 16:16,0,401,23,1,5,,"{'badge_counts': {'bronze': 23, 'silver': 5, 'gold': 1}, 'account_id': 4144534, 'is_employee': False, 'last_modified_date': 1671018000, 'last_access_date': 1710756255, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 401, 'creation_date': 1511111871, 'user_type': 'registered', 'user_id': 8967121, 'location': 'Italy', 'website_url': '', 'link': 'https://stackoverflow.com/users/8967121/cswah', 'profile_image': 'https://i.stack.imgur.com/mbBHf.jpg?s=256&g=1', 'display_name': 'cswah'}","I have the following code which I rewrite to work on a large scale dataset. I am using Python generator to Fit the model on data yielded batch-by-batch. I obtain the following error: I looked into a similar question posted here however, I am not able to resolve the error why StopIteration is raised.","def subtract_mean_gen(x_source,y_source,avg_image,batch):
    batch_list_x=[]
    batch_list_y=[]
    for line,y in zip(x_source,y_source):
        x=line.astype('float32')
        x=x-avg_image
        batch_list_x.append(x)
        batch_list_y.append(y)
        if len(batch_list_x) == batch:
            yield (np.array(batch_list_x),np.array(batch_list_y))
            batch_list_x=[]
            batch_list_y=[] 

model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

val = subtract_mean_gen(X_test,Y_test,avg_image_test,batch_size)
model.fit_generator(subtract_mean_gen(X_train,Y_train,avg_image_train,batch_size), steps_per_epoch=X_train.shape[0]//batch_size,epochs=nb_epoch,validation_data = val,
                    validation_steps = X_test.shape[0]//batch_size)
 239/249 [===========================&gt;..] - ETA: 60s - loss: 1.3318 - acc: 0.8330Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/utils/data_utils.py"", line 560, in data_generator_task
    generator_output = next(self._generator)
StopIteration

240/249 [===========================&gt;..] - ETA: 54s - loss: 1.3283 - acc: 0.8337Traceback (most recent call last):
  File ""cifa10-copy.py"", line 125, in &lt;module&gt;
    validation_steps = X_test.shape[0]//batch_size)
  File ""/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py"", line 87, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1809, in fit_generator
    generator_output = next(output_generator)
StopIteration
",37,48,0,1,
309,48343387,48343484,25097,ValueError and TypeError in python,5,<python><typeerror><valueerror>,29,"<p>I can't completely understand the difference between Type and Value error in Python3x. </p>

<p>Why do we get a ValueError when I try float('string') instead of TypeError? shouldn't this give also a TypeError because I am passing a variable of type 'str' to be converted into float?</p>

<pre><code>In [169]: float('string')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-169-f894e176bff2&gt; in &lt;module&gt;()
----&gt; 1 float('string')

ValueError: could not convert string to float: 'string'
</code></pre>
",9240584,631,19-01-2018 14:33,19-01-2018 14:38,0,631,18,3,8,62,"{'badge_counts': {'bronze': 18, 'silver': 8, 'gold': 3}, 'account_id': 12719725, 'is_employee': False, 'last_modified_date': 1680316200, 'last_access_date': 1709898908, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 631, 'creation_date': 1516371714, 'user_type': 'registered', 'user_id': 9240584, 'accept_rate': 62, 'location': 'Madurai, Tamil Nadu, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/9240584/thileepan', 'profile_image': 'https://www.gravatar.com/avatar/959e6b35956149adcb917edd05a2fbb2?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'thileepan'}",I can't completely understand the difference between Type and Value error in Python3x. Why do we get a ValueError when I try float('string') instead of TypeError? shouldn't this give also a TypeError because I am passing a variable of type 'str' to be converted into float?,"In [169]: float('string')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-169-f894e176bff2&gt; in &lt;module&gt;()
----&gt; 1 float('string')

ValueError: could not convert string to float: 'string'
",6,12,0,0,
310,48272452,48272472,19317,Sum columns by level in a pandas MultiIndex DataFrame,2,<python><pandas><dataframe><multi-index>,16,"<p>I have my df with multi-index columns. All of my values are in float, and I want to merge values with in first level of multi-index. Please see below for detail.</p>

<pre><code>first        bar                 baz                 foo   
second       one       two       one       two       one    
A       0.895717  0.805244  1.206412  2.565646  1.431256    
B       0.410835  0.813850  0.132003  0.827317  0.076467    
C       1.413681  1.607920  1.024180  0.569605  0.875906 

first        bar                 baz                 foo   

A       (0.895717+0.805244) (1.206412+2.565646)  1.431256    
B       (0.410835+0.813850) (0.132003+0.827317)  0.076467    
C       (1.413681+1.607920) (1.024180+0.569605)  0.875906 
</code></pre>

<p>The values are actually added (I just didn't feel like doing all this :)). Bottom line is that I just want to level-up(higher level I guess) and within the index, add all the values. Please let me know a good way to do this. Thank you!</p>
",9001495,465,16-01-2018 00:14,16-01-2018 00:16,0,485,19,2,5,100,"{'badge_counts': {'bronze': 19, 'silver': 5, 'gold': 2}, 'account_id': 12337068, 'is_employee': False, 'last_modified_date': 1676938468, 'last_access_date': 1710984999, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 485, 'creation_date': 1511501717, 'user_type': 'registered', 'user_id': 9001495, 'accept_rate': 100, 'location': 'Chicago, IL', 'website_url': '', 'link': 'https://stackoverflow.com/users/9001495/ej-kang', 'profile_image': 'https://i.stack.imgur.com/ei2Ee.jpg?s=256&g=1', 'display_name': 'EJ Kang'}","I have my df with multi-index columns. All of my values are in float, and I want to merge values with in first level of multi-index. Please see below for detail. The values are actually added (I just didn't feel like doing all this :)). Bottom line is that I just want to level-up(higher level I guess) and within the index, add all the values. Please let me know a good way to do this. Thank you!","first        bar                 baz                 foo   
second       one       two       one       two       one    
A       0.895717  0.805244  1.206412  2.565646  1.431256    
B       0.410835  0.813850  0.132003  0.827317  0.076467    
C       1.413681  1.607920  1.024180  0.569605  0.875906 

first        bar                 baz                 foo   

A       (0.895717+0.805244) (1.206412+2.565646)  1.431256    
B       (0.410835+0.813850) (0.132003+0.827317)  0.076467    
C       (1.413681+1.607920) (1.024180+0.569605)  0.875906 
",10,16,0,0,
311,49941426,49942523,94896,AttributeError: 'collections.OrderedDict' object has no attribute 'eval',2,<python><deep-learning><pytorch>,22,"<p>I have a model file which looks like this</p>
<pre><code>OrderedDict([('inp.conv1.conv.weight', 
          (0 ,0 ,0 ,.,.) = 
           -1.5073e-01  6.4760e-02  1.9156e-01
            1.2175e-01  3.5886e-02  1.3992e-01
           -1.5903e-01  8.2055e-02  1.7820e-01
          
          (0 ,0 ,1 ,.,.) = 
            1.0604e-01 -1.3653e-01  1.4803e-01
            6.0276e-02 -1.4674e-02  2.3059e-06
           -6.2192e-02 -5.1061e-03 -7.4145e-03
          
          (0 ,0 ,2 ,.,.) = 
           -5.5632e-02  3.5326e-02  6.5108e-02
            1.1411e-01 -4.4160e-02  8.2610e-02
            8.9979e-02 -3.5454e-02  4.2549e-02
          
          (1 ,0 ,0 ,.,.) = 
            4.8523e-02 -4.3961e-02  5.3614e-02
           -1.2644e-01  1.2777e-01  8.9547e-02
            3.8392e-02  2.7016e-02 -1.4552e-01
          
          (1 ,0 ,1 ,.,.) = 
            9.5537e-02  2.8748e-02  3.9772e-02
           -6.2410e-02  1.1264e-01  7.8663e-02
           -2.6374e-02  1.4401e-01 -1.7109e-01
          
          (1 ,0 ,2 ,.,.) = 
            5.1791e-02 -1.6388e-01 -1.7605e-01
            3.5028e-02  7.7164e-02 -1.4499e-01
           -2.9189e-02  2.7064e-03 -2.3228e-02
          
          (2 ,0 ,0 ,.,.) = 
           -7.4446e-03 -9.7202e-02 -1.4704e-01
           -1.0019e-02  8.1780e-02 -5.3530e-02
           -1.8412e-01  1.5988e-01 -1.3450e-01
          
          (2 ,0 ,1 ,.,.) = 
           -1.1075e-01 -5.2478e-02  6.0658e-02
            1.6739e-01 -2.9360e-02  1.2621e-01
            2.0686e-02  1.1468e-01  1.2282e-01
</code></pre>
<p>I want to do inference on this model, but when i do model.eval() i get,</p>
<pre><code>AttributeError: 'collections.OrderedDict' object has no attribute 'eval
</code></pre>
",8176285,9249,20-04-2018 12:14,20-04-2018 13:14,0,9339,68,14,40,73,"{'badge_counts': {'bronze': 68, 'silver': 40, 'gold': 14}, 'account_id': 11138885, 'is_employee': False, 'last_modified_date': 1671307500, 'last_access_date': 1591387367, 'reputation_change_year': 310, 'reputation_change_quarter': 310, 'reputation_change_month': 100, 'reputation_change_week': 50, 'reputation_change_day': 0, 'reputation': 9339, 'creation_date': 1497711892, 'user_type': 'registered', 'user_id': 8176285, 'accept_rate': 73, 'link': 'https://stackoverflow.com/users/8176285/ryan', 'profile_image': 'https://www.gravatar.com/avatar/51535938bd9cf87d44432562af9eb914?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ryan '}","I have a model file which looks like this I want to do inference on this model, but when i do model.eval() i get,","OrderedDict([('inp.conv1.conv.weight', 
          (0 ,0 ,0 ,.,.) = 
           -1.5073e-01  6.4760e-02  1.9156e-01
            1.2175e-01  3.5886e-02  1.3992e-01
           -1.5903e-01  8.2055e-02  1.7820e-01
          
          (0 ,0 ,1 ,.,.) = 
            1.0604e-01 -1.3653e-01  1.4803e-01
            6.0276e-02 -1.4674e-02  2.3059e-06
           -6.2192e-02 -5.1061e-03 -7.4145e-03
          
          (0 ,0 ,2 ,.,.) = 
           -5.5632e-02  3.5326e-02  6.5108e-02
            1.1411e-01 -4.4160e-02  8.2610e-02
            8.9979e-02 -3.5454e-02  4.2549e-02
          
          (1 ,0 ,0 ,.,.) = 
            4.8523e-02 -4.3961e-02  5.3614e-02
           -1.2644e-01  1.2777e-01  8.9547e-02
            3.8392e-02  2.7016e-02 -1.4552e-01
          
          (1 ,0 ,1 ,.,.) = 
            9.5537e-02  2.8748e-02  3.9772e-02
           -6.2410e-02  1.1264e-01  7.8663e-02
           -2.6374e-02  1.4401e-01 -1.7109e-01
          
          (1 ,0 ,2 ,.,.) = 
            5.1791e-02 -1.6388e-01 -1.7605e-01
            3.5028e-02  7.7164e-02 -1.4499e-01
           -2.9189e-02  2.7064e-03 -2.3228e-02
          
          (2 ,0 ,0 ,.,.) = 
           -7.4446e-03 -9.7202e-02 -1.4704e-01
           -1.0019e-02  8.1780e-02 -5.3530e-02
           -1.8412e-01  1.5988e-01 -1.3450e-01
          
          (2 ,0 ,1 ,.,.) = 
           -1.1075e-01 -5.2478e-02  6.0658e-02
            1.6739e-01 -2.9360e-02  1.2621e-01
            2.0686e-02  1.1468e-01  1.2282e-01
 AttributeError: 'collections.OrderedDict' object has no attribute 'eval
",39,45,0,0,
312,50149562,50247223,124955,jupyterlab interactive plot,6,<python><matplotlib><jupyter-lab>,77,"<p>With old Jupyter notebooks, I could create interactive plots via:</p>
<pre><code>import matplotlib.pyplot as plt
%matplotlib notebook
x = [1,2,3]
y = [4,5,6]
plt.figure()
plt.plot(x,y)
</code></pre>
<p>However, in JupyterLab, this gives an error:</p>
<pre><code>JavaScript output is disabled in JupyterLab
</code></pre>
<p>I have also tried the magic (with <a href=""https://github.com/matplotlib/jupyter-matplotlib"" rel=""noreferrer"" title=""jupyter-matplotlib""><code>jupyter-matplotlib</code></a> installed):</p>
<pre><code>%matplotlib ipympl
</code></pre>
<p>But that just returns:</p>
<pre><code>FigureCanvasNbAgg()
</code></pre>
<p>Inline plots work, but they are not interactive plots:</p>
<pre><code>%matplotlib inline
</code></pre>
",8233329,1015,03-05-2018 07:45,09-05-2018 06:55,6,1025,13,1,8,,"{'badge_counts': {'bronze': 13, 'silver': 8, 'gold': 1}, 'account_id': 11215243, 'is_employee': False, 'last_modified_date': 1672230300, 'last_access_date': 1675494378, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1025, 'creation_date': 1498758079, 'user_type': 'registered', 'user_id': 8233329, 'website_url': '', 'link': 'https://stackoverflow.com/users/8233329/albatross', 'profile_image': 'https://www.gravatar.com/avatar/2cb15869324cc1a34e5ffae0f93f497b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Albatross'}","With old Jupyter notebooks, I could create interactive plots via: However, in JupyterLab, this gives an error: I have also tried the magic (with installed): But that just returns: Inline plots work, but they are not interactive plots:","import matplotlib.pyplot as plt
%matplotlib notebook
x = [1,2,3]
y = [4,5,6]
plt.figure()
plt.plot(x,y)
 JavaScript output is disabled in JupyterLab
 jupyter-matplotlib %matplotlib ipympl
 FigureCanvasNbAgg()
 %matplotlib inline
",4,20,0,1,
313,49286135,49286165,36299,Writing text to gzip file,1,<python><python-3.x><file><gzip>,34,"<p>Following tutorials and examples found in blogs and in other threads here, it appears that the way to write to a <code>.gz</code> file is to open it in binary mode and write the string as is:</p>

<pre><code>import gzip
with gzip.open('file.gz', 'wb') as f:
    f.write('Hello world!')
</code></pre>

<p>I tried it, and got the following exception:</p>

<pre><code>  File ""C:\Users\Tal\Anaconda3\lib\gzip.py"", line 258, in write
    data = memoryview(data)
TypeError: memoryview: a bytes-like object is required, not 'str'
</code></pre>

<p>So I tried opening the file in text mode:</p>

<pre><code>import gzip
with gzip.open('file.gz', 'w') as f:
    f.write('Hello world!')
</code></pre>

<p>But I got the same error:</p>

<pre><code>  File ""C:\Users\Tal\Anaconda3\lib\gzip.py"", line 258, in write
    data = memoryview(data)
TypeError: memoryview: a bytes-like object is required, not 'str'
</code></pre>

<p>How can this problem be addressed in Python3?</p>
",9035298,575,14-03-2018 19:22,14-03-2018 19:24,0,575,12,1,5,,"{'badge_counts': {'bronze': 12, 'silver': 5, 'gold': 1}, 'account_id': 12395040, 'is_employee': False, 'last_modified_date': 1586273715, 'last_access_date': 1614432415, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 575, 'creation_date': 1512063994, 'user_type': 'registered', 'user_id': 9035298, 'link': 'https://stackoverflow.com/users/9035298/agvaniarekuva', 'profile_image': 'https://www.gravatar.com/avatar/c6771d8a5118b2ddbd021e2fff73053b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'AgvaniaRekuva'}","Following tutorials and examples found in blogs and in other threads here, it appears that the way to write to a file is to open it in binary mode and write the string as is: I tried it, and got the following exception: So I tried opening the file in text mode: But I got the same error: How can this problem be addressed in Python3?",".gz import gzip
with gzip.open('file.gz', 'wb') as f:
    f.write('Hello world!')
   File ""C:\Users\Tal\Anaconda3\lib\gzip.py"", line 258, in write
    data = memoryview(data)
TypeError: memoryview: a bytes-like object is required, not 'str'
 import gzip
with gzip.open('file.gz', 'w') as f:
    f.write('Hello world!')
   File ""C:\Users\Tal\Anaconda3\lib\gzip.py"", line 258, in write
    data = memoryview(data)
TypeError: memoryview: a bytes-like object is required, not 'str'
",7,29,0,0,
314,48152908,48153344,11336,How to filter choices in Django2's autocomplete_fields?,6,<python><django><django-admin><django-2.0>,31,"<p>In Django 2.0, <a href=""https://docs.djangoproject.com/en/2.0/ref/contrib/admin/#django.contrib.admin.ModelAdmin.autocomplete_fields"" rel=""noreferrer"">autocomplete_fields</a> was added, which is great.</p>

<p>Without autocomplete_fields, I can change the queryset of a ForeignKeyField using <a href=""https://docs.djangoproject.com/en/2.0/ref/contrib/admin/#django.contrib.admin.ModelAdmin.formfield_for_foreignkey"" rel=""noreferrer"">formfield_for_foreignkey</a>.</p>

<p>But combining the two together doesn't work - it looks like the list of options for autocomplete is dynamic and coming from a different url, instead of from the current form.</p>

<p>So the question is - </p>

<p>How can I change the queryset in the autocomplete widget? </p>
",9098769,473,08-01-2018 15:04,08-01-2018 15:31,0,473,6,1,5,,"{'badge_counts': {'bronze': 6, 'silver': 5, 'gold': 1}, 'account_id': 12500014, 'is_employee': False, 'last_modified_date': 1594038724, 'last_access_date': 1580635429, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 473, 'creation_date': 1513253138, 'user_type': 'registered', 'user_id': 9098769, 'link': 'https://stackoverflow.com/users/9098769/oren-shpigel', 'profile_image': 'https://www.gravatar.com/avatar/2f224946235f9f2ca9c00f76d94ad9a2?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Oren Shpigel'}","In Django 2.0, autocomplete_fields was added, which is great. Without autocomplete_fields, I can change the queryset of a ForeignKeyField using formfield_for_foreignkey. But combining the two together doesn't work - it looks like the list of options for autocomplete is dynamic and coming from a different url, instead of from the current form. So the question is - How can I change the queryset in the autocomplete widget?",,0,9,0,2,
315,49750107,49750196,33961,How to remove scientific notation on a matplotlib log-log plot,4,<python><matplotlib><plot><notation>,29,"<p>I know that this question has been asked before, but I tried all the possible solutions and none of them worked for me.</p>

<p>So, I have a log-log plot in matplotlib, and I would like to avoid scientific notation on the x-axis.</p>

<p>This is my code:</p>

<pre><code>from numpy import array, log, pi
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
import matplotlib.ticker as mticker

plt.rc('axes.formatter', useoffset=False)

tc = array([7499680.0, 12508380.0, 23858280.0, 34877020.0, 53970660.0, 89248580.0, 161032860.0, 326814160.0, 784460200.0])

theta = array([70, 60, 50, 45, 40, 35, 30, 25, 20])

plt.scatter(theta,tc)

ax=plt.gca()

ax.set_xscale('log')
ax.set_yscale('log')

ax.xaxis.set_major_formatter(mticker.ScalarFormatter())
ax.xaxis.get_major_formatter().set_scientific(False)
ax.xaxis.get_major_formatter().set_useOffset(False)

plt.show()
</code></pre>

<p>And this is the output:
<a href=""https://i.stack.imgur.com/dW00s.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dW00s.png"" alt=""Output""></a></p>

<p>As you can see, the numbers on the x-axis are still in scientific notation. I would like to display them as 20, 30, 40... I tried every possible solution with no result.</p>

<p>Thank you very much to everyone that will help.</p>

<p>NB. I can't use the plt.loglog() command, because I am doing some curve fitting on the data and I need it like that.</p>

<p>NB2. I noticed a very weird thing happening: if I change the code to yaxis.get_mayor_formatter()..., it works on the y-axis! It is just on the x one that it's not working. How is it possible?</p>

<p>Edit: maybe it is not clear, but if you look at the code, there are 3 methods that should affect the display of the x-ticks: <code>plt.rc('axes.formatter', useoffset=False)</code>, <code>ax.xaxis.set_major_formatter(mticker.ScalarFormatter())</code> and <code>ax.xaxis.get_major_formatter().set_scientific(False)</code>. They are 3 methods that should all do the trick alone, according to what I found around, but they don't. Of course I also tried them one by one and not all together. </p>
",9104884,1445,10-04-2018 09:30,10-04-2018 09:34,0,1445,28,1,10,,"{'badge_counts': {'bronze': 28, 'silver': 10, 'gold': 1}, 'account_id': 6875594, 'is_employee': False, 'last_modified_date': 1697643300, 'last_access_date': 1711104388, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1445, 'creation_date': 1508789673, 'user_type': 'registered', 'user_id': 9104884, 'location': 'Italy', 'website_url': 'https://www.francescoboccardo.it/', 'link': 'https://stackoverflow.com/users/9104884/tropilio', 'profile_image': 'https://i.stack.imgur.com/sVPmy.png?s=256&g=1', 'display_name': 'Tropilio'}","I know that this question has been asked before, but I tried all the possible solutions and none of them worked for me. So, I have a log-log plot in matplotlib, and I would like to avoid scientific notation on the x-axis. This is my code: And this is the output: As you can see, the numbers on the x-axis are still in scientific notation. I would like to display them as 20, 30, 40... I tried every possible solution with no result. Thank you very much to everyone that will help. NB. I can't use the plt.loglog() command, because I am doing some curve fitting on the data and I need it like that. NB2. I noticed a very weird thing happening: if I change the code to yaxis.get_mayor_formatter()..., it works on the y-axis! It is just on the x one that it's not working. How is it possible? Edit: maybe it is not clear, but if you look at the code, there are 3 methods that should affect the display of the x-ticks: , and . They are 3 methods that should all do the trick alone, according to what I found around, but they don't. Of course I also tried them one by one and not all together.","from numpy import array, log, pi
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
import matplotlib.ticker as mticker

plt.rc('axes.formatter', useoffset=False)

tc = array([7499680.0, 12508380.0, 23858280.0, 34877020.0, 53970660.0, 89248580.0, 161032860.0, 326814160.0, 784460200.0])

theta = array([70, 60, 50, 45, 40, 35, 30, 25, 20])

plt.scatter(theta,tc)

ax=plt.gca()

ax.set_xscale('log')
ax.set_yscale('log')

ax.xaxis.set_major_formatter(mticker.ScalarFormatter())
ax.xaxis.get_major_formatter().set_scientific(False)
ax.xaxis.get_major_formatter().set_useOffset(False)

plt.show()
 plt.rc('axes.formatter', useoffset=False) ax.xaxis.set_major_formatter(mticker.ScalarFormatter()) ax.xaxis.get_major_formatter().set_scientific(False)",19,43,1,1,
316,48523629,48524064,47939,Spark/PySpark: An error occurred while trying to connect to the Java server (127.0.0.1:39543),1,<python><apache-spark><pyspark><jupyter-notebook>,11,"<p>Good afternoon,</p>

<p>In the last two days occurs many connection problems to the Java server. It´s a little bit uncommon because the error occurs not always, only sometimes...</p>

<p>I am using PySpark combined with Jupyter Notebook. Everything is running on a VM instance in the Google Cloud. I am using this one in Google Cloud:</p>

<pre><code>custom (8 vCPUs, 200 GB) 
</code></pre>

<p>These are the other settings:</p>

<pre><code>conf = pyspark.SparkConf().setAppName(""App"")
conf = (conf.setMaster('local[*]')
        .set('spark.executor.memory', '180G')
        .set('spark.driver.memory', '180G')
        .set('spark.driver.maxResultSize', '180G'))

sc = pyspark.SparkContext(conf=conf)
sq = pyspark.sql.SQLContext(sc)
</code></pre>

<p>I trained a Random Forest Model and made predictions:</p>

<pre><code>model = rf.fit(train)
predictions = model.transform(test)
</code></pre>

<p>Afterwards I created the ROC-Curve and compute the AUC-value.</p>

<p>Then I wanted to see the confusion matrix:</p>

<pre><code>confusion_mat = metrics.confusionMatrix().toArray()
print(confusion_mat_train_rf)
</code></pre>

<p>And now the error occurs:</p>

<pre><code>    Traceback (most recent call last):
  File ""/usr/lib/python2.7/SocketServer.py"", line 290, in _handle_request_noblock
    self.process_request(request, client_address)
  File ""/usr/lib/python2.7/SocketServer.py"", line 318, in process_request
    self.finish_request(request, client_address)
  File ""/usr/lib/python2.7/SocketServer.py"", line 331, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File ""/usr/lib/python2.7/SocketServer.py"", line 652, in __init__
    self.handle()
  File ""/usr/local/lib/python2.7/dist-packages/pyspark/accumulators.py"", line 235, in handle
    num_updates = read_int(self.rfile)
  File ""/usr/local/lib/python2.7/dist-packages/pyspark/serializers.py"", line 577, in read_int
    raise EOFError
EOFError
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.py"", line 883, in send_command
    response = connection.send_command(command)
  File ""/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.py"", line 1040, in send_command
    ""Error while receiving"", e, proto.ERROR_ON_RECEIVE)
Py4JNetworkError: Error while receiving
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:39543)
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.py"", line 963, in start
    self.socket.connect((self.address, self.port))
  File ""/usr/lib/python2.7/socket.py"", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
</code></pre>

<p>Here is the output from the console:</p>

<pre><code>OpenJDK 64-Bit Server VM warning
: INFO: os::commit_memory(0x00007f4998300000, 603979776, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 603979776 bytes for committing reserved memory.
</code></pre>

<p>Logfile:</p>

<pre><code>#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 603979776 bytes for committing reserved memory.
# Possible reasons:
#   The system is out of physical RAM or swap space
#   In 32 bit mode, the process size limit was hit
# Possible solutions:
#   Reduce memory load on the system
#   Increase physical memory or swap space
#   Check if swap backing store is full
#   Use 64 bit Java on a 64 bit OS
#   Decrease Java heap size (-Xmx/-Xms)
#   Decrease number of Java threads
#   Decrease Java thread stack sizes (-Xss)
#   Set larger code cache with -XX:ReservedCodeCacheSize=
# This output file may be truncated or incomplete.
#
#  Out of Memory Error (os_linux.cpp:2643), pid=2377, tid=0x00007f1c94fac700
#
# JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12)
# Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 )
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#

---------------  S Y S T E M  ---------------

OS:DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=""Ubuntu 16.04.3 LTS""

uname:Linux 4.13.0-1008-gcp #11-Ubuntu SMP Thu Jan 25 11:08:44 UTC 2018 x86_64
libc:glibc 2.23 NPTL 2.23 
rlimit: STACK 8192k, CORE 0k, NPROC 805983, NOFILE 1048576, AS infinity
load average:7.69 4.51 3.57

/proc/meminfo:
MemTotal:       206348252 kB
MemFree:         1298460 kB
MemAvailable:     250308 kB
Buffers:            6812 kB
Cached:           438232 kB
SwapCached:            0 kB
Active:         203906416 kB
Inactive:         339540 kB
Active(anon):   203804300 kB
Inactive(anon):     8392 kB
Active(file):     102116 kB
Inactive(file):   331148 kB
Unevictable:        3652 kB
Mlocked:            3652 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:              4688 kB
Writeback:             0 kB
AnonPages:      203805168 kB
Mapped:            23076 kB
Shmem:              8776 kB
Slab:             114476 kB
SReclaimable:      50640 kB
SUnreclaim:        63836 kB
KernelStack:        4752 kB
PageTables:       404292 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    103174124 kB
Committed_AS:   205956256 kB
VmallocTotal:   34359738367 kB
VmallocUsed:           0 kB
VmallocChunk:          0 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       71628 kB
DirectMap2M:     4122624 kB
DirectMap1G:    207618048 kB


CPU:total 8 (initial active 8) (4 cores per cpu, 2 threads per core) family 6 model 85 stepping 3, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, rtm, 3dnowpref, lzcnt, ht, tsc, tscinvbit, bmi1, bmi2, adx
</code></pre>

<p>Does anyone have any idea what the problem might be and how i can solve this? I am desperate. :(</p>

<p>// I think the Java Runtime Environment has not enough memory to continue... But what can i do?</p>

<p>Thank you very much!</p>
",9195369,373,30-01-2018 14:20,30-01-2018 14:42,0,373,16,2,4,50,"{'badge_counts': {'bronze': 16, 'silver': 4, 'gold': 2}, 'account_id': 12651329, 'is_employee': False, 'last_modified_date': 1651188600, 'last_access_date': 1589234139, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 373, 'creation_date': 1515531475, 'user_type': 'registered', 'user_id': 9195369, 'accept_rate': 50, 'link': 'https://stackoverflow.com/users/9195369/qwertz', 'profile_image': 'https://www.gravatar.com/avatar/6a259119a88a58bc8a7b32af4a675517?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'qwertz'}","Good afternoon, In the last two days occurs many connection problems to the Java server. It´s a little bit uncommon because the error occurs not always, only sometimes... I am using PySpark combined with Jupyter Notebook. Everything is running on a VM instance in the Google Cloud. I am using this one in Google Cloud: These are the other settings: I trained a Random Forest Model and made predictions: Afterwards I created the ROC-Curve and compute the AUC-value. Then I wanted to see the confusion matrix: And now the error occurs: Here is the output from the console: Logfile: Does anyone have any idea what the problem might be and how i can solve this? I am desperate. :( // I think the Java Runtime Environment has not enough memory to continue... But what can i do? Thank you very much!","custom (8 vCPUs, 200 GB) 
 conf = pyspark.SparkConf().setAppName(""App"")
conf = (conf.setMaster('local[*]')
        .set('spark.executor.memory', '180G')
        .set('spark.driver.memory', '180G')
        .set('spark.driver.maxResultSize', '180G'))

sc = pyspark.SparkContext(conf=conf)
sq = pyspark.sql.SQLContext(sc)
 model = rf.fit(train)
predictions = model.transform(test)
 confusion_mat = metrics.confusionMatrix().toArray()
print(confusion_mat_train_rf)
     Traceback (most recent call last):
  File ""/usr/lib/python2.7/SocketServer.py"", line 290, in _handle_request_noblock
    self.process_request(request, client_address)
  File ""/usr/lib/python2.7/SocketServer.py"", line 318, in process_request
    self.finish_request(request, client_address)
  File ""/usr/lib/python2.7/SocketServer.py"", line 331, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File ""/usr/lib/python2.7/SocketServer.py"", line 652, in __init__
    self.handle()
  File ""/usr/local/lib/python2.7/dist-packages/pyspark/accumulators.py"", line 235, in handle
    num_updates = read_int(self.rfile)
  File ""/usr/local/lib/python2.7/dist-packages/pyspark/serializers.py"", line 577, in read_int
    raise EOFError
EOFError
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.py"", line 883, in send_command
    response = connection.send_command(command)
  File ""/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.py"", line 1040, in send_command
    ""Error while receiving"", e, proto.ERROR_ON_RECEIVE)
Py4JNetworkError: Error while receiving
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:39543)
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.py"", line 963, in start
    self.socket.connect((self.address, self.port))
  File ""/usr/lib/python2.7/socket.py"", line 228, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 111] Connection refused
 OpenJDK 64-Bit Server VM warning
: INFO: os::commit_memory(0x00007f4998300000, 603979776, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 603979776 bytes for committing reserved memory.
 #
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 603979776 bytes for committing reserved memory.
# Possible reasons:
#   The system is out of physical RAM or swap space
#   In 32 bit mode, the process size limit was hit
# Possible solutions:
#   Reduce memory load on the system
#   Increase physical memory or swap space
#   Check if swap backing store is full
#   Use 64 bit Java on a 64 bit OS
#   Decrease Java heap size (-Xmx/-Xms)
#   Decrease number of Java threads
#   Decrease Java thread stack sizes (-Xss)
#   Set larger code cache with -XX:ReservedCodeCacheSize=
# This output file may be truncated or incomplete.
#
#  Out of Memory Error (os_linux.cpp:2643), pid=2377, tid=0x00007f1c94fac700
#
# JRE version: OpenJDK Runtime Environment (8.0_151-b12) (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12)
# Java VM: OpenJDK 64-Bit Server VM (25.151-b12 mixed mode linux-amd64 )
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#

---------------  S Y S T E M  ---------------

OS:DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=""Ubuntu 16.04.3 LTS""

uname:Linux 4.13.0-1008-gcp #11-Ubuntu SMP Thu Jan 25 11:08:44 UTC 2018 x86_64
libc:glibc 2.23 NPTL 2.23 
rlimit: STACK 8192k, CORE 0k, NPROC 805983, NOFILE 1048576, AS infinity
load average:7.69 4.51 3.57

/proc/meminfo:
MemTotal:       206348252 kB
MemFree:         1298460 kB
MemAvailable:     250308 kB
Buffers:            6812 kB
Cached:           438232 kB
SwapCached:            0 kB
Active:         203906416 kB
Inactive:         339540 kB
Active(anon):   203804300 kB
Inactive(anon):     8392 kB
Active(file):     102116 kB
Inactive(file):   331148 kB
Unevictable:        3652 kB
Mlocked:            3652 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:              4688 kB
Writeback:             0 kB
AnonPages:      203805168 kB
Mapped:            23076 kB
Shmem:              8776 kB
Slab:             114476 kB
SReclaimable:      50640 kB
SUnreclaim:        63836 kB
KernelStack:        4752 kB
PageTables:       404292 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    103174124 kB
Committed_AS:   205956256 kB
VmallocTotal:   34359738367 kB
VmallocUsed:           0 kB
VmallocChunk:          0 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       71628 kB
DirectMap2M:     4122624 kB
DirectMap1G:    207618048 kB


CPU:total 8 (initial active 8) (4 cores per cpu, 2 threads per core) family 6 model 85 stepping 3, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, avx2, aes, clmul, erms, rtm, 3dnowpref, lzcnt, ht, tsc, tscinvbit, bmi1, bmi2, adx
",127,173,0,0,
317,48915003,48915493,53857,Get the bounding box coordinates in the TensorFlow object detection API tutorial,3,<python><tensorflow><bounding-box><object-detection-api>,29,"<p>I am new to both Python and Tensorflow. I am trying to run the object detection tutorial file from the <a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""nofollow noreferrer"">Tensorflow Object Detection API</a>,
but I cannot find where I can get the coordinates of the bounding boxes when objects are detected.</p>
<p>Relevant code:</p>
<pre><code> # The following processing is only for single image
 detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
 detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
</code></pre>
<p>The place where I assume bounding boxes are drawn is like this:</p>
<pre><code> # Visualization of the results of detection.
 vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
 plt.figure(figsize=IMAGE_SIZE)
 plt.imshow(image_np)
</code></pre>
<p>I tried printing <code>output_dict['detection_boxes']</code> but I am not sure what the numbers mean. There are a lot.</p>
<pre><code>array([[ 0.56213236,  0.2780568 ,  0.91445708,  0.69120586],
       [ 0.56261235,  0.86368728,  0.59286624,  0.8893863 ],
       [ 0.57073039,  0.87096912,  0.61292225,  0.90354401],
       [ 0.51422435,  0.78449738,  0.53994244,  0.79437423],
......

       [ 0.32784131,  0.5461576 ,  0.36972913,  0.56903434],
       [ 0.03005961,  0.02714229,  0.47211722,  0.44683522],
       [ 0.43143299, 0.09211366,  0.58121657,  0.3509962 ]], dtype=float32)
</code></pre>
<p>I found answers for similar questions, but I don't have a variable called boxes as they do. How can I get the coordinates?</p>
",9393009,325,21-02-2018 20:36,21-02-2018 21:08,0,325,5,1,4,,"{'badge_counts': {'bronze': 5, 'silver': 4, 'gold': 1}, 'account_id': 12995058, 'is_employee': False, 'last_modified_date': 1573678394, 'last_access_date': 1536019842, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 325, 'creation_date': 1519243337, 'user_type': 'registered', 'user_id': 9393009, 'location': 'Seattle, WA, USA', 'link': 'https://stackoverflow.com/users/9393009/mandy', 'profile_image': 'https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=256', 'display_name': 'Mandy'}","I am new to both Python and Tensorflow. I am trying to run the object detection tutorial file from the Tensorflow Object Detection API, but I cannot find where I can get the coordinates of the bounding boxes when objects are detected. Relevant code: The place where I assume bounding boxes are drawn is like this: I tried printing but I am not sure what the numbers mean. There are a lot. I found answers for similar questions, but I don't have a variable called boxes as they do. How can I get the coordinates?"," # The following processing is only for single image
 detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
 detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
  # Visualization of the results of detection.
 vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
 plt.figure(figsize=IMAGE_SIZE)
 plt.imshow(image_np)
 output_dict['detection_boxes'] array([[ 0.56213236,  0.2780568 ,  0.91445708,  0.69120586],
       [ 0.56261235,  0.86368728,  0.59286624,  0.8893863 ],
       [ 0.57073039,  0.87096912,  0.61292225,  0.90354401],
       [ 0.51422435,  0.78449738,  0.53994244,  0.79437423],
......

       [ 0.32784131,  0.5461576 ,  0.36972913,  0.56903434],
       [ 0.03005961,  0.02714229,  0.47211722,  0.44683522],
       [ 0.43143299, 0.09211366,  0.58121657,  0.3509962 ]], dtype=float32)
",20,33,0,1,
318,48743556,48743678,11569,python pandas percent change with columns of dataframe,2,<python><pandas><dataframe>,11,"<p>I just started studying pandas and have questions.
Firstly, I'd like to ask this.</p>

<p>I have dataframe and it's like below.</p>

<pre><code>  Date        Open        High         Low       Close    
2015-11-02  711.059998  721.619995  705.849976  721.109985   
2015-11-03  718.859985  724.650024  714.719971  722.159973   
2015-11-04  722.000000  733.099976  721.900024  728.109985   
2015-11-05  729.469971  739.479980  729.469971  731.250000   
2015-11-06  731.500000  735.409973  727.010010  733.760010   
</code></pre>

<p>I know</p>

<pre><code>df[""Close""].pct_change() 
</code></pre>

<p>make the percent change from Close to Close.</p>

<p>But,
I want to add a new column, ""CloseToOpen"" which is a percent change of ""yesterday Close to today Open"".</p>

<p>So, it is ""Open(Day 0) / Close(Day -1) -1"".
Of course, the first row should be ""NaN"" or Zero because there's no ""previous day's Close"".</p>

<p>How can I make this with python pandas code??</p>

<p>Thanks guys!</p>

<p>This is what I want.</p>

<pre><code>Date        Open        High        Low         Close       CloseToOpen
2015-11-02  711.059998  721.619995  705.849976  721.109985  0.000000
2015-11-03  718.859985  724.650024  714.719971  722.159973  -0.003120
2015-11-04  722.000000  733.099976  721.900024  728.109985  -0.000222
2015-11-05  729.469971  739.479980  729.469971  731.250000  0.001868
2015-11-06  731.500000  735.409973  727.010010  733.760010  0.000342
</code></pre>
",9316004,181,12-02-2018 09:51,12-02-2018 09:58,0,181,8,1,1,,"{'badge_counts': {'bronze': 8, 'silver': 1, 'gold': 1}, 'account_id': 12880954, 'is_employee': False, 'last_modified_date': 1573678407, 'last_access_date': 1708492647, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 181, 'creation_date': 1517825263, 'user_type': 'registered', 'user_id': 9316004, 'location': 'Korea', 'link': 'https://stackoverflow.com/users/9316004/b-a', 'profile_image': 'https://www.gravatar.com/avatar/9779b879dc093fdbc5061e4c21e462bd?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'B.A.'}","I just started studying pandas and have questions. Firstly, I'd like to ask this. I have dataframe and it's like below. I know make the percent change from Close to Close. But, I want to add a new column, ""CloseToOpen"" which is a percent change of ""yesterday Close to today Open"". So, it is ""Open(Day 0) / Close(Day -1) -1"". Of course, the first row should be ""NaN"" or Zero because there's no ""previous day's Close"". How can I make this with python pandas code?? Thanks guys! This is what I want.","  Date        Open        High         Low       Close    
2015-11-02  711.059998  721.619995  705.849976  721.109985   
2015-11-03  718.859985  724.650024  714.719971  722.159973   
2015-11-04  722.000000  733.099976  721.900024  728.109985   
2015-11-05  729.469971  739.479980  729.469971  731.250000   
2015-11-06  731.500000  735.409973  727.010010  733.760010   
 df[""Close""].pct_change() 
 Date        Open        High        Low         Close       CloseToOpen
2015-11-02  711.059998  721.619995  705.849976  721.109985  0.000000
2015-11-03  718.859985  724.650024  714.719971  722.159973  -0.003120
2015-11-04  722.000000  733.099976  721.900024  728.109985  -0.000222
2015-11-05  729.469971  739.479980  729.469971  731.250000  0.001868
2015-11-06  731.500000  735.409973  727.010010  733.760010  0.000342
",10,39,0,0,
319,48633951,48634041,1295,Why does chained assignment work this way?,2,<python><python-3.x><python-2.7>,15,"<p>I found the assignment <code>a = a[1:] = [2]</code> in an article.  I tried it in python3 and python2; it all works, but I don't understand <em>how</em> it works.  <code>=</code> here is not like in C; C processes <code>=</code> by right to left.  How does python process the <code>=</code> operator?</p>
",9319403,1285,06-02-2018 01:06,06-02-2018 01:17,0,1285,21,1,11,,"{'badge_counts': {'bronze': 21, 'silver': 11, 'gold': 1}, 'account_id': 12885913, 'is_employee': False, 'last_modified_date': 1607614407, 'last_access_date': 1710834922, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1285, 'creation_date': 1517876737, 'user_type': 'registered', 'user_id': 9319403, 'location': 'zhejang univeersity', 'website_url': '', 'link': 'https://stackoverflow.com/users/9319403/peter-zhang', 'profile_image': 'https://www.gravatar.com/avatar/f95c8269f7242720c9f1e2c69795799a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'peter zhang'}","I found the assignment in an article. I tried it in python3 and python2; it all works, but I don't understand how it works. here is not like in C; C processes by right to left. How does python process the operator?",a = a[1:] = [2] = = =,-4,1,0,0,
320,48810726,48810951,39631,Concatenate pandas DataFrames generated with a loop,3,<python><pandas><loops><dataframe><append>,15,"<p>I am creating a new DataFrame named <strong>data_day</strong>, containing new features, for each day extrapolated from the day-timestamp of a previous DataFrame <strong>df</strong>.</p>

<p>My new dataframes <strong>data_day</strong> are 30 independent DataFrames that I need to concatenate/append at the end in a unic dataframe (final_data_day). </p>

<p>The for loop for each day is defined as follow:</p>

<pre><code>num_days=len(list_day)

#list_day= random.sample(list_day,num_days_to_simulate)
data_frame = pd.DataFrame()

for i, day in enumerate(list_day):

    print('*** ',day,' ***')

    data_day=df[df.day==day]
    .....................
    final_data_day = pd.concat()
</code></pre>

<p>Hope I was clear. Mine is basically a problem of append/concatenation of data-frames generated in a non-trivial for loop</p>
",9327426,470,15-02-2018 15:27,15-02-2018 15:38,0,470,18,2,6,80,"{'badge_counts': {'bronze': 18, 'silver': 6, 'gold': 2}, 'collectives': [{'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 9430768, 'is_employee': False, 'last_modified_date': 1698459000, 'last_access_date': 1692200647, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 470, 'creation_date': 1518005673, 'user_type': 'registered', 'user_id': 9327426, 'accept_rate': 80, 'location': 'London, UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/9327426/annalix', 'profile_image': 'https://www.gravatar.com/avatar/e76aee47b047441d886e4f89617f8340?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Annalix'}","I am creating a new DataFrame named data_day, containing new features, for each day extrapolated from the day-timestamp of a previous DataFrame df. My new dataframes data_day are 30 independent DataFrames that I need to concatenate/append at the end in a unic dataframe (final_data_day). The for loop for each day is defined as follow: Hope I was clear. Mine is basically a problem of append/concatenation of data-frames generated in a non-trivial for loop","num_days=len(list_day)

#list_day= random.sample(list_day,num_days_to_simulate)
data_frame = pd.DataFrame()

for i, day in enumerate(list_day):

    print('*** ',day,' ***')

    data_day=df[df.day==day]
    .....................
    final_data_day = pd.concat()
",11,21,0,0,
321,48719776,48719922,32403,django-rest-framwork got AttributeError when attempting to get a value for field,4,<python><django><django-rest-framework>,17,"<p>I want to get all prodcut table values with join product_ratings table. I did somthing like this but this code give me <code>AttributeError</code>. So I did <code>product_ratings = ProductRatingSerializer(many=True)</code>
in product serializer and used this value in the field, but it's not working:</p>
<p>The full error message:</p>
<pre><code>Got AttributeError when attempting to get a value for field `product_ratings` on serializer `ProductSerializer`.
The serializer field might be named incorrectly and not match any attribute or key on the `Product` instance.
Original exception text was: 'Product' object has no attribute 'product_ratings'.
</code></pre>
<p>view :</p>
<pre><code>class StoreApiView(mixins.CreateModelMixin, generics.ListAPIView):
    lookup_field = 'pk'
    serializer_class = ProductSerializer

    def get_queryset(self):
        qs = Product.objects.all()
        query = self.request.GET.get('q')
        if query is not None:
            qs = qs.filter(
                Q(title__icontains=query) |
                Q(description__icontains=query)
            ).distinct()
        return qs
</code></pre>
<p>its serializer classes:</p>
<pre><code>class ProductRatingSerializer(ModelSerializer):
    class Meta:
        model = Product_ratings
        fields = [
            'p_id',
            'commenter',
            'comment',
            'rating',
            'created_date',
        ]
        read_only_fields = ['p_id']


class ProductSerializer(ModelSerializer):
    product_ratings = ProductRatingSerializer(many=True)
    author = serializers.SerializerMethodField()

    def get_author(self, obj):
        return obj.author.first_name
    class Meta:
        model = Product
        fields = [
            'product_id',
            'author',
            'category',
            'title',
            'description',
            'filepath',
            'price',
            'created_date',
            'updated_date',
            'product_ratings',
        ]
        read_only_fields = ['product_id', 'created_date', 'updated_date', 'author']
</code></pre>
<p>related model class :</p>
<pre><code>class Product(models.Model):
    product_id = models.AutoField(primary_key=True)
    author = models.ForeignKey(User, on_delete=models.CASCADE, db_index=True)
    category = models.ForeignKey(Category, on_delete=models.CASCADE, to_field='cat_id')
    title = models.CharField(max_length=120)
    description = models.TextField(null=True, blank=True)
    price = models.CharField(max_length=50, null=True, blank=True)
    filepath = models.CharField(max_length=100, null=True, blank=True)
    created_date = models.DateTimeField(auto_now_add=True)
    updated_date = models.DateTimeField(auto_now=True)

class Product_ratings(models.Model):
    p_id = models.ForeignKey(Product, on_delete=models.CASCADE, to_field='product_id')
    commenter = models.ForeignKey(User, on_delete=models.CASCADE)
    comment = models.CharField(max_length=200, null=True, blank=True)
    rating = models.IntegerField(null=True, blank=True)
    created_date = models.DateTimeField(auto_now_add=True)
</code></pre>
",9342041,1211,10-02-2018 10:34,10-02-2018 10:49,0,1211,46,4,20,78,"{'badge_counts': {'bronze': 46, 'silver': 20, 'gold': 4}, 'collectives': [{'collective': {'tags': ['php'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where developers working with PHP can learn and connect about the open source scripting language.', 'link': '/collectives/php', 'name': 'PHP', 'slug': 'php'}, 'role': 'member'}], 'account_id': 8973941, 'is_employee': False, 'last_modified_date': 1703313901, 'last_access_date': 1701192790, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1211, 'creation_date': 1518258286, 'user_type': 'registered', 'user_id': 9342041, 'accept_rate': 78, 'location': 'Iran', 'website_url': '', 'link': 'https://stackoverflow.com/users/9342041/devmrh', 'profile_image': 'https://www.gravatar.com/avatar/a26a0b3b7aec1b171e1573882f6d98e2?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'devmrh'}","I want to get all prodcut table values with join product_ratings table. I did somthing like this but this code give me . So I did in product serializer and used this value in the field, but it's not working: The full error message: view : its serializer classes: related model class :","AttributeError product_ratings = ProductRatingSerializer(many=True) Got AttributeError when attempting to get a value for field `product_ratings` on serializer `ProductSerializer`.
The serializer field might be named incorrectly and not match any attribute or key on the `Product` instance.
Original exception text was: 'Product' object has no attribute 'product_ratings'.
 class StoreApiView(mixins.CreateModelMixin, generics.ListAPIView):
    lookup_field = 'pk'
    serializer_class = ProductSerializer

    def get_queryset(self):
        qs = Product.objects.all()
        query = self.request.GET.get('q')
        if query is not None:
            qs = qs.filter(
                Q(title__icontains=query) |
                Q(description__icontains=query)
            ).distinct()
        return qs
 class ProductRatingSerializer(ModelSerializer):
    class Meta:
        model = Product_ratings
        fields = [
            'p_id',
            'commenter',
            'comment',
            'rating',
            'created_date',
        ]
        read_only_fields = ['p_id']


class ProductSerializer(ModelSerializer):
    product_ratings = ProductRatingSerializer(many=True)
    author = serializers.SerializerMethodField()

    def get_author(self, obj):
        return obj.author.first_name
    class Meta:
        model = Product
        fields = [
            'product_id',
            'author',
            'category',
            'title',
            'description',
            'filepath',
            'price',
            'created_date',
            'updated_date',
            'product_ratings',
        ]
        read_only_fields = ['product_id', 'created_date', 'updated_date', 'author']
 class Product(models.Model):
    product_id = models.AutoField(primary_key=True)
    author = models.ForeignKey(User, on_delete=models.CASCADE, db_index=True)
    category = models.ForeignKey(Category, on_delete=models.CASCADE, to_field='cat_id')
    title = models.CharField(max_length=120)
    description = models.TextField(null=True, blank=True)
    price = models.CharField(max_length=50, null=True, blank=True)
    filepath = models.CharField(max_length=100, null=True, blank=True)
    created_date = models.DateTimeField(auto_now_add=True)
    updated_date = models.DateTimeField(auto_now=True)

class Product_ratings(models.Model):
    p_id = models.ForeignKey(Product, on_delete=models.CASCADE, to_field='product_id')
    commenter = models.ForeignKey(User, on_delete=models.CASCADE)
    comment = models.CharField(max_length=200, null=True, blank=True)
    rating = models.IntegerField(null=True, blank=True)
    created_date = models.DateTimeField(auto_now_add=True)
",61,77,0,0,
322,48789686,48808459,4495,How to make cerberus required rule depends on condition,1,<python><json><validation><cerberus>,13,"<p>I have a big json document, where some fields should be required if other fields have exact values. E.g.</p>

<pre><code>document = {'is_realty_address': False, 'postcode': 111111}
</code></pre>

<p>postcode have to be required if is_realty_address == False.
All of the rules (except 'required') are applied to the fields that are exist in document, so my custom rules are silent, when I have</p>

<pre><code>document = {'is_realty_address': False}
</code></pre>

<p>of-rules won't help in my case, because I have plenty of ""conditional-required"" fields which depends on many different fields. So, of-rules will strongly complicate my schema.
Dependencies don't work too. I tried:</p>

<pre><code>{'postcode': {'dependencies': {'is_realty_address': False}, 'required': True}}
</code></pre>

<p>This returns error if postcode not appears in document, no matter what value has is_realty_address</p>

<pre><code>v = Validator()
print(v.validate({'is_realty_address': False}, schema))
print(v.errors)

print(v.validate({'is_realty_address': True}, schema))
print(v.errors)
</code></pre>

<p>this code returns:</p>

<pre><code>False
{'postcode': ['required field']}
False
{'postcode': ['required field']}
</code></pre>

<p>I also tried to implement validation method:</p>

<pre><code>def _validate_conditional_required(self, conditional_required, field, value):
    """"""
    :param conditional_required:
    :param field:
    :param value:
    :return:
    The rule's arguments are validated against this schema:
    {'type': 'dict'}
    """"""
    for conditional_field, conditional_value in conditional_required.items():
        if self.document[conditional_field] == conditional_value and field not in self.document:
            self._error(field, errors.REQUIRED_FIELD)
</code></pre>

<p>with schema</p>

<pre><code>schema = {
    'is_realty_address': {'required': True, 'type': 'boolean'},
    'postcode': {'conditional_required': {'is_realty_address': False}},
}
</code></pre>

<p>but this rule doesn't run if 'postcode' is not in the document.</p>

<p>Is there any way to set ""conditional-required"" rule?
I want to see this code:</p>

<pre><code>schema = {
    'is_realty_address': {'required': True, 'type': 'boolean'},
    'postcode': {'conditional_required': {'is_realty_address': False}},
}
v = Validator()
print(v.validate({'is_realty_address': False}, schema))
print(v.errors)

print(v.validate({'is_realty_address': True}, schema))
print(v.errors)
</code></pre>

<p>returns:</p>

<pre><code>True

False
{'postcode': ['required field']}
</code></pre>
",9349707,251,14-02-2018 14:33,15-02-2018 13:28,1,251,9,0,2,,"{'badge_counts': {'bronze': 9, 'silver': 2, 'gold': 0}, 'account_id': 12930551, 'is_employee': False, 'last_modified_date': 1573678401, 'last_access_date': 1621863547, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 251, 'creation_date': 1518435984, 'user_type': 'registered', 'user_id': 9349707, 'location': 'Moscow, Россия', 'link': 'https://stackoverflow.com/users/9349707/dmitry-bozhenko', 'profile_image': 'https://www.gravatar.com/avatar/9367d7862fa43b1c38c22b4ff8c0f330?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Dmitry Bozhenko'}","I have a big json document, where some fields should be required if other fields have exact values. E.g. postcode have to be required if is_realty_address == False. All of the rules (except 'required') are applied to the fields that are exist in document, so my custom rules are silent, when I have of-rules won't help in my case, because I have plenty of ""conditional-required"" fields which depends on many different fields. So, of-rules will strongly complicate my schema. Dependencies don't work too. I tried: This returns error if postcode not appears in document, no matter what value has is_realty_address this code returns: I also tried to implement validation method: with schema but this rule doesn't run if 'postcode' is not in the document. Is there any way to set ""conditional-required"" rule? I want to see this code: returns:","document = {'is_realty_address': False, 'postcode': 111111}
 document = {'is_realty_address': False}
 {'postcode': {'dependencies': {'is_realty_address': False}, 'required': True}}
 v = Validator()
print(v.validate({'is_realty_address': False}, schema))
print(v.errors)

print(v.validate({'is_realty_address': True}, schema))
print(v.errors)
 False
{'postcode': ['required field']}
False
{'postcode': ['required field']}
 def _validate_conditional_required(self, conditional_required, field, value):
    """"""
    :param conditional_required:
    :param field:
    :param value:
    :return:
    The rule's arguments are validated against this schema:
    {'type': 'dict'}
    """"""
    for conditional_field, conditional_value in conditional_required.items():
        if self.document[conditional_field] == conditional_value and field not in self.document:
            self._error(field, errors.REQUIRED_FIELD)
 schema = {
    'is_realty_address': {'required': True, 'type': 'boolean'},
    'postcode': {'conditional_required': {'is_realty_address': False}},
}
 schema = {
    'is_realty_address': {'required': True, 'type': 'boolean'},
    'postcode': {'conditional_required': {'is_realty_address': False}},
}
v = Validator()
print(v.validate({'is_realty_address': False}, schema))
print(v.errors)

print(v.validate({'is_realty_address': True}, schema))
print(v.errors)
 True

False
{'postcode': ['required field']}
",34,83,0,0,
323,48771502,48772174,35613,Is there a way to stack two tensorflow datasets?,3,<python><python-3.x><numpy><tensorflow><tensorflow-datasets>,22,"<p>I want to stack two datasets objects in Tensorflow (rbind function in R). I have created one dataset A from tfRecord files and one dataset B from numpy arrays. Both have same variables. Do you know if there is a way to stack these two datasets to create a bigger one ? Or to create an iterrator that will randomly read data from this two sources ?</p>

<p>Thanks</p>
",9355986,231,13-02-2018 16:37,13-02-2018 17:14,0,231,5,1,2,,"{'badge_counts': {'bronze': 5, 'silver': 2, 'gold': 1}, 'account_id': 12939826, 'is_employee': False, 'last_modified_date': 1573678400, 'last_access_date': 1528576079, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 231, 'creation_date': 1518539307, 'user_type': 'registered', 'user_id': 9355986, 'website_url': '', 'link': 'https://stackoverflow.com/users/9355986/kent930', 'profile_image': 'https://i.stack.imgur.com/mhtX4.jpg?s=256&g=1', 'display_name': 'Kent930'}",I want to stack two datasets objects in Tensorflow (rbind function in R). I have created one dataset A from tfRecord files and one dataset B from numpy arrays. Both have same variables. Do you know if there is a way to stack these two datasets to create a bigger one ? Or to create an iterrator that will randomly read data from this two sources ? Thanks,,0,3,0,0,
324,49373825,49374152,37575,kombu.exceptions.EncodeError: User is not JSON serializable,5,<python><json><django><celery><celery-task>,26,"<p>I have django 1.11.5 app with celery 4.1.0 and I recived all the time:</p>

<pre><code>kombu.exceptions.EncodeError: &lt;User: testuser&gt; is not JSON serializable
</code></pre>

<p>my settings.py:</p>

<pre><code>CELERY_BROKER_URL = 'amqp://localhost'
CELERY_RESULT_BACKEND = 'amqp://localhost'
CELERY_ACCEPT_CONTENT = ['application/json']
CELERY_RESULT_SERIALIZER = 'json'
CELERY_TASK_SERIALIZER = 'json'
CELERY_TIMEZONE = 'Asia/Makassar'
CELERY_BEAT_SCHEDULE = {}
</code></pre>

<p>tasks.py</p>

<pre><code>from __future__ import absolute_import, unicode_literals
from celery import task
from django.contrib.auth.models import User


@task(serializer='json')
def task_number_one():
    user = User.objects.create(username=""testuser"", email=""test@test.com"", password=""pass"")
    return user
</code></pre>

<p>I call task in the view:</p>

<pre><code>def form_valid(self, form):
    form.instance.user = self.request.user
    task_number_one.delay()
    return super().form_valid(form)
</code></pre>
",9357277,283,19-03-2018 23:07,19-03-2018 23:43,0,283,5,1,3,,"{'badge_counts': {'bronze': 5, 'silver': 3, 'gold': 1}, 'account_id': 12941638, 'is_employee': False, 'last_modified_date': 1573678400, 'last_access_date': 1522265784, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 283, 'creation_date': 1518559023, 'user_type': 'registered', 'user_id': 9357277, 'link': 'https://stackoverflow.com/users/9357277/user9357277', 'profile_image': 'https://www.gravatar.com/avatar/45875434146d76241524ea0e0a8b542e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user9357277'}",I have django 1.11.5 app with celery 4.1.0 and I recived all the time: my settings.py: tasks.py I call task in the view:,"kombu.exceptions.EncodeError: &lt;User: testuser&gt; is not JSON serializable
 CELERY_BROKER_URL = 'amqp://localhost'
CELERY_RESULT_BACKEND = 'amqp://localhost'
CELERY_ACCEPT_CONTENT = ['application/json']
CELERY_RESULT_SERIALIZER = 'json'
CELERY_TASK_SERIALIZER = 'json'
CELERY_TIMEZONE = 'Asia/Makassar'
CELERY_BEAT_SCHEDULE = {}
 from __future__ import absolute_import, unicode_literals
from celery import task
from django.contrib.auth.models import User


@task(serializer='json')
def task_number_one():
    user = User.objects.create(username=""testuser"", email=""test@test.com"", password=""pass"")
    return user
 def form_valid(self, form):
    form.instance.user = self.request.user
    task_number_one.delay()
    return super().form_valid(form)
",17,36,0,0,
325,49354232,49354406,69341,How to stream audio from a Youtube URL in Python (without download)?,4,<python><url><audio><youtube><stream>,13,"<p>I have been trying to create a way to stream a youtube url (preferably audio only, although this doesn't matter too much) right from python code. I have tried numerous things but none really seem to work. So far, I am able to search for videos or playlists using youtube data api, grab the first video or playlist and pass it into pafy to get different streaming urls. Does anyone know of a way to play youtube audio/video through python without downloading the video first? I would think it is possible with a cmd line tool such as mplayer or vlc using the sub process to pop open a cmd for the cmd line and pass in the url, but I am stuck. Any help is needed. Please! Here is my following code:</p>

<pre><code>import argparse
import pafy
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

DEVELOPER_KEY = 'DEVELOPER KEY'
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'

def pafy_video(video_id):
    url = 'https://www.youtube.com/watch?v={0}'.format(video_id)
    vid = pafy.new(url)

def pafy_playlist(playlist_id)
    url = ""https://www.youtube.com/playlist?list={0}"".format(playlist_id)
    playlist = pafy.get_playlist(url)


def youtube_search(options):
  youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)

  search_response = youtube.search().list(
    q='Hello world',
    part='id,snippet',
    maxResults=options.max_results
  ).execute()

  videos = []
  playlists = []
  channels = []
  for search_result in search_response.get('items', []):
    if search_result['id']['kind'] == 'youtube#video':
      videos.append('%s' % (search_result['id']['videoId']))
    elif search_result['id']['kind'] == 'youtube#channel':
      channels.append('%s' % (search_result['id']['channelId']))
    elif search_result['id']['kind'] == 'youtube#playlist':
      playlists.append('%s' % (search_result['id']['playlistId']))


  if videos:
    print('Videos:{0}'.format(videos))
    pafy_video(videos[0])
  elif playlists:
    print('Playlists:{0}'.format(playlists))
    pafy_video(playlists[0])

  #https://www.youtube.com/watch?v=rOU4YiuaxAM
  #url = 'https://www.youtube.com/watch?v={0}'.format(videos[0])
  #print(url)

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--q', help='Search term', default='Google')
  parser.add_argument('--max-results', help='Max results', default=3)
  args = parser.parse_args()
  youtube_search(args)
</code></pre>

<p>Tldr; I would like to stream a youtube video (using the url or id) straight from python code without downloading the video first</p>

<p>Thank you!</p>
",9137080,590,19-03-2018 00:28,19-03-2018 00:59,0,590,23,3,10,,"{'badge_counts': {'bronze': 23, 'silver': 10, 'gold': 3}, 'account_id': 12558266, 'is_employee': False, 'last_modified_date': 1660419900, 'last_access_date': 1710603003, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 590, 'creation_date': 1514141056, 'user_type': 'registered', 'user_id': 9137080, 'location': 'Connecticut, United States', 'link': 'https://stackoverflow.com/users/9137080/nick-d', 'profile_image': 'https://www.gravatar.com/avatar/79f0b31ab27b821fae754494e4973575?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nick D'}","I have been trying to create a way to stream a youtube url (preferably audio only, although this doesn't matter too much) right from python code. I have tried numerous things but none really seem to work. So far, I am able to search for videos or playlists using youtube data api, grab the first video or playlist and pass it into pafy to get different streaming urls. Does anyone know of a way to play youtube audio/video through python without downloading the video first? I would think it is possible with a cmd line tool such as mplayer or vlc using the sub process to pop open a cmd for the cmd line and pass in the url, but I am stuck. Any help is needed. Please! Here is my following code: Tldr; I would like to stream a youtube video (using the url or id) straight from python code without downloading the video first Thank you!","import argparse
import pafy
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

DEVELOPER_KEY = 'DEVELOPER KEY'
YOUTUBE_API_SERVICE_NAME = 'youtube'
YOUTUBE_API_VERSION = 'v3'

def pafy_video(video_id):
    url = 'https://www.youtube.com/watch?v={0}'.format(video_id)
    vid = pafy.new(url)

def pafy_playlist(playlist_id)
    url = ""https://www.youtube.com/playlist?list={0}"".format(playlist_id)
    playlist = pafy.get_playlist(url)


def youtube_search(options):
  youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)

  search_response = youtube.search().list(
    q='Hello world',
    part='id,snippet',
    maxResults=options.max_results
  ).execute()

  videos = []
  playlists = []
  channels = []
  for search_result in search_response.get('items', []):
    if search_result['id']['kind'] == 'youtube#video':
      videos.append('%s' % (search_result['id']['videoId']))
    elif search_result['id']['kind'] == 'youtube#channel':
      channels.append('%s' % (search_result['id']['channelId']))
    elif search_result['id']['kind'] == 'youtube#playlist':
      playlists.append('%s' % (search_result['id']['playlistId']))


  if videos:
    print('Videos:{0}'.format(videos))
    pafy_video(videos[0])
  elif playlists:
    print('Playlists:{0}'.format(playlists))
    pafy_video(playlists[0])

  #https://www.youtube.com/watch?v=rOU4YiuaxAM
  #url = 'https://www.youtube.com/watch?v={0}'.format(videos[0])
  #print(url)

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--q', help='Search term', default='Google')
  parser.add_argument('--max-results', help='Max results', default=3)
  args = parser.parse_args()
  youtube_search(args)
",55,63,0,0,
326,49907455,49916688,107632,Hide Code when exporting Jupyter notebook to HTML,12,<python><jupyter>,93,"<p>I'm looking for a way to hide code cells (inputs) when export my .iipynb file to a HTML. I don't want the code cells to be visible at all (not some button that turn them off/on). The output is for people that have no idea what a programming language is. I tried many things that I found on the internet but nothing seems to work. </p>

<p>Thanks</p>
",9160548,1991,18-04-2018 19:18,19-04-2018 08:49,1,1991,21,2,13,,"{'badge_counts': {'bronze': 21, 'silver': 13, 'gold': 2}, 'account_id': 12595894, 'is_employee': False, 'last_modified_date': 1607614411, 'last_access_date': 1686061818, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1991, 'creation_date': 1514811332, 'user_type': 'registered', 'user_id': 9160548, 'location': 'Paris, France', 'website_url': 'https://ahmedlahloum.github.io/', 'link': 'https://stackoverflow.com/users/9160548/ahmed-lahlou-mimi', 'profile_image': 'https://i.stack.imgur.com/lfVvT.jpg?s=256&g=1', 'display_name': 'Ahmed Lahlou Mimi'}",I'm looking for a way to hide code cells (inputs) when export my .iipynb file to a HTML. I don't want the code cells to be visible at all (not some button that turn them off/on). The output is for people that have no idea what a programming language is. I tried many things that I found on the internet but nothing seems to work. Thanks,,0,3,0,0,
327,48198021,48198034,122128,Filter pandas dataframe with specific column names in python,3,<python><pandas><dataframe>,29,"<p>I have a pandas dataframe and a list as follows</p>

<pre><code>mylist = ['nnn', 'mmm', 'yyy']
mydata =
   xxx   yyy zzz nnn ddd mmm
0  0  10      5    5   5  5
1  1   9      2    3   4  4
2  2   8      8    7   9  0
</code></pre>

<p>Now, I want to get only the columns mentioned in <code>mylist</code> and save it as a csv file.</p>

<p>i.e.</p>

<pre><code>     yyy  nnn   mmm
0    10     5     5
1    9      3     4
2    8      7     0
</code></pre>

<p>My current code is as follows.</p>

<pre><code>mydata = pd.read_csv( input_file, header=0)

for item in mylist:
    mydata_new = mydata[item]

print(mydata_new)
mydata_new.to_csv(file_name)
</code></pre>

<p>It seems to me that my new dataframe produces wrong results.Where I am making it wrong? Please help me!</p>
",9176239,1003,11-01-2018 00:04,11-01-2018 00:06,0,1003,25,2,12,100,"{'badge_counts': {'bronze': 25, 'silver': 12, 'gold': 2}, 'account_id': 12620593, 'is_employee': False, 'last_modified_date': 1607614411, 'last_access_date': 1544534154, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1003, 'creation_date': 1515135254, 'user_type': 'registered', 'user_id': 9176239, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/9176239/j-cena', 'profile_image': 'https://www.gravatar.com/avatar/1e8a221692a42a7799f7129a5eed0ef1?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'J Cena'}","I have a pandas dataframe and a list as follows Now, I want to get only the columns mentioned in and save it as a csv file. i.e. My current code is as follows. It seems to me that my new dataframe produces wrong results.Where I am making it wrong? Please help me!","mylist = ['nnn', 'mmm', 'yyy']
mydata =
   xxx   yyy zzz nnn ddd mmm
0  0  10      5    5   5  5
1  1   9      2    3   4  4
2  2   8      8    7   9  0
 mylist      yyy  nnn   mmm
0    10     5     5
1    9      3     4
2    8      7     0
 mydata = pd.read_csv( input_file, header=0)

for item in mylist:
    mydata_new = mydata[item]

print(mydata_new)
mydata_new.to_csv(file_name)
",13,32,0,0,
328,49414841,49414907,217238,Process finished with exit code 139 (interrupted by signal 11: SIGSEGV),21,<python><python-3.5><segmentation-fault><linux-mint>,81,"<p>I'm trying to execute a <a href=""https://pastebin.com/jSjn1aaD"" rel=""noreferrer"">Python script</a>, but I am getting the following error:</p>

<pre><code>Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
</code></pre>

<p>I'm using python 3.5.2 on a Linux Mint 18.1 Serena OS</p>

<p>Can someone tell me why this happens, and how can I solve?</p>
",9181998,1237,21-03-2018 19:14,21-03-2018 19:19,0,1237,18,2,11,,"{'badge_counts': {'bronze': 18, 'silver': 11, 'gold': 2}, 'account_id': 12630266, 'is_employee': False, 'last_modified_date': 1618675452, 'last_access_date': 1594296413, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1237, 'creation_date': 1515261181, 'user_type': 'registered', 'user_id': 9181998, 'location': 'Italia', 'website_url': '', 'link': 'https://stackoverflow.com/users/9181998/andre', 'profile_image': 'https://www.gravatar.com/avatar/175e591dbd5b40a7a27a1d63ed2c05ae?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Andre'}","I'm trying to execute a Python script, but I am getting the following error: I'm using python 3.5.2 on a Linux Mint 18.1 Serena OS Can someone tell me why this happens, and how can I solve?","Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
",0,8,0,1,
329,48864027,48864266,8871,How do I patch the `pathlib.Path.exists()` method?,1,<python><unit-testing><mocking><pathlib>,18,"<p>I want to patch the <code>exists()</code> method of a <code>pathlib.Path</code> object for a unit test but I have problems getting this to work.</p>

<p>What I am trying to do is this:</p>

<pre class=""lang-py prettyprint-override""><code>from unittest.mock import patch
from pathlib import Path

def test_move_basic():
    p = Path('~/test.py')
    with patch.object(p, 'exists') as mock_exists:
        mock_exists.return_value = True
</code></pre>

<p>But it fails with:</p>

<p><code>AttributeError: 'PosixPath' object attribute 'exists' is read-only</code>.</p>

<p>Any ideas?</p>
",300783,3186,19-02-2018 10:24,19-02-2018 10:38,0,3186,36,1,23,88,"{'badge_counts': {'bronze': 36, 'silver': 23, 'gold': 1}, 'account_id': 114366, 'is_employee': False, 'last_modified_date': 1710189900, 'last_access_date': 1711137266, 'reputation_change_year': 28, 'reputation_change_quarter': 28, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3186, 'creation_date': 1269432544, 'user_type': 'registered', 'user_id': 300783, 'accept_rate': 88, 'location': 'Germany', 'website_url': 'https://tfeldmann.de', 'link': 'https://stackoverflow.com/users/300783/tfeldmann', 'profile_image': 'https://www.gravatar.com/avatar/c4049f4a238bb5bb7f997edcc3bd59e3?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'tfeldmann'}",I want to patch the method of a object for a unit test but I have problems getting this to work. What I am trying to do is this: But it fails with: . Any ideas?,"exists() pathlib.Path from unittest.mock import patch
from pathlib import Path

def test_move_basic():
    p = Path('~/test.py')
    with patch.object(p, 'exists') as mock_exists:
        mock_exists.return_value = True
 AttributeError: 'PosixPath' object attribute 'exists' is read-only",3,18,0,0,
330,49097715,49097897,36800,"Module not found error in PyCharm, but it is installed as an Anaconda package",4,<python><pycharm><anaconda>,24,"<p>I have installed <a href=""https://en.wikipedia.org/wiki/Anaconda_(Python_distribution)"" rel=""nofollow noreferrer"">Anaconda</a> 3 and <a href=""https://en.wikipedia.org/wiki/JetBrains#IDEs"" rel=""nofollow noreferrer"">PyCharm Community Edition</a> after that. I am able to chose the interpreter to be a <a href=""https://en.wikipedia.org/wiki/Conda_(package_manager)"" rel=""nofollow noreferrer"">Conda</a> environment. But when I try using certain packages, such as <a href=""https://en.wikipedia.org/wiki/Matplotlib"" rel=""nofollow noreferrer"">Matplotlib</a>, it throws &quot;Module not found error&quot;. When I run pip, it returns saying that matplotlib <em>is</em> available.</p>
<pre class=""lang-none prettyprint-override""><code>pip install matplotlib
Requirement already satisfied: matplotlib in./anaconda3/lib/python3.6/site-packages
</code></pre>
<p>Clearly the package is there and for some reason it does not show up.</p>
",407198,2571,04-03-2018 16:40,04-03-2018 16:56,0,2581,38,2,29,100,"{'badge_counts': {'bronze': 38, 'silver': 29, 'gold': 2}, 'account_id': 176477, 'is_employee': False, 'last_modified_date': 1625315100, 'last_access_date': 1709744216, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2581, 'creation_date': 1280532014, 'user_type': 'registered', 'user_id': 407198, 'accept_rate': 100, 'location': 'London, United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/407198/ash', 'profile_image': 'https://i.stack.imgur.com/7qQ8T.jpg?s=256&g=1', 'display_name': 'Ash'}","I have installed Anaconda 3 and PyCharm Community Edition after that. I am able to chose the interpreter to be a Conda environment. But when I try using certain packages, such as Matplotlib, it throws &quot;Module not found error&quot;. When I run pip, it returns saying that matplotlib is available. Clearly the package is there and for some reason it does not show up.","pip install matplotlib
Requirement already satisfied: matplotlib in./anaconda3/lib/python3.6/site-packages
",1,5,0,4,
331,50170588,50208857,33304,List dependencies of Python wheel file,7,<python><python-wheel>,49,"<p>I have Python wheel file: <code>psutil-5.4.5-cp26-none-linux_x86_64.whl</code></p>

<p>How can I list the dependencies this wheel has?</p>
",633961,26880,04-05-2018 08:33,07-05-2018 07:12,3,27092,706,89,373,66,"{'badge_counts': {'bronze': 706, 'silver': 373, 'gold': 89}, 'account_id': 317335, 'is_employee': False, 'last_modified_date': 1709346000, 'last_access_date': 1711173143, 'reputation_change_year': 947, 'reputation_change_quarter': 947, 'reputation_change_month': 302, 'reputation_change_week': 80, 'reputation_change_day': 0, 'reputation': 27092, 'creation_date': 1298630183, 'user_type': 'registered', 'user_id': 633961, 'accept_rate': 66, 'location': 'Chemnitz, Germany', 'website_url': 'http://www.thomas-guettler.de/', 'link': 'https://stackoverflow.com/users/633961/guettli', 'profile_image': 'https://i.stack.imgur.com/j9Krc.jpg?s=256&g=1', 'display_name': 'guettli'}",I have Python wheel file: How can I list the dependencies this wheel has?,psutil-5.4.5-cp26-none-linux_x86_64.whl,-1,3,0,0,
332,49099637,49099726,14376,How to determine if an exception was raised once you're in the finally block?,6,<python><exception><logging><control-flow><try-finally>,47,"<p>Is it possible to tell if there was an exception once you're in the <code>finally</code> clause?  Something like:</p>

<pre><code>try:
    funky code
finally:
    if ???:
        print('the funky code raised')
</code></pre>

<p>I'm looking to make something like this more DRY:</p>

<pre><code>try:
    funky code
except HandleThis:
    # handle it
    raised = True
except DontHandleThis:
    raised = True
    raise
else:
    raised = False
finally:
    logger.info('funky code raised %s', raised)
</code></pre>

<p>I don't like that it requires to catch an exception, which you don't intend to handle, just to set a flag.</p>

<hr>

<p>Since some <a href=""https://stackoverflow.com/questions/49099637/how-to-determine-if-an-exception-was-raised-once-youre-in-the-finally-block#comment85224289_49099637"">comments</a> are asking for less ""M"" in the MCVE, here is some more background on the use-case.  The actual problem is about escalation of logging levels.  </p>

<ul>
<li>The funky code is third party and can't be changed.</li>
<li>The failure exception and stack trace does not contain any useful diagnostic information, so using <code>logger.exception</code> in an except block is not helpful here.</li>
<li>If the funky code raised then some information which I need to see has already been logged, at level DEBUG.  We do not and can not handle the error, but want to escalate the DEBUG logging because the information needed is in there.</li>
<li>The funky code does not raise, most of the time.  I don't want to escalate logging levels for the general case, because it is too verbose.  </li>
</ul>

<p>Hence, the code runs under a log capture context (which sets up custom handlers to intercept log records) and some debug info gets re-logged retrospectively:</p>

<pre><code>try:
    with LogCapture() as log:
        funky_code()  # &lt;-- third party badness
finally:
    # log events are buffered in memory. if there was an exception,
    # emit everything that was captured at a WARNING level
    for record in log.captured:
        if &lt;there was an exception&gt;:
            log_fn = mylogger.warning
        else:
            log_fn = getattr(mylogger, record.levelname.lower())
        log_fn(record.msg, record.args)
</code></pre>
",674039,348162,04-03-2018 19:42,04-03-2018 19:52,0,348698,767,105,631,94,"{'badge_counts': {'bronze': 767, 'silver': 631, 'gold': 105}, 'account_id': 342731, 'is_employee': False, 'last_modified_date': 1710872103, 'last_access_date': 1711143082, 'reputation_change_year': 4383, 'reputation_change_quarter': 4383, 'reputation_change_month': 1127, 'reputation_change_week': 227, 'reputation_change_day': 10, 'reputation': 348698, 'creation_date': 1300923627, 'user_type': 'registered', 'user_id': 674039, 'accept_rate': 94, 'location': 'ℂ&#120153;&#120154;&#120148;&#120146;&#120152;&#120160;, &#120128;&#120131;', 'website_url': 'http://www.wimglenn.com', 'link': 'https://stackoverflow.com/users/674039/wim', 'profile_image': 'https://i.stack.imgur.com/leoFi.gif?s=256&g=1', 'display_name': 'wim'}","Is it possible to tell if there was an exception once you're in the clause? Something like: I'm looking to make something like this more DRY: I don't like that it requires to catch an exception, which you don't intend to handle, just to set a flag. Since some comments are asking for less ""M"" in the MCVE, here is some more background on the use-case. The actual problem is about escalation of logging levels. The funky code is third party and can't be changed. The failure exception and stack trace does not contain any useful diagnostic information, so using in an except block is not helpful here. If the funky code raised then some information which I need to see has already been logged, at level DEBUG. We do not and can not handle the error, but want to escalate the DEBUG logging because the information needed is in there. The funky code does not raise, most of the time. I don't want to escalate logging levels for the general case, because it is too verbose. Hence, the code runs under a log capture context (which sets up custom handlers to intercept log records) and some debug info gets re-logged retrospectively:","finally try:
    funky code
finally:
    if ???:
        print('the funky code raised')
 try:
    funky code
except HandleThis:
    # handle it
    raised = True
except DontHandleThis:
    raised = True
    raise
else:
    raised = False
finally:
    logger.info('funky code raised %s', raised)
 logger.exception try:
    with LogCapture() as log:
        funky_code()  # &lt;-- third party badness
finally:
    # log events are buffered in memory. if there was an exception,
    # emit everything that was captured at a WARNING level
    for record in log.captured:
        if &lt;there was an exception&gt;:
            log_fn = mylogger.warning
        else:
            log_fn = getattr(mylogger, record.levelname.lower())
        log_fn(record.msg, record.args)
",24,53,0,1,
333,49000217,49001995,4021,Cross-argument validation in argparse,1,<python><argparse>,18,"<p>I'm looking for a Pythonic way to validate arguments when their validation logically depends on the value(s) parsed from other argument(s).  </p>

<p>Here's a simple example:</p>

<pre><code>parser.add_argument(
    '--animal', 
    choices=['raccoon', 'giraffe', 'snake'], 
    default='raccoon',
)
parser.add_argument(
    '--with-shoes', 
    action='store_true',
)
</code></pre>

<p>In this case, parsing this command should cause an error:</p>

<pre><code>my_script.py --animal snake --with-shoes
</code></pre>

<p>Adding a <a href=""https://docs.python.org/3/library/argparse.html#mutual-exclusion"" rel=""noreferrer"">mutually exclusive group</a> doesn't seem to help here, as the other combos are OK:</p>

<pre><code>my_script.py --animal raccoon --with-shoes
my_script.py --animal raccoon
my_script.py --animal snake
my_script.py --animal giraffe --with-shoes
my_script.py --animal giraffe
</code></pre>

<p>The validation error should ideally not be tied to <code>--animal</code> argument nor to <code>--with-shoes</code> argument, since the interface can not tell you <em>which</em> value needs to change here.  Each value is valid, yet they can't be used in combination.</p>

<p>We can do this with post-processing the <code>args</code> namespace, but I'm looking for a solution which would cause the <code>parser.parse_args()</code> call to fail, i.e. we actually fail <em>during</em> argument parsing, not afterwards.  </p>
",674039,348162,27-02-2018 02:33,27-02-2018 05:53,0,348698,767,105,631,94,"{'badge_counts': {'bronze': 767, 'silver': 631, 'gold': 105}, 'account_id': 342731, 'is_employee': False, 'last_modified_date': 1710872103, 'last_access_date': 1711143082, 'reputation_change_year': 4383, 'reputation_change_quarter': 4383, 'reputation_change_month': 1127, 'reputation_change_week': 227, 'reputation_change_day': 10, 'reputation': 348698, 'creation_date': 1300923627, 'user_type': 'registered', 'user_id': 674039, 'accept_rate': 94, 'location': 'ℂ&#120153;&#120154;&#120148;&#120146;&#120152;&#120160;, &#120128;&#120131;', 'website_url': 'http://www.wimglenn.com', 'link': 'https://stackoverflow.com/users/674039/wim', 'profile_image': 'https://i.stack.imgur.com/leoFi.gif?s=256&g=1', 'display_name': 'wim'}","I'm looking for a Pythonic way to validate arguments when their validation logically depends on the value(s) parsed from other argument(s). Here's a simple example: In this case, parsing this command should cause an error: Adding a mutually exclusive group doesn't seem to help here, as the other combos are OK: The validation error should ideally not be tied to argument nor to argument, since the interface can not tell you which value needs to change here. Each value is valid, yet they can't be used in combination. We can do this with post-processing the namespace, but I'm looking for a solution which would cause the call to fail, i.e. we actually fail during argument parsing, not afterwards.","parser.add_argument(
    '--animal', 
    choices=['raccoon', 'giraffe', 'snake'], 
    default='raccoon',
)
parser.add_argument(
    '--with-shoes', 
    action='store_true',
)
 my_script.py --animal snake --with-shoes
 my_script.py --animal raccoon --with-shoes
my_script.py --animal raccoon
my_script.py --animal snake
my_script.py --animal giraffe --with-shoes
my_script.py --animal giraffe
 --animal --with-shoes args parser.parse_args()",8,32,0,1,
334,48074094,48082204,21516,Use Sphinx autosummary recursively to generate API documentation,2,<python><jinja2><python-sphinx><restructuredtext><toctree>,24,"<p>I want to use Sphinx's <a href=""http://www.sphinx-doc.org/en/stable/ext/autosummary.html"" rel=""noreferrer"">autosummary extension</a> and <a href=""http://www.sphinx-doc.org/en/stable/templating.html"" rel=""noreferrer"">templates</a> to generate API docs recursively from docstrings. I want separate pages for each module, class, method, property and function. But it doesn't detect my templates at all. In fact, if I just remove the <code>module.rst</code> file from <code>_templates/autosummary/</code>, it renders the whole file exactly the same way as before. I've followed <a href=""https://stackoverflow.com/a/21665947/774273"">this SO question</a> to the letter. If you're interested, <a href=""https://github.com/nils-werner/sparse/tree/docs"" rel=""noreferrer"">the full repository is on GitHub</a>.</p>

<p>Edit: It seems it does generate a different file, I had to delete docs/_autosummary for it to read the new template. However, now it generates a file with the <code>sparse</code> header and <code>description</code> header. It doesn't go into the <code>{% if classes %}</code> and <code>{% if functions %}</code> directives.</p>

<p>My directory structure is as follows:</p>

<ul>
<li>sparse</li>
<li>docs

<ul>
<li>conf.py</li>
<li>index.rst</li>
<li>modules.rst</li>
<li>_templates/autosummary/module.rst</li>
</ul></li>
</ul>

<p>Here are the relevant files so far:</p>

<p><code>index.rst</code>:</p>

<pre><code>.. sparse documentation master file, created by
   sphinx-quickstart on Fri Dec 29 20:58:03 2017.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Welcome to sparse's documentation!
==================================

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   modules

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
</code></pre>

<p><code>modules.rst</code>:</p>

<pre><code>API Reference
=============

Modules
-------

.. autosummary::
   :toctree: _autosummary

   sparse
</code></pre>

<p><code>_templates/autosummary/module.rst</code>:</p>

<pre><code>{{ fullname | escape | underline }}

Description
-----------

.. automodule:: {{ fullname | escape }}

{% if classes %}
Classes
-------
.. autosummary:
    :toctree: _autosummary

    {% for class in classes %}
        {{ class }}
    {% endfor %}

{% endif %}

{% if functions %}
Functions
---------
.. autosummary:
    :toctree: _autosummary

    {% for function in functions %}
        {{ function }}
    {% endfor %}

{% endif %}
</code></pre>
",774273,1304,03-01-2018 08:57,03-01-2018 17:24,0,1304,35,1,12,69,"{'badge_counts': {'bronze': 35, 'silver': 12, 'gold': 1}, 'account_id': 405406, 'is_employee': False, 'last_modified_date': 1704806701, 'last_access_date': 1691755124, 'reputation_change_year': 2, 'reputation_change_quarter': 2, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1304, 'creation_date': 1306583035, 'user_type': 'registered', 'user_id': 774273, 'accept_rate': 69, 'location': 'Islamabad, Pakistan', 'website_url': 'https://hameerabbasi.github.io/', 'link': 'https://stackoverflow.com/users/774273/hameer-abbasi', 'profile_image': 'https://www.gravatar.com/avatar/09e8193f35628be825c37595484370da?s=256&d=identicon&r=PG', 'display_name': 'Hameer Abbasi'}","I want to use Sphinx's autosummary extension and templates to generate API docs recursively from docstrings. I want separate pages for each module, class, method, property and function. But it doesn't detect my templates at all. In fact, if I just remove the file from , it renders the whole file exactly the same way as before. I've followed this SO question to the letter. If you're interested, the full repository is on GitHub. Edit: It seems it does generate a different file, I had to delete docs/_autosummary for it to read the new template. However, now it generates a file with the header and header. It doesn't go into the and directives. My directory structure is as follows: sparse docs conf.py index.rst modules.rst _templates/autosummary/module.rst Here are the relevant files so far: : : :","module.rst _templates/autosummary/ sparse description {% if classes %} {% if functions %} index.rst .. sparse documentation master file, created by
   sphinx-quickstart on Fri Dec 29 20:58:03 2017.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Welcome to sparse's documentation!
==================================

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   modules

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
 modules.rst API Reference
=============

Modules
-------

.. autosummary::
   :toctree: _autosummary

   sparse
 _templates/autosummary/module.rst {{ fullname | escape | underline }}

Description
-----------

.. automodule:: {{ fullname | escape }}

{% if classes %}
Classes
-------
.. autosummary:
    :toctree: _autosummary

    {% for class in classes %}
        {{ class }}
    {% endfor %}

{% endif %}

{% if functions %}
Functions
---------
.. autosummary:
    :toctree: _autosummary

    {% for function in functions %}
        {{ function }}
    {% endfor %}

{% endif %}
",48,91,0,4,
335,48884766,48889369,14630,Pyinstaller on a setuptools package,5,<python><python-3.x><pyinstaller><setuptools><python-click>,18,"<p>I'm attempting to run PyInstaller on a CLI app I am building in Python using the <a href=""http://click.pocoo.org/6/"" rel=""noreferrer"">Click</a> library. I'm having trouble building the project using PyInstaller. PyInstaller has a document in their GitHub wiki titled <a href=""https://github.com/pyinstaller/pyinstaller/wiki/Recipe-Setuptools-Entry-Point"" rel=""noreferrer"">Recipe Setuptools Entry Point</a>, which gives information about how to use PyInstaller with a <code>setuptools</code> package, which I'm using for this project. However, it seems it cannot find the base module when I run <code>pyinstaller --onefile main.spec</code>.</p>

<p><strong>My question is: Is the problem simply an issue with the folder structure I have? Does the <a href=""https://github.com/pyinstaller/pyinstaller/wiki/Recipe-Setuptools-Entry-Point"" rel=""noreferrer"">Recipe Setuptools Entry Point</a> assume a certain file structure?</strong></p>

<p><em>Relevant information</em></p>

<p><strong>Pyinstaller output</strong></p>

<pre><code>184 INFO: PyInstaller: 3.3.1
184 INFO: Python: 3.6.4
189 INFO: Platform: Darwin-16.7.0-x86_64-i386-64bit
193 INFO: UPX is available.
Traceback (most recent call last):
  File ""/usr/local/bin/pyinstaller"", line 11, in &lt;module&gt;
    sys.exit(run())
  File ""/usr/local/lib/python3.6/site-packages/PyInstaller/__main__.py"", line 94, in run
    run_build(pyi_config, spec_file, **vars(args))
  File ""/usr/local/lib/python3.6/site-packages/PyInstaller/__main__.py"", line 46, in run_build
    PyInstaller.building.build_main.main(pyi_config, spec_file, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/PyInstaller/building/build_main.py"", line 791, in main
    build(specfile, kw.get('distpath'), kw.get('workpath'), kw.get('clean_build'))
  File ""/usr/local/lib/python3.6/site-packages/PyInstaller/building/build_main.py"", line 737, in build
    exec(text, spec_namespace)
  File ""&lt;string&gt;"", line 40, in &lt;module&gt;
  File ""&lt;string&gt;"", line 26, in Entrypoint
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 582, in get_entry_info
    return get_distribution(dist).get_entry_info(group, name)
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 564, in get_distribution
    dist = get_provider(dist)
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 436, in get_provider
    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 984, in require
    needed = self.resolve(parse_requirements(requirements))
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 870, in resolve
    raise DistributionNotFound(req, requirers)
pkg_resources.DistributionNotFound: The 'myapp' distribution was not found and is required by the application
</code></pre>

<p><strong>The <code>main.spec</code> file for <code>main.py</code>, which is the entrypoint for my CLI app:</strong></p>

<pre><code>block_cipher = None

def Entrypoint(dist, group, name,
               scripts=None, pathex=None, hiddenimports=None,
               hookspath=None, excludes=None, runtime_hooks=None):
    import pkg_resources

    # get toplevel packages of distribution from metadata
    def get_toplevel(dist):
        distribution = pkg_resources.get_distribution(dist)
        if distribution.has_metadata('top_level.txt'):
            return list(distribution.get_metadata('top_level.txt').split())
        else:
            return []

    hiddenimports = hiddenimports or []
    packages = []
    for distribution in hiddenimports:
        packages += get_toplevel(distribution)

    scripts = scripts or []
    pathex = pathex or []
    # get the entry point
    ep = pkg_resources.get_entry_info(dist, group, name)
    # insert path of the egg at the verify front of the search path
    pathex = [ep.dist.location] + pathex
    # script name must not be a valid module name to avoid name clashes on import
    script_path = os.path.join(workpath, name + '-script.py')
    print (""creating script for entry point"", dist, group, name)
    with open(script_path, 'w') as fh:
        print(""import"", ep.module_name, file=fh)
        print(""%s.%s()"" % (ep.module_name, '.'.join(ep.attrs)), file=fh)
        for package in packages:
            print (""import"", package, file=fh)

    return Analysis([script_path] + scripts, pathex, hiddenimports, hookspath, excludes, runtime_hooks)

a = Entrypoint('myapp', 'console_scripts', 'myapp')

pyz = PYZ(a.pure, a.zipped_data,
             cipher=block_cipher)
exe = EXE(pyz,
          a.scripts,
          exclude_binaries=True,
          name='main',
          debug=False,
          strip=False,
          upx=True,
          console=True )
coll = COLLECT(exe,
               a.binaries,
               a.zipfiles,
               a.datas,
               strip=False,
               upx=True,
               name='main')
</code></pre>

<p><strong>The contents of the <code>myapp</code> script generated when I run <code>pip3 install --editable .</code> in my virtual environment:</strong></p>

<pre><code>#!/some/path/to/myapp-cli/venv/bin/python3.6
# EASY-INSTALL-ENTRY-SCRIPT: 'myapp','console_scripts','myapp'
__requires__ = 'myapp'
import re
import sys
from pkg_resources import load_entry_point

if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
    sys.exit(
        load_entry_point('myapp', 'console_scripts', 'myapp')()
    )
</code></pre>

<p><strong>And finally, my repository structure:</strong></p>

<pre><code>myapp-cli/
├── README.md
├── myapp
│   ├── __init__.py
│   ├── main.py
│   ├── main.spec
│   ├── resources
│   │   ├── __init__.py
│   │   └── functions.py
│   ├── subcommands
│   │   ├── __init__.py
│   │   ├── config
│   │   │   ├── __init__.py
│   │   │   └── cli.py
│   │   ├── create
│   │   │   ├── __init__.py
│   │   │   └── cli.py
│   │   ├── destroy
│   │   │   ├── __init__.py
│   │   │   └── cli.py
│   │   └── switch
│   │       ├── __init__.py
│   │       └── cli.py
│   └── variables.py
├── requirements.txt
└── setup.py
</code></pre>

<p><strong>And my <code>setup.py</code> file:</strong></p>

<pre><code>from setuptools import find_packages
from setuptools import setup
import os

base_dir = os.path.dirname(__file__)

setup(
    entry_points = '''
        [console_scripts]
        myapp=myapp.main:entry_point
    ''',
    install_requires = [
        'packageone==1.0',
        'packagetwo==2.0',
    ],
    name = ""myapp"",
    packages=find_packages(),
    setup_requires=""setuptools"",
    version = ""0.1"",
)
</code></pre>
",928203,1573,20-02-2018 11:55,20-02-2018 16:01,0,1583,40,4,25,100,"{'badge_counts': {'bronze': 40, 'silver': 25, 'gold': 4}, 'account_id': 889674, 'is_employee': False, 'last_modified_date': 1695087300, 'last_access_date': 1711029813, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1583, 'creation_date': 1315193757, 'user_type': 'registered', 'user_id': 928203, 'accept_rate': 100, 'location': 'Amsterdam, Netherlands', 'website_url': '', 'link': 'https://stackoverflow.com/users/928203/scott-crooks', 'profile_image': 'https://www.gravatar.com/avatar/dd94ea334aa3cc0f45989abb13778a83?s=256&d=identicon&r=PG', 'display_name': 'Scott Crooks'}","I'm attempting to run PyInstaller on a CLI app I am building in Python using the Click library. I'm having trouble building the project using PyInstaller. PyInstaller has a document in their GitHub wiki titled Recipe Setuptools Entry Point, which gives information about how to use PyInstaller with a package, which I'm using for this project. However, it seems it cannot find the base module when I run . My question is: Is the problem simply an issue with the folder structure I have? Does the Recipe Setuptools Entry Point assume a certain file structure? Relevant information Pyinstaller output The file for , which is the entrypoint for my CLI app: The contents of the script generated when I run in my virtual environment: And finally, my repository structure: And my file:","setuptools pyinstaller --onefile main.spec 184 INFO: PyInstaller: 3.3.1
184 INFO: Python: 3.6.4
189 INFO: Platform: Darwin-16.7.0-x86_64-i386-64bit
193 INFO: UPX is available.
Traceback (most recent call last):
  File ""/usr/local/bin/pyinstaller"", line 11, in &lt;module&gt;
    sys.exit(run())
  File ""/usr/local/lib/python3.6/site-packages/PyInstaller/__main__.py"", line 94, in run
    run_build(pyi_config, spec_file, **vars(args))
  File ""/usr/local/lib/python3.6/site-packages/PyInstaller/__main__.py"", line 46, in run_build
    PyInstaller.building.build_main.main(pyi_config, spec_file, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/PyInstaller/building/build_main.py"", line 791, in main
    build(specfile, kw.get('distpath'), kw.get('workpath'), kw.get('clean_build'))
  File ""/usr/local/lib/python3.6/site-packages/PyInstaller/building/build_main.py"", line 737, in build
    exec(text, spec_namespace)
  File ""&lt;string&gt;"", line 40, in &lt;module&gt;
  File ""&lt;string&gt;"", line 26, in Entrypoint
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 582, in get_entry_info
    return get_distribution(dist).get_entry_info(group, name)
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 564, in get_distribution
    dist = get_provider(dist)
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 436, in get_provider
    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 984, in require
    needed = self.resolve(parse_requirements(requirements))
  File ""/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 870, in resolve
    raise DistributionNotFound(req, requirers)
pkg_resources.DistributionNotFound: The 'myapp' distribution was not found and is required by the application
 main.spec main.py block_cipher = None

def Entrypoint(dist, group, name,
               scripts=None, pathex=None, hiddenimports=None,
               hookspath=None, excludes=None, runtime_hooks=None):
    import pkg_resources

    # get toplevel packages of distribution from metadata
    def get_toplevel(dist):
        distribution = pkg_resources.get_distribution(dist)
        if distribution.has_metadata('top_level.txt'):
            return list(distribution.get_metadata('top_level.txt').split())
        else:
            return []

    hiddenimports = hiddenimports or []
    packages = []
    for distribution in hiddenimports:
        packages += get_toplevel(distribution)

    scripts = scripts or []
    pathex = pathex or []
    # get the entry point
    ep = pkg_resources.get_entry_info(dist, group, name)
    # insert path of the egg at the verify front of the search path
    pathex = [ep.dist.location] + pathex
    # script name must not be a valid module name to avoid name clashes on import
    script_path = os.path.join(workpath, name + '-script.py')
    print (""creating script for entry point"", dist, group, name)
    with open(script_path, 'w') as fh:
        print(""import"", ep.module_name, file=fh)
        print(""%s.%s()"" % (ep.module_name, '.'.join(ep.attrs)), file=fh)
        for package in packages:
            print (""import"", package, file=fh)

    return Analysis([script_path] + scripts, pathex, hiddenimports, hookspath, excludes, runtime_hooks)

a = Entrypoint('myapp', 'console_scripts', 'myapp')

pyz = PYZ(a.pure, a.zipped_data,
             cipher=block_cipher)
exe = EXE(pyz,
          a.scripts,
          exclude_binaries=True,
          name='main',
          debug=False,
          strip=False,
          upx=True,
          console=True )
coll = COLLECT(exe,
               a.binaries,
               a.zipfiles,
               a.datas,
               strip=False,
               upx=True,
               name='main')
 myapp pip3 install --editable . #!/some/path/to/myapp-cli/venv/bin/python3.6
# EASY-INSTALL-ENTRY-SCRIPT: 'myapp','console_scripts','myapp'
__requires__ = 'myapp'
import re
import sys
from pkg_resources import load_entry_point

if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
    sys.exit(
        load_entry_point('myapp', 'console_scripts', 'myapp')()
    )
 myapp-cli/
├── README.md
├── myapp
│   ├── __init__.py
│   ├── main.py
│   ├── main.spec
│   ├── resources
│   │   ├── __init__.py
│   │   └── functions.py
│   ├── subcommands
│   │   ├── __init__.py
│   │   ├── config
│   │   │   ├── __init__.py
│   │   │   └── cli.py
│   │   ├── create
│   │   │   ├── __init__.py
│   │   │   └── cli.py
│   │   ├── destroy
│   │   │   ├── __init__.py
│   │   │   └── cli.py
│   │   └── switch
│   │       ├── __init__.py
│   │       └── cli.py
│   └── variables.py
├── requirements.txt
└── setup.py
 setup.py from setuptools import find_packages
from setuptools import setup
import os

base_dir = os.path.dirname(__file__)

setup(
    entry_points = '''
        [console_scripts]
        myapp=myapp.main:entry_point
    ''',
    install_requires = [
        'packageone==1.0',
        'packagetwo==2.0',
    ],
    name = ""myapp"",
    packages=find_packages(),
    setup_requires=""setuptools"",
    version = ""0.1"",
)
",130,167,0,3,
336,48522640,48565404,13742,Failure to connect to Docker Postgresql instance from Python,5,<python><postgresql><docker><psycopg2>,20,"<p>I am using Docker to ""containerize"" a PostgreSQL deployment. I can spin up the container and connect to PostgreSQL via the command line as shown below:</p>

<pre><code>minime2@CEBERUS:~/Projects/skunkworks$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
dc176901052a        df:pg               ""docker-entrypoint...""   About an hour ago   Up About an hour    5432/tcp            vigilant_agnesi

minime2@CEBERUS:~/Projects/skunkworks$ CONTAINER_ID=dc176901052a
minime2@CEBERUS:~/Projects/skunkworks$ IP=$(docker inspect -f '{{.NetworkSettings.Networks.bridge.IPAddress}}' $CONTAINER_ID)

minime2@CEBERUS:~/Projects/skunkworks$ echo $IP
172.17.0.2


minime2@CEBERUS:~/Projects/skunkworks$ docker exec -it vigilant_agnesi psql -U postgres -W cookiebox
Passwod for user postgres:
psql (9.6.5)
Type ""help"" for help

cookiebox#
</code></pre>

<h1><strong>Now attempting connection with Python:</strong></h1>

<pre><code>Python 3.5.2 (default, Sep 14 2017, 22:51:06) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import psycopg2
&gt;&gt;&gt; conn = psycopg2.connect(""dbname='cookiebox' user='postgres' host='172.17.0.2' password='nunyabiznes'"")                                     Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/minime2/Projects/skunkworks/archivers/env/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host ""172.17.0.2"" and accepting
        TCP/IP connections on port 5432?

&gt;&gt;&gt; 
</code></pre>

<p>Can anyone explain why I can't connect to PostgreSQL using Python - even though I'm using the same arguments/parameters that enable a successful connection at the command line (using <code>docker exec</code>?).</p>

<p><strong>[[Additional Info]]</strong></p>

<p>As suggested by @Itvhillo, I tried to use a desktop application to connect to the PG service. I run the docker service using the following command:</p>

<p><code>docker run -i -p 5432:5432 --name $CONTAINER_NAME $DOCKER_IMAGE</code></p>

<p>I am using <a href=""https://www.dbvis.com/"" rel=""noreferrer"">Db Visualizer</a> to connect to the database, and I have set the hostname to 'localhost'. I can successfully ping the port, but still get an error message when I try to connect to the database (possible permissions related error):</p>

<pre><code>An error occurred while establishing the connection:

Long Message:
The connection attempt failed.

Details:
   Type: org.postgresql.util.PSQLException
   SQL State: 08001
</code></pre>

<p>Incidentally, this is the tail end of the output for the PG service instance:</p>

<pre><code>PostgreSQL init process complete; ready for start up.

LOG:  could not bind IPv6 socket: Cannot assign requested address
HINT:  Is another postmaster already running on port 5432? If not, wait a few seconds and retry.
LOG:  database system was shut down at 2018-01-30 16:21:59 UTC
LOG:  MultiXact member wraparound protections are now enabled
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started
</code></pre>

<p><strong>[[Additional Info2]]</strong></p>

<p>Here is the tail end of my Dockerfile:</p>

<pre><code># modified target locations (checked by login onto Docker container)
# show hba_file;
# show config_file;

#################################################################################
# From here: https://docs.docker.com/engine/examples/postgresql_service/
# Adjust PostgreSQL configuration so that remote connections to the
# database are possible.
RUN echo ""host all  all    0.0.0.0/0  md5"" &gt;&gt; /var/lib/postgresql/data/pg_hba.conf

# And add ``listen_addresses`` to ``/var/lib/postgresql/data/postgresql.conf``
RUN echo ""listen_addresses='*'"" &gt;&gt; /var/lib/postgresql/data/postgresql.conf
#################################################################################


EXPOSE 5432

# Add VOLUMEs to allow backup of config, logs and databases
VOLUME  [""/etc/postgresql"", ""/var/log/postgresql"", ""/var/lib/postgresql"", ""/usr/lib/postgresql/""]
</code></pre>
",962891,66497,30-01-2018 13:29,01-02-2018 14:42,2,66607,350,83,223,77,"{'badge_counts': {'bronze': 350, 'silver': 223, 'gold': 83}, 'account_id': 516016, 'is_employee': False, 'last_modified_date': 1698182100, 'last_access_date': 1711123795, 'reputation_change_year': 540, 'reputation_change_quarter': 540, 'reputation_change_month': 190, 'reputation_change_week': 50, 'reputation_change_day': 0, 'reputation': 66607, 'creation_date': 1316887811, 'user_type': 'registered', 'user_id': 962891, 'accept_rate': 77, 'location': 'London, United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/962891/homunculus-reticulli', 'profile_image': 'https://i.stack.imgur.com/7Bcxm.png?s=256&g=1', 'display_name': 'Homunculus Reticulli'}","I am using Docker to ""containerize"" a PostgreSQL deployment. I can spin up the container and connect to PostgreSQL via the command line as shown below: Now attempting connection with Python: Can anyone explain why I can't connect to PostgreSQL using Python - even though I'm using the same arguments/parameters that enable a successful connection at the command line (using ?). [[Additional Info]] As suggested by @Itvhillo, I tried to use a desktop application to connect to the PG service. I run the docker service using the following command: I am using Db Visualizer to connect to the database, and I have set the hostname to 'localhost'. I can successfully ping the port, but still get an error message when I try to connect to the database (possible permissions related error): Incidentally, this is the tail end of the output for the PG service instance: [[Additional Info2]] Here is the tail end of my Dockerfile:","minime2@CEBERUS:~/Projects/skunkworks$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
dc176901052a        df:pg               ""docker-entrypoint...""   About an hour ago   Up About an hour    5432/tcp            vigilant_agnesi

minime2@CEBERUS:~/Projects/skunkworks$ CONTAINER_ID=dc176901052a
minime2@CEBERUS:~/Projects/skunkworks$ IP=$(docker inspect -f '{{.NetworkSettings.Networks.bridge.IPAddress}}' $CONTAINER_ID)

minime2@CEBERUS:~/Projects/skunkworks$ echo $IP
172.17.0.2


minime2@CEBERUS:~/Projects/skunkworks$ docker exec -it vigilant_agnesi psql -U postgres -W cookiebox
Passwod for user postgres:
psql (9.6.5)
Type ""help"" for help

cookiebox#
 Python 3.5.2 (default, Sep 14 2017, 22:51:06) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import psycopg2
&gt;&gt;&gt; conn = psycopg2.connect(""dbname='cookiebox' user='postgres' host='172.17.0.2' password='nunyabiznes'"")                                     Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/minime2/Projects/skunkworks/archivers/env/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host ""172.17.0.2"" and accepting
        TCP/IP connections on port 5432?

&gt;&gt;&gt; 
 docker exec docker run -i -p 5432:5432 --name $CONTAINER_NAME $DOCKER_IMAGE An error occurred while establishing the connection:

Long Message:
The connection attempt failed.

Details:
   Type: org.postgresql.util.PSQLException
   SQL State: 08001
 PostgreSQL init process complete; ready for start up.

LOG:  could not bind IPv6 socket: Cannot assign requested address
HINT:  Is another postmaster already running on port 5432? If not, wait a few seconds and retry.
LOG:  database system was shut down at 2018-01-30 16:21:59 UTC
LOG:  MultiXact member wraparound protections are now enabled
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started
 # modified target locations (checked by login onto Docker container)
# show hba_file;
# show config_file;

#################################################################################
# From here: https://docs.docker.com/engine/examples/postgresql_service/
# Adjust PostgreSQL configuration so that remote connections to the
# database are possible.
RUN echo ""host all  all    0.0.0.0/0  md5"" &gt;&gt; /var/lib/postgresql/data/pg_hba.conf

# And add ``listen_addresses`` to ``/var/lib/postgresql/data/postgresql.conf``
RUN echo ""listen_addresses='*'"" &gt;&gt; /var/lib/postgresql/data/postgresql.conf
#################################################################################


EXPOSE 5432

# Add VOLUMEs to allow backup of config, logs and databases
VOLUME  [""/etc/postgresql"", ""/var/log/postgresql"", ""/var/lib/postgresql"", ""/usr/lib/postgresql/""]
",58,94,0,1,
337,49511933,49520024,2856,Better usage of `make_pass_decorator` in Python Click,2,<python><decorator><python-decorators><python-click>,11,"<p>I am looking for some advice to <strong>avoid having to instantiate a class twice</strong>; this is more of a design pattern question. I am creating an application using the <a href=""http://click.pocoo.org/"" rel=""noreferrer"">Python Click</a> library.</p>

<p>I have a <code>Settings</code> class that first loads all initial default settings into a dictionary (hard-coded into the application), then loads all settings overrides (if specified) from a TOML file on the user's computer into a dictionary, and then finally merges the two and makes them available as attributes of the class instance (<code>settings.&lt;something&gt;</code>).</p>

<p>For most of these settings, I <strong><em>also</em></strong> want to be able to specify a command-line flag. The priority then becomes:</p>

<ol>
<li>Command-line flag. If not specified, then fallback to...</li>
<li>User setting in TOML file. If not specified, then finally fallback to...</li>
<li>Application default</li>
</ol>

<p>In order to achieve this result, I am finding that, when using Click's decorators, I have to do something like this:</p>

<pre><code>import click
from myapp import Settings

settings = Settings()
pass_settings = click.make_pass_decorator(Settings, ensure=True)

@click.command()
@click.help_option('-h', '--help')
@click.option(
    '-s', '--disk-size',
    default=settings.instance_disk_size,
    help=""Disk size"",
    show_default=True,
    type=int
)
@click.option(
    '-t', '--disk-type',
    default=settings.instance_disk_type,
    help=""Disk type"",
    show_default=True,
    type=click.Choice(['pd-standard', 'pd-ssd'])
)
@pass_settings
def create(settings, disk_size, disk_type):
    print(disk_size)
    print(disk_type)
</code></pre>

<h3>Why twice?</h3>

<ul>
<li>The <code>settings = Settings()</code> line is needed to provide the <code>@click.option</code> decorators with the <code>default</code> value. The <code>default</code> value could either come from the user override TOML file (if present), or from the application default.</li>
<li>The <a href=""http://click.pocoo.org/6/api/#click.make_pass_decorator"" rel=""noreferrer"">click.make_pass_decorator</a> seems to be the recommended way for interleaved commands; it's even mentioned <a href=""http://click.pocoo.org/6/complex/#interleaved-commands"" rel=""noreferrer"">in their documentation</a>. Inside of the function, in addition to the CLI parameters passed, I also sometimes needs to reference other attributes in the <code>Settings</code> class.</li>
</ul>

<p>My question is, <em>which is better?</em> Is there a way to use the <code>pass_settings</code> decorator in the other <code>click.option</code> decorators? Or should I ditch using <code>click.make_pass_decorator</code> entirely?</p>
",928203,1573,27-03-2018 11:40,27-03-2018 18:26,0,1583,40,4,25,100,"{'badge_counts': {'bronze': 40, 'silver': 25, 'gold': 4}, 'account_id': 889674, 'is_employee': False, 'last_modified_date': 1695087300, 'last_access_date': 1711029813, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1583, 'creation_date': 1315193757, 'user_type': 'registered', 'user_id': 928203, 'accept_rate': 100, 'location': 'Amsterdam, Netherlands', 'website_url': '', 'link': 'https://stackoverflow.com/users/928203/scott-crooks', 'profile_image': 'https://www.gravatar.com/avatar/dd94ea334aa3cc0f45989abb13778a83?s=256&d=identicon&r=PG', 'display_name': 'Scott Crooks'}","I am looking for some advice to avoid having to instantiate a class twice; this is more of a design pattern question. I am creating an application using the Python Click library. I have a class that first loads all initial default settings into a dictionary (hard-coded into the application), then loads all settings overrides (if specified) from a TOML file on the user's computer into a dictionary, and then finally merges the two and makes them available as attributes of the class instance (). For most of these settings, I also want to be able to specify a command-line flag. The priority then becomes: Command-line flag. If not specified, then fallback to... User setting in TOML file. If not specified, then finally fallback to... Application default In order to achieve this result, I am finding that, when using Click's decorators, I have to do something like this: Why twice? The line is needed to provide the decorators with the value. The value could either come from the user override TOML file (if present), or from the application default. The click.make_pass_decorator seems to be the recommended way for interleaved commands; it's even mentioned in their documentation. Inside of the function, in addition to the CLI parameters passed, I also sometimes needs to reference other attributes in the class. My question is, which is better? Is there a way to use the decorator in the other decorators? Or should I ditch using entirely?","Settings settings.&lt;something&gt; import click
from myapp import Settings

settings = Settings()
pass_settings = click.make_pass_decorator(Settings, ensure=True)

@click.command()
@click.help_option('-h', '--help')
@click.option(
    '-s', '--disk-size',
    default=settings.instance_disk_size,
    help=""Disk size"",
    show_default=True,
    type=int
)
@click.option(
    '-t', '--disk-type',
    default=settings.instance_disk_type,
    help=""Disk type"",
    show_default=True,
    type=click.Choice(['pd-standard', 'pd-ssd'])
)
@pass_settings
def create(settings, disk_size, disk_type):
    print(disk_size)
    print(disk_type)
 settings = Settings() @click.option default default Settings pass_settings click.option click.make_pass_decorator",15,50,0,3,
338,48767143,48858192,37105,How to suppress warnings about lack of cert verification in a requests HTTPS call?,3,<python><python-3.x><https><python-requests><suppress-warnings>,14,"<p>I would like to disable the warning about a lack of certificate verification in a HTTPS call using <code>requests</code>.</p>

<p>The question has been asked in the past, leading to answers about <a href=""https://stackoverflow.com/a/32282390/903011"">disabling a relevant request logging</a> or the <a href=""https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings"" rel=""noreferrer""><code>urllib3</code> SSL warning</a>.</p>

<p>This used to work in the past (I remember successfully silencing the warnings) but seems not to work anymore?</p>

<p>I put together the two solutions which worked so far:</p>

<pre><code>Python 3.5.3 (default, Jan 19 2017, 14:11:04)
[GCC 6.3.0 20170124] on linux
&gt;&gt;&gt; import requests
&gt;&gt;&gt; import requests.packages
&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; urllib3.disable_warnings()
&gt;&gt;&gt; requests.packages.urllib3.disable_warnings()
&gt;&gt;&gt; requests.get('https://www.google.com', verify=False)
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:845: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
&lt;Response [200]&gt;
</code></pre>

<p><strong>Is there another (current) solution to silence these warnings?</strong></p>
",903011,28379,13-02-2018 12:44,19-02-2018 00:38,6,28449,362,53,193,85,"{'badge_counts': {'bronze': 362, 'silver': 193, 'gold': 53}, 'account_id': 486226, 'is_employee': False, 'last_modified_date': 1710436200, 'last_access_date': 1711121162, 'reputation_change_year': 616, 'reputation_change_quarter': 616, 'reputation_change_month': 164, 'reputation_change_week': 60, 'reputation_change_day': 0, 'reputation': 28449, 'creation_date': 1313782394, 'user_type': 'registered', 'user_id': 903011, 'accept_rate': 85, 'location': 'France', 'website_url': '', 'link': 'https://stackoverflow.com/users/903011/woj', 'profile_image': 'https://www.gravatar.com/avatar/6a8f2f5f292c75c5e81a6607530a0ab6?s=256&d=identicon&r=PG', 'display_name': 'WoJ'}","I would like to disable the warning about a lack of certificate verification in a HTTPS call using . The question has been asked in the past, leading to answers about disabling a relevant request logging or the SSL warning. This used to work in the past (I remember successfully silencing the warnings) but seems not to work anymore? I put together the two solutions which worked so far: Is there another (current) solution to silence these warnings?","requests urllib3 Python 3.5.3 (default, Jan 19 2017, 14:11:04)
[GCC 6.3.0 20170124] on linux
&gt;&gt;&gt; import requests
&gt;&gt;&gt; import requests.packages
&gt;&gt;&gt; import urllib3
&gt;&gt;&gt; urllib3.disable_warnings()
&gt;&gt;&gt; requests.packages.urllib3.disable_warnings()
&gt;&gt;&gt; requests.get('https://www.google.com', verify=False)
/usr/lib/python3/dist-packages/urllib3/connectionpool.py:845: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
&lt;Response [200]&gt;
",8,22,0,2,
339,49494337,49547872,27753,Encode numpy array using uncompressed RLE for COCO dataset,5,<python><image><numpy>,19,"<p>To create a <a href=""http://cocodataset.org/#download"" rel=""noreferrer"">COCO</a> dataset of annotated images, you need to convert binary masks into either polygons or uncompressed run length encoding representations depending on the type of object. </p>

<p>The <a href=""https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/mask.py"" rel=""noreferrer"">pycocotools</a> library has functions to encode and decode into and from <em>compressed</em> RLE, but nothing for polygons and uncompressed RLE.</p>

<p>I can use skimage's measure library to <a href=""https://github.com/cocodataset/cocoapi/issues/131"" rel=""noreferrer"">generate polygons of masks</a>, but I'm not sure how to create uncompressed RLEs. </p>

<p>I can use <a href=""https://www.kaggle.com/hackerpoet/even-faster-run-length-encoder"" rel=""noreferrer"">this RLE encoder</a> to create <em>a</em> representation of RLE from an image, but I'm not sure what format COCO expects. COCO just mentions that they use a ""custom Run Length Encoding (RLE) scheme""</p>

<p>for example,</p>

<pre><code>ground_truth_binary_mask = np.array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
                                     [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
                                     [  0,   0,   0,   0,   0,   1,   1,   1,   0,   0],
                                     [  0,   0,   0,   0,   0,   1,   1,   1,   0,   0],
                                     [  0,   0,   0,   0,   0,   1,   1,   1,   0,   0],
                                     [  0,   0,   0,   0,   0,   1,   1,   1,   0,   0],
                                     [  1,   0,   0,   0,   0,   0,   0,   0,   0,   0],
                                     [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
                                     [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=np.uint8)

fortran_ground_truth_binary_mask = np.asfortranarray(ground_truth_binary_mask)
rle(fortran_ground_truth_binary_mask)
</code></pre>

<p>outputs:</p>

<pre><code>(array([26, 36, 46, 56, 61]), array([3, 3, 3, 3, 1]))
</code></pre>

<p>this is what a coco RLE looks like:</p>

<pre><code>{
    ""segmentation"": {
        ""counts"": [
            272,
            2,
            4,
            4,
            4,
            4,
            2,
            9,
            1,
            2,
            16,
            43,
            143,
            24,
            5,
            8,
            16,
            44,
            141,
            25,
            8,
            5,
            17,
            44,
            140,
            26,
            10,
            2,
            17,
            45,
            129,
            4,
            5,
            27,
            24,
            5,
            1,
            45,
            127,
            38,
            23,
            52,
            125,
            40,
            22,
            53,
            123,
            43,
            20,
            54,
            122,
            46,
            18,
            54,
            121,
            54,
            12,
            53,
            119,
            57,
            11,
            53,
            117,
            59,
            13,
            51,
            117,
            59,
            13,
            51,
            117,
            60,
            11,
            52,
            117,
            60,
            10,
            52,
            118,
            60,
            9,
            53,
            118,
            61,
            8,
            52,
            119,
            62,
            7,
            52,
            119,
            64,
            1,
            2,
            2,
            51,
            120,
            120,
            120,
            101,
            139,
            98,
            142,
            96,
            144,
            93,
            147,
            90,
            150,
            87,
            153,
            85,
            155,
            82,
            158,
            76,
            164,
            66,
            174,
            61,
            179,
            57,
            183,
            54,
            186,
            52,
            188,
            49,
            191,
            47,
            193,
            21,
            8,
            16,
            195,
            20,
            13,
            8,
            199,
            18,
            222,
            17,
            223,
            16,
            224,
            16,
            224,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            14,
            226,
            14,
            226,
            14,
            39,
            1,
            186,
            14,
            39,
            3,
            184,
            14,
            39,
            4,
            183,
            13,
            40,
            6,
            181,
            14,
            39,
            7,
            180,
            14,
            39,
            9,
            178,
            14,
            39,
            10,
            177,
            14,
            39,
            11,
            176,
            14,
            38,
            14,
            174,
            14,
            36,
            19,
            171,
            15,
            33,
            32,
            160,
            16,
            30,
            35,
            159,
            18,
            26,
            38,
            158,
            19,
            23,
            41,
            157,
            20,
            19,
            45,
            156,
            21,
            15,
            48,
            156,
            22,
            10,
            53,
            155,
            23,
            9,
            54,
            154,
            23,
            8,
            55,
            154,
            24,
            7,
            56,
            153,
            24,
            6,
            57,
            153,
            25,
            5,
            57,
            153,
            25,
            5,
            58,
            152,
            25,
            4,
            59,
            152,
            26,
            3,
            59,
            152,
            26,
            3,
            59,
            152,
            27,
            1,
            60,
            152,
            27,
            1,
            60,
            152,
            86,
            154,
            80,
            160,
            79,
            161,
            42,
            8,
            29,
            161,
            41,
            11,
            22,
            2,
            3,
            161,
            40,
            13,
            18,
            5,
            3,
            161,
            40,
            15,
            2,
            5,
            8,
            7,
            2,
            161,
            40,
            24,
            6,
            170,
            35,
            30,
            4,
            171,
            34,
            206,
            34,
            41,
            1,
            164,
            34,
            39,
            3,
            164,
            34,
            37,
            5,
            164,
            34,
            35,
            10,
            161,
            36,
            1,
            3,
            28,
            17,
            155,
            41,
            27,
            16,
            156,
            41,
            26,
            17,
            156,
            41,
            26,
            16,
            157,
            27,
            4,
            10,
            25,
            16,
            158,
            27,
            6,
            8,
            11,
            2,
            12,
            6,
            2,
            7,
            159,
            27,
            7,
            14,
            3,
            4,
            19,
            6,
            160,
            26,
            8,
            22,
            18,
            5,
            161,
            26,
            8,
            22,
            18,
            4,
            162,
            26,
            8,
            23,
            15,
            4,
            164,
            23,
            11,
            23,
            11,
            7,
            165,
            19,
            17,
            22,
            9,
            6,
            167,
            19,
            22,
            18,
            8,
            3,
            170,
            18,
            25,
            16,
            7,
            1,
            173,
            17,
            28,
            15,
            180,
            17,
            30,
            12,
            181,
            16,
            34,
            6,
            184,
            15,
            225,
            14,
            226,
            13,
            227,
            12,
            228,
            11,
            229,
            10,
            230,
            9,
            231,
            9,
            231,
            9,
            231,
            9,
            231,
            8,
            232,
            8,
            232,
            8,
            232,
            8,
            232,
            8,
            232,
            8,
            232,
            7,
            233,
            7,
            233,
            7,
            233,
            7,
            233,
            8,
            232,
            8,
            232,
            8,
            232,
            9,
            231,
            9,
            231,
            9,
            231,
            10,
            230,
            10,
            230,
            11,
            229,
            13,
            227,
            14,
            226,
            16,
            224,
            17,
            223,
            19,
            221,
            23,
            217,
            31,
            3,
            5,
            201,
            39,
            201,
            39,
            201,
            39,
            201,
            39,
            201,
            39,
            201,
            40,
            200,
            40,
            200,
            41,
            199,
            41,
            199,
            41,
            199,
            22,
            8,
            12,
            198,
            22,
            12,
            8,
            198,
            22,
            14,
            6,
            198,
            22,
            15,
            6,
            197,
            22,
            16,
            5,
            197,
            22,
            17,
            5,
            196,
            22,
            18,
            4,
            196,
            22,
            19,
            4,
            195,
            22,
            19,
            5,
            194,
            22,
            20,
            4,
            194,
            25,
            21,
            1,
            193,
            27,
            213,
            29,
            211,
            30,
            210,
            35,
            6,
            6,
            193,
            49,
            191,
            50,
            190,
            50,
            190,
            51,
            189,
            51,
            189,
            52,
            188,
            53,
            187,
            53,
            187,
            54,
            186,
            54,
            186,
            54,
            186,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            28,
            1,
            26,
            185,
            23,
            11,
            21,
            185,
            20,
            17,
            17,
            186,
            18,
            21,
            15,
            186,
            16,
            23,
            14,
            187,
            14,
            25,
            14,
            187,
            14,
            26,
            12,
            188,
            14,
            28,
            10,
            188,
            14,
            226,
            14,
            226,
            16,
            224,
            17,
            223,
            19,
            221,
            20,
            220,
            22,
            218,
            24,
            18,
            3,
            12,
            3,
            180,
            25,
            10,
            1,
            4,
            6,
            10,
            6,
            178,
            28,
            7,
            12,
            8,
            8,
            177,
            49,
            3,
            12,
            176,
            65,
            175,
            67,
            173,
            69,
            171,
            53,
            3,
            14,
            170,
            37,
            20,
            9,
            4,
            1,
            169,
            36,
            21,
            8,
            175,
            35,
            22,
            7,
            176,
            34,
            23,
            7,
            176,
            34,
            23,
            6,
            177,
            35,
            22,
            6,
            177,
            35,
            22,
            8,
            175,
            35,
            23,
            9,
            173,
            35,
            205,
            36,
            204,
            39,
            201,
            43,
            197,
            48,
            36,
            1,
            155,
            48,
            35,
            3,
            154,
            49,
            33,
            5,
            154,
            48,
            32,
            6,
            155,
            49,
            27,
            10,
            155,
            51,
            24,
            11,
            154,
            54,
            21,
            11,
            155,
            56,
            19,
            11,
            155,
            56,
            18,
            11,
            156,
            56,
            17,
            11,
            157,
            56,
            16,
            12,
            157,
            56,
            14,
            13,
            159,
            56,
            12,
            13,
            160,
            61,
            5,
            14,
            162,
            78,
            165,
            75,
            167,
            73,
            168,
            72,
            170,
            70,
            171,
            69,
            173,
            67,
            176,
            64,
            179,
            61,
            182,
            58,
            183,
            57,
            185,
            54,
            187,
            53,
            188,
            51,
            191,
            49,
            192,
            47,
            195,
            45,
            196,
            43,
            198,
            42,
            199,
            40,
            201,
            38,
            203,
            36,
            205,
            34,
            207,
            32,
            210,
            28,
            213,
            26,
            216,
            22,
            221,
            16,
            228,
            8,
            10250
        ],
        ""size"": [
            240,
            320
        ]
    }
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/t511e.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/t511e.png"" alt=""enter image description here""></a></p>
",884269,6644,26-03-2018 14:48,29-03-2018 04:07,3,6664,79,11,54,80,"{'badge_counts': {'bronze': 79, 'silver': 54, 'gold': 11}, 'account_id': 474540, 'is_employee': False, 'last_modified_date': 1709341800, 'last_access_date': 1710972261, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 22, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 6664, 'creation_date': 1312814075, 'user_type': 'registered', 'user_id': 884269, 'accept_rate': 80, 'website_url': 'http://patrickwasp.com', 'link': 'https://stackoverflow.com/users/884269/waspinator', 'profile_image': 'https://www.gravatar.com/avatar/5692f3ff8b8c6348b701a518ce58d681?s=256&d=identicon&r=PG', 'display_name': 'waspinator'}","To create a COCO dataset of annotated images, you need to convert binary masks into either polygons or uncompressed run length encoding representations depending on the type of object. The pycocotools library has functions to encode and decode into and from compressed RLE, but nothing for polygons and uncompressed RLE. I can use skimage's measure library to generate polygons of masks, but I'm not sure how to create uncompressed RLEs. I can use this RLE encoder to create a representation of RLE from an image, but I'm not sure what format COCO expects. COCO just mentions that they use a ""custom Run Length Encoding (RLE) scheme"" for example, outputs: this is what a coco RLE looks like:","ground_truth_binary_mask = np.array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
                                     [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
                                     [  0,   0,   0,   0,   0,   1,   1,   1,   0,   0],
                                     [  0,   0,   0,   0,   0,   1,   1,   1,   0,   0],
                                     [  0,   0,   0,   0,   0,   1,   1,   1,   0,   0],
                                     [  0,   0,   0,   0,   0,   1,   1,   1,   0,   0],
                                     [  1,   0,   0,   0,   0,   0,   0,   0,   0,   0],
                                     [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
                                     [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=np.uint8)

fortran_ground_truth_binary_mask = np.asfortranarray(ground_truth_binary_mask)
rle(fortran_ground_truth_binary_mask)
 (array([26, 36, 46, 56, 61]), array([3, 3, 3, 3, 1]))
 {
    ""segmentation"": {
        ""counts"": [
            272,
            2,
            4,
            4,
            4,
            4,
            2,
            9,
            1,
            2,
            16,
            43,
            143,
            24,
            5,
            8,
            16,
            44,
            141,
            25,
            8,
            5,
            17,
            44,
            140,
            26,
            10,
            2,
            17,
            45,
            129,
            4,
            5,
            27,
            24,
            5,
            1,
            45,
            127,
            38,
            23,
            52,
            125,
            40,
            22,
            53,
            123,
            43,
            20,
            54,
            122,
            46,
            18,
            54,
            121,
            54,
            12,
            53,
            119,
            57,
            11,
            53,
            117,
            59,
            13,
            51,
            117,
            59,
            13,
            51,
            117,
            60,
            11,
            52,
            117,
            60,
            10,
            52,
            118,
            60,
            9,
            53,
            118,
            61,
            8,
            52,
            119,
            62,
            7,
            52,
            119,
            64,
            1,
            2,
            2,
            51,
            120,
            120,
            120,
            101,
            139,
            98,
            142,
            96,
            144,
            93,
            147,
            90,
            150,
            87,
            153,
            85,
            155,
            82,
            158,
            76,
            164,
            66,
            174,
            61,
            179,
            57,
            183,
            54,
            186,
            52,
            188,
            49,
            191,
            47,
            193,
            21,
            8,
            16,
            195,
            20,
            13,
            8,
            199,
            18,
            222,
            17,
            223,
            16,
            224,
            16,
            224,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            15,
            225,
            14,
            226,
            14,
            226,
            14,
            39,
            1,
            186,
            14,
            39,
            3,
            184,
            14,
            39,
            4,
            183,
            13,
            40,
            6,
            181,
            14,
            39,
            7,
            180,
            14,
            39,
            9,
            178,
            14,
            39,
            10,
            177,
            14,
            39,
            11,
            176,
            14,
            38,
            14,
            174,
            14,
            36,
            19,
            171,
            15,
            33,
            32,
            160,
            16,
            30,
            35,
            159,
            18,
            26,
            38,
            158,
            19,
            23,
            41,
            157,
            20,
            19,
            45,
            156,
            21,
            15,
            48,
            156,
            22,
            10,
            53,
            155,
            23,
            9,
            54,
            154,
            23,
            8,
            55,
            154,
            24,
            7,
            56,
            153,
            24,
            6,
            57,
            153,
            25,
            5,
            57,
            153,
            25,
            5,
            58,
            152,
            25,
            4,
            59,
            152,
            26,
            3,
            59,
            152,
            26,
            3,
            59,
            152,
            27,
            1,
            60,
            152,
            27,
            1,
            60,
            152,
            86,
            154,
            80,
            160,
            79,
            161,
            42,
            8,
            29,
            161,
            41,
            11,
            22,
            2,
            3,
            161,
            40,
            13,
            18,
            5,
            3,
            161,
            40,
            15,
            2,
            5,
            8,
            7,
            2,
            161,
            40,
            24,
            6,
            170,
            35,
            30,
            4,
            171,
            34,
            206,
            34,
            41,
            1,
            164,
            34,
            39,
            3,
            164,
            34,
            37,
            5,
            164,
            34,
            35,
            10,
            161,
            36,
            1,
            3,
            28,
            17,
            155,
            41,
            27,
            16,
            156,
            41,
            26,
            17,
            156,
            41,
            26,
            16,
            157,
            27,
            4,
            10,
            25,
            16,
            158,
            27,
            6,
            8,
            11,
            2,
            12,
            6,
            2,
            7,
            159,
            27,
            7,
            14,
            3,
            4,
            19,
            6,
            160,
            26,
            8,
            22,
            18,
            5,
            161,
            26,
            8,
            22,
            18,
            4,
            162,
            26,
            8,
            23,
            15,
            4,
            164,
            23,
            11,
            23,
            11,
            7,
            165,
            19,
            17,
            22,
            9,
            6,
            167,
            19,
            22,
            18,
            8,
            3,
            170,
            18,
            25,
            16,
            7,
            1,
            173,
            17,
            28,
            15,
            180,
            17,
            30,
            12,
            181,
            16,
            34,
            6,
            184,
            15,
            225,
            14,
            226,
            13,
            227,
            12,
            228,
            11,
            229,
            10,
            230,
            9,
            231,
            9,
            231,
            9,
            231,
            9,
            231,
            8,
            232,
            8,
            232,
            8,
            232,
            8,
            232,
            8,
            232,
            8,
            232,
            7,
            233,
            7,
            233,
            7,
            233,
            7,
            233,
            8,
            232,
            8,
            232,
            8,
            232,
            9,
            231,
            9,
            231,
            9,
            231,
            10,
            230,
            10,
            230,
            11,
            229,
            13,
            227,
            14,
            226,
            16,
            224,
            17,
            223,
            19,
            221,
            23,
            217,
            31,
            3,
            5,
            201,
            39,
            201,
            39,
            201,
            39,
            201,
            39,
            201,
            39,
            201,
            40,
            200,
            40,
            200,
            41,
            199,
            41,
            199,
            41,
            199,
            22,
            8,
            12,
            198,
            22,
            12,
            8,
            198,
            22,
            14,
            6,
            198,
            22,
            15,
            6,
            197,
            22,
            16,
            5,
            197,
            22,
            17,
            5,
            196,
            22,
            18,
            4,
            196,
            22,
            19,
            4,
            195,
            22,
            19,
            5,
            194,
            22,
            20,
            4,
            194,
            25,
            21,
            1,
            193,
            27,
            213,
            29,
            211,
            30,
            210,
            35,
            6,
            6,
            193,
            49,
            191,
            50,
            190,
            50,
            190,
            51,
            189,
            51,
            189,
            52,
            188,
            53,
            187,
            53,
            187,
            54,
            186,
            54,
            186,
            54,
            186,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            55,
            185,
            28,
            1,
            26,
            185,
            23,
            11,
            21,
            185,
            20,
            17,
            17,
            186,
            18,
            21,
            15,
            186,
            16,
            23,
            14,
            187,
            14,
            25,
            14,
            187,
            14,
            26,
            12,
            188,
            14,
            28,
            10,
            188,
            14,
            226,
            14,
            226,
            16,
            224,
            17,
            223,
            19,
            221,
            20,
            220,
            22,
            218,
            24,
            18,
            3,
            12,
            3,
            180,
            25,
            10,
            1,
            4,
            6,
            10,
            6,
            178,
            28,
            7,
            12,
            8,
            8,
            177,
            49,
            3,
            12,
            176,
            65,
            175,
            67,
            173,
            69,
            171,
            53,
            3,
            14,
            170,
            37,
            20,
            9,
            4,
            1,
            169,
            36,
            21,
            8,
            175,
            35,
            22,
            7,
            176,
            34,
            23,
            7,
            176,
            34,
            23,
            6,
            177,
            35,
            22,
            6,
            177,
            35,
            22,
            8,
            175,
            35,
            23,
            9,
            173,
            35,
            205,
            36,
            204,
            39,
            201,
            43,
            197,
            48,
            36,
            1,
            155,
            48,
            35,
            3,
            154,
            49,
            33,
            5,
            154,
            48,
            32,
            6,
            155,
            49,
            27,
            10,
            155,
            51,
            24,
            11,
            154,
            54,
            21,
            11,
            155,
            56,
            19,
            11,
            155,
            56,
            18,
            11,
            156,
            56,
            17,
            11,
            157,
            56,
            16,
            12,
            157,
            56,
            14,
            13,
            159,
            56,
            12,
            13,
            160,
            61,
            5,
            14,
            162,
            78,
            165,
            75,
            167,
            73,
            168,
            72,
            170,
            70,
            171,
            69,
            173,
            67,
            176,
            64,
            179,
            61,
            182,
            58,
            183,
            57,
            185,
            54,
            187,
            53,
            188,
            51,
            191,
            49,
            192,
            47,
            195,
            45,
            196,
            43,
            198,
            42,
            199,
            40,
            201,
            38,
            203,
            36,
            205,
            34,
            207,
            32,
            210,
            28,
            213,
            26,
            216,
            22,
            221,
            16,
            228,
            8,
            10250
        ],
        ""size"": [
            240,
            320
        ]
    }
}
",889,913,1,5,
340,48391750,48391751,57628,Disable Python requests SSL validation for an imported module,3,<python><ssl><python-requests>,22,"<p>I'm running a Python script that uses the <code>requests</code> package for making web requests. However, the web requests go through a proxy with a self-signed cert. As such, requests raise the following Exception:</p>

<p><code>requests.exceptions.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE', 'certificate verify failed')],)"",)</code></p>

<p>I know that SSL validation can be disabled in my own code by passing <code>verify=False</code>, e.g.: <code>requests.get(""https://www.google.com"", verify=False)</code>. I also know that if I had the certificate bundle, I could set the <code>REQUESTS_CA_BUNDLE</code> or <code>CURL_CA_BUNDLE</code> environment variables to point to those files. However, I do not have the certificate bundle available.</p>

<p>How can I disable SSL validation for external modules without editing their code?</p>
",875832,9613,22-01-2018 23:14,22-01-2018 23:14,0,9633,39,4,29,67,"{'badge_counts': {'bronze': 39, 'silver': 29, 'gold': 4}, 'account_id': 469316, 'is_employee': False, 'last_modified_date': 1634047501, 'last_access_date': 1711028875, 'reputation_change_year': 170, 'reputation_change_quarter': 170, 'reputation_change_month': 20, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 9633, 'creation_date': 1309968212, 'user_type': 'registered', 'user_id': 875832, 'accept_rate': 67, 'website_url': '', 'link': 'https://stackoverflow.com/users/875832/moshe', 'profile_image': 'https://www.gravatar.com/avatar/8df259c952186aa93179732461b8d1e7?s=256&d=identicon&r=PG', 'display_name': 'Moshe'}","I'm running a Python script that uses the package for making web requests. However, the web requests go through a proxy with a self-signed cert. As such, requests raise the following Exception: I know that SSL validation can be disabled in my own code by passing , e.g.: . I also know that if I had the certificate bundle, I could set the or environment variables to point to those files. However, I do not have the certificate bundle available. How can I disable SSL validation for external modules without editing their code?","requests requests.exceptions.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE', 'certificate verify failed')],)"",) verify=False requests.get(""https://www.google.com"", verify=False) REQUESTS_CA_BUNDLE CURL_CA_BUNDLE",-6,7,0,0,
341,49586991,49588198,4162,How to pass arguments with slash in Django2 urls,1,<python><django><django-2.0>,13,"<p>I would like to call a Django URL with an argument containing a forward slash. For example, I would like to call <code>mysite.com/test/</code> with <code>'test1/test2'</code>. (e.g. naively, the url would be <code>mysite.com/test/test1/test2</code>)</p>

<p><code>urls.py</code>:</p>

<pre><code>urlpatterns = [
    path('test/&lt;test_arg&gt;/', views.test, name='test'),
]
</code></pre>

<p>Accessing either of the following fails:</p>

<ol>
<li><code>mysite.com/test/test1/test2</code></li>
<li><code>mysite.com/test/test1%2Ftest2</code> (%2F is the standard URL encoding for forward slash)</li>
</ol>

<p>with the below error:</p>

<p>Error:</p>

<pre><code>Using the URLconf defined in test.urls, Django tried these URL patterns, in this order:

    reader/ test/&lt;test_arg&gt;/
    admin/

The current path, test/test1/test2/, didn't match any of these.
</code></pre>

<p>I would expect using the path in 1 to fail, but I am surprised the path in 2 fails.</p>

<p>What is the correct way of going about this?</p>

<p>Using Django 2.0.2</p>
",864112,5296,31-03-2018 11:01,31-03-2018 13:18,0,5306,77,7,44,79,"{'badge_counts': {'bronze': 77, 'silver': 44, 'gold': 7}, 'account_id': 462034, 'is_employee': False, 'last_modified_date': 1709473200, 'last_access_date': 1698500819, 'reputation_change_year': 52, 'reputation_change_quarter': 52, 'reputation_change_month': 11, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5306, 'creation_date': 1311706666, 'user_type': 'registered', 'user_id': 864112, 'accept_rate': 79, 'link': 'https://stackoverflow.com/users/864112/henry-henrinson', 'profile_image': 'https://www.gravatar.com/avatar/83455ac39608bfaba9cb73082dd0e697?s=256&d=identicon&r=PG', 'display_name': 'Henry Henrinson'}","I would like to call a Django URL with an argument containing a forward slash. For example, I would like to call with . (e.g. naively, the url would be ) : Accessing either of the following fails: (%2F is the standard URL encoding for forward slash) with the below error: Error: I would expect using the path in 1 to fail, but I am surprised the path in 2 fails. What is the correct way of going about this? Using Django 2.0.2","mysite.com/test/ 'test1/test2' mysite.com/test/test1/test2 urls.py urlpatterns = [
    path('test/&lt;test_arg&gt;/', views.test, name='test'),
]
 mysite.com/test/test1/test2 mysite.com/test/test1%2Ftest2 Using the URLconf defined in test.urls, Django tried these URL patterns, in this order:

    reader/ test/&lt;test_arg&gt;/
    admin/

The current path, test/test1/test2/, didn't match any of these.
",1,33,0,0,
342,48195727,48195769,36541,Running collectstatic on server : AttributeError: 'PosixPath' object has no attribute 'startswith',4,<python><django><nginx><gunicorn><collectstatic>,18,"<p>After deploying on a server on digital ocean using nginx, gunicorn, django, and virtualenv, I try to use collectstatic:</p>

<pre><code>python manage.py collectstatic --settings=config.settings.production
</code></pre>

<p>As you can see I have multiple setting files. One base, one local and one production setting file. Below is the error:</p>

<pre><code>    Traceback (most recent call last):
  File ""manage.py"", line 22, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/management/__init__.py"", line 363, in execute_from_command_line
    utility.execute()
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/management/__init__.py"", line 355, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/management/base.py"", line 283, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/management/base.py"", line 330, in execute
    output = self.handle(*args, **options)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py"", line 173, in handle
    if self.is_local_storage() and self.storage.location:
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/utils/functional.py"", line 239, in inner
    return func(self._wrapped, *args)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/utils/functional.py"", line 35, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/files/storage.py"", line 283, in location
    return abspathu(self.base_location)
  File ""/home/tony/vp/vpenv/lib/python3.5/posixpath.py"", line 357, in abspath
    if not isabs(path):
  File ""/home/tony/vp/vpenv/lib/python3.5/posixpath.py"", line 64, in isabs
    return s.startswith(sep)
AttributeError: 'PosixPath' object has no attribute 'startswith'
</code></pre>

<p>my <code>production.py</code> settings file contains the following:</p>

<pre><code>MEDIA_ROOT = BASE_DIR / 'media'
MEDIA_URL = 'media/'
STATIC_ROOT = BASE_DIR / 'static'
</code></pre>

<p>my base dir is as follows (imported from the base setting file):</p>

<pre><code>BASE_DIR = Path(__file__).resolve().parent.parent.parent
</code></pre>

<p>What could be the cause?</p>
",1067213,2442,10-01-2018 20:41,10-01-2018 20:44,0,2452,52,7,35,87,"{'badge_counts': {'bronze': 52, 'silver': 35, 'gold': 7}, 'account_id': 1066901, 'is_employee': False, 'last_modified_date': 1703299200, 'last_access_date': 1711046023, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2452, 'creation_date': 1322334518, 'user_type': 'registered', 'user_id': 1067213, 'accept_rate': 87, 'location': 'The Hague Center, The Hague, Netherlands', 'website_url': '', 'link': 'https://stackoverflow.com/users/1067213/tony', 'profile_image': 'https://i.stack.imgur.com/7vHjc.jpg?s=256&g=1', 'display_name': 'Tony'}","After deploying on a server on digital ocean using nginx, gunicorn, django, and virtualenv, I try to use collectstatic: As you can see I have multiple setting files. One base, one local and one production setting file. Below is the error: my settings file contains the following: my base dir is as follows (imported from the base setting file): What could be the cause?","python manage.py collectstatic --settings=config.settings.production
     Traceback (most recent call last):
  File ""manage.py"", line 22, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/management/__init__.py"", line 363, in execute_from_command_line
    utility.execute()
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/management/__init__.py"", line 355, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/management/base.py"", line 283, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/management/base.py"", line 330, in execute
    output = self.handle(*args, **options)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/contrib/staticfiles/management/commands/collectstatic.py"", line 173, in handle
    if self.is_local_storage() and self.storage.location:
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/utils/functional.py"", line 239, in inner
    return func(self._wrapped, *args)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/utils/functional.py"", line 35, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/tony/vp/vpenv/lib/python3.5/site-packages/django/core/files/storage.py"", line 283, in location
    return abspathu(self.base_location)
  File ""/home/tony/vp/vpenv/lib/python3.5/posixpath.py"", line 357, in abspath
    if not isabs(path):
  File ""/home/tony/vp/vpenv/lib/python3.5/posixpath.py"", line 64, in isabs
    return s.startswith(sep)
AttributeError: 'PosixPath' object has no attribute 'startswith'
 production.py MEDIA_ROOT = BASE_DIR / 'media'
MEDIA_URL = 'media/'
STATIC_ROOT = BASE_DIR / 'static'
 BASE_DIR = Path(__file__).resolve().parent.parent.parent
",24,46,0,0,
343,48290403,48313226,54763,"Python node2vec (Gensim Word2Vec) ""Process finished with exit code 134 (interrupted by signal 6: SIGABRT)""",5,<python><pycharm><word2vec><gensim>,14,"<p>I am working on node2vec in Python, which uses Gensim's <code>Word2Vec</code> internally.</p>
<p>When I am using a small dataset, the code works well. But as soon as I try to run the same code on a large dataset, the code crashes:</p>
<blockquote>
<p>Error:  Process finished with exit code 134 (interrupted by signal 6: SIGABRT).</p>
</blockquote>
<p>The line which is giving the error is</p>
<pre><code>model = Word2Vec(walks, size=args.dimensions,
                 window=args.window_size, min_count=0, sg=1,
                 workers=args.workers, iter=args.iter)
</code></pre>
<p>I am using <a href=""https://en.wikipedia.org/wiki/PyCharm"" rel=""nofollow noreferrer"">PyCharm</a> and Python 3.5.</p>
<p>What is happening? I could not find any post which could solve my problem.</p>
",1070734,574,16-01-2018 21:51,18-01-2018 03:06,2,574,15,1,7,62,"{'badge_counts': {'bronze': 15, 'silver': 7, 'gold': 1}, 'account_id': 1071625, 'is_employee': False, 'last_modified_date': 1634885700, 'last_access_date': 1636978770, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 574, 'creation_date': 1322548878, 'user_type': 'registered', 'user_id': 1070734, 'accept_rate': 62, 'location': 'Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/1070734/zohaib-brohi', 'profile_image': 'https://i.stack.imgur.com/vuDMX.jpg?s=256&g=1', 'display_name': 'Zohaib Brohi'}","I am working on node2vec in Python, which uses Gensim's internally. When I am using a small dataset, the code works well. But as soon as I try to run the same code on a large dataset, the code crashes: Error: Process finished with exit code 134 (interrupted by signal 6: SIGABRT). The line which is giving the error is I am using PyCharm and Python 3.5. What is happening? I could not find any post which could solve my problem.","Word2Vec model = Word2Vec(walks, size=args.dimensions,
                 window=args.window_size, min_count=0, sg=1,
                 workers=args.workers, iter=args.iter)
",1,12,0,1,
344,48452933,48453042,20812,Python comparison ignoring nan,3,<python><python-2.7><pandas><nan><equality>,17,"<p>While <code>nan == nan</code> is always <code>False</code>, in many cases people want to treat them as equal, and this is enshrined in <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.equals.html"" rel=""noreferrer""><code>pandas.DataFrame.equals</code></a>:</p>

<blockquote>
  <p>NaNs in the same location are considered equal.</p>
</blockquote>

<p>Of course, I can write</p>

<pre><code>def equalp(x, y):
    return (x == y) or (math.isnan(x) and math.isnan(y))
</code></pre>

<p>However, this will fail on containers like <code>[float(""nan"")]</code> and <code>isnan</code> barfs on non-numbers (so <a href=""https://stackoverflow.com/q/4187185/850781"">the complexity increases</a>).</p>

<p>So, what do people do to compare complex Python objects which may contain <code>nan</code>?</p>

<p><strong>PS</strong>. Motivation: when comparing two rows in a pandas <code>DataFrame</code>, I would <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html"" rel=""noreferrer"">convert them into <code>dict</code>s</a> and compare dicts element-wise. </p>

<p><strong>PPS</strong>. When I say ""<em>compare</em>"", I am thinking <a href=""https://www.unix.com/man-page/all/1/diff/"" rel=""noreferrer""><code>diff</code></a>, not <a href=""http://clhs.lisp.se/Body/f_equalp.htm"" rel=""noreferrer""><code>equalp</code></a>.</p>
",850781,59158,25-01-2018 22:27,25-01-2018 22:37,0,59184,288,30,163,83,"{'badge_counts': {'bronze': 288, 'silver': 163, 'gold': 30}, 'account_id': 453661, 'is_employee': False, 'last_modified_date': 1709876400, 'last_access_date': 1710964998, 'reputation_change_year': 199, 'reputation_change_quarter': 199, 'reputation_change_month': 20, 'reputation_change_week': -2, 'reputation_change_day': 0, 'reputation': 59184, 'creation_date': 1311020307, 'user_type': 'registered', 'user_id': 850781, 'accept_rate': 83, 'location': 'United States', 'website_url': 'http://sds.podval.org', 'link': 'https://stackoverflow.com/users/850781/sds', 'profile_image': 'https://i.stack.imgur.com/pwfBo.jpg?s=256&g=1', 'display_name': 'sds'}","While is always , in many cases people want to treat them as equal, and this is enshrined in : NaNs in the same location are considered equal. Of course, I can write However, this will fail on containers like and barfs on non-numbers (so the complexity increases). So, what do people do to compare complex Python objects which may contain ? PS. Motivation: when comparing two rows in a pandas , I would convert them into s and compare dicts element-wise. PPS. When I say ""compare"", I am thinking , not .","nan == nan False pandas.DataFrame.equals def equalp(x, y):
    return (x == y) or (math.isnan(x) and math.isnan(y))
 [float(""nan"")] isnan nan DataFrame dict diff equalp",-9,19,0,5,
345,48966891,48967848,4896,Difference between slash operator and comma separator in pathlib Path,1,<python><python-3.x><pathlib>,14,"<p>I <a href=""https://docs.python.org/3/library/pathlib.html#operators"" rel=""noreferrer"">read</a> that in <code>pathlib</code> we can create a child path by using <code>/</code> between two paths, where a comma would also work. However, I can't find out if there is a difference between the two cases. In the following example, the output is the same:</p>

<pre><code>from pathlib import Path

p = Path('/hello', 'world')
s = Path(p, 'how', 'are', 'you')
ns = Path(p / 'how', 'are', 'you')

print(s)
print(ns)
</code></pre>

<p>But considering that <code>pathlib</code> is heavily object-oriented, I guess there might be something different behind the scenes. Is there a difference between using <code>/</code> in Path in contrast with a comma?</p>
",1150683,27565,24-02-2018 19:45,24-02-2018 21:33,0,27595,254,25,142,96,"{'badge_counts': {'bronze': 254, 'silver': 142, 'gold': 25}, 'collectives': [{'collective': {'tags': ['sentiment-analysis', 'topic-modeling', 'opennlp', 'named-entity-recognition', 'word-embedding', 'nlp', 'tf-idf', 'gensim', 'spacy', 'word2vec', 'bert-language-model', 'nlp-question-answering', 'nltk', 'huggingface-transformers', 'stanford-nlp', 'spacy-3'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective focused on NLP (natural language processing), the transformation or extraction of useful information from natural language data.', 'link': '/collectives/nlp', 'name': 'NLP', 'slug': 'nlp'}, 'role': 'member'}], 'account_id': 1174306, 'is_employee': False, 'last_modified_date': 1698404700, 'last_access_date': 1711119311, 'reputation_change_year': 240, 'reputation_change_quarter': 240, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 27595, 'creation_date': 1326650625, 'user_type': 'registered', 'user_id': 1150683, 'accept_rate': 96, 'location': 'Gent, Belgi&#235;', 'website_url': 'https://bramvanroy.github.io/', 'link': 'https://stackoverflow.com/users/1150683/bram-vanroy', 'profile_image': 'https://i.stack.imgur.com/5Tbtq.jpg?s=256&g=1', 'display_name': 'Bram Vanroy'}","I read that in we can create a child path by using between two paths, where a comma would also work. However, I can't find out if there is a difference between the two cases. In the following example, the output is the same: But considering that is heavily object-oriented, I guess there might be something different behind the scenes. Is there a difference between using in Path in contrast with a comma?","pathlib / from pathlib import Path

p = Path('/hello', 'world')
s = Path(p, 'how', 'are', 'you')
ns = Path(p / 'how', 'are', 'you')

print(s)
print(ns)
 pathlib /",3,13,0,1,
346,48783271,48783423,9915,"In python assert, how to print the condition when the assertion failed?",3,<python><python-3.x><assert>,13,"<p>In Python, I can do:</p>

<pre><code>assert result==100, ""result should be 100""
</code></pre>

<p>but this is redundant since I have to type the condition twice.</p>

<p>Is there a way to tell Python to automatically show the condition when the assertion fails?</p>
",827927,35143,14-02-2018 09:07,14-02-2018 09:15,0,35305,189,37,118,74,"{'badge_counts': {'bronze': 189, 'silver': 118, 'gold': 37}, 'account_id': 439169, 'is_employee': False, 'last_modified_date': 1705716600, 'last_access_date': 1710961555, 'reputation_change_year': 536, 'reputation_change_quarter': 536, 'reputation_change_month': 272, 'reputation_change_week': 122, 'reputation_change_day': 0, 'reputation': 35305, 'creation_date': 1309774923, 'user_type': 'registered', 'user_id': 827927, 'accept_rate': 74, 'location': 'Israel', 'website_url': 'http://erelsgl.github.io', 'link': 'https://stackoverflow.com/users/827927/erel-segal-halevi', 'profile_image': 'https://www.gravatar.com/avatar/ffb6ad39ce95d58890d2e7ea2cb72a04?s=256&d=identicon&r=PG', 'display_name': 'Erel Segal-Halevi'}","In Python, I can do: but this is redundant since I have to type the condition twice. Is there a way to tell Python to automatically show the condition when the assertion fails?","assert result==100, ""result should be 100""
",0,8,0,0,
347,48382122,48382169,23919,pandas dataframe check if index exists in a multi index,3,<python><pandas><dataframe><multi-index>,14,"<p>I have a pandas Dataframe which has a multiindex created using the columns <code>userid</code> and <code>itemid</code>. df looks like this </p>

<pre><code>                  0     1     2
userid  itemid
007     5000      9     4     3
007     4000      6     7     1
009     3000      1     2     3
</code></pre>

<p>I want to check if the index [007, 6000] exists in the dataframe df. How can I do that. If I run the following code there is an error <code>TypeError: unhashable type: 'list'</code>. </p>

<pre><code>if [007, 6000] in df.index:
    print('it works')
</code></pre>
",775755,1819,22-01-2018 12:56,22-01-2018 12:59,0,1819,26,4,19,52,"{'badge_counts': {'bronze': 26, 'silver': 19, 'gold': 4}, 'account_id': 406316, 'is_employee': False, 'last_modified_date': 1607615081, 'last_access_date': 1696264692, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1819, 'creation_date': 1306732204, 'user_type': 'registered', 'user_id': 775755, 'accept_rate': 52, 'link': 'https://stackoverflow.com/users/775755/user77005', 'profile_image': 'https://www.gravatar.com/avatar/57d8329515134d4c741c8e62af0f85d6?s=256&d=identicon&r=PG', 'display_name': 'user77005'}","I have a pandas Dataframe which has a multiindex created using the columns and . df looks like this I want to check if the index [007, 6000] exists in the dataframe df. How can I do that. If I run the following code there is an error .","userid itemid                   0     1     2
userid  itemid
007     5000      9     4     3
007     4000      6     7     1
009     3000      1     2     3
 TypeError: unhashable type: 'list' if [007, 6000] in df.index:
    print('it works')
",2,14,0,0,
348,48066322,48066323,36362,Descriptive flake8 errors in PyCharm,5,<python><pycharm><static-code-analysis><flake8>,30,"<p><code>PyCharm</code> does not have a <a href=""https://stackoverflow.com/q/41899007/771848"">built-in support for <code>flake8</code> at the moment</a>. But, <code>flake8</code> can be configured to run as an external tool.</p>

<p>Sometimes, especially for Python newcomers, not every <code>flake8</code> warning is understandable and additional clarification is required. </p>

<p>We've recently stumbled upon the <a href=""https://lintlyci.github.io/Flake8Rules/"" rel=""noreferrer""><code>Flake8Rules</code> project</a> which attempts to describe every single warning in a detailed way with supportive examples.</p>

<p>Is there a way to combine PyCharm, <code>flake8</code> and <code>Flake8Rules</code> altogether to have static code analysis warnings displayed with additional descriptions or links to the <code>Flake8Rules</code> catalog?</p>
",771848,467904,02-01-2018 18:58,02-01-2018 18:58,0,468304,1210,121,1105,100,"{'badge_counts': {'bronze': 1210, 'silver': 1105, 'gold': 121}, 'account_id': 403854, 'is_employee': False, 'last_modified_date': 1703299200, 'last_access_date': 1710017178, 'reputation_change_year': 2299, 'reputation_change_quarter': 2299, 'reputation_change_month': 600, 'reputation_change_week': 180, 'reputation_change_day': 0, 'reputation': 468304, 'creation_date': 1306434854, 'user_type': 'registered', 'user_id': 771848, 'accept_rate': 100, 'location': 'New York, United States', 'website_url': 'http://www.linkedin.com/in/alexanderafanasiev', 'link': 'https://stackoverflow.com/users/771848/alecxe', 'profile_image': 'https://i.stack.imgur.com/2DTBv.jpg?s=256&g=1', 'display_name': 'alecxe'}","does not have a built-in support for at the moment. But, can be configured to run as an external tool. Sometimes, especially for Python newcomers, not every warning is understandable and additional clarification is required. We've recently stumbled upon the project which attempts to describe every single warning in a detailed way with supportive examples. Is there a way to combine PyCharm, and altogether to have static code analysis warnings displayed with additional descriptions or links to the catalog?",PyCharm flake8 flake8 flake8 Flake8Rules flake8 Flake8Rules Flake8Rules,-8,7,0,2,
349,50253517,50254135,253,How to group functions without side effects?,1,<python>,11,"<p>I have a function with several helper functions. That's fairly common case. I want to group them in a common context for readability and I'm wondering how to do it right.</p>

<ul>
<li>they take ~15 lines</li>
<li>only the main function is called from somewhere else</li>
<li>no plans on reusing the helper functions in the near future</li>
</ul>

<p>Simplified example:</p>

<pre><code>def create_filled_template_in_temp(path, values_mapping):
    template_text = path.read_text()
    filled_template = _fill_template(template_text, values_mapping)
    result_path = _save_in_temp(filled_template)
    return result_path

def _fill_template(template_text, values_mapping):
    ...

def _save_in_temp(filled_template):
    _, pathname = tempfile.mkstemp(suffix='.ini', text=True)
    path = pathlib.Path(pathname)
    path.write_text(text)
    return path

...
create_filled_template_in_temp(path, values_mapping)
</code></pre>

<p>Please note that I don't want the helper methods on the module level because they belong to only one method. Imagine having several such examples as above in the same module. Maany non-public functions on module level. A mess (and this happens many times). Also I'd like to give them context and use the context's name to simplify the naming inside.</p>

<p><strong>Solution #0: A module</strong></p>

<p>Just put it in another module:</p>

<pre><code>template_fillers.create_in_temp(path, values_mapping)
</code></pre>

<p>Problems:</p>

<ul>
<li>that's too little code to add a file, especially when there are many files already (this creates a mess)</li>
<li>this is an action and now I'm forced to create a noun-based name for the module (or break the modules naming rule). Moreover making it simple will make it too broad (in this case creating a set that really is a singleton).</li>
</ul>

<p>Finally this is just too little code to add a module for it.</p>

<p><strong>Solution #1: A class</strong></p>

<p>Create a class with no <code>__init__</code> and only one public (by naming convention) method:</p>

<pre><code>class TemplateFillerIntoTemp:
    def run(self, path, values_mapping):
        template_text = path.read_text()
        filled_template = self._fill_template(template_text, values_mapping)
        result_path = self._save_in_temp(filled_template)
        return result_path

    def _fill_template(self, template_text, values_mapping):
        ...

    def _save_in_temp(self, filled_template):
        _, pathname = tempfile.mkstemp(suffix='.ini', text=True)
        path = pathlib.Path(pathname)
        path.write_text(text)
        return path

 ...
 TemplateFillerIntoTemp().run(path, values_mapping)
</code></pre>

<p>This is what I did many times in the past. Problems:</p>

<ul>
<li>there are no side effects, so there's no need to have the class' instance</li>
<li>this is an action and now I'm forced to create a noun-based name for the class (or break the classes naming rule). This leads to many of those ""managers"" or ""creators"".</li>
<li>this is a misuse of a class concept, this is just a little execution tree with a single function-interface, not a class of things. Misusing concepts slows down understanding and may lead to further blending between uses. I know that in OOP this is common because in some languages you can't really make a function outside of a class, but this is too radical approach to order in code. Objects are useful when they are the closest expression of your idea. This isn't the case. Forcing not fitting order paradoxically generates disorder of a different kind :)</li>
</ul>

<p><strong>Solution #2: Static class</strong></p>

<p>Take solution #1, add <code>@staticmethod</code> everywhere. Possibly also ABC metaclass.</p>

<pre><code> TemplateFillerIntoTemp.run(path, values_mapping)
</code></pre>

<p>Pro: there is a clear indication that this all is instance-independent. Con: there's more code.</p>

<p><strong>Solution #3: Class with a __call__</strong></p>

<p>Take solution #1, create a <code>__call__</code> function with the main method, then create on module level a single instance called <code>create_filled_template_in_temp</code>.</p>

<pre><code>create_filled_template_in_temp(path, values_mapping)
</code></pre>

<p>Pro: calls like a single function. Con: implementation is overblown, not really fit for the purpose.</p>

<p><strong>Solution #4: Insert helper functions into main function</strong></p>

<p>Add them inside.</p>

<pre><code>def create_filled_template_in_temp(path, values_mapping):
    def _fill_template(template_text, values_mapping):
        ...

    def _save_in_temp(filled_template):
        _, pathname = tempfile.mkstemp(suffix='.ini', text=True)
        path = pathlib.Path(pathname)
        path.write_text(text)
        return path

    template_text = path.read_text()
    filled_template = _fill_template(template_text, values_mapping)
    result_path = _save_in_temp(filled_template)
    return result_path

...
create_filled_template_in_temp(path, values_mapping)
</code></pre>

<p>Pro: this looks well if total number of lines is small and there are very few helper functions. Con: it doesn't otherwise.</p>
",731634,4152,09-05-2018 12:31,09-05-2018 13:04,0,4152,30,1,26,25,"{'badge_counts': {'bronze': 30, 'silver': 26, 'gold': 1}, 'account_id': 378729, 'is_employee': False, 'last_modified_date': 1684144200, 'last_access_date': 1711006480, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4152, 'creation_date': 1304104025, 'user_type': 'registered', 'user_id': 731634, 'accept_rate': 25, 'location': 'Warszawa, Polska', 'website_url': '', 'link': 'https://stackoverflow.com/users/731634/ctrl-c', 'profile_image': 'https://www.gravatar.com/avatar/fb52adb469e2994efc68fd18296982bc?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ctrl-C'}","I have a function with several helper functions. That's fairly common case. I want to group them in a common context for readability and I'm wondering how to do it right. they take ~15 lines only the main function is called from somewhere else no plans on reusing the helper functions in the near future Simplified example: Please note that I don't want the helper methods on the module level because they belong to only one method. Imagine having several such examples as above in the same module. Maany non-public functions on module level. A mess (and this happens many times). Also I'd like to give them context and use the context's name to simplify the naming inside. Solution #0: A module Just put it in another module: Problems: that's too little code to add a file, especially when there are many files already (this creates a mess) this is an action and now I'm forced to create a noun-based name for the module (or break the modules naming rule). Moreover making it simple will make it too broad (in this case creating a set that really is a singleton). Finally this is just too little code to add a module for it. Solution #1: A class Create a class with no and only one public (by naming convention) method: This is what I did many times in the past. Problems: there are no side effects, so there's no need to have the class' instance this is an action and now I'm forced to create a noun-based name for the class (or break the classes naming rule). This leads to many of those ""managers"" or ""creators"". this is a misuse of a class concept, this is just a little execution tree with a single function-interface, not a class of things. Misusing concepts slows down understanding and may lead to further blending between uses. I know that in OOP this is common because in some languages you can't really make a function outside of a class, but this is too radical approach to order in code. Objects are useful when they are the closest expression of your idea. This isn't the case. Forcing not fitting order paradoxically generates disorder of a different kind :) Solution #2: Static class Take solution #1, add everywhere. Possibly also ABC metaclass. Pro: there is a clear indication that this all is instance-independent. Con: there's more code. Solution #3: Class with a __call__ Take solution #1, create a function with the main method, then create on module level a single instance called . Pro: calls like a single function. Con: implementation is overblown, not really fit for the purpose. Solution #4: Insert helper functions into main function Add them inside. Pro: this looks well if total number of lines is small and there are very few helper functions. Con: it doesn't otherwise.","def create_filled_template_in_temp(path, values_mapping):
    template_text = path.read_text()
    filled_template = _fill_template(template_text, values_mapping)
    result_path = _save_in_temp(filled_template)
    return result_path

def _fill_template(template_text, values_mapping):
    ...

def _save_in_temp(filled_template):
    _, pathname = tempfile.mkstemp(suffix='.ini', text=True)
    path = pathlib.Path(pathname)
    path.write_text(text)
    return path

...
create_filled_template_in_temp(path, values_mapping)
 template_fillers.create_in_temp(path, values_mapping)
 __init__ class TemplateFillerIntoTemp:
    def run(self, path, values_mapping):
        template_text = path.read_text()
        filled_template = self._fill_template(template_text, values_mapping)
        result_path = self._save_in_temp(filled_template)
        return result_path

    def _fill_template(self, template_text, values_mapping):
        ...

    def _save_in_temp(self, filled_template):
        _, pathname = tempfile.mkstemp(suffix='.ini', text=True)
        path = pathlib.Path(pathname)
        path.write_text(text)
        return path

 ...
 TemplateFillerIntoTemp().run(path, values_mapping)
 @staticmethod  TemplateFillerIntoTemp.run(path, values_mapping)
 __call__ create_filled_template_in_temp create_filled_template_in_temp(path, values_mapping)
 def create_filled_template_in_temp(path, values_mapping):
    def _fill_template(template_text, values_mapping):
        ...

    def _save_in_temp(filled_template):
        _, pathname = tempfile.mkstemp(suffix='.ini', text=True)
        path = pathlib.Path(pathname)
        path.write_text(text)
        return path

    template_text = path.read_text()
    filled_template = _fill_template(template_text, values_mapping)
    result_path = _save_in_temp(filled_template)
    return result_path

...
create_filled_template_in_temp(path, values_mapping)
",45,121,0,0,
350,48770822,48771070,8834,"How to make holes in a Polygon in shapely python, having Polygons",1,<python><polygon><shapely>,17,"<p>In python, I have a plain Polygon ""outer"" and a list of Polygons ""inners"". I want to make holes in my polygon using this list.</p>

<pre><code>from shapely.geometry import Polygon

# polygon with 1 hole in the middle
p = Polygon(((0,0),(10,0),(10,10),(0,10)), (((4,4),(4,6),(6,6),(6,4)), ))
print p.wkt
# POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0), (4 4, 4 6, 6 6, 6 4, 4 4))

# other constructor, does not work (no hole) :
outer = Polygon(((0,0),(10,0),(10,10),(0,10),(0,0)))
inners = (Polygon(((4,4),(4,6),(6,6),(6,4),(4,4))), )
p = Polygon(outer, inners)
print p.wkt
# POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0))
</code></pre>

<p>How to build p given outer and inners ?</p>
",703421,2162,13-02-2018 16:01,13-02-2018 16:13,0,2162,34,4,22,53,"{'badge_counts': {'bronze': 34, 'silver': 22, 'gold': 4}, 'account_id': 361057, 'is_employee': False, 'last_modified_date': 1711122097, 'last_access_date': 1711127986, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2162, 'creation_date': 1302586771, 'user_type': 'registered', 'user_id': 703421, 'accept_rate': 53, 'location': 'Font-Romeu, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/703421/eric-h', 'profile_image': 'https://www.gravatar.com/avatar/cc9d3832c6a535c30622f37b9de309bf?s=256&d=identicon&r=PG', 'display_name': 'Eric H.'}","In python, I have a plain Polygon ""outer"" and a list of Polygons ""inners"". I want to make holes in my polygon using this list. How to build p given outer and inners ?","from shapely.geometry import Polygon

# polygon with 1 hole in the middle
p = Polygon(((0,0),(10,0),(10,10),(0,10)), (((4,4),(4,6),(6,6),(6,4)), ))
print p.wkt
# POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0), (4 4, 4 6, 6 6, 6 4, 4 4))

# other constructor, does not work (no hole) :
outer = Polygon(((0,0),(10,0),(10,10),(0,10),(0,0)))
inners = (Polygon(((4,4),(4,6),(6,6),(6,4),(4,4))), )
p = Polygon(outer, inners)
print p.wkt
# POLYGON ((0 0, 10 0, 10 10, 0 10, 0 0))
",12,18,0,0,
351,48935907,48936334,46353,tqdm not showing bar,3,<python><tqdm>,32,"<p>I'm using the tqdm library and it doesn't give me the progress bar, instead it gives me output that looks like this where it just tells me the iteration:</p>

<p><code>251it [01:44,  2.39it/s]</code></p>

<p>Any idea why the code would do this? I thought it was maybe because I was passing it a generator but then again I've used generators in the past that have worked. I've never really messed with tdqm formatting before. Here is part of the source code:</p>

<pre><code>train_iter = zip(train_x, train_y) #train_x and train_y are just lists of elements
....
def train(train_iter, model, criterion, optimizer):
    model.train()
    total_loss = 0
    for x, y in tqdm(train_iter):
        x = x.transpose(0, 1)
        y = y.transpose(0, 1)
        optimizer.zero_grad()
        bloss = model.forward(x, y, criterion)   
        bloss.backward()
        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)
        optimizer.step()        
        total_loss += bloss.data[0]
    return total_loss
</code></pre>
",699168,1679,22-02-2018 19:41,22-02-2018 20:09,0,1679,33,3,22,88,"{'badge_counts': {'bronze': 33, 'silver': 22, 'gold': 3}, 'account_id': 358413, 'is_employee': False, 'last_modified_date': 1695090300, 'last_access_date': 1692050754, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1679, 'creation_date': 1302287627, 'user_type': 'registered', 'user_id': 699168, 'accept_rate': 88, 'website_url': '', 'link': 'https://stackoverflow.com/users/699168/matt', 'profile_image': 'https://www.gravatar.com/avatar/200251a89493e51a1ba46339d8a1f75e?s=256&d=identicon&r=PG', 'display_name': 'Matt'}","I'm using the tqdm library and it doesn't give me the progress bar, instead it gives me output that looks like this where it just tells me the iteration: Any idea why the code would do this? I thought it was maybe because I was passing it a generator but then again I've used generators in the past that have worked. I've never really messed with tdqm formatting before. Here is part of the source code:","251it [01:44,  2.39it/s] train_iter = zip(train_x, train_y) #train_x and train_y are just lists of elements
....
def train(train_iter, model, criterion, optimizer):
    model.train()
    total_loss = 0
    for x, y in tqdm(train_iter):
        x = x.transpose(0, 1)
        y = y.transpose(0, 1)
        optimizer.zero_grad()
        bloss = model.forward(x, y, criterion)   
        bloss.backward()
        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)
        optimizer.step()        
        total_loss += bloss.data[0]
    return total_loss
",13,22,0,0,
352,48567245,48696622,19095,pipenv: deployment workflow,5,<python><pipenv>,25,"<p>I am thinking about switching from pip &amp; virtualenv to pipenv.  But after studying the documentation I am still at a loss on how the creators of pipenv structured the deployment workflow.</p>

<p>For example, in development I have a <code>Pipfile</code> &amp; a <code>Pipfile.lock</code> that define the environment.  Using a deployment script I want to deploy</p>

<ol>
<li><code>git pull</code> via Github to production server</li>
<li><code>pipenv install</code> creates/refreshes the environment in the home directory of the deployment user</li>
</ol>

<p>But I need a venv in a <strong>specific</strong> directory which is already configured in systemd or supervisor.  E.g.: <code>command=/home/ubuntu/production/application_xy/env/bin/gunicorn module:app</code></p>

<p>pipenv creates the env in some location such as 
<code>/home/ultimo/.local/share/virtualenvs/application_xy-jvrv1OSi</code></p>

<p><strong>What is the intended workflow to deploy an application with <code>pipenv</code>?</strong></p>
",1352932,1409,01-02-2018 16:17,08-02-2018 23:45,7,1409,55,3,30,66,"{'badge_counts': {'bronze': 55, 'silver': 30, 'gold': 3}, 'account_id': 1430404, 'is_employee': False, 'last_modified_date': 1706321700, 'last_access_date': 1679651677, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1409, 'creation_date': 1335247239, 'user_type': 'registered', 'user_id': 1352932, 'accept_rate': 66, 'website_url': '', 'link': 'https://stackoverflow.com/users/1352932/caliph', 'profile_image': 'https://www.gravatar.com/avatar/3b1ed1b8d18f1fee723f1ff434b96a4c?s=256&d=identicon&r=PG', 'display_name': 'caliph'}","I am thinking about switching from pip &amp; virtualenv to pipenv. But after studying the documentation I am still at a loss on how the creators of pipenv structured the deployment workflow. For example, in development I have a &amp; a that define the environment. Using a deployment script I want to deploy via Github to production server creates/refreshes the environment in the home directory of the deployment user But I need a venv in a specific directory which is already configured in systemd or supervisor. E.g.: pipenv creates the env in some location such as What is the intended workflow to deploy an application with ?",Pipfile Pipfile.lock git pull pipenv install command=/home/ubuntu/production/application_xy/env/bin/gunicorn module:app /home/ultimo/.local/share/virtualenvs/application_xy-jvrv1OSi pipenv,-7,15,0,0,
353,48505458,48672858,1965,Coverage of Cython module using py.test and coverage.py,1,<python><code-coverage><cython><coverage.py>,19,"<p>I want to get coverage information of a Cython module using some (unit) tests written in Python. What I have right now is coverage of the <em>tests themselves</em>, i.e. which lines of the tests are executed by running <code>py.test</code>. While nice to look at, I would rather get coverage of the <code>.pyx</code> file, i.e. which lines of the C/Python interface are covered by my tests.</p>

<p>I found some info already but wasn't able to get it running for my project:</p>

<p><a href=""http://blog.behnel.de/posts/coverage-analysis-for-cython-modules.html"" rel=""noreferrer"">http://blog.behnel.de/posts/coverage-analysis-for-cython-modules.html</a></p>

<p><a href=""https://medium.com/@dfdeshom/better-test-coverage-workflow-for-cython-modules-631615eb197a"" rel=""noreferrer"">https://medium.com/@dfdeshom/better-test-coverage-workflow-for-cython-modules-631615eb197a</a></p>

<p><a href=""https://stackoverflow.com/questions/34935796/how-to-use-coverage-analysis-with-cython"">How to use coverage analysis with Cython</a></p>

<p>This is the code in question: <a href=""https://github.com/SCIP-Interfaces/PySCIPOpt/tree/coverage"" rel=""noreferrer"">https://github.com/SCIP-Interfaces/PySCIPOpt/tree/coverage</a></p>
",673271,6425,29-01-2018 16:08,07-02-2018 20:39,9,6435,67,3,37,91,"{'badge_counts': {'bronze': 67, 'silver': 37, 'gold': 3}, 'account_id': 342225, 'is_employee': False, 'last_modified_date': 1699271814, 'last_access_date': 1711118355, 'reputation_change_year': 48, 'reputation_change_quarter': 48, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6435, 'creation_date': 1300892166, 'user_type': 'registered', 'user_id': 673271, 'accept_rate': 91, 'location': 'Berlin, Germany', 'website_url': 'https://mattmilten.github.io/', 'link': 'https://stackoverflow.com/users/673271/mattmilten', 'profile_image': 'https://www.gravatar.com/avatar/659e21e1995421c6d1615b142b066400?s=256&d=identicon&r=PG', 'display_name': 'mattmilten'}","I want to get coverage information of a Cython module using some (unit) tests written in Python. What I have right now is coverage of the tests themselves, i.e. which lines of the tests are executed by running . While nice to look at, I would rather get coverage of the file, i.e. which lines of the C/Python interface are covered by my tests. I found some info already but wasn't able to get it running for my project: http://blog.behnel.de/posts/coverage-analysis-for-cython-modules.html https://medium.com/@dfdeshom/better-test-coverage-workflow-for-cython-modules-631615eb197a How to use coverage analysis with Cython This is the code in question: https://github.com/SCIP-Interfaces/PySCIPOpt/tree/coverage",py.test .pyx,-2,11,0,4,
354,48925328,48941123,14345,How to get all noun phrases in Spacy,4,<python><nlp><spacy>,11,"<p>I am new to <code>Spacy</code> and I would like to extract ""all"" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:</p>

<pre><code>import spacy

nlp = spacy.load(""en"")

file = open(""E:/test.txt"", ""r"")
doc = nlp(file.read())
for np in doc.noun_chunks:
    print(np.text)
</code></pre>

<p>But it returns only the base noun phrases, that is, phrases which don't have any other <code>NP</code> in them. That is, for the following phrase, I get the result below:</p>

<p>Phrase: <code>We try to explicitly describe the geometry of the edges of the images.</code></p>

<p>Result: <code>We, the geometry, the edges, the images</code>.</p>

<p>Expected result: <code>We, the geometry, the edges, the images, the geometry of the edges of the images, the edges of the images.</code></p>

<p>How can I get all the noun phrases, including nested phrases?</p>
",1419243,1665,22-02-2018 10:41,23-02-2018 04:28,1,1665,35,3,19,59,"{'badge_counts': {'bronze': 35, 'silver': 19, 'gold': 3}, 'account_id': 1518987, 'is_employee': False, 'last_modified_date': 1665800700, 'last_access_date': 1675616393, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1665, 'creation_date': 1338049761, 'user_type': 'registered', 'user_id': 1419243, 'accept_rate': 59, 'link': 'https://stackoverflow.com/users/1419243/user1419243', 'profile_image': 'https://www.gravatar.com/avatar/efed377c0e334e42eea910486e1d5f0e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user1419243'}","I am new to and I would like to extract ""all"" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code: But it returns only the base noun phrases, that is, phrases which don't have any other in them. That is, for the following phrase, I get the result below: Phrase: Result: . Expected result: How can I get all the noun phrases, including nested phrases?","Spacy import spacy

nlp = spacy.load(""en"")

file = open(""E:/test.txt"", ""r"")
doc = nlp(file.read())
for np in doc.noun_chunks:
    print(np.text)
 NP We try to explicitly describe the geometry of the edges of the images. We, the geometry, the edges, the images We, the geometry, the edges, the images, the geometry of the edges of the images, the edges of the images.",2,21,0,0,
355,48373532,48373612,1908,Where are the inaccuracies in math.sqrt() and math.pow() coming from for large numbers?,2,<python><pow><sqrt>,12,"<p>If you take a number, take its square root, drop the decimal, and then raise it to the second power, the result should always be less than or equal to the original number.</p>

<p>This seems to hold true in python until you try it on <code>99999999999999975425</code> for some reason.</p>

<pre><code>import math

def check(n):
    assert math.pow(math.floor(math.sqrt(n)), 2) &lt;= n

check(99999999999999975424)  # No exception.
check(99999999999999975425)  # Throws AssertionError.
</code></pre>

<p>It looks like <code>math.pow(math.floor(math.sqrt(99999999999999975425)), 2)</code> returns <code>1e+20</code>.</p>

<p>I assume this has something to do with the way we store values in python... something related to floating point arithmetic, but I can't reason about specifically how that affects this case.</p>
",612192,1368,22-01-2018 01:21,22-01-2018 01:33,0,1368,20,2,10,31,"{'badge_counts': {'bronze': 20, 'silver': 10, 'gold': 2}, 'account_id': 303326, 'is_employee': False, 'last_modified_date': 1652193275, 'last_access_date': 1702484367, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1368, 'creation_date': 1297375958, 'user_type': 'registered', 'user_id': 612192, 'accept_rate': 31, 'location': 'United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/612192/evan', 'profile_image': 'https://www.gravatar.com/avatar/791db340b99a41843ac0922ed74aa883?s=256&d=identicon&r=PG', 'display_name': 'Evan'}","If you take a number, take its square root, drop the decimal, and then raise it to the second power, the result should always be less than or equal to the original number. This seems to hold true in python until you try it on for some reason. It looks like returns . I assume this has something to do with the way we store values in python... something related to floating point arithmetic, but I can't reason about specifically how that affects this case.","99999999999999975425 import math

def check(n):
    assert math.pow(math.floor(math.sqrt(n)), 2) &lt;= n

check(99999999999999975424)  # No exception.
check(99999999999999975425)  # Throws AssertionError.
 math.pow(math.floor(math.sqrt(99999999999999975425)), 2) 1e+20",3,16,0,0,
356,50168532,50168890,7481,keep both merging keys after pandas.merge_asof,2,<python><pandas><merge>,22,"<p>I have found this nice function  <code>pandas.merge_asof</code>.
From the documentation</p>

<pre><code>pandas.merge_asof(left, right, on=None, left_on=None, right_on=None)

Parameters: 

left : DataFrame
right : DataFrame
on : label

Field name to join on. Must be found in both DataFrames.
The data MUST be ordered. 
Furthermore this must be a numeric column,such as datetimelike, integer, or float. 
On or left_on/right_on must be given.
</code></pre>

<p>and it works as expected.</p>

<p>However, my merged dataframe keeps as columns <code>on</code> only the one that originally was in <code>left</code>. I would need to keep them both, so to have </p>

<pre><code>   mydf=pandas.merge_asof(left, right, on='Time')
</code></pre>

<p>and <code>mydf</code> to contain both <code>Time</code> from <code>left</code> and <code>right</code></p>

<p>Example data:</p>

<pre><code>a=pd.DataFrame(data=pd.date_range('20100201', periods=100, freq='6h3min'),columns=['Time'])
b=pd.DataFrame(data=
                  pd.date_range('20100201', periods=24, freq='1h'),columns=['Time'])
b['val']=range(b.shape[0])
out=pd.merge_asof(a,b,on='Time',direction='forward',tolerance=pd.Timedelta('30min'))
</code></pre>
",1506850,4997,04-05-2018 06:28,04-05-2018 06:51,0,4997,95,9,44,94,"{'badge_counts': {'bronze': 95, 'silver': 44, 'gold': 9}, 'account_id': 1632591, 'is_employee': False, 'last_modified_date': 1706004900, 'last_access_date': 1711050060, 'reputation_change_year': 73, 'reputation_change_quarter': 73, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4997, 'creation_date': 1341580556, 'user_type': 'registered', 'user_id': 1506850, 'accept_rate': 94, 'location': 'Chiang Mai Thailand', 'website_url': '', 'link': 'https://stackoverflow.com/users/1506850/00-00-00', 'profile_image': 'https://www.gravatar.com/avatar/643e79721a0b7cf15b842b56ea1bfb35?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': '00__00__00'}","I have found this nice function . From the documentation and it works as expected. However, my merged dataframe keeps as columns only the one that originally was in . I would need to keep them both, so to have and to contain both from and Example data:","pandas.merge_asof pandas.merge_asof(left, right, on=None, left_on=None, right_on=None)

Parameters: 

left : DataFrame
right : DataFrame
on : label

Field name to join on. Must be found in both DataFrames.
The data MUST be ordered. 
Furthermore this must be a numeric column,such as datetimelike, integer, or float. 
On or left_on/right_on must be given.
 on left    mydf=pandas.merge_asof(left, right, on='Time')
 mydf Time left right a=pd.DataFrame(data=pd.date_range('20100201', periods=100, freq='6h3min'),columns=['Time'])
b=pd.DataFrame(data=
                  pd.date_range('20100201', periods=24, freq='1h'),columns=['Time'])
b['val']=range(b.shape[0])
out=pd.merge_asof(a,b,on='Time',direction='forward',tolerance=pd.Timedelta('30min'))
",8,34,0,0,
357,50089400,50092991,19189,Introspecting a WSDL with Python Zeep,2,<python><soap><wsdl><zeep><soappy>,24,"<p>I am attempting to use Zeep to describe the operations and types in a given WSDL, so that a program knows the operation names, their parameter names, the parameter types, and parameter attributes. </p>

<p>This info will be used to dynamically generate a UI for a given WSDL.</p>

<p>What I have got so far is just the string representations of the operations and types. Using code similar to what is found in <a href=""https://stackoverflow.com/questions/44885439/parse-wsdl-with-zeep"">this answer</a>.</p>

<p>Here's an example:</p>

<pre><code>from zeep import Client
import operator

wsdl = 'http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.wsdl'
client = Client(wsdl)

# get each operation signature
for service in client.wsdl.services.values():
    print(""service:"", service.name)
    for port in service.ports.values():
        operations = sorted(
            port.binding._operations.values(),
            key=operator.attrgetter('name'))

        for operation in operations:
            print(""method :"", operation.name)
            print(""  input :"", operation.input.signature())
            print()
    print()

# get a specific type signature by name
complextype = client.get_type('ns0:CartGetRequest')
print(complextype.name)
print(complextype.signature())
</code></pre>

<p>This gives an output like the following (shortened for brevity)</p>

<pre><code>[...]

method : CartCreate
  input : MarketplaceDomain: xsd:string, AWSAccessKeyId: xsd:string, AssociateTag: xsd:string, Validate: xsd:string, XMLEscaping: xsd:string, Shared: ns0:CartCreateRequest, Request: ns0:CartCreateRequest[]

method : CartGet
  input : MarketplaceDomain: xsd:string, AWSAccessKeyId: xsd:string, AssociateTag: xsd:string, Validate: xsd:string, XMLEscaping: xsd:string, Shared: ns0:CartGetRequest, Request: ns0:CartGetRequest[]

[...]


CartGetRequest
{http://webservices.amazon.com/AWSECommerceService/2011-08-01}CartGetRequest(CartId: xsd:string, HMAC: xsd:string, MergeCart: xsd:string, ResponseGroup: xsd:string[])
</code></pre>

<p>The string representations returned by .signature() have  the names and types, but I don't know how to parse them out individually. I have tried looping over each objects attrs with dir() as well, and they don't contain this info. It seems to be nested much deeper.</p>

<p>I could parse the string representations themselves, but then I am also missing whether or not the parameter is optional (more specifically, if it has the attribute minOccurs=0 </p>

<p>It seems like <a href=""http://www.diveintopython.net/soap_web_services/introspection.html"" rel=""noreferrer"">SOAPpy actually has this functionality</a>, but isn't maintained anymore.</p>

<p>So is there a way to introspect a WSDL with zeep that provides granular information about each operation, it's parameter names, types, and attributes similar to the SOAPpy implementation? Or should I parse the signature, or alternatively, parse the WSDL with a regular XML parser.</p>
",608725,1676,29-04-2018 17:26,30-04-2018 01:42,1,1676,30,4,19,61,"{'badge_counts': {'bronze': 30, 'silver': 19, 'gold': 4}, 'account_id': 301169, 'is_employee': False, 'last_modified_date': 1573684316, 'last_access_date': 1622741902, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1676, 'creation_date': 1297194990, 'user_type': 'registered', 'user_id': 608725, 'accept_rate': 61, 'location': 'Toronto, Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/608725/jesse-galley', 'profile_image': 'https://www.gravatar.com/avatar/29568b4c87880b41a8722a8058760a5f?s=256&d=identicon&r=PG', 'display_name': 'jesse_galley'}","I am attempting to use Zeep to describe the operations and types in a given WSDL, so that a program knows the operation names, their parameter names, the parameter types, and parameter attributes. This info will be used to dynamically generate a UI for a given WSDL. What I have got so far is just the string representations of the operations and types. Using code similar to what is found in this answer. Here's an example: This gives an output like the following (shortened for brevity) The string representations returned by .signature() have the names and types, but I don't know how to parse them out individually. I have tried looping over each objects attrs with dir() as well, and they don't contain this info. It seems to be nested much deeper. I could parse the string representations themselves, but then I am also missing whether or not the parameter is optional (more specifically, if it has the attribute minOccurs=0 It seems like SOAPpy actually has this functionality, but isn't maintained anymore. So is there a way to introspect a WSDL with zeep that provides granular information about each operation, it's parameter names, types, and attributes similar to the SOAPpy implementation? Or should I parse the signature, or alternatively, parse the WSDL with a regular XML parser.","from zeep import Client
import operator

wsdl = 'http://webservices.amazon.com/AWSECommerceService/AWSECommerceService.wsdl'
client = Client(wsdl)

# get each operation signature
for service in client.wsdl.services.values():
    print(""service:"", service.name)
    for port in service.ports.values():
        operations = sorted(
            port.binding._operations.values(),
            key=operator.attrgetter('name'))

        for operation in operations:
            print(""method :"", operation.name)
            print(""  input :"", operation.input.signature())
            print()
    print()

# get a specific type signature by name
complextype = client.get_type('ns0:CartGetRequest')
print(complextype.name)
print(complextype.signature())
 [...]

method : CartCreate
  input : MarketplaceDomain: xsd:string, AWSAccessKeyId: xsd:string, AssociateTag: xsd:string, Validate: xsd:string, XMLEscaping: xsd:string, Shared: ns0:CartCreateRequest, Request: ns0:CartCreateRequest[]

method : CartGet
  input : MarketplaceDomain: xsd:string, AWSAccessKeyId: xsd:string, AssociateTag: xsd:string, Validate: xsd:string, XMLEscaping: xsd:string, Shared: ns0:CartGetRequest, Request: ns0:CartGetRequest[]

[...]


CartGetRequest
{http://webservices.amazon.com/AWSECommerceService/2011-08-01}CartGetRequest(CartId: xsd:string, HMAC: xsd:string, MergeCart: xsd:string, ResponseGroup: xsd:string[])
",35,58,0,2,
358,49433936,49433937,457728,How do I initialize weights in PyTorch?,10,<python><machine-learning><deep-learning><neural-network><pytorch>,268,"<p>How do I initialize weights and biases of a network (via e.g. He or Xavier initialization)?</p>
",604734,24850,22-03-2018 16:34,22-03-2018 16:34,0,24878,100,22,78,100,"{'badge_counts': {'bronze': 100, 'silver': 78, 'gold': 22}, 'collectives': [{'collective': {'tags': ['sentiment-analysis', 'topic-modeling', 'opennlp', 'named-entity-recognition', 'word-embedding', 'nlp', 'tf-idf', 'gensim', 'spacy', 'word2vec', 'bert-language-model', 'nlp-question-answering', 'nltk', 'huggingface-transformers', 'stanford-nlp', 'spacy-3'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective focused on NLP (natural language processing), the transformation or extraction of useful information from natural language data.', 'link': '/collectives/nlp', 'name': 'NLP', 'slug': 'nlp'}, 'role': 'member'}], 'account_id': 298619, 'is_employee': False, 'last_modified_date': 1705929000, 'last_access_date': 1709236086, 'reputation_change_year': 298, 'reputation_change_quarter': 298, 'reputation_change_month': 58, 'reputation_change_week': 28, 'reputation_change_day': 0, 'reputation': 24878, 'creation_date': 1296940209, 'user_type': 'registered', 'user_id': 604734, 'accept_rate': 100, 'location': 'S&#227;o Paulo, State of S&#227;o Paulo, Brazil', 'website_url': 'http://www.fabioperez.com.br/', 'link': 'https://stackoverflow.com/users/604734/f%c3%a1bio-perez', 'profile_image': 'https://www.gravatar.com/avatar/b7b889367f06fa0457298888c3402ee9?s=256&d=identicon&r=PG', 'display_name': 'F&#225;bio Perez'}",How do I initialize weights and biases of a network (via e.g. He or Xavier initialization)?,,0,1,0,0,
359,48298009,48298230,5821,Can I give a click option another name?,1,<python><command-line-interface><python-click>,18,"<p>I use <code>click</code> like this:</p>

<pre><code>import click

@click.command(name='foo')
@click.option('--bar', required=True)
def do_something(bar):
    print(bar)
</code></pre>

<p>So the name of the option and the name of the function parameter is the same. When it is not the same (e.g. when you want <code>--id</code> instead of <code>--bar</code>), I get:</p>

<pre><code>TypeError: do_something() got an unexpected keyword argument 'id'
</code></pre>

<p>I don't want the parameter to be called <code>id</code> because that is a Python function. I don't want the CLI parameter to be called different, because it would be more cumbersome / less intuitive. How can I fix it?</p>
",562769,129983,17-01-2018 09:46,17-01-2018 09:58,0,130438,995,162,641,61,"{'badge_counts': {'bronze': 995, 'silver': 641, 'gold': 162}, 'account_id': 271958, 'is_employee': False, 'last_modified_date': 1710522300, 'last_access_date': 1711142867, 'reputation_change_year': 2083, 'reputation_change_quarter': 2083, 'reputation_change_month': 649, 'reputation_change_week': 305, 'reputation_change_day': 30, 'reputation': 130438, 'creation_date': 1294155598, 'user_type': 'registered', 'user_id': 562769, 'accept_rate': 61, 'location': 'M&#252;nchen, Deutschland', 'website_url': 'http://www.martin-thoma.de', 'link': 'https://stackoverflow.com/users/562769/martin-thoma', 'profile_image': 'https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG', 'display_name': 'Martin Thoma'}","I use like this: So the name of the option and the name of the function parameter is the same. When it is not the same (e.g. when you want instead of ), I get: I don't want the parameter to be called because that is a Python function. I don't want the CLI parameter to be called different, because it would be more cumbersome / less intuitive. How can I fix it?","click import click

@click.command(name='foo')
@click.option('--bar', required=True)
def do_something(bar):
    print(bar)
 --id --bar TypeError: do_something() got an unexpected keyword argument 'id'
 id",1,16,0,0,
360,49381661,49381947,38684,How do I calculate the Adjusted R-squared score using scikit-learn?,4,<python><scikit-learn>,19,"<p>I'm already using the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html"" rel=""noreferrer"">r2_score</a> function but don't understand how I can get the ""adjusted"" R^2 score from this. The description at this page doesn't mention it - maybe it's the adjusted score by default?</p>
",543315,9563,20-03-2018 10:23,20-03-2018 10:36,0,9573,78,7,55,61,"{'badge_counts': {'bronze': 78, 'silver': 55, 'gold': 7}, 'account_id': 260010, 'is_employee': False, 'last_modified_date': 1687777504, 'last_access_date': 1711111583, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 9573, 'creation_date': 1284718506, 'user_type': 'registered', 'user_id': 543315, 'accept_rate': 61, 'location': 'Normandy, France', 'website_url': 'http://www.niccottrell.com', 'link': 'https://stackoverflow.com/users/543315/nic-cottrell', 'profile_image': 'https://www.gravatar.com/avatar/1b1cc141d08272ff09997044dc980e8c?s=256&d=identicon&r=PG', 'display_name': 'Nic Cottrell'}","I'm already using the r2_score function but don't understand how I can get the ""adjusted"" R^2 score from this. The description at this page doesn't mention it - maybe it's the adjusted score by default?",,0,1,0,1,
361,49722196,49722254,43876,How does python compute the hash of a tuple,5,<python><hash><tuples>,58,"<p>In python, if I have a tuple with many elements, is its hash calculated from its elements' <code>id</code>s or its elements' content? </p>

<p>In this example,</p>

<pre><code>a = (1, [1,2])
hash(a)
</code></pre>

<p>It errors out saying list is unhashable. So I guess it's not computed by id, or probably there is a check on whether the element is mutable.</p>

<p>Now see this example</p>

<pre><code>class A: pass
a0 = A()
ta = (1, a0)
hash(ta)  # -1122968024
a0.x = 20
hash(ta)  # -1122968024
</code></pre>

<p>Here it turns out the hash of <code>ta</code> does not change with the modification of its element, i.e., <code>a0</code>. So maybe <code>a0</code>'s id is used for the hash calculation? Is <code>a0</code> somehow considered as immutable? How does python know if a type is mutable?</p>

<p>Now consider this case</p>

<pre><code>b = (1, 2)
id(b)  # 3980742764
c = (1, 2)
id(c)  # 3980732588
tb = (1, b)
tc = (1, c) 
hash(tb)  # -1383040070
hash(tc)  # -1383040070
</code></pre>

<p>It seems the content of <code>b</code> and <code>c</code> are used for the hash calculation.</p>

<p>How should I understand these examples?</p>
",534298,20345,08-04-2018 20:01,08-04-2018 20:07,0,20395,139,28,102,92,"{'badge_counts': {'bronze': 139, 'silver': 102, 'gold': 28}, 'account_id': 254713, 'is_employee': False, 'last_modified_date': 1695087000, 'last_access_date': 1711043630, 'reputation_change_year': 240, 'reputation_change_quarter': 240, 'reputation_change_month': 90, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 20395, 'creation_date': 1291760381, 'user_type': 'registered', 'user_id': 534298, 'accept_rate': 92, 'location': 'New York, NY, United States', 'website_url': 'https://nosarthur.github.io', 'link': 'https://stackoverflow.com/users/534298/nos', 'profile_image': 'https://www.gravatar.com/avatar/4ba32c06ed067564a6c6512de5217657?s=256&d=identicon&r=PG', 'display_name': 'nos'}","In python, if I have a tuple with many elements, is its hash calculated from its elements' s or its elements' content? In this example, It errors out saying list is unhashable. So I guess it's not computed by id, or probably there is a check on whether the element is mutable. Now see this example Here it turns out the hash of does not change with the modification of its element, i.e., . So maybe 's id is used for the hash calculation? Is somehow considered as immutable? How does python know if a type is mutable? Now consider this case It seems the content of and are used for the hash calculation. How should I understand these examples?","id a = (1, [1,2])
hash(a)
 class A: pass
a0 = A()
ta = (1, a0)
hash(ta)  # -1122968024
a0.x = 20
hash(ta)  # -1122968024
 ta a0 a0 a0 b = (1, 2)
id(b)  # 3980742764
c = (1, 2)
id(c)  # 3980732588
tb = (1, b)
tc = (1, c) 
hash(tb)  # -1383040070
hash(tc)  # -1383040070
 b c",6,37,0,0,
362,48380452,48503066,21766,Mask out sensitive information in python log,1,<python><security><logging><python-logging>,17,"<p>Consider the following code</p>

<pre><code>try:
    r = requests.get('https://sensitive:passw0rd@what.ever/')
    r.raise_for_status()
except requests.HTTPError:
    logging.exception(""Failed to what.ever"")
</code></pre>

<p>Here, if the endpoint returns non-successful http status code, the following will be logged</p>

<pre><code>Traceback (most recent call last):
  File ""a.py"", line 5, in &lt;module&gt;
    r.raise_for_status()
  File ""venv/lib/python3.5/site-packages/requests/models.py"", line 928, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://sensitive:passw0rd@what.ever/
</code></pre>

<p>The problem is that the <em>password</em> is logged. I could create a logging filter to filter out this line completely. However, it would be more convenient if the password was just masked out somehow. As no string is passed to <code>logging.exception</code> filtering on the app side is tricky. Where in the logging framwork can I transform a log record?</p>
",363437,12747,22-01-2018 11:23,29-01-2018 14:02,7,12757,114,9,66,62,"{'badge_counts': {'bronze': 114, 'silver': 66, 'gold': 9}, 'collectives': [{'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 150318, 'is_employee': False, 'last_modified_date': 1708024800, 'last_access_date': 1711104451, 'reputation_change_year': 96, 'reputation_change_quarter': 96, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 12757, 'creation_date': 1276173241, 'user_type': 'registered', 'user_id': 363437, 'accept_rate': 62, 'location': 'Stockholm, Sweden', 'website_url': 'http://vidstige.se/', 'link': 'https://stackoverflow.com/users/363437/vidstige', 'profile_image': 'https://www.gravatar.com/avatar/f866fca5a7395a360cca27b5e9da0ed7?s=256&d=identicon&r=PG', 'display_name': 'vidstige'}","Consider the following code Here, if the endpoint returns non-successful http status code, the following will be logged The problem is that the password is logged. I could create a logging filter to filter out this line completely. However, it would be more convenient if the password was just masked out somehow. As no string is passed to filtering on the app side is tricky. Where in the logging framwork can I transform a log record?","try:
    r = requests.get('https://sensitive:passw0rd@what.ever/')
    r.raise_for_status()
except requests.HTTPError:
    logging.exception(""Failed to what.ever"")
 Traceback (most recent call last):
  File ""a.py"", line 5, in &lt;module&gt;
    r.raise_for_status()
  File ""venv/lib/python3.5/site-packages/requests/models.py"", line 928, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://sensitive:passw0rd@what.ever/
 logging.exception",8,20,0,0,
363,49274650,49280552,10272,Directly load spacy model from packaged tar.gz file,2,<python><model><nlp><load><spacy>,15,"<p>Is it possible to load a packaged spacy model (i.e. <code>foo.tar.gz</code>) directly from the tar file instead of installing it beforehand? I would imagine something like:</p>

<pre><code>import spacy 

nlp = spacy.load(/some/path/foo.tar.gz)
</code></pre>
",350403,1295,14-03-2018 09:59,14-03-2018 14:38,0,1295,51,1,22,84,"{'badge_counts': {'bronze': 51, 'silver': 22, 'gold': 1}, 'account_id': 142604, 'is_employee': False, 'last_modified_date': 1661877600, 'last_access_date': 1681563956, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1295, 'creation_date': 1274827519, 'user_type': 'registered', 'user_id': 350403, 'accept_rate': 84, 'website_url': 'http://orkus.wordpress.com/', 'link': 'https://stackoverflow.com/users/350403/evermean', 'profile_image': 'https://www.gravatar.com/avatar/67a4a63776c9c4461b088397bf626066?s=256&d=identicon&r=PG', 'display_name': 'evermean'}",Is it possible to load a packaged spacy model (i.e. ) directly from the tar file instead of installing it beforehand? I would imagine something like:,"foo.tar.gz import spacy 

nlp = spacy.load(/some/path/foo.tar.gz)
",1,6,0,0,
364,49833866,49843108,6475,Making dashboards using altair,3,<python><altair>,12,"<p>I would like to use the excellent altair library to create dashboards. Is there a way to create the dashboards and not show any code? I see some really nice examples here: <a href=""https://altair-viz.github.io/case_studies/exploring-weather.html"" rel=""noreferrer"">https://altair-viz.github.io/case_studies/exploring-weather.html</a> but the code is visible too. Also, what is the best (well tested/easy to use) frontend for altair? Colab? Jupyter?</p>
",308827,21597,14-04-2018 16:50,15-04-2018 14:35,1,21707,431,93,265,96,"{'badge_counts': {'bronze': 431, 'silver': 265, 'gold': 93}, 'account_id': 118069, 'is_employee': False, 'last_modified_date': 1695033300, 'last_access_date': 1710715265, 'reputation_change_year': 538, 'reputation_change_quarter': 538, 'reputation_change_month': 140, 'reputation_change_week': 60, 'reputation_change_day': 0, 'reputation': 21707, 'creation_date': 1270407253, 'user_type': 'registered', 'user_id': 308827, 'accept_rate': 96, 'link': 'https://stackoverflow.com/users/308827/user308827', 'profile_image': 'https://www.gravatar.com/avatar/28f523030f3ecd9c9f2ad23ca745cc65?s=256&d=identicon&r=PG', 'display_name': 'user308827'}","I would like to use the excellent altair library to create dashboards. Is there a way to create the dashboards and not show any code? I see some really nice examples here: https://altair-viz.github.io/case_studies/exploring-weather.html but the code is visible too. Also, what is the best (well tested/easy to use) frontend for altair? Colab? Jupyter?",,0,1,0,1,
365,49139044,49159195,51676,"GeoDjango on Windows: ""Could not find the GDAL library"" / ""OSError: [WinError 126] The specified module could not be found""",11,<python><django><postgresql><postgis><geodjango>,32,"<p>I've been trying to setup my windows computer such that I can have a local postgreSQL with PostGIS extension. With this installed I hope to be able to create a project with geodjango locally before putting it in the cloud. I have been working with Django for a little while now on my local machine with the SQLite DB, but since the next project will partly be based on coordinate based data I wanted to setup the right environment.</p>

<p><strong>Import note: I've installed mini-conda to run in a seperate environment. I do activate this environment ""development"" when I work though</strong></p>

<p>I've tried to follow most of the geodjango information/tutorials online, but can't get it to work. What I've done (mostly followed this: <a href=""https://docs.djangoproject.com/en/2.0/ref/contrib/gis/install/#windows"" rel=""noreferrer"">https://docs.djangoproject.com/en/2.0/ref/contrib/gis/install/#windows</a>):</p>

<ol>
<li>Download and install the latest (10.3) PostgreSQL setup from <a href=""https://www.enterprisedb.com/downloads/postgres-postgresql-downloads"" rel=""noreferrer"">https://www.enterprisedb.com/downloads/postgres-postgresql-downloads</a></li>
<li>After installation I also installed used the Application Stack Builder to install PostGis</li>
<li>I've installed OSGeo4W from <a href=""https://trac.osgeo.org/osgeo4w/"" rel=""noreferrer"">https://trac.osgeo.org/osgeo4w/</a></li>
<li>I've created a batch script as described on the geodjango website (<a href=""https://docs.djangoproject.com/en/2.0/ref/contrib/gis/install/#windows"" rel=""noreferrer"">https://docs.djangoproject.com/en/2.0/ref/contrib/gis/install/#windows</a>) and ran it as administrator (except for the part where it sets the path to python, cause python was already in there since I've been using python for a while now)</li>
<li>I've tried some command in psql shell and I think I've created a database with name: geodjango, username: **** and pass: ****.</li>
<li>I don't know if I have given the geodjango user all priveleges, but I suspect so.</li>
</ol>

<p>After all of this I created a new django project and in settings.py I've added some parts:</p>

<pre><code>INSTALLED_APPS = [
'django.contrib.admin',
'django.contrib.auth',
'django.contrib.contenttypes',
'django.contrib.sessions',
'django.contrib.messages',
'django.contrib.staticfiles',
'django.contrib.gis',
'nameOfMyApp',
]
</code></pre>

<p>I've also got this in settings.py:</p>

<pre><code>DATABASES = {
'default': {
    'ENGINE': 'django.contrib.gis.db.backends.postgis',
    'NAME': 'geodjango',
    'USER': '****',
    'PASSWORD': '****',
    'HOST': 'localhost',
}
}

# FOR GEODJANGO
POSTGIS_VERSION = (2, 4, 3)
</code></pre>

<p>When I try to set up the database in django I run (in the right folder):</p>

<pre><code>python manage.py makemigrations
</code></pre>

<p>I get the following error:</p>

<pre><code>django.core.exceptions.ImproperlyConfigured: Could not find the GDAL library (tried ""gdal202"", ""gdal201"", ""gdal20"", ""gdal111"", ""gdal110"", ""gdal19""). Is GDAL installed? If it is, try setting GDAL_LIBRARY_PATH in your settings.
</code></pre>

<p>I've tried to fix that, but nothing seems to work.
Can anybody give me some help in setting this all up locally?</p>

<p>Update 7-3-2018:</p>

<ul>
<li>I've tried installing GDAL manually myself, (from: <a href=""http://www.gisinternals.com/query.html?content=filelist&amp;file=release-1911-x64-gdal-2-2-3-mapserver-7-0-7.zip"" rel=""noreferrer"">http://www.gisinternals.com/query.html?content=filelist&amp;file=release-1911-x64-gdal-2-2-3-mapserver-7-0-7.zip</a> the generic core components)</li>
<li>I've installed (what I assume are) the python bindings from <a href=""https://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""noreferrer"">https://www.lfd.uci.edu/~gohlke/pythonlibs/</a>. Still I get the same error. </li>
<li>I've also tried setting the GDAL_LIBRARY_PATH to the GDAL directory or the gdal-data directory (which resides in the GDAL directory).</li>
</ul>

<p>Now I get the following error:</p>

<pre><code>OSError: [WinError 126] The specified module could not be found
</code></pre>

<p>(while the .dll is there...)</p>
",301175,2032,06-03-2018 20:00,07-03-2018 18:46,1,2032,61,6,35,40,"{'badge_counts': {'bronze': 61, 'silver': 35, 'gold': 6}, 'account_id': 599157, 'is_employee': False, 'last_modified_date': 1698182700, 'last_access_date': 1705349882, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2032, 'creation_date': 1269460801, 'user_type': 'registered', 'user_id': 301175, 'accept_rate': 40, 'location': 'Netherlands', 'website_url': 'http://web-develop.nl', 'link': 'https://stackoverflow.com/users/301175/yorian', 'profile_image': 'https://www.gravatar.com/avatar/6e040ed4f71fe26b38811d538e428098?s=256&d=identicon&r=PG', 'display_name': 'Yorian'}","I've been trying to setup my windows computer such that I can have a local postgreSQL with PostGIS extension. With this installed I hope to be able to create a project with geodjango locally before putting it in the cloud. I have been working with Django for a little while now on my local machine with the SQLite DB, but since the next project will partly be based on coordinate based data I wanted to setup the right environment. Import note: I've installed mini-conda to run in a seperate environment. I do activate this environment ""development"" when I work though I've tried to follow most of the geodjango information/tutorials online, but can't get it to work. What I've done (mostly followed this: https://docs.djangoproject.com/en/2.0/ref/contrib/gis/install/#windows): Download and install the latest (10.3) PostgreSQL setup from https://www.enterprisedb.com/downloads/postgres-postgresql-downloads After installation I also installed used the Application Stack Builder to install PostGis I've installed OSGeo4W from https://trac.osgeo.org/osgeo4w/ I've created a batch script as described on the geodjango website (https://docs.djangoproject.com/en/2.0/ref/contrib/gis/install/#windows) and ran it as administrator (except for the part where it sets the path to python, cause python was already in there since I've been using python for a while now) I've tried some command in psql shell and I think I've created a database with name: geodjango, username: **** and pass: ****. I don't know if I have given the geodjango user all priveleges, but I suspect so. After all of this I created a new django project and in settings.py I've added some parts: I've also got this in settings.py: When I try to set up the database in django I run (in the right folder): I get the following error: I've tried to fix that, but nothing seems to work. Can anybody give me some help in setting this all up locally? Update 7-3-2018: I've tried installing GDAL manually myself, (from: http://www.gisinternals.com/query.html?content=filelist&amp;file=release-1911-x64-gdal-2-2-3-mapserver-7-0-7.zip the generic core components) I've installed (what I assume are) the python bindings from https://www.lfd.uci.edu/~gohlke/pythonlibs/. Still I get the same error. I've also tried setting the GDAL_LIBRARY_PATH to the GDAL directory or the gdal-data directory (which resides in the GDAL directory). Now I get the following error: (while the .dll is there...)","INSTALLED_APPS = [
'django.contrib.admin',
'django.contrib.auth',
'django.contrib.contenttypes',
'django.contrib.sessions',
'django.contrib.messages',
'django.contrib.staticfiles',
'django.contrib.gis',
'nameOfMyApp',
]
 DATABASES = {
'default': {
    'ENGINE': 'django.contrib.gis.db.backends.postgis',
    'NAME': 'geodjango',
    'USER': '****',
    'PASSWORD': '****',
    'HOST': 'localhost',
}
}

# FOR GEODJANGO
POSTGIS_VERSION = (2, 4, 3)
 python manage.py makemigrations
 django.core.exceptions.ImproperlyConfigured: Could not find the GDAL library (tried ""gdal202"", ""gdal201"", ""gdal20"", ""gdal111"", ""gdal110"", ""gdal19""). Is GDAL installed? If it is, try setting GDAL_LIBRARY_PATH in your settings.
 OSError: [WinError 126] The specified module could not be found
",20,72,0,6,
366,48770035,48770057,74085,Adding a 'count' column to the result of a groupby in pandas?,2,<python><pandas>,37,"<p>I think this is a fairly basic question, but I can't seem to find the solution.</p>

<p>I have a pandas dataframe similar to the following:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'A' : ['x','x','y','z','z'],
                   'B' : ['p','p','q','r','r']})
df
</code></pre>

<p>which creates a table like this:</p>

<pre><code>    A   B
0   x   p
1   x   p
2   y   q
3   z   r
4   z   r
</code></pre>

<p>I'm trying to create a table that represents the number of distinct values in that dataframe. So my goal is something like this:</p>

<pre><code>    A   B   c
0   x   p   2
1   y   q   1
2   z   r   2
</code></pre>

<p>I can't find the correct functions to achieve this, though. I've tried:</p>

<pre><code>df.groupby(['A','B']).agg('count')
</code></pre>

<p>This produces a table with 3 rows (as expected) but without a 'count' column. I don't know how to add in that count column. Could someone point me in the right direction?</p>
",284741,11407,13-02-2018 15:18,13-02-2018 15:20,0,11447,125,18,73,95,"{'badge_counts': {'bronze': 125, 'silver': 73, 'gold': 18}, 'account_id': 107014, 'is_employee': False, 'last_modified_date': 1700879101, 'last_access_date': 1707469761, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 40, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 11447, 'creation_date': 1267561419, 'user_type': 'registered', 'user_id': 284741, 'accept_rate': 95, 'website_url': '', 'link': 'https://stackoverflow.com/users/284741/oliver', 'profile_image': 'https://www.gravatar.com/avatar/e209bcc34ced5624bc4240b713d2ab73?s=256&d=identicon&r=PG', 'display_name': 'Oliver'}","I think this is a fairly basic question, but I can't seem to find the solution. I have a pandas dataframe similar to the following: which creates a table like this: I'm trying to create a table that represents the number of distinct values in that dataframe. So my goal is something like this: I can't find the correct functions to achieve this, though. I've tried: This produces a table with 3 rows (as expected) but without a 'count' column. I don't know how to add in that count column. Could someone point me in the right direction?","import pandas as pd

df = pd.DataFrame({'A' : ['x','x','y','z','z'],
                   'B' : ['p','p','q','r','r']})
df
     A   B
0   x   p
1   x   p
2   y   q
3   z   r
4   z   r
     A   B   c
0   x   p   2
1   y   q   1
2   z   r   2
 df.groupby(['A','B']).agg('count')
",12,35,0,0,
367,49141083,49289090,1350,Controlling Sankey diagram connections,1,<python><matplotlib><sankey-diagram>,11,"<p>I'm trying to control which flows connect to each other using the Matplotlib Sankey diagram. I'm modifying the basic two systems example.</p>

<p>I think my confusion comes down to misunderstanding what this actually means:</p>

<blockquote>
  <p>Notice that only one connection is specified, but the systems form a circuit since: (1) the lengths of the paths are justified and (2) the orientation and ordering of the flows is mirrored.</p>
</blockquote>

<p>I've made a toy example that uses a single data set and then modifies it for the second systems to make sure that the numbers all match up.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

from matplotlib.sankey import Sankey

plt.rcParams[""figure.figsize""] = (15,10)


system_1 = [
    {""label"": ""1st"",  ""value"":  2.00, ""orientation"":  0},
    {""label"": ""2nd"",  ""value"":  0.15, ""orientation"": -1},
    {""label"": ""3rd"",  ""value"":  0.60, ""orientation"": -1},
    {""label"": ""4th"",  ""value"": -0.10, ""orientation"": -1},
    {""label"": ""5th"",  ""value"":  0.25, ""orientation"": -1},
    {""label"": ""6th"",  ""value"":  0.25, ""orientation"": -1},
    {""label"": ""7th"",  ""value"":  0.25, ""orientation"": -1},
    {""label"": ""8th"",  ""value"":  0.25, ""orientation"": -1},
    {""label"": ""9th"",  ""value"":  0.25, ""orientation"": -1}
]

system_2 = system_1[:4]
system_2.append({""label"": ""new"",  ""value"":  -0.25, ""orientation"": 1})


fig = plt.figure()
ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[], title=""Where are all my cows?"")
flows  = [x[""value""] for x in system_1]
labels = [x[""label""] for x in system_1]
orientations=[x[""orientation""] for x in system_1]
sankey = Sankey(ax=ax, unit=""cow"")
sankey.add(flows=flows, 
           labels=labels,
           label='one',
           orientations=orientations)

sankey.add(flows=[-x[""value""] for x in system_2], 
           labels=[x[""label""] for x in system_2],
           label='two',
           orientations=[-x[""orientation""] for x in system_2], 
           prior=0, 
           connect= (0,0)
          )

diagrams = sankey.finish()
diagrams[-1].patch.set_hatch('/')
plt.legend(loc='best')


plt.show()
</code></pre>

<p>This gives me:</p>

<p><a href=""https://i.stack.imgur.com/3MYVq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3MYVq.png"" alt=""A sankey diagram that doesn&#39;t really work""></a></p>

<p>It <em>should</em> join up the flows with matching labels.</p>

<p>I've read <a href=""https://stackoverflow.com/questions/20425103/connecting-two-sankey-diagrams-in-matplotlib"">this</a> and <a href=""https://stackoverflow.com/questions/26677690/connecting-flows-in-matplotlib-sankey-diagram"">this</a> but they aren't helping me understand what is actually happening.</p>
",1835727,12922,06-03-2018 22:30,14-03-2018 23:14,8,12982,71,5,38,67,"{'badge_counts': {'bronze': 71, 'silver': 38, 'gold': 5}, 'account_id': 1940652, 'is_employee': False, 'last_modified_date': 1666402200, 'last_access_date': 1711097372, 'reputation_change_year': 168, 'reputation_change_quarter': 168, 'reputation_change_month': 70, 'reputation_change_week': 50, 'reputation_change_day': 0, 'reputation': 12982, 'creation_date': 1353327296, 'user_type': 'registered', 'user_id': 1835727, 'accept_rate': 67, 'location': 'Sydney', 'website_url': 'http://notionparallax.co.uk', 'link': 'https://stackoverflow.com/users/1835727/ben', 'profile_image': 'https://i.stack.imgur.com/oTOuc.jpg?s=256&g=1', 'display_name': 'Ben'}","I'm trying to control which flows connect to each other using the Matplotlib Sankey diagram. I'm modifying the basic two systems example. I think my confusion comes down to misunderstanding what this actually means: Notice that only one connection is specified, but the systems form a circuit since: (1) the lengths of the paths are justified and (2) the orientation and ordering of the flows is mirrored. I've made a toy example that uses a single data set and then modifies it for the second systems to make sure that the numbers all match up. This gives me: It should join up the flows with matching labels. I've read this and this but they aren't helping me understand what is actually happening.","import numpy as np
import matplotlib.pyplot as plt

from matplotlib.sankey import Sankey

plt.rcParams[""figure.figsize""] = (15,10)


system_1 = [
    {""label"": ""1st"",  ""value"":  2.00, ""orientation"":  0},
    {""label"": ""2nd"",  ""value"":  0.15, ""orientation"": -1},
    {""label"": ""3rd"",  ""value"":  0.60, ""orientation"": -1},
    {""label"": ""4th"",  ""value"": -0.10, ""orientation"": -1},
    {""label"": ""5th"",  ""value"":  0.25, ""orientation"": -1},
    {""label"": ""6th"",  ""value"":  0.25, ""orientation"": -1},
    {""label"": ""7th"",  ""value"":  0.25, ""orientation"": -1},
    {""label"": ""8th"",  ""value"":  0.25, ""orientation"": -1},
    {""label"": ""9th"",  ""value"":  0.25, ""orientation"": -1}
]

system_2 = system_1[:4]
system_2.append({""label"": ""new"",  ""value"":  -0.25, ""orientation"": 1})


fig = plt.figure()
ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[], title=""Where are all my cows?"")
flows  = [x[""value""] for x in system_1]
labels = [x[""label""] for x in system_1]
orientations=[x[""orientation""] for x in system_1]
sankey = Sankey(ax=ax, unit=""cow"")
sankey.add(flows=flows, 
           labels=labels,
           label='one',
           orientations=orientations)

sankey.add(flows=[-x[""value""] for x in system_2], 
           labels=[x[""label""] for x in system_2],
           label='two',
           orientations=[-x[""orientation""] for x in system_2], 
           prior=0, 
           connect= (0,0)
          )

diagrams = sankey.finish()
diagrams[-1].patch.set_hatch('/')
plt.legend(loc='best')


plt.show()
",48,68,1,3,
368,48145924,48146987,56403,Different colors for points and line in Seaborn regplot,3,<python><matplotlib><seaborn>,45,"<p>All examples listed in <a href=""https://seaborn.pydata.org/generated/seaborn.regplot.html"" rel=""noreferrer"">Seaborn's <code>regplot</code> documentation</a> show the same color for dots and the regression line. Changing the <code>color</code> argument changes both. How can one set a different color for the points as the line?</p>
",1840471,15344,08-01-2018 07:29,08-01-2018 09:01,0,15324,138,17,87,66,"{'badge_counts': {'bronze': 138, 'silver': 87, 'gold': 17}, 'account_id': 2064712, 'is_employee': False, 'last_modified_date': 1710556419, 'last_access_date': 1710733368, 'reputation_change_year': 241, 'reputation_change_quarter': 241, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 15324, 'creation_date': 1353456537, 'user_type': 'registered', 'user_id': 1840471, 'accept_rate': 66, 'location': 'Washington, DC', 'website_url': 'http://policyengine.org', 'link': 'https://stackoverflow.com/users/1840471/max-ghenis', 'profile_image': 'https://i.stack.imgur.com/axWtd.jpg?s=256&g=1', 'display_name': 'Max Ghenis'}",All examples listed in Seaborn's documentation show the same color for dots and the regression line. Changing the argument changes both. How can one set a different color for the points as the line?,regplot color,-2,1,0,1,
369,49862560,49862640,18509,Does Django Debug Toolbar work with DRF?,3,<python><django><django-rest-framework><django-debug-toolbar>,26,"<p>I'm trying to setup <a href=""https://django-debug-toolbar.readthedocs.io/en/stable/index.html"" rel=""noreferrer"">Debug Toolbar</a> to debug some API methods via DRF's Browsable API. I've went through the steps described on the <a href=""https://django-debug-toolbar.readthedocs.io/en/stable/installation.html"" rel=""noreferrer"">Installation</a> page (like updating <code>INSTALLED_APPS</code>, <code>MIDDLEWARE</code>, etc.) but still can't see any toolbar. So does Debug Toolbar work with DRF? How to debug the issue with it not showing up?</p>
",275088,15115,16-04-2018 16:55,16-04-2018 17:00,0,15165,167,21,92,79,"{'badge_counts': {'bronze': 167, 'silver': 92, 'gold': 21}, 'account_id': 102486, 'is_employee': False, 'last_modified_date': 1670442300, 'last_access_date': 1711111489, 'reputation_change_year': 360, 'reputation_change_quarter': 360, 'reputation_change_month': 50, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 15165, 'creation_date': 1266398103, 'user_type': 'registered', 'user_id': 275088, 'accept_rate': 79, 'website_url': '', 'link': 'https://stackoverflow.com/users/275088/planetp', 'profile_image': 'https://www.gravatar.com/avatar/b931a4a3164d36350c78dd67ed8a9f99?s=256&d=identicon&r=PG', 'display_name': 'planetp'}","I'm trying to setup Debug Toolbar to debug some API methods via DRF's Browsable API. I've went through the steps described on the Installation page (like updating , , etc.) but still can't see any toolbar. So does Debug Toolbar work with DRF? How to debug the issue with it not showing up?",INSTALLED_APPS MIDDLEWARE,-2,1,0,2,
370,49028611,49029629,82936,pytest cannot find module,2,<python><python-3.x><pytest>,65,"<p>I am following the <a href=""https://docs.pytest.org/en/latest/explanation/goodpractices.html#test-discovery"" rel=""noreferrer"">pytest good practices</a> or at least I <em>think I am</em>. However, pytest cannot find my module. It seems not to include the current directory in its <code>PYTHONPATH</code>.</p>
<p>The source file:</p>
<pre><code>def add(x, y):
    return x + y
</code></pre>
<p>The test file:</p>
<pre><code>import pytest
from junk.ook import add


def test_add_true():
    assert add(1, 1) == 2
</code></pre>
<p>And the shell output with a Python 3 virtual environment called &quot;p3&quot;.</p>
<pre><code>p3; pwd          
/home/usr/tmp/junk
p3; ls           
total 0
0 junk/  0 tests/
p3; ls junk      
total 4.0K
4.0K ook.py     0 __init__.py
p3; ls tests 
total 4.0K
4.0K test_ook.py     0 __pycache__/
p3; pytest
============================= test session starts ==============================
platform linux -- Python 3.4.5, pytest-3.4.1, py-1.5.2, pluggy-0.6.0
rootdir: /home/usr/tmp/junk, inifile:
collected 0 items / 1 errors                                                   

==================================== ERRORS ====================================
______________________ ERROR collecting tests/test_ook.py ______________________
ImportError while importing test module '/home/usr/tmp/junk/tests/test_ook.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
tests/test_ook.py:2: in &lt;module&gt;
    from junk.ook import add
E   ImportError: No module named 'junk'
!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!
=========================== 1 error in 0.08 seconds ============================
    
    def test_add_true():
        assert add(1, 1) == 2
</code></pre>
<p>However, running the following does work fine.</p>
<pre><code>p3; python -m pytest tests/
============================= test session starts ==============================
platform linux -- Python 3.4.5, pytest-3.4.1, py-1.5.2, pluggy-0.6.0
rootdir: /home/usr/tmp/junk, inifile:
collected 1 item                                                               

tests/test_ook.py .                                                      [100%]

=========================== 1 passed in 0.02 seconds ===========================
</code></pre>
<p>What am I doing wrong?</p>
",232794,17695,28-02-2018 11:34,28-02-2018 12:28,0,17715,156,29,103,90,"{'badge_counts': {'bronze': 156, 'silver': 103, 'gold': 29}, 'account_id': 82907, 'is_employee': False, 'last_modified_date': 1703304900, 'last_access_date': 1710329248, 'reputation_change_year': 200, 'reputation_change_quarter': 200, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 17715, 'creation_date': 1260955439, 'user_type': 'registered', 'user_id': 232794, 'accept_rate': 90, 'website_url': '', 'link': 'https://stackoverflow.com/users/232794/sardathrion-against-se-abuse', 'profile_image': 'https://i.stack.imgur.com/o1pji.png?s=256&g=1', 'display_name': 'Sardathrion - against SE abuse'}","I am following the pytest good practices or at least I think I am. However, pytest cannot find my module. It seems not to include the current directory in its . The source file: The test file: And the shell output with a Python 3 virtual environment called &quot;p3&quot;. However, running the following does work fine. What am I doing wrong?","PYTHONPATH def add(x, y):
    return x + y
 import pytest
from junk.ook import add


def test_add_true():
    assert add(1, 1) == 2
 p3; pwd          
/home/usr/tmp/junk
p3; ls           
total 0
0 junk/  0 tests/
p3; ls junk      
total 4.0K
4.0K ook.py     0 __init__.py
p3; ls tests 
total 4.0K
4.0K test_ook.py     0 __pycache__/
p3; pytest
============================= test session starts ==============================
platform linux -- Python 3.4.5, pytest-3.4.1, py-1.5.2, pluggy-0.6.0
rootdir: /home/usr/tmp/junk, inifile:
collected 0 items / 1 errors                                                   

==================================== ERRORS ====================================
______________________ ERROR collecting tests/test_ook.py ______________________
ImportError while importing test module '/home/usr/tmp/junk/tests/test_ook.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
tests/test_ook.py:2: in &lt;module&gt;
    from junk.ook import add
E   ImportError: No module named 'junk'
!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!
=========================== 1 error in 0.08 seconds ============================
    
    def test_add_true():
        assert add(1, 1) == 2
 p3; python -m pytest tests/
============================= test session starts ==============================
platform linux -- Python 3.4.5, pytest-3.4.1, py-1.5.2, pluggy-0.6.0
rootdir: /home/usr/tmp/junk, inifile:
collected 1 item                                                               

tests/test_ook.py .                                                      [100%]

=========================== 1 passed in 0.02 seconds ===========================
",42,57,0,1,
371,48466421,48466568,18698,Python: How to decompress a GZIP file to an uncompressed file on disk?,2,<python><python-3.x><gzip>,14,"<p>I want to emulate the behavior of <code>gzip -d &lt;file.gz&gt;</code> within a Python script.</p>

<p>The compressed GZIP file is decompressed and written as a file with the same file name as the original GZIP file without the .gz extension.</p>

<p>file.abc.gz --> file.abc</p>

<p>It's not obvious how to do this using the gzip library, all the examples in the docs are for compressing data arrays, and I've not yet found a good example from research. </p>

<h2>Edit</h2>

<p>I've tried the below using tarfile module, but unfortunately it's not working, I think since the GZIP file wasn't created with tar.</p>

<pre><code># get the zipped file's contents list, extract the file
with tarfile.TarFile(local_zipped_filename) as tar_file:

    # list the contents, make sure we only extract the expected named file
    members = tar_file.getmembers()
    for member in members:
        if member.name == filename_unzipped:
            members_to_extract = [member]
            tar_file.extractall(path=destination_dir, members=members_to_extract)
            break   # we've extracted our file, done
</code></pre>
",85248,8547,26-01-2018 17:21,26-01-2018 17:31,0,8557,151,24,92,55,"{'badge_counts': {'bronze': 151, 'silver': 92, 'gold': 24}, 'account_id': 31125, 'is_employee': False, 'last_modified_date': 1694572200, 'last_access_date': 1706389599, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 8557, 'creation_date': 1238522329, 'user_type': 'registered', 'user_id': 85248, 'accept_rate': 55, 'location': 'Asheville, NC', 'website_url': '', 'link': 'https://stackoverflow.com/users/85248/james-adams', 'profile_image': 'https://www.gravatar.com/avatar/1e7fc3010b5ee1e1e9350ae5d47a74f9?s=256&d=identicon&r=PG', 'display_name': 'James Adams'}","I want to emulate the behavior of within a Python script. The compressed GZIP file is decompressed and written as a file with the same file name as the original GZIP file without the .gz extension. file.abc.gz --> file.abc It's not obvious how to do this using the gzip library, all the examples in the docs are for compressing data arrays, and I've not yet found a good example from research. Edit I've tried the below using tarfile module, but unfortunately it's not working, I think since the GZIP file wasn't created with tar.","gzip -d &lt;file.gz&gt; # get the zipped file's contents list, extract the file
with tarfile.TarFile(local_zipped_filename) as tar_file:

    # list the contents, make sure we only extract the expected named file
    members = tar_file.getmembers()
    for member in members:
        if member.name == filename_unzipped:
            members_to_extract = [member]
            tar_file.extractall(path=destination_dir, members=members_to_extract)
            break   # we've extracted our file, done
",8,23,0,0,
372,49992300,49992362,296012,Python - How to show graph in Visual Studio Code itself?,6,<python><matplotlib><visual-studio-code>,63,"<p>When I try to run this example:</p>
<pre><code>import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np

x = np.linspace(0, 20, 100)
plt.plot(x, np.sin(x))
plt.show()
</code></pre>
<p>I see the result in a new window.</p>
<p>Is there any way to see the result graphs in the Visual Studio Code itself directly?</p>
<p>Thank you.</p>
",46503,5029,24-04-2018 01:50,24-04-2018 01:59,0,5035,100,7,56,47,"{'badge_counts': {'bronze': 100, 'silver': 56, 'gold': 7}, 'collectives': [{'collective': {'tags': ['amazon-elastic-beanstalk', 'aws-fargate', 'aws-sam', 'aws-codecommit', 'amazon-glacier', 'amazon-ami', 'aws-security-hub', 'alexa-sdk-nodejs', 'amazon-iam', 'amazon-guardduty', 'aws-glue', 'aws-sdk-ruby', 'amazon-cloudfront', 'aws-batch', 'aws-mediatailor', 'aws-global-accelerator', 'amazon-neptune', 'aws-sdk-go-v2', 'amazon-dynamodb-dax', 'aws-vpn', 'amazon-sumerian', 'aws-ssm', 'amazon-route53', 'aws-app-config', 'aws-sdk-ios', 'aws-cli', 'aws-app-mesh', 'aws-event-bridge', 'aws-directory-services', 'amazon-web-services', 'aws-copilot-cli', 'aws-transfer-family', 'aws-parameter-store', 'amazon-imagebuilder', 'amazon-sagemaker', 'amazon-workdocs', 'amazon-keyspaces', 'amazon-ecr', 'amazon-elb', 'aws-cloudformation', 'aws-config', 'aws-snowball', 'aws-sdk-mock', 'amazon-app-runner', 'aws-iot-analytics', 'amazon-sns', 'amazon-memory-db', 'aws-pinpoint', 'aws-deeplens', 'amazon-elasticache', 'aws-mediastore', 'amazon-bedrock', 'amazon-athena', 'amazon-gamelift', 'aws-codecatalyst', 'aws-graviton', 'aws-codeartifact', 'aws-elb', 'aws-cloudmap', 'aws-codeguru', 'amazon-translate', 'aws-sdk-java', 'aws-resource-group', 'amazon-data-pipeline', 'aws-sdk-js', 'amazon-ivs', 'aws-sdk-cpp', 'amazon-kinesis-analytics', 'amazon-cloudwatch', 'amazon-cloudhsm', 'aws-iot-sitewise', 'amazon-vpc', 'alexa-smart-home-skill', 'amazon-kendra', 'amazon-inspector', 'aws-datasync', 'aws-cloud9', 'amazon-ecs', 'amazon-rekognition', 'amazon-swf', 'aws-media-live', 'aws-sdk-js-v3', 'amazon-fsx', 'amazon-s3-select', 'aws-sdk-nodejs', 'aws-iam-identity-center', 'aws-chatbot', 'amazon-opensearch', 'aws-lambda', 'aws-lake-formation', 'aws-cdk', 'amazon-ses', 'aws-security-group', 'aws-mediapackage', 'amazon-connect', 'amazon-qldb', 'aws-iot-core', 'aws-sdk-rust', 'amazon-elastic-transcoder', 'aws-code-deploy', 'aws-serverless', 'amazon-honeycode', 'amazon-ec2', 'alexa-interaction-model', 'aws-xray', 'amazon-waf', 'aws-elemental', 'amazon-sqs', 'amazon-kms', 'aws-certificate-manager', 'aws-sam-cli', 'amazon-kinesis-video-streams', 'aws-lambda-powertools', 'amazon-ec2-spot-market', 'aws-documentdb', 'aws-control-tower', 'aws-service-catalog', 'aws-direct-connect', 'aws-billing', 'aws-iot', 'amazon-cloudwatchlogs', 'amazon-textract', 'alexa-sdk-python', 'aws-lambda-edge', 'amazon-dynamodb', 'amazon-rds', 'aws-iot-events', 'alexa-smapi', 'alexa-flash-briefing-skill', 'aws-dms', 'aws-mediaconnect', 'aws-organizations', 'amazon-macie', 'aws-sdk-comprehend', 'aws-device-farm', 'amazon-redshift-spectrum', 'aws-appsync', 'alexa-account-linking', 'amazon-transcribe', 'aws-acm', 'aws-opsworks', 'aws-step-functions', 'amazon-simpledb', 'amazon-lightsail', 'alexa-presentation-language', 'aws-amplify', 'amazon-workspaces', 'amazon-aurora', 'elastic-ip', 'aws-codepipeline', 'amazon-managed-blockchain', 'aws-application-load-balancer', 'amazon-forecast', 'aws-cloudshell', 'aws-mobilehub', 'aws-reserved-instances', 'amazon-efs', 'aws-sdk', 'aws-backup', 'amazon-timestream', 'amazon-cloudtrail', 'aws-sdk-go', 'amazon-appflow', 'amazon-emr', 'amazon-elasticsearch', 'aws-iot-greengrass', 'aws-sct', 'aws-private-link', 'amazon-quicksight', 'aws-fis', 'aws-sdk-net', 'alexa-skills-kit', 'amazon-kinesis-firehose', 'aws-sdk-java-2.0', 'amazon-ebs', 'aws-codestar', 'aws-sdk-android', 'aws-appstream', 'amazon-s3', 'amazon-lex', 'amazon-cloudsearch', 'aws-databrew', 'amazon-cognito', 'aws-elastictranscoder', 'amazon-workmail', 'amazon-comprehend', 'aws-auto-scaling', 'aws-codebuild', 'aws-api-gateway', 'aws-sso', 'amazon-eks', 'aws-storage-gateway', 'amazon-mq', 'aws-data-exchange', 'amazon-location-service', 'amazon-kinesis', 'amazon-sagemaker-compilers', 'aws-secrets-manager', 'aws-msk', 'amazon-personalize', 'aws-nlb', 'amazon-redshift', 'aws-copilot', 'aws-media-convert', 'amazon-polly'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 19738, 'is_employee': False, 'last_modified_date': 1706321700, 'last_access_date': 1711144373, 'reputation_change_year': 52, 'reputation_change_quarter': 52, 'reputation_change_month': 6, 'reputation_change_week': 6, 'reputation_change_day': 0, 'reputation': 5035, 'creation_date': 1229388230, 'user_type': 'registered', 'user_id': 46503, 'accept_rate': 47, 'website_url': '', 'link': 'https://stackoverflow.com/users/46503/mimic', 'profile_image': 'https://www.gravatar.com/avatar/eed5385d86b251196cb98c8b22c5e82b?s=256&d=identicon&r=PG', 'display_name': 'mimic'}",When I try to run this example: I see the result in a new window. Is there any way to see the result graphs in the Visual Studio Code itself directly? Thank you.,"import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np

x = np.linspace(0, 20, 100)
plt.plot(x, np.sin(x))
plt.show()
",6,12,0,0,
373,48524196,48837032,26984,How to get `python` to run Python 3 in WSL bash?,2,<python><bash><windows-10><windows-subsystem-for-linux>,16,"<p>When I type <code>python</code> into my bash shell (Windows Subsystem for Linux) in Windows 10 Home, I get the following error message:</p>

<pre><code>The program 'python' can be found in the following packages:
 * python-minimal
 * python3
Try: sudo apt install &lt;selected package&gt;
</code></pre>

<p>I've tried installing <code>python3</code> but am told it's already installed and up to date.</p>

<p>I've tried uninstalling <code>python-minimal</code> but am told it's not installed (!)</p>

<p>Why am I seeing two ""competing"" packages for Python? How can I fix the conflict and configure my WSL bash to run Python 3 from <code>python</code>?</p>
",33404,16280,30-01-2018 14:48,17-02-2018 01:23,18,16310,190,28,116,71,"{'badge_counts': {'bronze': 190, 'silver': 116, 'gold': 28}, 'account_id': 15694, 'is_employee': False, 'last_modified_date': 1710551700, 'last_access_date': 1710949210, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 50, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 16310, 'creation_date': 1225618447, 'user_type': 'registered', 'user_id': 33404, 'accept_rate': 71, 'location': 'Israel', 'website_url': 'https://urig.io/', 'link': 'https://stackoverflow.com/users/33404/urig', 'profile_image': 'https://i.stack.imgur.com/x3972.jpg?s=256&g=1', 'display_name': 'urig'}","When I type into my bash shell (Windows Subsystem for Linux) in Windows 10 Home, I get the following error message: I've tried installing but am told it's already installed and up to date. I've tried uninstalling but am told it's not installed (!) Why am I seeing two ""competing"" packages for Python? How can I fix the conflict and configure my WSL bash to run Python 3 from ?","python The program 'python' can be found in the following packages:
 * python-minimal
 * python3
Try: sudo apt install &lt;selected package&gt;
 python3 python-minimal python",-1,13,0,0,
374,48511076,48511103,5347,"In the Django REST framework, how are the default permission classes combined with per-view(set) ones?",4,<python><django><django-rest-framework><oauth2-toolkit>,17,"<p>I'm reading <a href=""http://www.django-rest-framework.org/api-guide/permissions/"" rel=""noreferrer"">http://www.django-rest-framework.org/api-guide/permissions/</a> and trying to relate it to the OAuth2 toolkit documentation, <a href=""http://django-oauth-toolkit.readthedocs.io/en/latest/rest-framework/getting_started.html"" rel=""noreferrer"">http://django-oauth-toolkit.readthedocs.io/en/latest/rest-framework/getting_started.html</a>. The latter has an example in which in <code>settings.py</code> one specifies</p>

<pre><code>REST_FRAMEWORK = {
    # ...

    'DEFAULT_PERMISSION_CLASSES': (
        'rest_framework.permissions.IsAuthenticated',
    )
}
</code></pre>

<p>and in addition, <code>IsAuthenticated</code> is also specified added to the <code>permission_classes</code> list of a <code>ModelViewSet</code>:</p>

<pre><code>class UserViewSet(viewsets.ModelViewSet):
    permission_classes = [permissions.IsAuthenticated, TokenHasReadWriteScope]
    queryset = User.objects.all()
    serializer_class = UserSerializer
</code></pre>

<p>Do I infer correctly from this example that the <code>DEFAULT_PERMISSION_CLASSES</code> are not prepended / postpended to a <code>ModelViewSet</code>'s permission classes, but are instead replaced by it?</p>
",995862,54143,29-01-2018 22:28,29-01-2018 22:31,0,54393,547,95,320,89,"{'badge_counts': {'bronze': 547, 'silver': 320, 'gold': 95}, 'account_id': 973980, 'is_employee': False, 'last_modified_date': 1703313301, 'last_access_date': 1710970031, 'reputation_change_year': 958, 'reputation_change_quarter': 958, 'reputation_change_month': 320, 'reputation_change_week': 170, 'reputation_change_day': 10, 'reputation': 54393, 'creation_date': 1318612149, 'user_type': 'registered', 'user_id': 995862, 'accept_rate': 89, 'location': 'San Francisco, CA, USA', 'website_url': 'http://www.kurtpeek.com', 'link': 'https://stackoverflow.com/users/995862/kurt-peek', 'profile_image': 'https://www.gravatar.com/avatar/d5efb6c1054808f7a79aaefbc64cce76?s=256&d=identicon&r=PG', 'display_name': 'Kurt Peek'}","I'm reading http://www.django-rest-framework.org/api-guide/permissions/ and trying to relate it to the OAuth2 toolkit documentation, http://django-oauth-toolkit.readthedocs.io/en/latest/rest-framework/getting_started.html. The latter has an example in which in one specifies and in addition, is also specified added to the list of a : Do I infer correctly from this example that the are not prepended / postpended to a 's permission classes, but are instead replaced by it?","settings.py REST_FRAMEWORK = {
    # ...

    'DEFAULT_PERMISSION_CLASSES': (
        'rest_framework.permissions.IsAuthenticated',
    )
}
 IsAuthenticated permission_classes ModelViewSet class UserViewSet(viewsets.ModelViewSet):
    permission_classes = [permissions.IsAuthenticated, TokenHasReadWriteScope]
    queryset = User.objects.all()
    serializer_class = UserSerializer
 DEFAULT_PERMISSION_CLASSES ModelViewSet",3,20,0,2,
375,49531286,49537098,26463,Cannot batch tensors with different shapes in component 0 with tf.data.Dataset,2,<python><tensorflow><tensorflow-datasets>,12,"<p>I have the following error in my input pipeline:</p>

<blockquote>
  <p>tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot
  batch tensors with different shapes in component 0. First element had
  shape [2,48,48,3] and element 1 had shape [27,48,48,3].</p>
</blockquote>

<p>with this code</p>

<pre><code>dataset = tf.data.Dataset.from_generator(generator,
                                         (tf.float32, tf.int64, tf.int64, tf.float32, tf.int64, tf.float32))

dataset = dataset.batch(max_buffer_size)
</code></pre>

<p>This is completely logical as the batch method tries to create a (batch_size, ?, 48, 48, 3) Tensor. However I want that it creates a [29,48,48,3] Tensor for this case. So concatenate instead of stack. Is this possible with tf.data?</p>

<p>I can do the concatenation in Python in the generator function, but I was wondering if this is also possible with the tf.data pipeline</p>
",987397,1395,28-03-2018 09:47,28-03-2018 14:23,0,1395,39,3,20,32,"{'badge_counts': {'bronze': 39, 'silver': 20, 'gold': 3}, 'account_id': 963556, 'is_employee': False, 'last_modified_date': 1703299800, 'last_access_date': 1700492624, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1395, 'creation_date': 1318238129, 'user_type': 'registered', 'user_id': 987397, 'accept_rate': 32, 'website_url': '', 'link': 'https://stackoverflow.com/users/987397/derk', 'profile_image': 'https://www.gravatar.com/avatar/fa7ae7d9bd13c2d04335c3209865c262?s=256&d=identicon&r=PG', 'display_name': 'Derk'}","I have the following error in my input pipeline: tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [2,48,48,3] and element 1 had shape [27,48,48,3]. with this code This is completely logical as the batch method tries to create a (batch_size, ?, 48, 48, 3) Tensor. However I want that it creates a [29,48,48,3] Tensor for this case. So concatenate instead of stack. Is this possible with tf.data? I can do the concatenation in Python in the generator function, but I was wondering if this is also possible with the tf.data pipeline","dataset = tf.data.Dataset.from_generator(generator,
                                         (tf.float32, tf.int64, tf.int64, tf.float32, tf.int64, tf.float32))

dataset = dataset.batch(max_buffer_size)
",3,19,0,0,
376,50155464,50156706,34838,Using pytest with a src layer,4,<python><pytest><python-packaging>,98,"<p><a href=""https://docs.pytest.org/en/7.2.x/explanation/goodpractices.html#tests-outside-application-code"" rel=""noreferrer"">pytest recommends</a> including an additional directory to separate the source code within a project:</p>
<pre><code>my_package
├── src  # &lt;-- no __init__.py on this layer
│   └── my_package
│       ├── __init__.py
│       └── util_module
│           ├── __init__.py
│           └── utils.py
└── tests
    ├── __init__.py
    └── test_util_module
        ├── __init__.py
        └── test_utils.py
</code></pre>
<p>Sadly, they say nothing<sup>[1]</sup> about how imports in the test code should work in such a case, which work for my IDE just fine in <a href=""https://gitlab.com/arecknag/my_package"" rel=""noreferrer"">this naive example</a><sup>[2]</sup>, but causes the following error with pytest:</p>
<pre class=""lang-bash prettyprint-override""><code>~/my_package$ pytest

====================== test session starts ======================
platform linux -- Python 3.6.4, pytest-3.5.1, py-1.5.3, pluggy-0.6.0
rootdir: /home/user/workspace/my_package, inifile:
collected 0 items / 1 errors     
                                                                                                                                                                      
============================ ERRORS =============================
___ ERROR collecting tests/test_util_module/test_utils.py ___
ImportError while importing test module '/home/user/workspace/my_package/tests/test_util_module/test_utils.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
tests/test_util_module/test_utils.py:1: in &lt;module&gt;
    from test.test_module.some_file import starify
E   ModuleNotFoundError: No module named 'my_package.util_module'
!!!! Interrupted: 1 errors during collection !!!!!
</code></pre>
<p>I can fix the issue by changing the import of the test to</p>
<pre class=""lang-py prettyprint-override""><code>from src.my_package.util_module.utils import starify
</code></pre>
<p>but then my IDE complaints about the <code>src</code> part being redundant, so I'd like to keep it out.</p>
<hr />
<p><sup>[1]<sub>: Not the case any more. As of version 3.7.3, pytest recommends the editable install also featured in @hoefling's answer at the top of its <a href=""https://docs.pytest.org/en/stable/explanation/goodpractices.html#install-package-with-pip"" rel=""noreferrer"">good practices</a>.</sub></sup></p>
<p><sup>[2]<sub>: Setup is <code>virtualenv env -p python3.6; source env/bin/activate; pip install pytest</code></sub></sup></p>
",962190,18635,03-05-2018 12:47,03-05-2018 13:50,0,18695,103,6,89,94,"{'badge_counts': {'bronze': 103, 'silver': 89, 'gold': 6}, 'collectives': [{'collective': {'tags': ['continuous-integration', 'gitlab-ci-runner', 'github-actions', 'jenkins-groovy', 'continuous-delivery', 'jenkins', 'google-cloud-build', 'octopus-deploy', 'jenkins-plugins', 'argocd', 'teamcity', 'circleci', 'continuous-deployment', 'bitbucket-pipelines', 'tfsbuild', 'codemagic', 'hudson', 'cicd', 'azure-pipelines', 'continuous-testing', 'jenkins-pipeline', 'gitlab-ci'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where developers focused on continuous integration, delivery, and deployment can find, share, and learn about simultaneous development.', 'link': '/collectives/ci-cd', 'name': 'CI/CD', 'slug': 'ci-cd'}, 'role': 'member'}], 'account_id': 932653, 'is_employee': False, 'last_modified_date': 1703299800, 'last_access_date': 1711103333, 'reputation_change_year': 486, 'reputation_change_quarter': 486, 'reputation_change_month': 110, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 18695, 'creation_date': 1316830201, 'user_type': 'registered', 'user_id': 962190, 'accept_rate': 94, 'location': 'D&#252;sseldorf, Germany', 'website_url': 'https://de.linkedin.com/in/arne-caratti-918776105', 'link': 'https://stackoverflow.com/users/962190/arne', 'profile_image': 'https://i.stack.imgur.com/sxvRS.png?s=256&g=1', 'display_name': 'Arne'}","pytest recommends including an additional directory to separate the source code within a project: Sadly, they say nothing[1] about how imports in the test code should work in such a case, which work for my IDE just fine in this naive example[2], but causes the following error with pytest: I can fix the issue by changing the import of the test to but then my IDE complaints about the part being redundant, so I'd like to keep it out. [1]: Not the case any more. As of version 3.7.3, pytest recommends the editable install also featured in @hoefling's answer at the top of its good practices. [2]: Setup is","my_package
├── src  # &lt;-- no __init__.py on this layer
│   └── my_package
│       ├── __init__.py
│       └── util_module
│           ├── __init__.py
│           └── utils.py
└── tests
    ├── __init__.py
    └── test_util_module
        ├── __init__.py
        └── test_utils.py
 ~/my_package$ pytest

====================== test session starts ======================
platform linux -- Python 3.6.4, pytest-3.5.1, py-1.5.3, pluggy-0.6.0
rootdir: /home/user/workspace/my_package, inifile:
collected 0 items / 1 errors     
                                                                                                                                                                      
============================ ERRORS =============================
___ ERROR collecting tests/test_util_module/test_utils.py ___
ImportError while importing test module '/home/user/workspace/my_package/tests/test_util_module/test_utils.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
tests/test_util_module/test_utils.py:1: in &lt;module&gt;
    from test.test_module.some_file import starify
E   ModuleNotFoundError: No module named 'my_package.util_module'
!!!! Interrupted: 1 errors during collection !!!!!
 from src.my_package.util_module.utils import starify
 src virtualenv env -p python3.6; source env/bin/activate; pip install pytest",24,39,0,3,
377,48234032,48310939,10598,Run py.test test in different process,1,<python><tensorflow><pytest>,11,"<p>I'm trying to test tensorflow program. I'm setting up tensorflow session using parametrized py.test fixture:</p>

<pre><code>@pytest.fixture(scope=""session"", params=configuration)
def session(request):
    if request.param == 'tensorflow':
        return tf.Session()
    elif request.param == 'tensorflow-eager':
        tfe.enable_eager_execution()
        return tf.Session()
    elif ...
</code></pre>

<p>Tensorflow has global state, thus several test launches can pollute it. For example there is no way to disable eager execution after it has been enabled. Is there a way to instruct py.test to create a new process for each test? Or another way to configure environment for the test besides using parametrized fixture? Example usage:</p>

<pre><code>@pytest.mark.parametrize(""bias_type"", ['variable', 'ndarray', 'list', 'tuple'])
@pytest.mark.parametrize(""kernel_type"", ['variable', 'ndarray', 'list', 'tuple'])
@pytest.mark.parametrize(""input_type"", ['variable', 'ndarray', 'list', 'tuple'])
def test_convolution(session, input_type, kernel_type, bias_type):
    ...
</code></pre>
",1008902,3577,12-01-2018 21:10,17-01-2018 22:18,5,3607,56,6,37,67,"{'badge_counts': {'bronze': 56, 'silver': 37, 'gold': 6}, 'account_id': 991847, 'is_employee': False, 'last_modified_date': 1689991200, 'last_access_date': 1711152632, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 3607, 'creation_date': 1319313679, 'user_type': 'registered', 'user_id': 1008902, 'accept_rate': 67, 'website_url': '', 'link': 'https://stackoverflow.com/users/1008902/dikobraz', 'profile_image': 'https://www.gravatar.com/avatar/71cad26db42145bae6acca07bd2688d3?s=256&d=identicon&r=PG', 'display_name': 'DikobrAz'}","I'm trying to test tensorflow program. I'm setting up tensorflow session using parametrized py.test fixture: Tensorflow has global state, thus several test launches can pollute it. For example there is no way to disable eager execution after it has been enabled. Is there a way to instruct py.test to create a new process for each test? Or another way to configure environment for the test besides using parametrized fixture? Example usage:","@pytest.fixture(scope=""session"", params=configuration)
def session(request):
    if request.param == 'tensorflow':
        return tf.Session()
    elif request.param == 'tensorflow-eager':
        tfe.enable_eager_execution()
        return tf.Session()
    elif ...
 @pytest.mark.parametrize(""bias_type"", ['variable', 'ndarray', 'list', 'tuple'])
@pytest.mark.parametrize(""kernel_type"", ['variable', 'ndarray', 'list', 'tuple'])
@pytest.mark.parametrize(""input_type"", ['variable', 'ndarray', 'list', 'tuple'])
def test_convolution(session, input_type, kernel_type, bias_type):
    ...
",11,20,0,0,
378,50385276,50400262,115993,How to stop execution of python script in visual studio code?,5,<python><visual-studio-code>,22,"<p>I have the following code which I am running from within Visual Studio Code using <code>Right click &gt; Run Python File in Terminal</code></p>

<pre><code>import threading


def worker(tid):
    """"""This is what the thread actually executes""""""
    for i in range(tid * 100000):
        print(""I'm working on thread {} with count {}"".format(tid, i))
    return


def main():
    threads = list()
    for i in range(32):
        t = threading.Thread(target=worker, args=(i,))
        threads.append(t)
        t.start()


if __name__ == ""__main__"":
    main()
</code></pre>

<p>However I want to stop the execution of the script so I have tried with <code>Ctrl+C</code> but the program is still running in the integrated terminal of Visual Studio Code. Is there a way to actually force the stop?</p>
",1016721,6463,17-05-2018 07:01,17-05-2018 20:46,0,6463,164,17,95,91,"{'badge_counts': {'bronze': 164, 'silver': 95, 'gold': 17}, 'account_id': 1002078, 'is_employee': False, 'last_modified_date': 1705716021, 'last_access_date': 1629126213, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6463, 'creation_date': 1319726807, 'user_type': 'registered', 'user_id': 1016721, 'accept_rate': 91, 'link': 'https://stackoverflow.com/users/1016721/brabbit27', 'profile_image': 'https://www.gravatar.com/avatar/277cb58c2d96ac6e6a6bd540a51daa55?s=256&d=identicon&r=PG', 'display_name': 'BRabbit27'}",I have the following code which I am running from within Visual Studio Code using However I want to stop the execution of the script so I have tried with but the program is still running in the integrated terminal of Visual Studio Code. Is there a way to actually force the stop?,"Right click &gt; Run Python File in Terminal import threading


def worker(tid):
    """"""This is what the thread actually executes""""""
    for i in range(tid * 100000):
        print(""I'm working on thread {} with count {}"".format(tid, i))
    return


def main():
    threads = list()
    for i in range(32):
        t = threading.Thread(target=worker, args=(i,))
        threads.append(t)
        t.start()


if __name__ == ""__main__"":
    main()
 Ctrl+C",17,25,0,0,
379,49101342,49114348,8023,Implementing the Prolog Unification algorithm in Python? Backtracking,3,<python><algorithm><prolog><backtracking><unification>,13,"<p>I'm trying to implement Unification, but having problems.. already got dozen of examples,but all they do is to muddy the water. I get more confused than enlightened :</p>

<p><a href=""http://www.cs.trincoll.edu/~ram/cpsc352/notes/unification.html"" rel=""noreferrer"">http://www.cs.trincoll.edu/~ram/cpsc352/notes/unification.html</a></p>

<p><a href=""https://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture8.html"" rel=""noreferrer"">https://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture8.html</a>
[code below is based on this intro]</p>

<p><a href=""http://www.cs.bham.ac.uk/research/projects/poplog/paradigms_lectures/lecture20.html#representing"" rel=""noreferrer"">http://www.cs.bham.ac.uk/research/projects/poplog/paradigms_lectures/lecture20.html#representing</a></p>

<p><a href=""https://norvig.com/unify-bug.pdf"" rel=""noreferrer"">https://norvig.com/unify-bug.pdf</a></p>

<p><a href=""https://stackoverflow.com/questions/1396558/how-can-i-implement-the-unification-algorithm-in-a-language-like-java-or-c"">How can I implement the unification algorithm in a language like Java or C#?</a></p>

<p>The art of Prolog one ...and several others.
The biggest problem is that I could not figure clear statement of the problem. More mathy or lispy explanations are confusing me even more.</p>

<p>As a good start it seems a good idea to follow the representation to be list based (like in the lispy cases) i.e. :</p>

<pre><code>pred(Var, val)  =becomes=&gt; [pred, Var, val] 
p1(val1, p2(val2, Var1)) ==&gt; [p1, val1, [p2, val2, Var1]]
</code></pre>

<blockquote>
  <p>except how do you represent lists themselves !? i.e. [H|T]</p>
</blockquote>

<p>I would love if you can show me a Python pseudo code and/or more detailed algorithm description or a pointer to one.</p>

<p>Some points I grasp is the need to separate the code in general-unifier and var-unification, but then I cant see the mutual-recusive case ! ... and so on.</p>

<hr>

<p>As a side note : I would also love for you to mention how would you handle Unification on Backtracking. I think I have backtracking squared-away, but I know something has to happen to substitution-frame on backtracking. </p>

<hr>

<p>Added an answer with the current code.</p>

<p><a href=""http://www.igrok.site/bi/Bi_language.html"" rel=""noreferrer"">http://www.igrok.site/bi/Bi_language.html</a></p>

<p><a href=""http://www.igrok.site/bi/TOC.html"" rel=""noreferrer"">http://www.igrok.site/bi/TOC.html</a></p>

<p><a href=""https://github.com/vsraptor/bi/blob/master/lib/bi_engine.py"" rel=""noreferrer"">https://github.com/vsraptor/bi/blob/master/lib/bi_engine.py</a></p>
",1019129,7304,04-03-2018 22:48,05-03-2018 15:52,1,7314,68,10,44,51,"{'badge_counts': {'bronze': 68, 'silver': 44, 'gold': 10}, 'account_id': 1005158, 'is_employee': False, 'last_modified_date': 1709342100, 'last_access_date': 1703343866, 'reputation_change_year': 136, 'reputation_change_quarter': 136, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 7314, 'creation_date': 1319836341, 'user_type': 'registered', 'user_id': 1019129, 'accept_rate': 51, 'website_url': '', 'link': 'https://stackoverflow.com/users/1019129/sten', 'profile_image': 'https://www.gravatar.com/avatar/357b7f0f0ca6e2c0f0ebccbec9b1a87b?s=256&d=identicon&r=PG', 'display_name': 'sten'}","I'm trying to implement Unification, but having problems.. already got dozen of examples,but all they do is to muddy the water. I get more confused than enlightened : http://www.cs.trincoll.edu/~ram/cpsc352/notes/unification.html https://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture8.html [code below is based on this intro] http://www.cs.bham.ac.uk/research/projects/poplog/paradigms_lectures/lecture20.html#representing https://norvig.com/unify-bug.pdf How can I implement the unification algorithm in a language like Java or C#? The art of Prolog one ...and several others. The biggest problem is that I could not figure clear statement of the problem. More mathy or lispy explanations are confusing me even more. As a good start it seems a good idea to follow the representation to be list based (like in the lispy cases) i.e. : except how do you represent lists themselves !? i.e. [H|T] I would love if you can show me a Python pseudo code and/or more detailed algorithm description or a pointer to one. Some points I grasp is the need to separate the code in general-unifier and var-unification, but then I cant see the mutual-recusive case ! ... and so on. As a side note : I would also love for you to mention how would you handle Unification on Backtracking. I think I have backtracking squared-away, but I know something has to happen to substitution-frame on backtracking. Added an answer with the current code. http://www.igrok.site/bi/Bi_language.html http://www.igrok.site/bi/TOC.html https://github.com/vsraptor/bi/blob/master/lib/bi_engine.py","pred(Var, val)  =becomes=&gt; [pred, Var, val] 
p1(val1, p2(val2, Var1)) ==&gt; [p1, val1, [p2, val2, Var1]]
",1,43,0,8,
380,48054392,48071103,26737,pytest: How to get a list of all failed tests at the end of the session? (and while using xdist),7,<python><unit-testing><testing><pytest><xdist>,32,"<p>I would like to have a <strong>list of all the tests that have failed</strong> to be used <strong>at the end of session</strong>.</p>

<p>Pytest lets you define a hook <code>pytest_sessionfinish(session, exitstatus)</code>, that is called at the end of the session, where I wish to have that list. </p>

<p><code>session</code> is a <code>_pytest.main.Session</code> instance that has the attribute <code>items</code> (type <code>list</code>), but I couldn't find whether the each <code>item</code> in that list passed of failed.</p>

<ol>
<li>How can a list of all failed tests could be retrieved at the end of the session?</li>
<li><p>How can it be done while using <code>pytest-xdist</code> plugin, where I would like to get that list in the master process. Using this plugin, <code>session</code> does not even have <code>items</code> attribute in the master:</p>

<pre class=""lang-py prettyprint-override""><code>def pytest_sessionfinish(session, exitstatus):
    if os.environ.get(""PYTEST_XDIST_WORKER"", ""master"") == ""master"":
         print(hasattr(session, ""items""))  # False
</code></pre>

</li>
</ol>
",1062003,355,02-01-2018 00:36,03-01-2018 04:19,1,355,6,1,3,,"{'badge_counts': {'bronze': 6, 'silver': 3, 'gold': 1}, 'account_id': 1060210, 'is_employee': False, 'last_modified_date': 1573683073, 'last_access_date': 1711146368, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 355, 'creation_date': 1322055300, 'user_type': 'registered', 'user_id': 1062003, 'link': 'https://stackoverflow.com/users/1062003/itay', 'profile_image': 'https://www.gravatar.com/avatar/4237b108aabcb84cabf63519ca511481?s=256&d=identicon&r=PG', 'display_name': 'Itay'}","I would like to have a list of all the tests that have failed to be used at the end of session. Pytest lets you define a hook , that is called at the end of the session, where I wish to have that list. is a instance that has the attribute (type ), but I couldn't find whether the each in that list passed of failed. How can a list of all failed tests could be retrieved at the end of the session? How can it be done while using plugin, where I would like to get that list in the master process. Using this plugin, does not even have attribute in the master:","pytest_sessionfinish(session, exitstatus) session _pytest.main.Session items list item pytest-xdist session items def pytest_sessionfinish(session, exitstatus):
    if os.environ.get(""PYTEST_XDIST_WORKER"", ""master"") == ""master"":
         print(hasattr(session, ""items""))  # False
",-7,17,0,0,
381,49031954,49129766,8497,Django OAuth Toolkit - Register a user,4,<python><django><django-rest-framework><django-registration><django-oauth>,11,"<p>I've gone through the docs of Provider and Resource of Django OAuth Toolkit, but all I'm able to find is <code>how to 'authenticate' a user</code>, not how to <code>register</code> a user.</p>

<p>I'm able to set up everything on my machine, but not sure how to register a user using username &amp; password. I know I'm missing something very subtle. How do I exactly register a user and get an access token in return to talk to my resource servers.</p>

<p>OR </p>

<p>Is it like that I've to first register the user using normal Django mechanism and then get the token of the same?</p>
",1162512,16942,28-02-2018 14:31,06-03-2018 11:28,6,16972,271,52,141,56,"{'badge_counts': {'bronze': 271, 'silver': 141, 'gold': 52}, 'account_id': 1189269, 'is_employee': False, 'last_modified_date': 1703317800, 'last_access_date': 1705287297, 'reputation_change_year': 188, 'reputation_change_quarter': 188, 'reputation_change_month': 40, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 16972, 'creation_date': 1327163131, 'user_type': 'registered', 'user_id': 1162512, 'accept_rate': 56, 'website_url': '', 'link': 'https://stackoverflow.com/users/1162512/praful-bagai', 'profile_image': 'https://www.gravatar.com/avatar/d9d2995febb517ad79ee1cf6bb6ee97f?s=256&d=identicon&r=PG', 'display_name': 'Praful Bagai'}","I've gone through the docs of Provider and Resource of Django OAuth Toolkit, but all I'm able to find is , not how to a user. I'm able to set up everything on my machine, but not sure how to register a user using username &amp; password. I know I'm missing something very subtle. How do I exactly register a user and get an access token in return to talk to my resource servers. OR Is it like that I've to first register the user using normal Django mechanism and then get the token of the same?",how to 'authenticate' a user register,-2,7,0,0,
382,49922460,49922520,26428,Scale a numpy array with from -0.1 - 0.2 to 0-255,2,<python><image><numpy>,15,"<p>I have an numpy array in python that represent an image its size is 28x28x3 while the max value of it is 0.2 and the min is -0.1. I want to scale that image between 0-255. How can I do so?</p>
",1194864,5624,19-04-2018 13:34,19-04-2018 13:37,0,5636,156,26,80,89,"{'badge_counts': {'bronze': 156, 'silver': 80, 'gold': 26}, 'account_id': 1230089, 'is_employee': False, 'last_modified_date': 1709919601, 'last_access_date': 1711128924, 'reputation_change_year': 14, 'reputation_change_quarter': 14, 'reputation_change_month': 12, 'reputation_change_week': 12, 'reputation_change_day': 0, 'reputation': 5636, 'creation_date': 1328624786, 'user_type': 'registered', 'user_id': 1194864, 'accept_rate': 89, 'website_url': '', 'link': 'https://stackoverflow.com/users/1194864/jose-ramon', 'profile_image': 'https://i.stack.imgur.com/uley8.jpg?s=256&g=1', 'display_name': 'Jose Ramon'}",I have an numpy array in python that represent an image its size is 28x28x3 while the max value of it is 0.2 and the min is -0.1. I want to scale that image between 0-255. How can I do so?,,0,1,0,0,
383,50065484,50066703,16287,"Getting precision, recall and F1 score per class in Keras",2,<python><tensorflow><neural-network><deep-learning><keras>,11,"<p>I have trained a neural network using the TensorFlow backend in Keras (2.1.5) and I have also used the keras-contrib (2.0.8) library in order to add a CRF layer as an output for the network.</p>

<p>I would like to know how can I get the precision, recall and f1 score for each class after making the predictions on a test set using the NN.  </p>
",1218046,1722,27-04-2018 15:16,27-04-2018 16:28,0,1722,50,7,32,75,"{'badge_counts': {'bronze': 50, 'silver': 32, 'gold': 7}, 'account_id': 1259271, 'is_employee': False, 'last_modified_date': 1709344500, 'last_access_date': 1626254085, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 2, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1722, 'creation_date': 1329568430, 'user_type': 'registered', 'user_id': 1218046, 'accept_rate': 75, 'link': 'https://stackoverflow.com/users/1218046/haritz', 'profile_image': 'https://www.gravatar.com/avatar/2feb5b477b063c4d4bb67087baef2802?s=256&d=identicon&r=PG', 'display_name': 'Haritz'}","I have trained a neural network using the TensorFlow backend in Keras (2.1.5) and I have also used the keras-contrib (2.0.8) library in order to add a CRF layer as an output for the network. I would like to know how can I get the precision, recall and f1 score for each class after making the predictions on a test set using the NN.",,0,3,0,0,
384,49671053,49671552,19846,How to set the line style for each kdeplot in a jointgrid,2,<python><matplotlib><seaborn><jointplot>,11,"<p>I am using seaborn to create a kdeplot with marginal distribution as described in <a href=""https://stackoverflow.com/a/49658133/1232833"">this answer</a>. I adapted the code a little to give me this:</p>

<pre><code>import matplotlib.pyplot as plt
import seaborn as sns

iris = sns.load_dataset(""iris"")
setosa = iris.loc[iris.species == ""setosa""]
virginica = iris.loc[iris.species == ""virginica""]

g = sns.JointGrid(x=""sepal_width"", y=""petal_length"", data=iris)
sns.kdeplot(setosa.sepal_width, setosa.sepal_length, cmap=""Reds"",
        shade=False, shade_lowest=False, ax=g.ax_joint)
sns.kdeplot(virginica.sepal_width, virginica.sepal_length, cmap=""Blues"",
        shade=False, shade_lowest=False, ax=g.ax_joint)
sns.distplot(setosa.sepal_width, kde=True, hist=False, color=""r"", ax=g.ax_marg_x)
sns.distplot(virginica.sepal_width, kde=True, hist=False, color=""b"", ax=g.ax_marg_x)
sns.distplot(setosa.sepal_length, kde=True, hist=False, color=""r"", ax=g.ax_marg_y, vertical=True)
sns.distplot(virginica.sepal_length, kde=True, hist=False, color=""b"", ax=g.ax_marg_y, vertical=True)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/TzfAw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TzfAw.png"" alt=""enter image description here""></a></p>

<p>This is impossible to print in black and white. <strong>How can I get seaborn to print the kdeplot and distplot lines in a specifically styled (dotted / dashed / ...) way to make them distinguishable when printed in black and white?</strong></p>

<p>The <a href=""https://stackoverflow.com/questions/44261803/change-line-style-in-seaborn-facet-grid"">related questions</a> deal with other types of plots which appear to support this, but this does not seem to be supported by <a href=""https://seaborn.pydata.org/generated/seaborn.kdeplot.html"" rel=""noreferrer"">kdeplot</a> and <a href=""https://seaborn.pydata.org/generated/seaborn.distplot.html"" rel=""noreferrer"">distplot</a>.</p>
",1232833,1293,05-04-2018 11:17,05-04-2018 11:44,0,1293,37,2,17,91,"{'badge_counts': {'bronze': 37, 'silver': 17, 'gold': 2}, 'account_id': 1278020, 'is_employee': False, 'last_modified_date': 1635462300, 'last_access_date': 1662392849, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1293, 'creation_date': 1330191627, 'user_type': 'registered', 'user_id': 1232833, 'accept_rate': 91, 'location': 'Darmstadt, Germany', 'website_url': 'http://twitter.com/malexmave', 'link': 'https://stackoverflow.com/users/1232833/malexmave', 'profile_image': 'https://www.gravatar.com/avatar/1c007b4947642a2dded390b84799441a?s=256&d=identicon&r=PG', 'display_name': 'malexmave'}","I am using seaborn to create a kdeplot with marginal distribution as described in this answer. I adapted the code a little to give me this: This is impossible to print in black and white. How can I get seaborn to print the kdeplot and distplot lines in a specifically styled (dotted / dashed / ...) way to make them distinguishable when printed in black and white? The related questions deal with other types of plots which appear to support this, but this does not seem to be supported by kdeplot and distplot.","import matplotlib.pyplot as plt
import seaborn as sns

iris = sns.load_dataset(""iris"")
setosa = iris.loc[iris.species == ""setosa""]
virginica = iris.loc[iris.species == ""virginica""]

g = sns.JointGrid(x=""sepal_width"", y=""petal_length"", data=iris)
sns.kdeplot(setosa.sepal_width, setosa.sepal_length, cmap=""Reds"",
        shade=False, shade_lowest=False, ax=g.ax_joint)
sns.kdeplot(virginica.sepal_width, virginica.sepal_length, cmap=""Blues"",
        shade=False, shade_lowest=False, ax=g.ax_joint)
sns.distplot(setosa.sepal_width, kde=True, hist=False, color=""r"", ax=g.ax_marg_x)
sns.distplot(virginica.sepal_width, kde=True, hist=False, color=""b"", ax=g.ax_marg_x)
sns.distplot(setosa.sepal_length, kde=True, hist=False, color=""r"", ax=g.ax_marg_y, vertical=True)
sns.distplot(virginica.sepal_length, kde=True, hist=False, color=""b"", ax=g.ax_marg_y, vertical=True)
plt.show()
",16,26,1,5,
385,49545358,49559115,13029,How do I repair conda after a system crash?,1,<python><anaconda><conda>,11,"<p>I was running this command <code>conda install -c xonsh/channel/dev xonsh</code> when I got a blue screen (black on my macOS :D).
Now running it results in this output:</p>

<pre><code>Solving environment: done

## Package Plan ##

  environment location: /Users/evar/anaconda

  added / updated specs:
    - xonsh


The following NEW packages will be INSTALLED:

    more-itertools:                     4.1.0-py35_0
    setproctitle:                       1.1.9-py35_0               xonsh/channel/dev
    xonsh:                              0.4.4.dev0-py35_gaa8b735   xonsh/channel/dev

The following packages will be UPDATED:

    _license:                           1.1-py36_1                                   --&gt; 1.1-py35_1
    alabaster:                          0.7.10-py36h174008c_0                        --&gt; 0.7.10-py35hb692fe1_0
    anaconda-client:                    1.6.11-py36_0                                --&gt; 1.6.14-py35_0
    anaconda-navigator:                 1.7.0-py36_0                                 --&gt; 1.8.2-py35_0
    anaconda-project:                   0.8.2-py36h9ee5d53_0                         --&gt; 0.8.2-py35ha24014a_0
    appnope:                            0.1.0-py36hf537a9a_0                         --&gt; 0.1.0-py35hd172556_0
    appscript:                          1.0.1-py36h9e71e49_1                         --&gt; 1.0.1-py35hdfff33c_1
    asn1crypto:                         0.24.0-py36_0                                --&gt; 0.24.0-py35_0
    astroid:                            1.6.1-py36_0                                 --&gt; 1.6.2-py35_0
    astropy:                            3.0-py36h917ab60_1                           --&gt; 3.0.1-py35h917ab60_1
    attrs:                              17.4.0-py36_0                                --&gt; 17.4.0-py35_0
    babel:                              2.5.3-py36_0                                 --&gt; 2.5.3-py35_0
    backports:                          1.0-py36ha3c1827_1                           --&gt; 1.0-py35h2556c16_1
    backports.shutil_get_terminal_size: 1.0.0-py36hd7a2ee4_2                         --&gt; 1.0.0-py35h40fcb1f_2
    beautifulsoup4:                     4.6.0-py36h72d3c9f_1                         --&gt; 4.6.0-py35hb75f182_1
    bitarray:                           0.8.1-py36h1de35cc_1                         --&gt; 0.8.1-py35h1de35cc_1
    blaze:                              0.11.3-py36h02e7a37_0                        --&gt; 0.11.3-py35h385aa17_0
    bleach:                             2.1.3-py36_0                                 --&gt; 2.1.3-py35_0
    bokeh:                              0.12.14-py36_0                               --&gt; 0.12.14-py35_0
    boto:                               2.48.0-py36hdbc59ac_1                        --&gt; 2.48.0-py35h10f7326_1
    bottleneck:                         1.2.1-py36hbd380ad_0                         --&gt; 1.2.1-py35h969083f_0
    certifi:                            2018.1.18-py36_0                             --&gt; 2018.1.18-py35_0
    cffi:                               1.11.5-py36h342bebf_0                        --&gt; 1.11.5-py35h342bebf_0
    chardet:                            3.0.4-py36h96c241c_1                         --&gt; 3.0.4-py35h16a84c2_1
    click:                              6.7-py36hec950be_0                           --&gt; 6.7-py35hcc65ea6_0
    cloudpickle:                        0.5.2-py36_1                                 --&gt; 0.5.2-py35_1
    clyent:                             1.2.2-py36hae3ad88_0                         --&gt; 1.2.2-py35h63ae3d7_0
    colorama:                           0.3.9-py36hd29a30c_0                         --&gt; 0.3.9-py35h1d66b2b_0
    conda:                              4.4.11-py36_0                                --&gt; 4.5.0-py35_0
    contextlib2:                        0.5.5-py36hd66e5e7_0                         --&gt; 0.5.5-py35hc2c67b4_0
    cryptography:                       2.1.4-py36h842514c_0                         --&gt; 2.2.1-py35h1de35cc_0
    cycler:                             0.10.0-py36hfc81398_0                        --&gt; 0.10.0-py35hb89929e_0
    cython:                             0.27.3-py36h6ecb376_0                        --&gt; 0.28.1-py35h1de35cc_0
    cytoolz:                            0.9.0.1-py36h1de35cc_0                       --&gt; 0.9.0.1-py35h1de35cc_0
    dask:                               0.17.1-py36_0                                --&gt; 0.17.2-py35_0
    dask-core:                          0.17.1-py36_0                                --&gt; 0.17.2-py35_0
    datashape:                          0.5.4-py36hfb22df8_0                         --&gt; 0.5.4-py35hd065018_0
    decorator:                          4.2.1-py36_0                                 --&gt; 4.2.1-py35_0
    distributed:                        1.21.2-py36_0                                --&gt; 1.21.4-py35_0
    docutils:                           0.14-py36hbfde631_0                          --&gt; 0.14-py35hb13dbd9_0
    entrypoints:                        0.2.3-py36hd81d71f_2                         --&gt; 0.2.3-py35h837ec6f_2
    et_xmlfile:                         1.0.1-py36h1315bdc_0                         --&gt; 1.0.1-py35h40eb147_0
    fastcache:                          1.0.2-py36h1de35cc_2                         --&gt; 1.0.2-py35h1de35cc_2
    flask:                              0.12.2-py36h5658096_0                        --&gt; 0.12.2-py35h7284a24_0
    flask-cors:                         3.0.3-py36h7387b97_0                         --&gt; 3.0.3-py35h7e589ad_0
    gevent:                             1.2.2-py36ha70b9d6_0                         --&gt; 1.2.2-py35h8412070_0
    gmpy2:                              2.0.8-py36hf9c35bd_2                         --&gt; 2.0.8-py35h212fb8a_2
    greenlet:                           0.4.13-py36h1de35cc_0                        --&gt; 0.4.13-py35h1de35cc_0
    h5py:                               2.7.1-py36h39cdac5_0                         --&gt; 2.7.1-py35he1c7800_0
    heapdict:                           1.0.0-py36_2                                 --&gt; 1.0.0-py35_2
    html5lib:                           1.0.1-py36h2f9c1c0_0                         --&gt; 1.0.1-py35h2f9c1c0_0
    idna:                               2.6-py36h8628d0a_1                           --&gt; 2.6-py35h01aacb0_1
    imageio:                            2.2.0-py36h5e01289_0                         --&gt; 2.3.0-py35_0
    imagesize:                          1.0.0-py36_0                                 --&gt; 1.0.0-py35_0
    ipykernel:                          4.8.2-py36_0                                 --&gt; 4.8.2-py35_0
    ipython:                            6.2.1-py36h3dda519_1                         --&gt; 6.2.1-py35h9470683_1
    ipython_genutils:                   0.2.0-py36h241746c_0                         --&gt; 0.2.0-py35hf129286_0
    ipywidgets:                         7.1.2-py36_0                                 --&gt; 7.1.2-py35_0
    isort:                              4.3.4-py36_0                                 --&gt; 4.3.4-py35_0
    itsdangerous:                       0.24-py36h49fbb8d_1                          --&gt; 0.24-py35hfbd69cd_1
    jdcal:                              1.3-py36h1986823_0                           --&gt; 1.3-py35h9028778_0
    jedi:                               0.11.1-py36_0                                --&gt; 0.11.1-py35_1
    jinja2:                             2.10-py36hd36f9c5_0                          --&gt; 2.10-py35h6ff70ae_0
    jsonschema:                         2.6.0-py36hb385e00_0                         --&gt; 2.6.0-py35h2dd9e4b_0
    jupyter:                            1.0.0-py36_4                                 --&gt; 1.0.0-py35_4
    jupyter_client:                     5.2.2-py36_0                                 --&gt; 5.2.3-py35_0
    jupyter_console:                    5.2.0-py36hccf5b1c_1                         --&gt; 5.2.0-py35hd2aa692_1
    jupyter_core:                       4.4.0-py36h79cf704_0                         --&gt; 4.4.0-py35h4ad9194_0
    kiwisolver:                         1.0.1-py36h792292d_0                         --&gt; 1.0.1-py35h219a9d8_0
    lazy-object-proxy:                  1.3.1-py36h2fbbe47_0                         --&gt; 1.3.1-py35h7293e74_0
    llvmlite:                           0.22.0-py36h35728e8_0                        --&gt; 0.22.0-py35h4df07f0_0
    locket:                             0.2.0-py36hca03003_1                         --&gt; 0.2.0-py35h58cf053_1
    lxml:                               4.1.1-py36hef8c89e_1                         --&gt; 4.2.1-py35h7166777_0
    markupsafe:                         1.0-py36h3a1e703_1                           --&gt; 1.0-py35h9ba0a7f_1
    matplotlib:                         2.2.0-py36hfa7797c_0                         --&gt; 2.2.2-py35ha7267d0_0
    mccabe:                             0.6.1-py36hdaeb55d_0                         --&gt; 0.6.1-py35h3f6a9a1_0
    mistune:                            0.8.3-py36_0                                 --&gt; 0.8.3-py35_0
    mkl-service:                        1.1.2-py36h7ea6df4_4                         --&gt; 1.1.2-py35h6a6947a_4
    mpmath:                             1.0.0-py36hf1b8295_2                         --&gt; 1.0.0-py35he743aed_2
    msgpack-python:                     0.5.5-py36h04f5b5a_0                         --&gt; 0.5.6-py35h04f5b5a_0
    multipledispatch:                   0.5.0-py36_0                                 --&gt; 0.5.0-py35_0
    navigator-updater:                  0.1.0-py36h7aee5fb_0                         --&gt; 0.1.0-py35hd04e0bf_0
    nbconvert:                          5.3.1-py36h810822e_0                         --&gt; 5.3.1-py35h63fb950_0
    nbformat:                           4.4.0-py36h827af21_0                         --&gt; 4.4.0-py35h41c2038_0
    networkx:                           2.1-py36_0                                   --&gt; 2.1-py35_0
    nltk:                               3.2.5-py36h1190bce_0                         --&gt; 3.2.5-py35h87b897b_0
    nose:                               1.3.7-py36h73fae2b_2                         --&gt; 1.3.7-py35h9ce1e3a_2
    notebook:                           5.4.0-py36_0                                 --&gt; 5.4.1-py35_0
    numba:                              0.37.0-np114py36h210bcc1_0                   --&gt; 0.37.0-np114py35hc1fb402_0
    numexpr:                            2.6.4-py36habcfcfe_0                         --&gt; 2.6.4-py35hcf51bc4_0
    numpy:                              1.14.1-py36ha726252_2                        --&gt; 1.14.2-py35ha9ae307_0
    numpydoc:                           0.7.0-py36he54d08e_0                         --&gt; 0.7.0-py35h296b98c_0
    odo:                                0.5.1-py36hc1af34a_0                         --&gt; 0.5.1-py35h7f7a387_0
    olefile:                            0.45.1-py36_0                                --&gt; 0.45.1-py35_0
    openpyxl:                           2.5.0-py36_0                                 --&gt; 2.5.1-py35_0
    openssl:                            1.0.2n-hdbc3d79_0                            --&gt; 1.0.2o-h26aff7b_0
    packaging:                          17.1-py36_0                                  --&gt; 17.1-py35_0
    pandas:                             0.22.0-py36h0a44026_0                        --&gt; 0.22.0-py35h0a44026_0
    pandocfilters:                      1.4.2-py36h3b0b094_1                         --&gt; 1.4.2-py35hff87490_1
    parso:                              0.1.1-py36hc90e01c_0                         --&gt; 0.1.1-py35hbda7c10_0
    partd:                              0.3.8-py36hf5c4cb8_0                         --&gt; 0.3.8-py35h6fadee7_0
    path.py:                            11.0-py36_0                                  --&gt; 11.0-py35_0
    pathlib2:                           2.3.0-py36h877a6d8_0                         --&gt; 2.3.0-py35hba2ddec_0
    patsy:                              0.5.0-py36_0                                 --&gt; 0.5.0-py35_0
    pep8:                               1.7.1-py36_0                                 --&gt; 1.7.1-py35_0
    pexpect:                            4.4.0-py36_0                                 --&gt; 4.4.0-py35_0
    pickleshare:                        0.7.4-py36hf512f8e_0                         --&gt; 0.7.4-py35h9517181_0
    pillow:                             5.0.0-py36hfcce615_0                         --&gt; 5.0.0-py35hfcce615_0
    pip:                                9.0.1-py36_5                                 --&gt; 9.0.1-py35_5
    pluggy:                             0.6.0-py36hb1d0581_0                         --&gt; 0.6.0-py35hf57b818_0
    ply:                                3.11-py36_0                                  --&gt; 3.11-py35_0
    prompt_toolkit:                     1.0.15-py36haeda067_0                        --&gt; 1.0.15-py35h93950c5_0
    psutil:                             5.4.3-py36h1de35cc_0                         --&gt; 5.4.3-py35h1de35cc_0
    ptyprocess:                         0.5.2-py36he6521c3_0                         --&gt; 0.5.2-py35hfc37984_0
    py:                                 1.5.2-py36ha69170d_0                         --&gt; 1.5.3-py35_0
    pycodestyle:                        2.3.1-py36h83e8646_0                         --&gt; 2.3.1-py35he0976b1_0
    pycosat:                            0.6.3-py36hee92d8f_0                         --&gt; 0.6.3-py35h745f8c1_0
    pycparser:                          2.18-py36h724b2fc_1                          --&gt; 2.18-py35hab820b0_1
    pycrypto:                           2.6.1-py36h1de35cc_7                         --&gt; 2.6.1-py35h1de35cc_7
    pycurl:                             7.43.0.1-py36hdbc3d79_0                      --&gt; 7.43.0.1-py35hdbc3d79_0
    pyflakes:                           1.6.0-py36hea45e83_0                         --&gt; 1.6.0-py35hc517269_0
    pygments:                           2.2.0-py36h240cd3f_0                         --&gt; 2.2.0-py35h392a662_0
    pylint:                             1.8.2-py36_0                                 --&gt; 1.8.3-py35_0
    pyodbc:                             4.0.22-py36h0a44026_0                        --&gt; 4.0.22-py35h0a44026_0
    pyopenssl:                          17.5.0-py36h51e4350_0                        --&gt; 17.5.0-py35h4065cf8_0
    pyparsing:                          2.2.0-py36hb281f35_0                         --&gt; 2.2.0-py35h31fab04_0
    pyqt:                               5.6.0-py36he5c6137_6                         --&gt; 5.6.0-py35hbd126f6_6
    pysocks:                            1.6.8-py36_0                                 --&gt; 1.6.8-py35_0
    pytables:                           3.4.2-py36hfbd7ab0_2                         --&gt; 3.4.2-py35hda701c8_2
    pytest:                             3.4.2-py36_0                                 --&gt; 3.5.0-py35_0
    pytest-arraydiff:                   0.2-py36_0                                   --&gt; 0.2-py35_0
    pytest-astropy:                     0.2.1-py36_0                                 --&gt; 0.2.1-py35_0
    pytest-doctestplus:                 0.1.2-py36_0                                 --&gt; 0.1.2-py35_0
    pytest-openfiles:                   0.2.0-py36_0                                 --&gt; 0.2.0-py35_0
    pytest-remotedata:                  0.2.0-py36_0                                 --&gt; 0.2.0-py35_0
    python-dateutil:                    2.6.1-py36h86d2abb_1                         --&gt; 2.7.2-py35_0
    python.app:                         2-py36h54569d5_7                             --&gt; 2-py35he4d1c94_7
    pytz:                               2018.3-py36_0                                --&gt; 2018.3-py35_0
    pywavelets:                         0.5.2-py36h2710a04_0                         --&gt; 0.5.2-py35h9dc8fb8_0
    pyyaml:                             3.12-py36h2ba1e63_1                          --&gt; 3.12-py35hf8cec8a_1
    pyzmq:                              17.0.0-py36h1de35cc_0                        --&gt; 17.0.0-py35h1de35cc_0
    qtawesome:                          0.4.4-py36h468c6fb_0                         --&gt; 0.4.4-py35h21e61ad_0
    qtconsole:                          4.3.1-py36hd96c0ff_0                         --&gt; 4.3.1-py35hd6d667b_0
    qtpy:                               1.3.1-py36h16bb863_0                         --&gt; 1.4.0-py35_0
    requests:                           2.18.4-py36h4516966_1                        --&gt; 2.18.4-py35h0d65e6b_1
    rope:                               0.10.7-py36h68959ac_0                        --&gt; 0.10.7-py35h27868a4_0
    ruamel_yaml:                        0.15.35-py36h1de35cc_1                       --&gt; 0.15.35-py35h1de35cc_1
    scikit-image:                       0.13.1-py36h1de35cc_1                        --&gt; 0.13.1-py35h1de35cc_1
    scikit-learn:                       0.19.1-py36hffbff8c_0                        --&gt; 0.19.1-py35h2b554eb_0
    scipy:                              1.0.0-py36h1de22e9_0                         --&gt; 1.0.0-py35h8b35106_0
    seaborn:                            0.8.1-py36h595ecd9_0                         --&gt; 0.8.1-py35he0bbe96_0
    send2trash:                         1.5.0-py36_0                                 --&gt; 1.5.0-py35_0
    setuptools:                         38.5.1-py36_0                                --&gt; 38.5.1-py35_0
    simplegeneric:                      0.8.1-py36_2                                 --&gt; 0.8.1-py35_2
    singledispatch:                     3.4.0.3-py36hf20db9d_0                       --&gt; 3.4.0.3-py35h0acf360_0
    sip:                                4.18.1-py36h2824476_2                        --&gt; 4.18.1-py35h79e1f92_2
    six:                                1.11.0-py36h0e22d5e_1                        --&gt; 1.11.0-py35h39a4c60_1
    snowballstemmer:                    1.2.1-py36h6c7b616_0                         --&gt; 1.2.1-py35hbb7be01_0
    sortedcollections:                  0.5.3-py36he9c3ed6_0                         --&gt; 0.6.1-py35_0
    sortedcontainers:                   1.5.9-py36_0                                 --&gt; 1.5.9-py35_0
    sphinx:                             1.7.1-py36_0                                 --&gt; 1.7.2-py35_0
    sphinxcontrib:                      1.0-py36h9364dc8_1                           --&gt; 1.0-py35h3eabf46_1
    sphinxcontrib-websupport:           1.0.1-py36h92f4a7a_1                         --&gt; 1.0.1-py35hcb4ca16_1
    spyder:                             3.2.7-py36_0                                 --&gt; 3.2.8-py35_0
    sqlalchemy:                         1.2.4-py36h1de35cc_0                         --&gt; 1.2.5-py35h1de35cc_0
    statsmodels:                        0.8.0-py36h9c68fc9_0                         --&gt; 0.8.0-py35ha7c9052_0
    sympy:                              1.1.1-py36h7f3cf04_0                         --&gt; 1.1.1-py35he478fab_0
    tblib:                              1.3.2-py36hda67792_0                         --&gt; 1.3.2-py35h1b9c5fd_0
    terminado:                          0.8.1-py36_1                                 --&gt; 0.8.1-py35_1
    testpath:                           0.3.1-py36h625a49b_0                         --&gt; 0.3.1-py35hf8009f4_0
    toolz:                              0.9.0-py36_0                                 --&gt; 0.9.0-py35_0
    tornado:                            5.0-py36_0                                   --&gt; 5.0-py35_0
    traitlets:                          4.3.2-py36h65bd3ce_0                         --&gt; 4.3.2-py35hd3d1486_0
    typing:                             3.6.4-py36_0                                 --&gt; 3.6.4-py35_0
    unicodecsv:                         0.14.1-py36he531d66_0                        --&gt; 0.14.1-py35h2154ad0_0
    urllib3:                            1.22-py36h68b9469_0                          --&gt; 1.22-py35he002d57_0
    webencodings:                       0.5.1-py36h3b9701d_1                         --&gt; 0.5.1-py35hcf8ebf9_1
    werkzeug:                           0.14.1-py36_0                                --&gt; 0.14.1-py35_0
    wheel:                              0.30.0-py36h5eb2c71_1                        --&gt; 0.30.0-py35h5c0b906_1
    widgetsnbextension:                 3.1.4-py36_0                                 --&gt; 3.1.4-py35_0
    wrapt:                              1.10.11-py36hc29e774_0                       --&gt; 1.10.11-py35ha18cf31_0
    xlrd:                               1.1.0-py36h336f4a2_1                         --&gt; 1.1.0-py35h892fcf7_1
    xlsxwriter:                         1.0.2-py36h3736301_0                         --&gt; 1.0.2-py35h4d68a89_0
    xlwings:                            0.11.7-py36_0                                --&gt; 0.11.7-py35_0
    xlwt:                               1.2.0-py36h5ad1178_0                         --&gt; 1.2.0-py35hf70d7fa_0
    zict:                               0.1.3-py36h71da714_0                         --&gt; 0.1.3-py35h1ae85d2_0

The following packages will be DOWNGRADED:

    anaconda:                           custom-py36ha4fed55_0                        --&gt; custom-py35hd7b5ba2_0
    python:                             3.6.4-hc167b69_1                             --&gt; 3.5.5-hc167b69_1
    wcwidth:                            0.1.7-py36h8c6ec74_0                         --&gt; 0.1.5-py35_5               xonsh/channel/dev

Proceed ([y]/n)? y

Preparing transaction: done
Verifying transaction: failed

SafetyError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/__pycache__/_tokenizer.cpython-35.pyc'
has a sha256 mismatch.
  reported sha256: 95475d9a871d7e07cb2b8af6d985068c5a1ceb9d593e5c21e42fa8019c7c14c0
  actual sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/__pycache__/constants.cpython-35.pyc'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/__pycache__/html5parser.cpython-35.pyc'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/_tokenizer.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/constants.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/filters/__init__.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/html5parser.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'bin/pylupdate5'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtBluetooth.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtCore.pyi'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtCore.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtDesigner.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtGui.pyi'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtGui.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtHelp.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtMultimedia.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtMultimediaWidgets.so'
specified in the package manifest cannot be found.

SafetyError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtNetwork.pyi'
has a sha256 mismatch.
  reported sha256: 6d1d0e8dab63eb82e73de1d2e7346ee0584e3796537d5c1d4c015ec22a7da356
  actual sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtNetwork.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtNfc.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtOpenGL.so'
specified in the package manifest cannot be found.
...
</code></pre>

<p>I looked at <a href=""https://conda.io/docs/user-guide/troubleshooting.html#"" rel=""noreferrer"">https://conda.io/docs/user-guide/troubleshooting.html#</a> but I didn't find anything about these errors or what to do in the case of a system crash.</p>
",1410221,3734,28-03-2018 22:46,29-03-2018 14:49,1,3750,47,3,24,86,"{'badge_counts': {'bronze': 47, 'silver': 24, 'gold': 3}, 'account_id': 1507082, 'is_employee': False, 'last_modified_date': 1685755800, 'last_access_date': 1711136122, 'reputation_change_year': 63, 'reputation_change_quarter': 63, 'reputation_change_month': 15, 'reputation_change_week': 6, 'reputation_change_day': 0, 'reputation': 3750, 'creation_date': 1337690278, 'user_type': 'registered', 'user_id': 1410221, 'accept_rate': 86, 'website_url': '', 'link': 'https://stackoverflow.com/users/1410221/happyface', 'profile_image': 'https://i.stack.imgur.com/CW5Pu.jpg?s=256&g=1', 'display_name': 'HappyFace'}",I was running this command when I got a blue screen (black on my macOS :D). Now running it results in this output: I looked at https://conda.io/docs/user-guide/troubleshooting.html# but I didn't find anything about these errors or what to do in the case of a system crash.,"conda install -c xonsh/channel/dev xonsh Solving environment: done

## Package Plan ##

  environment location: /Users/evar/anaconda

  added / updated specs:
    - xonsh


The following NEW packages will be INSTALLED:

    more-itertools:                     4.1.0-py35_0
    setproctitle:                       1.1.9-py35_0               xonsh/channel/dev
    xonsh:                              0.4.4.dev0-py35_gaa8b735   xonsh/channel/dev

The following packages will be UPDATED:

    _license:                           1.1-py36_1                                   --&gt; 1.1-py35_1
    alabaster:                          0.7.10-py36h174008c_0                        --&gt; 0.7.10-py35hb692fe1_0
    anaconda-client:                    1.6.11-py36_0                                --&gt; 1.6.14-py35_0
    anaconda-navigator:                 1.7.0-py36_0                                 --&gt; 1.8.2-py35_0
    anaconda-project:                   0.8.2-py36h9ee5d53_0                         --&gt; 0.8.2-py35ha24014a_0
    appnope:                            0.1.0-py36hf537a9a_0                         --&gt; 0.1.0-py35hd172556_0
    appscript:                          1.0.1-py36h9e71e49_1                         --&gt; 1.0.1-py35hdfff33c_1
    asn1crypto:                         0.24.0-py36_0                                --&gt; 0.24.0-py35_0
    astroid:                            1.6.1-py36_0                                 --&gt; 1.6.2-py35_0
    astropy:                            3.0-py36h917ab60_1                           --&gt; 3.0.1-py35h917ab60_1
    attrs:                              17.4.0-py36_0                                --&gt; 17.4.0-py35_0
    babel:                              2.5.3-py36_0                                 --&gt; 2.5.3-py35_0
    backports:                          1.0-py36ha3c1827_1                           --&gt; 1.0-py35h2556c16_1
    backports.shutil_get_terminal_size: 1.0.0-py36hd7a2ee4_2                         --&gt; 1.0.0-py35h40fcb1f_2
    beautifulsoup4:                     4.6.0-py36h72d3c9f_1                         --&gt; 4.6.0-py35hb75f182_1
    bitarray:                           0.8.1-py36h1de35cc_1                         --&gt; 0.8.1-py35h1de35cc_1
    blaze:                              0.11.3-py36h02e7a37_0                        --&gt; 0.11.3-py35h385aa17_0
    bleach:                             2.1.3-py36_0                                 --&gt; 2.1.3-py35_0
    bokeh:                              0.12.14-py36_0                               --&gt; 0.12.14-py35_0
    boto:                               2.48.0-py36hdbc59ac_1                        --&gt; 2.48.0-py35h10f7326_1
    bottleneck:                         1.2.1-py36hbd380ad_0                         --&gt; 1.2.1-py35h969083f_0
    certifi:                            2018.1.18-py36_0                             --&gt; 2018.1.18-py35_0
    cffi:                               1.11.5-py36h342bebf_0                        --&gt; 1.11.5-py35h342bebf_0
    chardet:                            3.0.4-py36h96c241c_1                         --&gt; 3.0.4-py35h16a84c2_1
    click:                              6.7-py36hec950be_0                           --&gt; 6.7-py35hcc65ea6_0
    cloudpickle:                        0.5.2-py36_1                                 --&gt; 0.5.2-py35_1
    clyent:                             1.2.2-py36hae3ad88_0                         --&gt; 1.2.2-py35h63ae3d7_0
    colorama:                           0.3.9-py36hd29a30c_0                         --&gt; 0.3.9-py35h1d66b2b_0
    conda:                              4.4.11-py36_0                                --&gt; 4.5.0-py35_0
    contextlib2:                        0.5.5-py36hd66e5e7_0                         --&gt; 0.5.5-py35hc2c67b4_0
    cryptography:                       2.1.4-py36h842514c_0                         --&gt; 2.2.1-py35h1de35cc_0
    cycler:                             0.10.0-py36hfc81398_0                        --&gt; 0.10.0-py35hb89929e_0
    cython:                             0.27.3-py36h6ecb376_0                        --&gt; 0.28.1-py35h1de35cc_0
    cytoolz:                            0.9.0.1-py36h1de35cc_0                       --&gt; 0.9.0.1-py35h1de35cc_0
    dask:                               0.17.1-py36_0                                --&gt; 0.17.2-py35_0
    dask-core:                          0.17.1-py36_0                                --&gt; 0.17.2-py35_0
    datashape:                          0.5.4-py36hfb22df8_0                         --&gt; 0.5.4-py35hd065018_0
    decorator:                          4.2.1-py36_0                                 --&gt; 4.2.1-py35_0
    distributed:                        1.21.2-py36_0                                --&gt; 1.21.4-py35_0
    docutils:                           0.14-py36hbfde631_0                          --&gt; 0.14-py35hb13dbd9_0
    entrypoints:                        0.2.3-py36hd81d71f_2                         --&gt; 0.2.3-py35h837ec6f_2
    et_xmlfile:                         1.0.1-py36h1315bdc_0                         --&gt; 1.0.1-py35h40eb147_0
    fastcache:                          1.0.2-py36h1de35cc_2                         --&gt; 1.0.2-py35h1de35cc_2
    flask:                              0.12.2-py36h5658096_0                        --&gt; 0.12.2-py35h7284a24_0
    flask-cors:                         3.0.3-py36h7387b97_0                         --&gt; 3.0.3-py35h7e589ad_0
    gevent:                             1.2.2-py36ha70b9d6_0                         --&gt; 1.2.2-py35h8412070_0
    gmpy2:                              2.0.8-py36hf9c35bd_2                         --&gt; 2.0.8-py35h212fb8a_2
    greenlet:                           0.4.13-py36h1de35cc_0                        --&gt; 0.4.13-py35h1de35cc_0
    h5py:                               2.7.1-py36h39cdac5_0                         --&gt; 2.7.1-py35he1c7800_0
    heapdict:                           1.0.0-py36_2                                 --&gt; 1.0.0-py35_2
    html5lib:                           1.0.1-py36h2f9c1c0_0                         --&gt; 1.0.1-py35h2f9c1c0_0
    idna:                               2.6-py36h8628d0a_1                           --&gt; 2.6-py35h01aacb0_1
    imageio:                            2.2.0-py36h5e01289_0                         --&gt; 2.3.0-py35_0
    imagesize:                          1.0.0-py36_0                                 --&gt; 1.0.0-py35_0
    ipykernel:                          4.8.2-py36_0                                 --&gt; 4.8.2-py35_0
    ipython:                            6.2.1-py36h3dda519_1                         --&gt; 6.2.1-py35h9470683_1
    ipython_genutils:                   0.2.0-py36h241746c_0                         --&gt; 0.2.0-py35hf129286_0
    ipywidgets:                         7.1.2-py36_0                                 --&gt; 7.1.2-py35_0
    isort:                              4.3.4-py36_0                                 --&gt; 4.3.4-py35_0
    itsdangerous:                       0.24-py36h49fbb8d_1                          --&gt; 0.24-py35hfbd69cd_1
    jdcal:                              1.3-py36h1986823_0                           --&gt; 1.3-py35h9028778_0
    jedi:                               0.11.1-py36_0                                --&gt; 0.11.1-py35_1
    jinja2:                             2.10-py36hd36f9c5_0                          --&gt; 2.10-py35h6ff70ae_0
    jsonschema:                         2.6.0-py36hb385e00_0                         --&gt; 2.6.0-py35h2dd9e4b_0
    jupyter:                            1.0.0-py36_4                                 --&gt; 1.0.0-py35_4
    jupyter_client:                     5.2.2-py36_0                                 --&gt; 5.2.3-py35_0
    jupyter_console:                    5.2.0-py36hccf5b1c_1                         --&gt; 5.2.0-py35hd2aa692_1
    jupyter_core:                       4.4.0-py36h79cf704_0                         --&gt; 4.4.0-py35h4ad9194_0
    kiwisolver:                         1.0.1-py36h792292d_0                         --&gt; 1.0.1-py35h219a9d8_0
    lazy-object-proxy:                  1.3.1-py36h2fbbe47_0                         --&gt; 1.3.1-py35h7293e74_0
    llvmlite:                           0.22.0-py36h35728e8_0                        --&gt; 0.22.0-py35h4df07f0_0
    locket:                             0.2.0-py36hca03003_1                         --&gt; 0.2.0-py35h58cf053_1
    lxml:                               4.1.1-py36hef8c89e_1                         --&gt; 4.2.1-py35h7166777_0
    markupsafe:                         1.0-py36h3a1e703_1                           --&gt; 1.0-py35h9ba0a7f_1
    matplotlib:                         2.2.0-py36hfa7797c_0                         --&gt; 2.2.2-py35ha7267d0_0
    mccabe:                             0.6.1-py36hdaeb55d_0                         --&gt; 0.6.1-py35h3f6a9a1_0
    mistune:                            0.8.3-py36_0                                 --&gt; 0.8.3-py35_0
    mkl-service:                        1.1.2-py36h7ea6df4_4                         --&gt; 1.1.2-py35h6a6947a_4
    mpmath:                             1.0.0-py36hf1b8295_2                         --&gt; 1.0.0-py35he743aed_2
    msgpack-python:                     0.5.5-py36h04f5b5a_0                         --&gt; 0.5.6-py35h04f5b5a_0
    multipledispatch:                   0.5.0-py36_0                                 --&gt; 0.5.0-py35_0
    navigator-updater:                  0.1.0-py36h7aee5fb_0                         --&gt; 0.1.0-py35hd04e0bf_0
    nbconvert:                          5.3.1-py36h810822e_0                         --&gt; 5.3.1-py35h63fb950_0
    nbformat:                           4.4.0-py36h827af21_0                         --&gt; 4.4.0-py35h41c2038_0
    networkx:                           2.1-py36_0                                   --&gt; 2.1-py35_0
    nltk:                               3.2.5-py36h1190bce_0                         --&gt; 3.2.5-py35h87b897b_0
    nose:                               1.3.7-py36h73fae2b_2                         --&gt; 1.3.7-py35h9ce1e3a_2
    notebook:                           5.4.0-py36_0                                 --&gt; 5.4.1-py35_0
    numba:                              0.37.0-np114py36h210bcc1_0                   --&gt; 0.37.0-np114py35hc1fb402_0
    numexpr:                            2.6.4-py36habcfcfe_0                         --&gt; 2.6.4-py35hcf51bc4_0
    numpy:                              1.14.1-py36ha726252_2                        --&gt; 1.14.2-py35ha9ae307_0
    numpydoc:                           0.7.0-py36he54d08e_0                         --&gt; 0.7.0-py35h296b98c_0
    odo:                                0.5.1-py36hc1af34a_0                         --&gt; 0.5.1-py35h7f7a387_0
    olefile:                            0.45.1-py36_0                                --&gt; 0.45.1-py35_0
    openpyxl:                           2.5.0-py36_0                                 --&gt; 2.5.1-py35_0
    openssl:                            1.0.2n-hdbc3d79_0                            --&gt; 1.0.2o-h26aff7b_0
    packaging:                          17.1-py36_0                                  --&gt; 17.1-py35_0
    pandas:                             0.22.0-py36h0a44026_0                        --&gt; 0.22.0-py35h0a44026_0
    pandocfilters:                      1.4.2-py36h3b0b094_1                         --&gt; 1.4.2-py35hff87490_1
    parso:                              0.1.1-py36hc90e01c_0                         --&gt; 0.1.1-py35hbda7c10_0
    partd:                              0.3.8-py36hf5c4cb8_0                         --&gt; 0.3.8-py35h6fadee7_0
    path.py:                            11.0-py36_0                                  --&gt; 11.0-py35_0
    pathlib2:                           2.3.0-py36h877a6d8_0                         --&gt; 2.3.0-py35hba2ddec_0
    patsy:                              0.5.0-py36_0                                 --&gt; 0.5.0-py35_0
    pep8:                               1.7.1-py36_0                                 --&gt; 1.7.1-py35_0
    pexpect:                            4.4.0-py36_0                                 --&gt; 4.4.0-py35_0
    pickleshare:                        0.7.4-py36hf512f8e_0                         --&gt; 0.7.4-py35h9517181_0
    pillow:                             5.0.0-py36hfcce615_0                         --&gt; 5.0.0-py35hfcce615_0
    pip:                                9.0.1-py36_5                                 --&gt; 9.0.1-py35_5
    pluggy:                             0.6.0-py36hb1d0581_0                         --&gt; 0.6.0-py35hf57b818_0
    ply:                                3.11-py36_0                                  --&gt; 3.11-py35_0
    prompt_toolkit:                     1.0.15-py36haeda067_0                        --&gt; 1.0.15-py35h93950c5_0
    psutil:                             5.4.3-py36h1de35cc_0                         --&gt; 5.4.3-py35h1de35cc_0
    ptyprocess:                         0.5.2-py36he6521c3_0                         --&gt; 0.5.2-py35hfc37984_0
    py:                                 1.5.2-py36ha69170d_0                         --&gt; 1.5.3-py35_0
    pycodestyle:                        2.3.1-py36h83e8646_0                         --&gt; 2.3.1-py35he0976b1_0
    pycosat:                            0.6.3-py36hee92d8f_0                         --&gt; 0.6.3-py35h745f8c1_0
    pycparser:                          2.18-py36h724b2fc_1                          --&gt; 2.18-py35hab820b0_1
    pycrypto:                           2.6.1-py36h1de35cc_7                         --&gt; 2.6.1-py35h1de35cc_7
    pycurl:                             7.43.0.1-py36hdbc3d79_0                      --&gt; 7.43.0.1-py35hdbc3d79_0
    pyflakes:                           1.6.0-py36hea45e83_0                         --&gt; 1.6.0-py35hc517269_0
    pygments:                           2.2.0-py36h240cd3f_0                         --&gt; 2.2.0-py35h392a662_0
    pylint:                             1.8.2-py36_0                                 --&gt; 1.8.3-py35_0
    pyodbc:                             4.0.22-py36h0a44026_0                        --&gt; 4.0.22-py35h0a44026_0
    pyopenssl:                          17.5.0-py36h51e4350_0                        --&gt; 17.5.0-py35h4065cf8_0
    pyparsing:                          2.2.0-py36hb281f35_0                         --&gt; 2.2.0-py35h31fab04_0
    pyqt:                               5.6.0-py36he5c6137_6                         --&gt; 5.6.0-py35hbd126f6_6
    pysocks:                            1.6.8-py36_0                                 --&gt; 1.6.8-py35_0
    pytables:                           3.4.2-py36hfbd7ab0_2                         --&gt; 3.4.2-py35hda701c8_2
    pytest:                             3.4.2-py36_0                                 --&gt; 3.5.0-py35_0
    pytest-arraydiff:                   0.2-py36_0                                   --&gt; 0.2-py35_0
    pytest-astropy:                     0.2.1-py36_0                                 --&gt; 0.2.1-py35_0
    pytest-doctestplus:                 0.1.2-py36_0                                 --&gt; 0.1.2-py35_0
    pytest-openfiles:                   0.2.0-py36_0                                 --&gt; 0.2.0-py35_0
    pytest-remotedata:                  0.2.0-py36_0                                 --&gt; 0.2.0-py35_0
    python-dateutil:                    2.6.1-py36h86d2abb_1                         --&gt; 2.7.2-py35_0
    python.app:                         2-py36h54569d5_7                             --&gt; 2-py35he4d1c94_7
    pytz:                               2018.3-py36_0                                --&gt; 2018.3-py35_0
    pywavelets:                         0.5.2-py36h2710a04_0                         --&gt; 0.5.2-py35h9dc8fb8_0
    pyyaml:                             3.12-py36h2ba1e63_1                          --&gt; 3.12-py35hf8cec8a_1
    pyzmq:                              17.0.0-py36h1de35cc_0                        --&gt; 17.0.0-py35h1de35cc_0
    qtawesome:                          0.4.4-py36h468c6fb_0                         --&gt; 0.4.4-py35h21e61ad_0
    qtconsole:                          4.3.1-py36hd96c0ff_0                         --&gt; 4.3.1-py35hd6d667b_0
    qtpy:                               1.3.1-py36h16bb863_0                         --&gt; 1.4.0-py35_0
    requests:                           2.18.4-py36h4516966_1                        --&gt; 2.18.4-py35h0d65e6b_1
    rope:                               0.10.7-py36h68959ac_0                        --&gt; 0.10.7-py35h27868a4_0
    ruamel_yaml:                        0.15.35-py36h1de35cc_1                       --&gt; 0.15.35-py35h1de35cc_1
    scikit-image:                       0.13.1-py36h1de35cc_1                        --&gt; 0.13.1-py35h1de35cc_1
    scikit-learn:                       0.19.1-py36hffbff8c_0                        --&gt; 0.19.1-py35h2b554eb_0
    scipy:                              1.0.0-py36h1de22e9_0                         --&gt; 1.0.0-py35h8b35106_0
    seaborn:                            0.8.1-py36h595ecd9_0                         --&gt; 0.8.1-py35he0bbe96_0
    send2trash:                         1.5.0-py36_0                                 --&gt; 1.5.0-py35_0
    setuptools:                         38.5.1-py36_0                                --&gt; 38.5.1-py35_0
    simplegeneric:                      0.8.1-py36_2                                 --&gt; 0.8.1-py35_2
    singledispatch:                     3.4.0.3-py36hf20db9d_0                       --&gt; 3.4.0.3-py35h0acf360_0
    sip:                                4.18.1-py36h2824476_2                        --&gt; 4.18.1-py35h79e1f92_2
    six:                                1.11.0-py36h0e22d5e_1                        --&gt; 1.11.0-py35h39a4c60_1
    snowballstemmer:                    1.2.1-py36h6c7b616_0                         --&gt; 1.2.1-py35hbb7be01_0
    sortedcollections:                  0.5.3-py36he9c3ed6_0                         --&gt; 0.6.1-py35_0
    sortedcontainers:                   1.5.9-py36_0                                 --&gt; 1.5.9-py35_0
    sphinx:                             1.7.1-py36_0                                 --&gt; 1.7.2-py35_0
    sphinxcontrib:                      1.0-py36h9364dc8_1                           --&gt; 1.0-py35h3eabf46_1
    sphinxcontrib-websupport:           1.0.1-py36h92f4a7a_1                         --&gt; 1.0.1-py35hcb4ca16_1
    spyder:                             3.2.7-py36_0                                 --&gt; 3.2.8-py35_0
    sqlalchemy:                         1.2.4-py36h1de35cc_0                         --&gt; 1.2.5-py35h1de35cc_0
    statsmodels:                        0.8.0-py36h9c68fc9_0                         --&gt; 0.8.0-py35ha7c9052_0
    sympy:                              1.1.1-py36h7f3cf04_0                         --&gt; 1.1.1-py35he478fab_0
    tblib:                              1.3.2-py36hda67792_0                         --&gt; 1.3.2-py35h1b9c5fd_0
    terminado:                          0.8.1-py36_1                                 --&gt; 0.8.1-py35_1
    testpath:                           0.3.1-py36h625a49b_0                         --&gt; 0.3.1-py35hf8009f4_0
    toolz:                              0.9.0-py36_0                                 --&gt; 0.9.0-py35_0
    tornado:                            5.0-py36_0                                   --&gt; 5.0-py35_0
    traitlets:                          4.3.2-py36h65bd3ce_0                         --&gt; 4.3.2-py35hd3d1486_0
    typing:                             3.6.4-py36_0                                 --&gt; 3.6.4-py35_0
    unicodecsv:                         0.14.1-py36he531d66_0                        --&gt; 0.14.1-py35h2154ad0_0
    urllib3:                            1.22-py36h68b9469_0                          --&gt; 1.22-py35he002d57_0
    webencodings:                       0.5.1-py36h3b9701d_1                         --&gt; 0.5.1-py35hcf8ebf9_1
    werkzeug:                           0.14.1-py36_0                                --&gt; 0.14.1-py35_0
    wheel:                              0.30.0-py36h5eb2c71_1                        --&gt; 0.30.0-py35h5c0b906_1
    widgetsnbextension:                 3.1.4-py36_0                                 --&gt; 3.1.4-py35_0
    wrapt:                              1.10.11-py36hc29e774_0                       --&gt; 1.10.11-py35ha18cf31_0
    xlrd:                               1.1.0-py36h336f4a2_1                         --&gt; 1.1.0-py35h892fcf7_1
    xlsxwriter:                         1.0.2-py36h3736301_0                         --&gt; 1.0.2-py35h4d68a89_0
    xlwings:                            0.11.7-py36_0                                --&gt; 0.11.7-py35_0
    xlwt:                               1.2.0-py36h5ad1178_0                         --&gt; 1.2.0-py35hf70d7fa_0
    zict:                               0.1.3-py36h71da714_0                         --&gt; 0.1.3-py35h1ae85d2_0

The following packages will be DOWNGRADED:

    anaconda:                           custom-py36ha4fed55_0                        --&gt; custom-py35hd7b5ba2_0
    python:                             3.6.4-hc167b69_1                             --&gt; 3.5.5-hc167b69_1
    wcwidth:                            0.1.7-py36h8c6ec74_0                         --&gt; 0.1.5-py35_5               xonsh/channel/dev

Proceed ([y]/n)? y

Preparing transaction: done
Verifying transaction: failed

SafetyError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/__pycache__/_tokenizer.cpython-35.pyc'
has a sha256 mismatch.
  reported sha256: 95475d9a871d7e07cb2b8af6d985068c5a1ceb9d593e5c21e42fa8019c7c14c0
  actual sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/__pycache__/constants.cpython-35.pyc'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/__pycache__/html5parser.cpython-35.pyc'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/_tokenizer.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/constants.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/filters/__init__.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for html5lib located at /Users/evar/anaconda/pkgs/html5lib-1.0.1-py35h2f9c1c0_0
appears to be corrupted. The path 'lib/python3.5/site-packages/html5lib/html5parser.py'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'bin/pylupdate5'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtBluetooth.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtCore.pyi'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtCore.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtDesigner.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtGui.pyi'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtGui.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtHelp.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtMultimedia.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtMultimediaWidgets.so'
specified in the package manifest cannot be found.

SafetyError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtNetwork.pyi'
has a sha256 mismatch.
  reported sha256: 6d1d0e8dab63eb82e73de1d2e7346ee0584e3796537d5c1d4c015ec22a7da356
  actual sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtNetwork.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtNfc.so'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pyqt located at /Users/evar/anaconda/pkgs/pyqt-5.6.0-py35hbd126f6_6
appears to be corrupted. The path 'lib/python3.5/site-packages/PyQt5/QtOpenGL.so'
specified in the package manifest cannot be found.
...
",302,310,0,1,
386,49748063,49748407,447674,"'pip install' fails for every package (""Could not find a version that satisfies the requirement"")",2,<python><pip><pypi>,131,"<p><code>pip install &lt;package name&gt;</code> is failing for every package for me. This is what I get:</p>

<pre><code>Could not find a version that satisfies the requirement &lt;package-name
(from versions: )
No matching distribution found for &lt;package-name&gt;
</code></pre>

<p>I <a href=""https://stackoverflow.com/q/38903415/1526703"">saw</a> <a href=""https://stackoverflow.com/q/31719816/1526703"">similar</a> <a href=""https://stackoverflow.com/q/43218164/1526703"">questions</a> on Stack&nbsp;Overflow, but they don't seem to be fully related to this one.</p>

<p>Also, <a href=""https://bhch.github.io/posts/2017/04/fix-the-pip-error-couldnt-find-a-version-that-satisfies-the-requirement/"" rel=""noreferrer"">this post</a> suggests that this might happen if PyPI is down or my IP address is blacklisted. It seems both are not true for my case.</p>

<p>pip shows up-to-date on running <code>pip install --upgrade pip</code>.</p>
",1526703,15198,10-04-2018 07:43,10-04-2018 08:02,0,15218,97,20,68,88,"{'badge_counts': {'bronze': 97, 'silver': 68, 'gold': 20}, 'account_id': 1657620, 'is_employee': False, 'last_modified_date': 1705384350, 'last_access_date': 1711041728, 'reputation_change_year': 140, 'reputation_change_quarter': 140, 'reputation_change_month': 70, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 15218, 'creation_date': 1342344856, 'user_type': 'registered', 'user_id': 1526703, 'accept_rate': 88, 'location': 'Gurgaon, Haryana, India', 'website_url': 'http://linkedin.com/in/anupam12', 'link': 'https://stackoverflow.com/users/1526703/anupam', 'profile_image': 'https://www.gravatar.com/avatar/dfd04c3353249c5f7636646dafdc822d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Anupam'}","is failing for every package for me. This is what I get: I saw similar questions on Stack&nbsp;Overflow, but they don't seem to be fully related to this one. Also, this post suggests that this might happen if PyPI is down or my IP address is blacklisted. It seems both are not true for my case. pip shows up-to-date on running .","pip install &lt;package name&gt; Could not find a version that satisfies the requirement &lt;package-name
(from versions: )
No matching distribution found for &lt;package-name&gt;
 pip install --upgrade pip",0,12,0,4,
387,49600611,49600711,79944,Python Anaconda: should I use `conda activate` or `source activate` in linux,3,<python><anaconda><conda>,68,"<p>So I am used to typing <code>source activate &lt;environment&gt;</code> when starting a python Anaconda environment. That works just fine. But when I create new conda environments I am seeing the message on Ubuntu 16.04 to start the environments with <code>conda activate</code> instead. Besides the errors about how to set up my shell to use <code>conda activate</code> instead, I am still not clear on what is the difference between <code>source activate ...</code> and <code>conda activate ...</code> Is there a reason to change? Does anyone know the difference between these two commands? Thanks.</p>
",1610428,9630,01-04-2018 17:14,01-04-2018 17:27,0,9630,126,13,70,75,"{'badge_counts': {'bronze': 126, 'silver': 70, 'gold': 13}, 'account_id': 1764146, 'is_employee': False, 'last_modified_date': 1708740600, 'last_access_date': 1710636897, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 9630, 'creation_date': 1345405752, 'user_type': 'registered', 'user_id': 1610428, 'accept_rate': 75, 'location': 'los angeles', 'link': 'https://stackoverflow.com/users/1610428/krishnab', 'profile_image': 'https://i.stack.imgur.com/e6gnk.jpg?s=256&g=1', 'display_name': 'krishnab'}","So I am used to typing when starting a python Anaconda environment. That works just fine. But when I create new conda environments I am seeing the message on Ubuntu 16.04 to start the environments with instead. Besides the errors about how to set up my shell to use instead, I am still not clear on what is the difference between and Is there a reason to change? Does anyone know the difference between these two commands? Thanks.",source activate &lt;environment&gt; conda activate conda activate source activate ... conda activate ...,-5,1,0,0,
388,48677128,48682691,7533,What is the right way to preprocess images in Keras while fine-tuning pre-trained models,1,<python><machine-learning><deep-learning><keras>,16,"<p>What is the right way to preprocess the data in Keras while fine-tuning the pre-trained models in keras.applications for our own data?</p>

<p>Keras provides the following <code>preprocess_input</code> functions</p>

<p><a href=""https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py#L149"" rel=""noreferrer"">keras.applications.imagenet_utils.preprocess_input</a></p>

<p><a href=""https://github.com/keras-team/keras/blob/master/keras/applications/inception_v3.py#L398"" rel=""noreferrer"">keras.applications.inception_v3.preprocess_input</a></p>

<p><a href=""https://github.com/keras-team/keras/blob/master/keras/applications/xception.py#L272"" rel=""noreferrer"">keras.applications.xception.preprocess_input</a></p>

<p><a href=""https://github.com/keras-team/keras/blob/master/keras/applications/inception_resnet_v2.py#L46"" rel=""noreferrer"">keras.applications.inception_resnet_v2.preprocess_input</a></p>

<p>Looking inside it seems like for inception_v3, xception, and inception_resnet_v2, it calls <a href=""https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py#L149"" rel=""noreferrer"">keras.applications.imagenet_utils.preprocess_input</a> with <code>mode='tf'</code>. While for other models it sets <code>mode='caffe'</code> each of which perform a different transformation. </p>

<p>In the blog post about transfer learning from Francois chollet -- <a href=""https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"" rel=""noreferrer"">https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</a> -- it is normalized to <code>[0, 1]</code> through a division with 255. Shouldn't the preprocess_input functions in Keras be used instead?</p>

<p>Also it is not clear whether the input images should be in RGB or BGR? Is there any consistency regarding this or is it specific to the pre-trained model being used?</p>
",1615732,2203,08-02-2018 03:37,08-02-2018 10:04,0,2203,23,3,19,55,"{'badge_counts': {'bronze': 23, 'silver': 19, 'gold': 3}, 'account_id': 1770956, 'is_employee': False, 'last_modified_date': 1649953501, 'last_access_date': 1636522327, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2203, 'creation_date': 1345603101, 'user_type': 'registered', 'user_id': 1615732, 'accept_rate': 55, 'website_url': '', 'link': 'https://stackoverflow.com/users/1615732/cdeepakroy', 'profile_image': 'https://www.gravatar.com/avatar/58592e3186eee86dcfe53e5973442959?s=256&d=identicon&r=PG', 'display_name': 'cdeepakroy'}","What is the right way to preprocess the data in Keras while fine-tuning the pre-trained models in keras.applications for our own data? Keras provides the following functions keras.applications.imagenet_utils.preprocess_input keras.applications.inception_v3.preprocess_input keras.applications.xception.preprocess_input keras.applications.inception_resnet_v2.preprocess_input Looking inside it seems like for inception_v3, xception, and inception_resnet_v2, it calls keras.applications.imagenet_utils.preprocess_input with . While for other models it sets each of which perform a different transformation. In the blog post about transfer learning from Francois chollet -- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html -- it is normalized to through a division with 255. Shouldn't the preprocess_input functions in Keras be used instead? Also it is not clear whether the input images should be in RGB or BGR? Is there any consistency regarding this or is it specific to the pre-trained model being used?","preprocess_input mode='tf' mode='caffe' [0, 1]",-4,17,0,6,
389,49188162,49197763,842,Efficiently yield elements from large list in (pseudo) random order,1,<python><python-3.x><random><generator>,11,"<p>I am experimenting with unrolling a few nested loops for (potentially) better performance at the expense of memory. In my scenario, I would end up with a list of about 300M elements (tuples), which I'd have to yield in (more or less) random order. </p>

<p>At this order of magnitude, <code>random.shuffle(some_list)</code> really is not the way to go anymore.</p>

<p>The below example illustrates the issue. Be aware, on an x86_64 Linux and CPython 3.6.4, it will eat about 11 GByte of memory.</p>

<pre><code>def get_random_element():
    some_long_list = list(range(0, 300000000))
    for random_item in some_long_list:
        yield random_item
</code></pre>

<p>My thinking so far is to simply generate one random index per iteration and yield randomly picked elements (indefinitely) from the list. It may yield certain elements multiple times and totally skip others, which would be a trade-off worth considering.</p>

<p>What other options do I have within reasonable bounds of memory and CPU time to possibly yield every element of the list only once?</p>
",1672565,3483,09-03-2018 06:54,09-03-2018 16:08,0,3493,75,2,38,95,"{'badge_counts': {'bronze': 75, 'silver': 38, 'gold': 2}, 'account_id': 1843697, 'is_employee': False, 'last_modified_date': 1695090300, 'last_access_date': 1711105978, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 3493, 'creation_date': 1347660461, 'user_type': 'registered', 'user_id': 1672565, 'accept_rate': 95, 'location': 'Europe', 'website_url': 'https://www.pleiszenburg.de/', 'link': 'https://stackoverflow.com/users/1672565/s-m-e', 'profile_image': 'https://i.stack.imgur.com/OEMWS.png?s=256&g=1', 'display_name': 's-m-e'}","I am experimenting with unrolling a few nested loops for (potentially) better performance at the expense of memory. In my scenario, I would end up with a list of about 300M elements (tuples), which I'd have to yield in (more or less) random order. At this order of magnitude, really is not the way to go anymore. The below example illustrates the issue. Be aware, on an x86_64 Linux and CPython 3.6.4, it will eat about 11 GByte of memory. My thinking so far is to simply generate one random index per iteration and yield randomly picked elements (indefinitely) from the list. It may yield certain elements multiple times and totally skip others, which would be a trade-off worth considering. What other options do I have within reasonable bounds of memory and CPU time to possibly yield every element of the list only once?","random.shuffle(some_list) def get_random_element():
    some_long_list = list(range(0, 300000000))
    for random_item in some_long_list:
        yield random_item
",2,15,0,0,
390,48091693,48601819,6844,tensorflow Dataset API diff between make_initializable_iterator and make_one_shot_iterator,1,<python><tensorflow><dataset>,15,"<p>I want to know the difference between <code>make_initializable_iterator</code> and <code>make_one_shot_iterator</code>.<br>
1. Tensorflow documentations said that <code>A ""one-shot"" iterator does not currently support re-initialization.</code> What exactly does that mean?<br>
2.  Are the following 2 snippets equivalent?<br>
Use <code>make_initializable_iterator</code>  </p>

<pre><code>iterator = data_ds.make_initializable_iterator()
data_iter = iterator.get_next()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for e in range(1, epoch+1):
    sess.run(iterator.initializer)
    while True:
        try:
            x_train, y_train = sess.run([data_iter])
            _, cost = sess.run([train_op, loss_op], feed_dict={X: x_train,
                                                               Y: y_train})
        except tf.errors.OutOfRangeError:   
            break
sess.close()
</code></pre>

<p>Use <code>make_one_shot_iterator</code>  </p>

<pre><code>iterator = data_ds.make_one_shot_iterator()
data_iter = iterator.get_next()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for e in range(1, epoch+1):
    while True:
        try:
            x_train, y_train = sess.run([data_iter])
            _, cost = sess.run([train_op, loss_op], feed_dict={X: x_train,
                                                               Y: y_train})
        except tf.errors.OutOfRangeError:   
            break
sess.close()
</code></pre>
",2641038,1993,04-01-2018 08:52,03-02-2018 20:42,30,2005,44,2,22,89,"{'badge_counts': {'bronze': 44, 'silver': 22, 'gold': 2}, 'account_id': 3121331, 'is_employee': False, 'last_modified_date': 1702868100, 'last_access_date': 1711176413, 'reputation_change_year': 81, 'reputation_change_quarter': 81, 'reputation_change_month': 12, 'reputation_change_week': 12, 'reputation_change_day': 0, 'reputation': 2005, 'creation_date': 1375342284, 'user_type': 'registered', 'user_id': 2641038, 'accept_rate': 89, 'website_url': '', 'link': 'https://stackoverflow.com/users/2641038/lion-lai', 'profile_image': 'https://www.gravatar.com/avatar/3d737f33994195915863709ddf876d0b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Lion Lai'}",I want to know the difference between and . 1. Tensorflow documentations said that What exactly does that mean? 2. Are the following 2 snippets equivalent? Use Use,"make_initializable_iterator make_one_shot_iterator A ""one-shot"" iterator does not currently support re-initialization. make_initializable_iterator iterator = data_ds.make_initializable_iterator()
data_iter = iterator.get_next()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for e in range(1, epoch+1):
    sess.run(iterator.initializer)
    while True:
        try:
            x_train, y_train = sess.run([data_iter])
            _, cost = sess.run([train_op, loss_op], feed_dict={X: x_train,
                                                               Y: y_train})
        except tf.errors.OutOfRangeError:   
            break
sess.close()
 make_one_shot_iterator iterator = data_ds.make_one_shot_iterator()
data_iter = iterator.get_next()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for e in range(1, epoch+1):
    while True:
        try:
            x_train, y_train = sess.run([data_iter])
            _, cost = sess.run([train_op, loss_op], feed_dict={X: x_train,
                                                               Y: y_train})
        except tf.errors.OutOfRangeError:   
            break
sess.close()
",20,37,0,0,
391,48466151,48480837,17142,Python refactoring fails in Visual Studio Code,3,<python><python-2.7><visual-studio-code><automated-refactoring>,13,"<p>I switched to Visual Studio Code for Python programming recently. Below is my Python configuration in Visual Studio Code settings:</p>

<pre><code>""python.pythonPath"": ""/Users/hzhang/.virtualenvs/env-2.7/bin/python"",
""python.autoComplete.extraPaths"": [
    ""/Users/hzhang/Work/xxx/shared_modules""
],
</code></pre>

<p>Basically, I just configure the Python interpreter and add one extra shared module path.</p>

<p>When I try to refactor a variable name, it throws this <em>error</em> which says <code>rope</code> is not installed, and it doesn't work even I install it. Based on my understanding, refactor variables is a feature of Visual Studio Code, and it shouldn't rely on any specific language.</p>

<p>How can I fix this problem?</p>

<p><a href=""https://i.stack.imgur.com/cjWIP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cjWIP.png"" alt=""Enter image description here""></a></p>

<p>Once I installed <code>rope</code>, refactor was still not working. It popups this error:</p>

<p><a href=""https://i.stack.imgur.com/3dUJ3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3dUJ3.png"" alt=""Enter image description here""></a></p>

<pre class=""lang-none prettyprint-override""><code>I am on Python 2.7
Visual Studio Code: Version 1.19.3 (1.19.3)
Rope version: 0.10.7
</code></pre>
",1735756,30817,26-01-2018 17:06,27-01-2018 21:38,1,30877,130,20,86,65,"{'badge_counts': {'bronze': 130, 'silver': 86, 'gold': 20}, 'collectives': [{'collective': {'tags': ['twilio-php', 'sendgrid', 'sendgrid-rails', 'twilio-conference', 'twilio-javascript', 'twilio-programmable-voice', 'twilio-twiml', 'twilioflexwebchat', 'twilio-functions', 'twilio-api', 'twilio-cli', 'twilio-flex', 'twilio-python', 'sendgrid-ruby', 'segment-io', 'twilio-click-to-call', 'twilio-video', 'sendgrid-api-v2', 'twilio', 'twilio-programmable-chat', 'twilio-node', 'sendgrid-templates', 'twilio-taskrouter', 'twilio-amd', 'twimlet', 'sendgrid-api-v3', 'twilio-studio', 'twilio-conversations', 'authy'], 'external_links': [{'type': 'website', 'link': 'https://www.twilio.com/'}, {'type': 'support', 'link': 'mailto:stackoverflow@twilio.com'}, {'type': 'twitter', 'link': 'https://twitter.com/TwilioDevs'}, {'type': 'github', 'link': 'https://github.com/twilio'}, {'type': 'facebook', 'link': 'https://facebook.com/TeamTwilio'}, {'type': 'instagram', 'link': 'https://instagram.com/twilio'}], 'description': 'Twilio has democratized channels like voice, text, chat, video, and email by virtualizing the world’s communications infrastructure through APIs that are simple enough for any developer, yet robust enough to power the world’s most demanding applications.', 'link': '/collectives/twilio', 'name': 'Twilio', 'slug': 'twilio'}, 'role': 'member'}, {'collective': {'tags': ['azure-synapse-link', 'azure-management-groups', 'azure-deployment-slots', 'azure-data-lake-gen2', 'azure-ad-v2', 'azure-databoxfamily', 'azure-purview', 'azure-spring-boot', 'azure-table-storage', 'azure-blob-trigger', 'azure-application-registration', 'azure-calculator', 'azure-hub', 'azure-ad-b2c-custom-policy', 'azure-batch', 'azure-form-recognizer', 'azure-app-service-plans', 'azureshell', 'azure-notificationhub', 'azure-sql-database', 'azure-workflow-automation', 'azure-storage', 'azure-static-web-app', 'azure-nsg', 'azure-databricks', 'azure-app-api', 'azure-alerts', 'azure-virtual-machine', 'azure-cli', 'azure-git-deployment', 'azure-waf', 'azure-connect', 'azure-cosmosdb-cassandra-api', 'azure-database-mysql', 'azure-iot-suite', 'azure-ad-role', 'azure-dns', 'azure-emulator', 'azure-sas', 'azure-search-.net-sdk', 'azure-availability-zones', 'azure-bot-service', 'kql', 'azure-authentication', 'sql-azure-alerts', 'azure-service-fabric', 'azure-communication-services', 'azure-metrics-advisor', 'azure-iot-central', 'azure-cloud-services', 'azure-blob-storage', 'azure-regions', 'azure-store', 'azure-feature-manager', 'azure-cdn', 'azure-ad-graph-api', 'azure-industrial-iot', 'azure-ad-b2c', 'azure-synapse', 'azure-app-registration', 'azure-remote-rendering', 'azure-rtos', 'azure-storage-account', 'azure-iot-sdk', 'azure-ai-translator', 'azure-container-service', 'azure-text-translation', 'azure-fluent-api', 'azure-rest-api', 'azure-sphere', 'azure-gov', 'azure-hybrid-connections', 'azure-quantum', 'azure-digital-twins', 'azurekinect', 'azure-postgresql', 'azure-integration-account', 'microsoft-entra-private-access', 'azure-sql-server', 'azure-servicebus-topics', 'sql-server-azure', 'azure-log-analytics-workspace', 'azure-rbac', 'azure-policy', 'azure-mcd', 'azure-sdk-go', 'spark-bash-azure-databricks', 'azure-app-configuration', 'azure-iot-hub', 'azure-role-environment', 'azure-powershell', 'azure-identity', 'microsoft-entra-external-id', 'azure-deployment', 'azure-sdk-ruby', 'azure-free-services', 'azure-webjobs-triggered', 'azureadgraph-deprecation', 'azure-data-lake', 'azure-data-factory', 'azure-service-hooks', 'azure-image-builder', 'azure-backup-vault', 'azure-data-share', 'azure-data-catalog', 'azure-storage-files', 'azure-appservice', 'azure-files', 'azure-ad-domain-services', 'adal.js', 'azure-configuration', 'azure-qna-maker', 'spring-cloud-azure', 'azurerm-app-service', 'azure-sdk', 'azure-secrets', 'azure-affinity-group', 'kitchen-azurerm', 'azure-web-roles', 'azure-cli2', 'azure-log-analytics', 'azure-hdinsight', 'azure-application-gateway', 'azure-python-sdk', 'azure-sql-reporting', 'azure-disk', 'azure-devtest-labs', 'passport-azure-ad', 'azure-elasticpool', 'azure-function-app', 'azure-signalr', 'azure-webjobs-continuous', 'azure-compute-emulator', 'azure-speech', 'microsoft-entra-internet-access', 'azure-functions-runtime', 'azure-web-app-service', 'azure-front-door', 'azure-function-queue', 'azure-resource-group', 'azure-pack', 'azure-caching', 'azure-cosmosdb-mongoapi', 'azure-webjobs', 'azure-billing-api', 'azure-appfabric', 'azure-cloud-shell', 'azure-sdk-for-ruby', 'azure-virtual-network', 'azure-servicebus-subscriptions', 'azure-arc', 'azure-private-dns', 'azure-static-web-app-routing', 'azureservicebus', 'azure-defender', 'azure-data-studio', 'azure-iot-hub-device-management', 'azure-site-recovery', 'azure-application-settings', 'azure-cosmosdb-mongovcore', 'azure-security-center', 'azure-web-app-firewall', 'azure-queues', 'azure-functions', 'azure-management', 'azure-sdk-for-go', 'azure-blockchain-service', 'defaultazurecredential', 'azure-acs', 'azure-functions-core-tools', 'azure-servicebusrelay', 'pulumi-azure', 'azure-sql-edge', 'azure-subscription', 'azure-managed-database', 'azure-management-api', 'azure-bastion', 'azure-iot-dps', 'azure-vm-templates', 'azure-static-website-routing', 'azure-stack', 'azure-managed-app', 'azure-private-link', 'azure-ml-component', 'azure-function-async', 'azure-cosmosdb-tables', 'azure-timeseries-insights', 'azure-video-indexer', 'azure-ai', 'azure-elastic-scale', 'azure-clouddrive', 'azure-api-apps', 'azure-service-fabric-mesh', 'azure-functions-docker', 'microsoft-custom-vision', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-app-service-envrmnt', 'azure-sdk-js', 'azure-sdk-for-java', 'azure-advisor', 'azure-function-app-proxy', 'azure-functions-proxies', 'azure-sentinel', 'azure-anomaly-detector', 'azure-container-instances', 'azure-managed-disk', 'azure-active-directory', 'azureclicredential', 'azure-webapps', 'azure-ml-pipelines', 'azure-redis-cache', 'azure-http-trigger', 'azure-dsvm', 'azureportal', 'azure-servicebus-queues', 'azure-media-services', 'azure-ase', 'azure-node-sdk', 'azure-sql-managed-instance', 'sql-azure-federations', 'azure-debugger', 'azure-service-principal', 'azure-monitor-workbooks', 'azure-web-app-for-containers', 'azure-vm', 'azure-application-insights-profiler', 'azure-cost-calculation', 'azure-mobile-engagement', 'azure-file-copy', 'azure-diagnostics', 'azure-security', 'azure-analytics', 'azure-logic-app-standard', 'azure-vm-scale-set', 'azure-java-tools', 'azure-cognitive-services', 'django-pyodbc-azure', 'azure-application-proxy', 'azure-resource-graph', 'azure-ad-b2b', 'azure-compliance-policy', 'azure-durable-functions', 'azure-database-postgresql', 'azure-promptflow', 'azure-eventhub', 'azure-tablequery', 'azure-sdk-php', 'azure-storage-queues', 'azure-service-plan', 'azure-cosmosdb-emulator', 'azure-performancecounters', 'azure-scheduler', 'azure-availability-set', 'azure-dashboard', 'azure-mysql-database', 'azure-managed-grafana', 'azure-monitoring', 'azure-worker-roles', 'azure-service-runtime', 'azure-ddos', 'azure-data-sync', 'azure-machine-learning-service', 'azure-billing', 'azure-packaging', 'azure-container-apps', 'microsoft-entra-id', 'azure-sql', 'azure-bicep', 'azure-cosmosdb', 'azure-update-management-center', 'azure-mapping-data-flow', 'azure-lab-services', 'azure-custom-providers', 'azure-sdk-.net', 'azure-autoscaling-block', 'azure-data-explorer', 'azureml-python-sdk', 'azure-pipelines-release-pipeline', 'azure-load-balancer', 'azure-managed-identity', 'azure-ad-verifiable-credentials', 'azure-webjobssdk', 'azure-agent', 'azuremlsdk', 'azure-blueprints', 'azure-vpn', 'azure-automation', 'azure-blockchain-workbench', 'azure-api-management', 'azure-rm', 'azure-application-roles', 'azure-public-ip', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-sdk-python', 'terraform-provider-azure', 'azure-marketplace', 'azure-information-protection', 'azure-analysis-services', 'azure-zulu', 'azure-batch-account', 'azure-china', 'azure-android-sdk', 'azure-rm-template', 'azure-spring-cloud', 'azure-stream-analytics', 'azure-keyvault', 'azure-oms', 'azure-ad-msal', 'azure-webhooks', 'azure-adal-deprecation', 'azure-language-understanding', 'azure-container-registry', 'azure-web-pubsub', 'azure-maps', 'azure-migrate', 'azure-dev-spaces', 'fhir-server-for-azure', 'sitecore-azure', 'azure-cosmosdb-gremlinapi', 'azure-aks', 'azure.data.tables', 'azure-java-sdk', 'azure-static-website-hosting', 'azure-mobile-services', 'azure-triggers', 'azure-ad-powershell-v2', 'azure-adf', 'azure-in-role-cache', 'azure-iot-edge', 'azure-linux', 'azure-media-player', 'azure-storage-emulator', 'azure-eventgrid', 'azure-object-anchors', 'azure-management-portal', 'azure-notebooks', 'azure-custom-domain', 'azure-xplat-cli', 'azure-runbook', 'rebus-azureservicebus', 'azure-cognitive-search', 'azure-oauth', 'azure-application-insights', 'azure-traffic-manager', 'azure-anomaly-detection', 'azure-acr', 'adal', 'azure-storage-explorer', 'azure-private-dns-zone', 'azure', 'azure-auto-ml', 'azure-iot-hub-device-update', 'azure-logic-apps', 'azure-relay', 'azure-spatial-anchors', 'azure-monitor', 'azure-load-testing', 'azure-cosmosdb-changefeed', 'azure-function-http'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}, {'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}, {'collective': {'tags': ['amazon-elastic-beanstalk', 'aws-fargate', 'aws-sam', 'aws-codecommit', 'amazon-glacier', 'amazon-ami', 'aws-security-hub', 'alexa-sdk-nodejs', 'amazon-iam', 'amazon-guardduty', 'aws-glue', 'aws-sdk-ruby', 'amazon-cloudfront', 'aws-batch', 'aws-mediatailor', 'aws-global-accelerator', 'amazon-neptune', 'aws-sdk-go-v2', 'amazon-dynamodb-dax', 'aws-vpn', 'amazon-sumerian', 'aws-ssm', 'amazon-route53', 'aws-app-config', 'aws-sdk-ios', 'aws-cli', 'aws-app-mesh', 'aws-event-bridge', 'aws-directory-services', 'amazon-web-services', 'aws-copilot-cli', 'aws-transfer-family', 'aws-parameter-store', 'amazon-imagebuilder', 'amazon-sagemaker', 'amazon-workdocs', 'amazon-keyspaces', 'amazon-ecr', 'amazon-elb', 'aws-cloudformation', 'aws-config', 'aws-snowball', 'aws-sdk-mock', 'amazon-app-runner', 'aws-iot-analytics', 'amazon-sns', 'amazon-memory-db', 'aws-pinpoint', 'aws-deeplens', 'amazon-elasticache', 'aws-mediastore', 'amazon-bedrock', 'amazon-athena', 'amazon-gamelift', 'aws-codecatalyst', 'aws-graviton', 'aws-codeartifact', 'aws-elb', 'aws-cloudmap', 'aws-codeguru', 'amazon-translate', 'aws-sdk-java', 'aws-resource-group', 'amazon-data-pipeline', 'aws-sdk-js', 'amazon-ivs', 'aws-sdk-cpp', 'amazon-kinesis-analytics', 'amazon-cloudwatch', 'amazon-cloudhsm', 'aws-iot-sitewise', 'amazon-vpc', 'alexa-smart-home-skill', 'amazon-kendra', 'amazon-inspector', 'aws-datasync', 'aws-cloud9', 'amazon-ecs', 'amazon-rekognition', 'amazon-swf', 'aws-media-live', 'aws-sdk-js-v3', 'amazon-fsx', 'amazon-s3-select', 'aws-sdk-nodejs', 'aws-iam-identity-center', 'aws-chatbot', 'amazon-opensearch', 'aws-lambda', 'aws-lake-formation', 'aws-cdk', 'amazon-ses', 'aws-security-group', 'aws-mediapackage', 'amazon-connect', 'amazon-qldb', 'aws-iot-core', 'aws-sdk-rust', 'amazon-elastic-transcoder', 'aws-code-deploy', 'aws-serverless', 'amazon-honeycode', 'amazon-ec2', 'alexa-interaction-model', 'aws-xray', 'amazon-waf', 'aws-elemental', 'amazon-sqs', 'amazon-kms', 'aws-certificate-manager', 'aws-sam-cli', 'amazon-kinesis-video-streams', 'aws-lambda-powertools', 'amazon-ec2-spot-market', 'aws-documentdb', 'aws-control-tower', 'aws-service-catalog', 'aws-direct-connect', 'aws-billing', 'aws-iot', 'amazon-cloudwatchlogs', 'amazon-textract', 'alexa-sdk-python', 'aws-lambda-edge', 'amazon-dynamodb', 'amazon-rds', 'aws-iot-events', 'alexa-smapi', 'alexa-flash-briefing-skill', 'aws-dms', 'aws-mediaconnect', 'aws-organizations', 'amazon-macie', 'aws-sdk-comprehend', 'aws-device-farm', 'amazon-redshift-spectrum', 'aws-appsync', 'alexa-account-linking', 'amazon-transcribe', 'aws-acm', 'aws-opsworks', 'aws-step-functions', 'amazon-simpledb', 'amazon-lightsail', 'alexa-presentation-language', 'aws-amplify', 'amazon-workspaces', 'amazon-aurora', 'elastic-ip', 'aws-codepipeline', 'amazon-managed-blockchain', 'aws-application-load-balancer', 'amazon-forecast', 'aws-cloudshell', 'aws-mobilehub', 'aws-reserved-instances', 'amazon-efs', 'aws-sdk', 'aws-backup', 'amazon-timestream', 'amazon-cloudtrail', 'aws-sdk-go', 'amazon-appflow', 'amazon-emr', 'amazon-elasticsearch', 'aws-iot-greengrass', 'aws-sct', 'aws-private-link', 'amazon-quicksight', 'aws-fis', 'aws-sdk-net', 'alexa-skills-kit', 'amazon-kinesis-firehose', 'aws-sdk-java-2.0', 'amazon-ebs', 'aws-codestar', 'aws-sdk-android', 'aws-appstream', 'amazon-s3', 'amazon-lex', 'amazon-cloudsearch', 'aws-databrew', 'amazon-cognito', 'aws-elastictranscoder', 'amazon-workmail', 'amazon-comprehend', 'aws-auto-scaling', 'aws-codebuild', 'aws-api-gateway', 'aws-sso', 'amazon-eks', 'aws-storage-gateway', 'amazon-mq', 'aws-data-exchange', 'amazon-location-service', 'amazon-kinesis', 'amazon-sagemaker-compilers', 'aws-secrets-manager', 'aws-msk', 'amazon-personalize', 'aws-nlb', 'amazon-redshift', 'aws-copilot', 'aws-media-convert', 'amazon-polly'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 1925103, 'is_employee': False, 'last_modified_date': 1687740412, 'last_access_date': 1711164793, 'reputation_change_year': 340, 'reputation_change_quarter': 340, 'reputation_change_month': 70, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 30877, 'creation_date': 1349891590, 'user_type': 'registered', 'user_id': 1735756, 'accept_rate': 65, 'location': 'Calgary, AB, Canada', 'website_url': 'https://haifzhan.ca/', 'link': 'https://stackoverflow.com/users/1735756/haifeng-zhang', 'profile_image': 'https://i.stack.imgur.com/ytS6H.png?s=256&g=1', 'display_name': 'Haifeng Zhang'}","I switched to Visual Studio Code for Python programming recently. Below is my Python configuration in Visual Studio Code settings: Basically, I just configure the Python interpreter and add one extra shared module path. When I try to refactor a variable name, it throws this error which says is not installed, and it doesn't work even I install it. Based on my understanding, refactor variables is a feature of Visual Studio Code, and it shouldn't rely on any specific language. How can I fix this problem? Once I installed , refactor was still not working. It popups this error:","""python.pythonPath"": ""/Users/hzhang/.virtualenvs/env-2.7/bin/python"",
""python.autoComplete.extraPaths"": [
    ""/Users/hzhang/Work/xxx/shared_modules""
],
 rope rope I am on Python 2.7
Visual Studio Code: Version 1.19.3 (1.19.3)
Rope version: 0.10.7
",3,24,2,2,
392,49673345,49673613,57663,Python Pandas: get rows of a DataFrame where a column is not null,1,<python><pandas><dataframe>,19,"<p>I'm filtering my DataFrame dropping those rows in which the cell value of a specific column is None.</p>

<pre><code>df = df[df['my_col'].isnull() == False]
</code></pre>

<p>Works fine, but PyCharm tells me:</p>

<blockquote>
  <p>PEP8: comparison to False should be 'if cond is False:' or 'if not cond:'</p>
</blockquote>

<p>But I wonder how I should apply this to my use-case? Using 'not ...' or ' is False' did not work. My current solution is:</p>

<pre><code>df = df[df['my_col'].notnull()]
</code></pre>
",1739297,5634,05-04-2018 13:15,05-04-2018 13:28,0,5644,122,8,63,86,"{'badge_counts': {'bronze': 122, 'silver': 63, 'gold': 8}, 'account_id': 1929598, 'is_employee': False, 'last_modified_date': 1672939351, 'last_access_date': 1710674472, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5644, 'creation_date': 1349985210, 'user_type': 'registered', 'user_id': 1739297, 'accept_rate': 86, 'location': 'Vienna, Austria', 'website_url': 'https://at.linkedin.com/in/matthiasheise', 'link': 'https://stackoverflow.com/users/1739297/matthias', 'profile_image': 'https://i.stack.imgur.com/PG6Q6.png?s=256&g=1', 'display_name': 'Matthias'}","I'm filtering my DataFrame dropping those rows in which the cell value of a specific column is None. Works fine, but PyCharm tells me: PEP8: comparison to False should be 'if cond is False:' or 'if not cond:' But I wonder how I should apply this to my use-case? Using 'not ...' or ' is False' did not work. My current solution is:","df = df[df['my_col'].isnull() == False]
 df = df[df['my_col'].notnull()]
",0,15,0,0,
393,48053955,60388520,11277,Alembic Migrations on Multiple Models,3,<python><sqlalchemy><alembic>,21,"<p>I am attempting to create a revision with <code>--autogenerate</code> using Alembic for two Models, but am receiving a duplicate table keys error. Does, a schema need to be specified?  If so, how can it be set?  The documentation I've read says to use <code>__table_args__ = {'schema': 'somename'}</code>, but that hasn't helped.  Any tips or suggestions are greatly appreciated.</p>

<p>My current setup is:</p>

<p><strong>base.py</strong></p>

<pre><code>from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base()
</code></pre>

<p><strong>workspace.py</strong></p>

<pre><code>from sqlalchemy import Column, Integer, String
from base import Base

class WorkspaceModel(Base):

    __tablename__ = 'workspaces'

    id = Column(Integer, primary_key=True)
    name = Column(String)
</code></pre>

<p><strong>host.py</strong></p>

<pre><code>from sqlalchemy import Column, Integer, String
from base import Base

class HostModel(Base):

    __tablename__ = 'hosts'

    id = Column(Integer, primary_key=true)
    ip = Column(String)
</code></pre>

<p><strong>alembic/env.py</strong></p>

<pre><code>from host import HostModel
from workspace import WorkspaceModel
target_metadata = [HostModel.metadata, WorkspaceModel.metadata]
</code></pre>

<p><strong>Error</strong></p>

<pre><code>ValueError: Duplicate table keys across multiple MetaData objects: ""hosts"", ""workspaces""
</code></pre>
",1822062,4468,01-01-2018 22:59,25-02-2020 06:14,785,4478,39,3,29,89,"{'badge_counts': {'bronze': 39, 'silver': 29, 'gold': 3}, 'account_id': 2040810, 'is_employee': False, 'last_modified_date': 1654805700, 'last_access_date': 1711035686, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4478, 'creation_date': 1352840622, 'user_type': 'registered', 'user_id': 1822062, 'accept_rate': 89, 'website_url': 'http://chirinosky.com', 'link': 'https://stackoverflow.com/users/1822062/chirinosky', 'profile_image': 'https://www.gravatar.com/avatar/f12b215864a8adf95b3e00711f121983?s=256&d=identicon&r=PG', 'display_name': 'chirinosky'}","I am attempting to create a revision with using Alembic for two Models, but am receiving a duplicate table keys error. Does, a schema need to be specified? If so, how can it be set? The documentation I've read says to use , but that hasn't helped. Any tips or suggestions are greatly appreciated. My current setup is: base.py workspace.py host.py alembic/env.py Error","--autogenerate __table_args__ = {'schema': 'somename'} from sqlalchemy.ext.declarative import declarative_base
Base = declarative_base()
 from sqlalchemy import Column, Integer, String
from base import Base

class WorkspaceModel(Base):

    __tablename__ = 'workspaces'

    id = Column(Integer, primary_key=True)
    name = Column(String)
 from sqlalchemy import Column, Integer, String
from base import Base

class HostModel(Base):

    __tablename__ = 'hosts'

    id = Column(Integer, primary_key=true)
    ip = Column(String)
 from host import HostModel
from workspace import WorkspaceModel
target_metadata = [HostModel.metadata, WorkspaceModel.metadata]
 ValueError: Duplicate table keys across multiple MetaData objects: ""hosts"", ""workspaces""
",17,47,0,0,
394,49665757,49675283,37307,How to add report_tensor_allocations_upon_oom to RunOptions in Keras,5,<python><tensorflow><keras><gpu>,30,"<p>I'm trying to train a neural net on a GPU using Keras and am getting a ""Resource exhausted: OOM when allocating tensor"" error.  The specific tensor it's trying to allocate isn't very big, so I assume some previous tensor consumed almost all the VRAM.  The error message comes with a hint that suggests this:</p>

<blockquote>
  <p>Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</p>
</blockquote>

<p>That sounds good, but how do I do it?  RunOptions appears to be a Tensorflow thing, and what little documentation I can find for it associates it with a ""session"".  I'm using Keras, so Tensorflow is hidden under a layer of abstraction and its sessions under another layer below that.</p>

<p>How do I dig underneath everything to set this option in such a way that it will take effect?</p>
",1944458,2956,05-04-2018 06:45,05-04-2018 14:47,0,2966,25,1,19,25,"{'badge_counts': {'bronze': 25, 'silver': 19, 'gold': 1}, 'account_id': 124056, 'is_employee': False, 'last_modified_date': 1687367400, 'last_access_date': 1710318177, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2966, 'creation_date': 1357179334, 'user_type': 'registered', 'user_id': 1944458, 'accept_rate': 25, 'website_url': '', 'link': 'https://stackoverflow.com/users/1944458/dspeyer', 'profile_image': 'https://www.gravatar.com/avatar/2c148127606d9c8d88cc74991ef1f796?s=256&d=identicon&r=PG', 'display_name': 'dspeyer'}","I'm trying to train a neural net on a GPU using Keras and am getting a ""Resource exhausted: OOM when allocating tensor"" error. The specific tensor it's trying to allocate isn't very big, so I assume some previous tensor consumed almost all the VRAM. The error message comes with a hint that suggests this: Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. That sounds good, but how do I do it? RunOptions appears to be a Tensorflow thing, and what little documentation I can find for it associates it with a ""session"". I'm using Keras, so Tensorflow is hidden under a layer of abstraction and its sessions under another layer below that. How do I dig underneath everything to set this option in such a way that it will take effect?",,0,9,0,0,
395,49584917,49584927,11033,How to merge two pandas time series objects with different date time indices?,2,<python><pandas>,11,"<p>I have two disjoint time series objects, for example</p>

<p>-ts1</p>

<pre><code> Date           Price
 2010-01-01     1800.0
 2010-01-04     1500.0
 2010-01-08     1600.0
 2010-01-09     1400.0
 Name: Price, dtype: float64
</code></pre>

<p>-ts2</p>

<pre><code> Date           Price
 2010-01-02     2000.0
 2010-01-03     2200.0
 2010-01-05     2010.0
 2010-01-07     2100.0
 2010-01-10     2110.0
</code></pre>

<p>How I could merge the two into a single time series that should be sorted on date? like </p>

<p>-ts3</p>

<pre><code> Date           Price
 2010-01-01     1800.0
 2010-01-02     2000.0
 2010-01-03     2200.0
 2010-01-04     1500.0
 2010-01-05     2010.0
 2010-01-07     2100.0
 2010-01-08     1600.0
 2010-01-09     1400.0
 2010-01-10     2110.0
</code></pre>
",1953806,512,31-03-2018 06:26,31-03-2018 06:27,0,532,12,1,5,,"{'badge_counts': {'bronze': 12, 'silver': 5, 'gold': 1}, 'account_id': 2212308, 'is_employee': False, 'last_modified_date': 1607614796, 'last_access_date': 1699624406, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 532, 'creation_date': 1357522775, 'user_type': 'registered', 'user_id': 1953806, 'location': 'India', 'website_url': '', 'link': 'https://stackoverflow.com/users/1953806/prabhakar', 'profile_image': 'https://www.gravatar.com/avatar/09b0d1e2cd3096e1c1da900ce7951354?s=256&d=identicon&r=PG', 'display_name': 'prabhakar'}","I have two disjoint time series objects, for example -ts1 -ts2 How I could merge the two into a single time series that should be sorted on date? like -ts3"," Date           Price
 2010-01-01     1800.0
 2010-01-04     1500.0
 2010-01-08     1600.0
 2010-01-09     1400.0
 Name: Price, dtype: float64
  Date           Price
 2010-01-02     2000.0
 2010-01-03     2200.0
 2010-01-05     2010.0
 2010-01-07     2100.0
 2010-01-10     2110.0
  Date           Price
 2010-01-01     1800.0
 2010-01-02     2000.0
 2010-01-03     2200.0
 2010-01-04     1500.0
 2010-01-05     2010.0
 2010-01-07     2100.0
 2010-01-08     1600.0
 2010-01-09     1400.0
 2010-01-10     2110.0
",19,37,0,0,
396,48323493,48323568,11928,What is this odd colon behavior doing?,2,<python><python-3.x>,121,"<p>I am using Python 3.6.1, and I have come across something very strange. I had a simple dictionary assignment typo that took me a long time to find.</p>

<pre><code>context = {}
context[""a""]: 2
print(context)
</code></pre>

<p>Output</p>

<pre><code>{}
</code></pre>

<p>What is the code <code>context[""a""]: 2</code> doing? It doesn't raise a <code>SyntaxError</code> when it should IMO. At first I thought it was creating a slice. However, typing <code>repr(context[""a""]: 2)</code> raises a <code>SyntaxError</code>. I also typed <code>context[""a""]: 2</code> in the console and the console didn't print anything. I thought maybe it returned <code>None</code>, but I'm not so sure.</p>

<p>I've also thought it could be a single line if statement, but that shouldn't be the right syntax either.</p>

<p>Additionally, <code>context[""a""]</code> should raise a <code>KeyError</code>.</p>

<p>I am perplexed. What is going on?</p>
",1965288,6202,18-01-2018 14:23,18-01-2018 14:27,0,6222,43,4,27,83,"{'badge_counts': {'bronze': 43, 'silver': 27, 'gold': 4}, 'account_id': 2226730, 'is_employee': False, 'last_modified_date': 1637338800, 'last_access_date': 1711118287, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6222, 'creation_date': 1357784226, 'user_type': 'registered', 'user_id': 1965288, 'accept_rate': 83, 'location': 'Michigan, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1965288/justengel', 'profile_image': 'https://www.gravatar.com/avatar/ff23b27533d1ce3088552319a2ec311e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'justengel'}","I am using Python 3.6.1, and I have come across something very strange. I had a simple dictionary assignment typo that took me a long time to find. Output What is the code doing? It doesn't raise a when it should IMO. At first I thought it was creating a slice. However, typing raises a . I also typed in the console and the console didn't print anything. I thought maybe it returned , but I'm not so sure. I've also thought it could be a single line if statement, but that shouldn't be the right syntax either. Additionally, should raise a . I am perplexed. What is going on?","context = {}
context[""a""]: 2
print(context)
 {}
 context[""a""]: 2 SyntaxError repr(context[""a""]: 2) SyntaxError context[""a""]: 2 None context[""a""] KeyError",-6,19,0,0,
397,48207561,48235275,8024,How can I add type-annotations to dynamically created classes?,1,<python><mypy>,18,"<p>In one application I have code which generates dynamic classes which reduces the amount of duplicated code considerably. But adding type-hints for mypy checking resulted in an error. Consider the following example code (simplified to focus on the relevant bits):</p>

<pre><code>class Mapper:

    @staticmethod
    def action() -&gt; None:
        raise NotImplementedError('Not yet implemnented')


def magic(new_name: str) -&gt; type:

    cls = type('%sMapper' % new_name.capitalize(), (Mapper,), {})

    def action() -&gt; None:
        print('Hello')

    cls.action = staticmethod(action)
    return cls


MyCls = magic('My')
MyCls.action()
</code></pre>

<p>Checking this with mypy will result in the following error:</p>

<pre><code>dynamic_type.py:15: error: ""type"" has no attribute ""action""
dynamic_type.py:21: error: ""type"" has no attribute ""action""
</code></pre>

<p>mypy is obviously unable to tell that the return-value from the <code>type</code> call is a subclass of <code>Mapper</code>, so it complains that ""type"" has not attribute ""action"" when I assign to it.</p>

<p>Note that the code functions perfectly and does what it is supposed to but mypy still complains.</p>

<p>Is there a way to flag <code>cls</code> as being a type of <code>Mapper</code>? I tried to simply append <code># type: Mapper</code> to the line which creates the class:</p>

<pre><code>cls = type('%sMapper' % new_name.capitalize(), (Mapper,), {})  # type: Mapper
</code></pre>

<p>But then I get the following errors:</p>

<pre><code>dynamic_type.py:10: error: Incompatible types in assignment (expression has type ""type"", variable has type ""Mapper"")
dynamic_type.py:15: error: Cannot assign to a method
dynamic_type.py:15: error: Incompatible types in assignment (expression has type ""staticmethod"", variable has type ""Callable[[], None]"")
dynamic_type.py:16: error: Incompatible return value type (got ""Mapper"", expected ""type"")
dynamic_type.py:21: error: ""type"" has no attribute ""action""
</code></pre>
",160665,20739,11-01-2018 12:51,12-01-2018 23:12,1,20839,130,14,92,68,"{'badge_counts': {'bronze': 130, 'silver': 92, 'gold': 14}, 'account_id': 53720, 'is_employee': False, 'last_modified_date': 1704895820, 'last_access_date': 1710423756, 'reputation_change_year': 298, 'reputation_change_quarter': 298, 'reputation_change_month': 110, 'reputation_change_week': 60, 'reputation_change_day': 0, 'reputation': 20839, 'creation_date': 1250847063, 'user_type': 'registered', 'user_id': 160665, 'accept_rate': 68, 'location': 'Luxembourg City, Luxemburg, Luxembourg', 'website_url': 'http://michel.albert.lu', 'link': 'https://stackoverflow.com/users/160665/exhuma', 'profile_image': 'https://www.gravatar.com/avatar/70888c4c7063a4f8be70a01125e7cb74?s=256&d=identicon&r=PG', 'display_name': 'exhuma'}","In one application I have code which generates dynamic classes which reduces the amount of duplicated code considerably. But adding type-hints for mypy checking resulted in an error. Consider the following example code (simplified to focus on the relevant bits): Checking this with mypy will result in the following error: mypy is obviously unable to tell that the return-value from the call is a subclass of , so it complains that ""type"" has not attribute ""action"" when I assign to it. Note that the code functions perfectly and does what it is supposed to but mypy still complains. Is there a way to flag as being a type of ? I tried to simply append to the line which creates the class: But then I get the following errors:","class Mapper:

    @staticmethod
    def action() -&gt; None:
        raise NotImplementedError('Not yet implemnented')


def magic(new_name: str) -&gt; type:

    cls = type('%sMapper' % new_name.capitalize(), (Mapper,), {})

    def action() -&gt; None:
        print('Hello')

    cls.action = staticmethod(action)
    return cls


MyCls = magic('My')
MyCls.action()
 dynamic_type.py:15: error: ""type"" has no attribute ""action""
dynamic_type.py:21: error: ""type"" has no attribute ""action""
 type Mapper cls Mapper # type: Mapper cls = type('%sMapper' % new_name.capitalize(), (Mapper,), {})  # type: Mapper
 dynamic_type.py:10: error: Incompatible types in assignment (expression has type ""type"", variable has type ""Mapper"")
dynamic_type.py:15: error: Cannot assign to a method
dynamic_type.py:15: error: Incompatible types in assignment (expression has type ""staticmethod"", variable has type ""Callable[[], None]"")
dynamic_type.py:16: error: Incompatible return value type (got ""Mapper"", expected ""type"")
dynamic_type.py:21: error: ""type"" has no attribute ""action""
",19,47,0,0,
398,50305725,52679517,54350,CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://repo.continuum.io/pk gs/r/win-64/repodata.json.bz2>,10,<python><anaconda><conda>,19,"<p>I need to install Tensorflow and was trying to add up environment first. But I get HTTP Connection Failed error. I'm behind a corporate proxy and already defined them well in .condarc file. Here is the error I'm getting:</p>
<pre><code>C:\Users\Rahul\Downloads&gt;conda create -n tensorflow python=3.6 anaconda
Solving environment: failed
CondaHTTPError: HTTP 000 CONNECTION FAILED for url 
&lt;https://repo.continuum.io/pk
gs/r/win-64/repodata.json.bz2&gt;
Elapsed: -
An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your 
way.
ConnectionError(MaxRetryError(&quot;HTTPSConnectionPool(host='repo.continuum.io, por
t=443): Max retries exceeded with url: /pkgs/r/win-64/repodata.json.bz2 (Caused
by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x0
000001A00393C88&gt;: Failed to establish a new connection: [Errno 11002] getaddrinf
o failed',))&quot;,),)
</code></pre>
<p>I posted this issue in their issue tracker here in detail:
<a href=""https://github.com/conda/conda/issues/7283"" rel=""nofollow noreferrer"">https://github.com/conda/conda/issues/7283</a></p>
<p>As mentioned in issue tracker, I already tried resetting ssl, adding condarc file etc. But no luck so far. My proxy is working as expected and my entries in condarc file are correct. Also continuum repository is accessible via browser without issues.</p>
<p>None of the commands like <code>conda update</code> or <code>conda install</code> works and gives the same error stack while executing.</p>
<p>What might be going wrong here?</p>
",1973779,3249,12-05-2018 11:28,06-10-2018 13:24,147,3269,60,6,38,27,"{'badge_counts': {'bronze': 60, 'silver': 38, 'gold': 6}, 'account_id': 2237989, 'is_employee': False, 'last_modified_date': 1702548001, 'last_access_date': 1707817767, 'reputation_change_year': 42, 'reputation_change_quarter': 42, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 3269, 'creation_date': 1358066621, 'user_type': 'registered', 'user_id': 1973779, 'accept_rate': 27, 'location': 'Bengaluru, Karnataka, India', 'website_url': 'https://www.linkedin.com/in/rahulraj90/', 'link': 'https://stackoverflow.com/users/1973779/rahul-raj', 'profile_image': 'https://i.stack.imgur.com/2pBNf.jpg?s=256&g=1', 'display_name': 'Rahul Raj'}","I need to install Tensorflow and was trying to add up environment first. But I get HTTP Connection Failed error. I'm behind a corporate proxy and already defined them well in .condarc file. Here is the error I'm getting: I posted this issue in their issue tracker here in detail: https://github.com/conda/conda/issues/7283 As mentioned in issue tracker, I already tried resetting ssl, adding condarc file etc. But no luck so far. My proxy is working as expected and my entries in condarc file are correct. Also continuum repository is accessible via browser without issues. None of the commands like or works and gives the same error stack while executing. What might be going wrong here?","C:\Users\Rahul\Downloads&gt;conda create -n tensorflow python=3.6 anaconda
Solving environment: failed
CondaHTTPError: HTTP 000 CONNECTION FAILED for url 
&lt;https://repo.continuum.io/pk
gs/r/win-64/repodata.json.bz2&gt;
Elapsed: -
An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your 
way.
ConnectionError(MaxRetryError(&quot;HTTPSConnectionPool(host='repo.continuum.io, por
t=443): Max retries exceeded with url: /pkgs/r/win-64/repodata.json.bz2 (Caused
by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x0
000001A00393C88&gt;: Failed to establish a new connection: [Errno 11002] getaddrinf
o failed',))&quot;,),)
 conda update conda install",11,21,0,1,
399,49037902,49038999,14452,How to interpolate a line between two other lines in python,4,<python><algorithm><math><geometry><interpolation>,20,"<p>Note: I asked this question before but it was closed as a duplicate, however, I, along with several others believe it was unduely closed, I explain why in an edit in my original <a href=""https://stackoverflow.com/questions/47493154/interpolating-a-line-between-two-other-lines-in-python"">post</a>. So I would like to re-ask this question here again.</p>

<p>Does anyone know of a python library that can interpolate between two lines. For example, given the two solid lines below, I would like to produce the dashed line in the middle. In other words, I'd like to get the centreline. The input is a just two <code>numpy</code> arrays of coordinates with size <code>N x 2</code> and <code>M x 2</code> respectively.</p>

<p><a href=""https://i.stack.imgur.com/yGZH9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yGZH9.png"" alt=""enter image description here""></a></p>

<p>Furthermore, I'd like to know if someone has written a function for this in some optimized python library. Although optimization isn't exactly a necessary.</p>

<p>Here is an example of two lines that I might have, you can assume they do not overlap with each other and an x/y can have multiple y/x coordinates.</p>

<pre><code>array([[ 1233.87375018,  1230.07095987],
       [ 1237.63559365,  1253.90749041],
       [ 1240.87500801,  1264.43925132],
       [ 1245.30875975,  1274.63795396],
       [ 1256.1449357 ,  1294.48254424],
       [ 1264.33600095,  1304.47893299],
       [ 1273.38192911,  1313.71468591],
       [ 1283.12411536,  1322.35942538],
       [ 1293.2559388 ,  1330.55873344],
       [ 1309.4817002 ,  1342.53074698],
       [ 1325.7074616 ,  1354.50276051],
       [ 1341.93322301,  1366.47477405],
       [ 1358.15898441,  1378.44678759],
       [ 1394.38474581,  1390.41880113]])

array([[ 1152.27115094,  1281.52899302],
       [ 1155.53345506,  1295.30515742],
       [ 1163.56506781,  1318.41642169],
       [ 1168.03497425,  1330.03181319],
       [ 1173.26135672,  1341.30559949],
       [ 1184.07110925,  1356.54121651],
       [ 1194.88086178,  1371.77683353],
       [ 1202.58908737,  1381.41765447],
       [ 1210.72465255,  1390.65097106],
       [ 1227.81309742,  1403.2904646 ],
       [ 1244.90154229,  1415.92995815],
       [ 1261.98998716,  1428.56945169],
       [ 1275.89219696,  1438.21626352],
       [ 1289.79440676,  1447.86307535],
       [ 1303.69661656,  1457.50988719],
       [ 1323.80994319,  1470.41028655],
       [ 1343.92326983,  1488.31068591],
       [ 1354.31738934,  1499.33260989],
       [ 1374.48879779,  1516.93734053],
       [ 1394.66020624,  1534.54207116]])
</code></pre>

<p>Visualizing this we have:
<a href=""https://i.stack.imgur.com/kiHiG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kiHiG.png"" alt=""enter image description here""></a></p>

<p>So my attempt at this has been using the <code>skeletonize</code> function in the <code>skimage.morphology</code> library by first rasterizing the coordinates into a filled in polygon. However, I get branching at the ends like this:</p>

<p><a href=""https://i.stack.imgur.com/JgXlh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JgXlh.png"" alt=""enter image description here""></a></p>
",2158231,1718,28-02-2018 20:21,28-02-2018 21:36,0,1728,53,6,23,86,"{'badge_counts': {'bronze': 53, 'silver': 23, 'gold': 6}, 'account_id': 2478055, 'is_employee': False, 'last_modified_date': 1696644900, 'last_access_date': 1701559150, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1728, 'creation_date': 1363029800, 'user_type': 'registered', 'user_id': 2158231, 'accept_rate': 86, 'link': 'https://stackoverflow.com/users/2158231/jlcv', 'profile_image': 'https://www.gravatar.com/avatar/dcedee8ed60c0e8d8c6d0a80df2b217d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jlcv'}","Note: I asked this question before but it was closed as a duplicate, however, I, along with several others believe it was unduely closed, I explain why in an edit in my original post. So I would like to re-ask this question here again. Does anyone know of a python library that can interpolate between two lines. For example, given the two solid lines below, I would like to produce the dashed line in the middle. In other words, I'd like to get the centreline. The input is a just two arrays of coordinates with size and respectively. Furthermore, I'd like to know if someone has written a function for this in some optimized python library. Although optimization isn't exactly a necessary. Here is an example of two lines that I might have, you can assume they do not overlap with each other and an x/y can have multiple y/x coordinates. Visualizing this we have: So my attempt at this has been using the function in the library by first rasterizing the coordinates into a filled in polygon. However, I get branching at the ends like this:","numpy N x 2 M x 2 array([[ 1233.87375018,  1230.07095987],
       [ 1237.63559365,  1253.90749041],
       [ 1240.87500801,  1264.43925132],
       [ 1245.30875975,  1274.63795396],
       [ 1256.1449357 ,  1294.48254424],
       [ 1264.33600095,  1304.47893299],
       [ 1273.38192911,  1313.71468591],
       [ 1283.12411536,  1322.35942538],
       [ 1293.2559388 ,  1330.55873344],
       [ 1309.4817002 ,  1342.53074698],
       [ 1325.7074616 ,  1354.50276051],
       [ 1341.93322301,  1366.47477405],
       [ 1358.15898441,  1378.44678759],
       [ 1394.38474581,  1390.41880113]])

array([[ 1152.27115094,  1281.52899302],
       [ 1155.53345506,  1295.30515742],
       [ 1163.56506781,  1318.41642169],
       [ 1168.03497425,  1330.03181319],
       [ 1173.26135672,  1341.30559949],
       [ 1184.07110925,  1356.54121651],
       [ 1194.88086178,  1371.77683353],
       [ 1202.58908737,  1381.41765447],
       [ 1210.72465255,  1390.65097106],
       [ 1227.81309742,  1403.2904646 ],
       [ 1244.90154229,  1415.92995815],
       [ 1261.98998716,  1428.56945169],
       [ 1275.89219696,  1438.21626352],
       [ 1289.79440676,  1447.86307535],
       [ 1303.69661656,  1457.50988719],
       [ 1323.80994319,  1470.41028655],
       [ 1343.92326983,  1488.31068591],
       [ 1354.31738934,  1499.33260989],
       [ 1374.48879779,  1516.93734053],
       [ 1394.66020624,  1534.54207116]])
 skeletonize skimage.morphology",29,53,3,4,
400,50299143,50299165,1290,"Difference between ""from x.y import z"" and ""import x.y.z as z""",2,<python><python-import>,14,"<p>In situations where you want to import a nested module into your namespace, I've always written it like this:</p>

<pre class=""lang-py prettyprint-override""><code>from concurrent import futures
</code></pre>

<p>However, I recently realized that this can be expressed using the ""as"" syntax as well. See the following:</p>

<pre class=""lang-py prettyprint-override""><code>import concurrent.futures as futures
</code></pre>

<p>Which has the subjective advantage of looking more similar to other imports:</p>

<pre class=""lang-py prettyprint-override""><code>import sys
import os
import concurrent.futures as futures
</code></pre>

<p>... with the disadvantage of added verbosity.</p>

<p>Is there a functional difference between the two, or is one officially preferred in a PEP or otherwise?</p>
",2159348,527,11-05-2018 19:37,11-05-2018 19:40,0,527,20,1,6,50,"{'badge_counts': {'bronze': 20, 'silver': 6, 'gold': 1}, 'account_id': 2479601, 'is_employee': False, 'last_modified_date': 1639763754, 'last_access_date': 1684895696, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 527, 'creation_date': 1363062096, 'user_type': 'registered', 'user_id': 2159348, 'accept_rate': 50, 'website_url': 'https://velovix.com', 'link': 'https://stackoverflow.com/users/2159348/velovix', 'profile_image': 'https://www.gravatar.com/avatar/8703a88e1c178112625bcb6970ed40e4?s=256&d=identicon&r=PG', 'display_name': 'Velovix'}","In situations where you want to import a nested module into your namespace, I've always written it like this: However, I recently realized that this can be expressed using the ""as"" syntax as well. See the following: Which has the subjective advantage of looking more similar to other imports: ... with the disadvantage of added verbosity. Is there a functional difference between the two, or is one officially preferred in a PEP or otherwise?","from concurrent import futures
 import concurrent.futures as futures
 import sys
import os
import concurrent.futures as futures
",2,20,0,0,
401,48646745,48705933,11603,Celery multi inside docker container,3,<python><docker><docker-compose><celery>,15,"<p>I have python app with celery in docker containers. I want have few workers with different queue.
For example:</p>

<pre><code>celery worker -c 3 -Q queue1
celery worker -c 7 -Q queue2,queue3
</code></pre>

<p>But I don't do this in docker compose. I found out about celery multi. I tried use it.</p>

<pre><code>version: '3.2'
services:
  app:
    image: ""app""
    build:
      context: .
    networks:
      - net
    ports:
      - 5004:5000
    stdin_open: true
    tty: true
    environment:
      FLASK_APP: app/app.py
      FLASK_DEBUG: 1
    volumes:
      - .:/home/app
  app__celery:
    image: ""app""
    build:
      context: .
    command: sh -c 'celery multi start 2 -l INFO -c:1 3 -c:2 7 -Q:1 queue1 -Q:2 queue2,queue3'
</code></pre>

<p>But I get it... </p>

<pre><code>app__celery_1  |    &gt; celery1@1ab37081acb9: OK
app__celery_1  |    &gt; celery2@1ab37081acb9: OK
app__celery_1 exited with code 0
</code></pre>

<p>And my container with celery closes. How not to let him close and get his logs from him?</p>

<p>UPD: Celery multi created background processes. How to start celery multi in foreground?</p>
",2223918,545,06-02-2018 15:42,09-02-2018 12:32,3,545,19,1,4,,"{'badge_counts': {'bronze': 19, 'silver': 4, 'gold': 1}, 'account_id': 2563225, 'is_employee': False, 'last_modified_date': 1674151502, 'last_access_date': 1706347130, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 545, 'creation_date': 1364551296, 'user_type': 'registered', 'user_id': 2223918, 'location': 'Rostov-on-Don, Rostov Oblast, Russia', 'website_url': '', 'link': 'https://stackoverflow.com/users/2223918/dluhhbiu', 'profile_image': 'https://i.stack.imgur.com/XvOxF.jpg?s=256&g=1', 'display_name': 'dluhhbiu'}",I have python app with celery in docker containers. I want have few workers with different queue. For example: But I don't do this in docker compose. I found out about celery multi. I tried use it. But I get it... And my container with celery closes. How not to let him close and get his logs from him? UPD: Celery multi created background processes. How to start celery multi in foreground?,"celery worker -c 3 -Q queue1
celery worker -c 7 -Q queue2,queue3
 version: '3.2'
services:
  app:
    image: ""app""
    build:
      context: .
    networks:
      - net
    ports:
      - 5004:5000
    stdin_open: true
    tty: true
    environment:
      FLASK_APP: app/app.py
      FLASK_DEBUG: 1
    volumes:
      - .:/home/app
  app__celery:
    image: ""app""
    build:
      context: .
    command: sh -c 'celery multi start 2 -l INFO -c:1 3 -c:2 7 -Q:1 queue1 -Q:2 queue2,queue3'
 app__celery_1  |    &gt; celery1@1ab37081acb9: OK
app__celery_1  |    &gt; celery2@1ab37081acb9: OK
app__celery_1 exited with code 0
",24,43,0,0,
402,48744165,48744470,19125,Uneven subplot in python,3,<python><matplotlib><subplot>,13,"<p>What is the best way to create a (3,3) subplot matrix in python with the following catch:</p>

<ul>
<li>first column contains 3 subplots</li>
<li>second column contains 3 subplots</li>
<li>third column contains <strong>2 subplots</strong> </li>
</ul>

<p>The last two subplots should have equal size. This means they will meet in the middle of the middle plot for the other two columns. </p>

<p>I tried to do this with gridspec but did not managed so far. </p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from matplotlib import gridspec

# generate some data
x = np.arange(0, 10, 0.2)
y = np.sin(x)

# plot it
fig = plt.figure(figsize=(8, 6))

gs = gridspec.GridSpec(3, 3)

ax0 = plt.subplot(gs[0])
ax0.plot(x, y)
ax1 = plt.subplot(gs[1])
ax1.plot(y, x)

ax3 = plt.subplot(gs[3])
ax3.plot(y, x)
ax4 = plt.subplot(gs[4])
ax4.plot(y, x)

ax6 = plt.subplot(gs[6])
ax6.plot(y, x)
ax7 = plt.subplot(gs[7])
ax7.plot(y, x)

plt.tight_layout()
plt.savefig('grid_figure.png')

plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/aN20B.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/aN20B.png"" alt=""enter image description here""></a></p>
",2359301,1379,12-02-2018 10:25,12-02-2018 10:40,0,1379,18,2,11,77,"{'badge_counts': {'bronze': 18, 'silver': 11, 'gold': 2}, 'account_id': 2487649, 'is_employee': False, 'last_modified_date': 1607614748, 'last_access_date': 1670024564, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1379, 'creation_date': 1367946557, 'user_type': 'registered', 'user_id': 2359301, 'accept_rate': 77, 'website_url': '', 'link': 'https://stackoverflow.com/users/2359301/sponce', 'profile_image': 'https://i.stack.imgur.com/dRQ4q.jpg?s=256&g=1', 'display_name': 'sponce'}","What is the best way to create a (3,3) subplot matrix in python with the following catch: first column contains 3 subplots second column contains 3 subplots third column contains 2 subplots The last two subplots should have equal size. This means they will meet in the middle of the middle plot for the other two columns. I tried to do this with gridspec but did not managed so far.","import numpy as np
import matplotlib.pyplot as plt
from matplotlib import gridspec

# generate some data
x = np.arange(0, 10, 0.2)
y = np.sin(x)

# plot it
fig = plt.figure(figsize=(8, 6))

gs = gridspec.GridSpec(3, 3)

ax0 = plt.subplot(gs[0])
ax0.plot(x, y)
ax1 = plt.subplot(gs[1])
ax1.plot(y, x)

ax3 = plt.subplot(gs[3])
ax3.plot(y, x)
ax4 = plt.subplot(gs[4])
ax4.plot(y, x)

ax6 = plt.subplot(gs[6])
ax6.plot(y, x)
ax7 = plt.subplot(gs[7])
ax7.plot(y, x)

plt.tight_layout()
plt.savefig('grid_figure.png')

plt.show()
",31,47,1,1,
403,50156550,50156656,29468,"Test if dictionary key exists, is not None and isn't blank",8,<python><python-3.x><dictionary>,18,"<p>I have code that works but I'm wondering if there is a more pythonic way to do this.  I have a dictionary and I want to see if:</p>

<ul>
<li>a key exists</li>
<li>that value isn't None (NULL from SQL in this case)</li>
<li>that value isn't simply quote quote (blank?)</li>
<li>that value doesn't solely consist of spaces</li>
</ul>

<p>So in my code the keys of ""a"", ""b"", and ""c"" would succeed, which is correct.</p>

<pre><code>import re

mydict = {
""a"":""alpha"",
""b"":0,
""c"":False,
""d"":None,
""e"":"""",
""g"":""   "",
}

#a,b,c should succeed
for k in mydict.keys():
    if k in mydict and mydict[k] is not None and not re.search(""^\s*$"", str(mydict[k])):
        print(k)
    else:
        print(""I am incomplete and sad"")
</code></pre>

<p>What I have above works, but that seems like an awfully long set of conditions.  Maybe this simply is the right solution but I'm wondering if there is a more pythonic ""exists and has stuff"" or better way to do this?</p>

<p><strong>UPDATE</strong>
Thank you all for wonderful answers and thoughtful comments.  With some of the points and tips, I've updated the question a little bit as there some conditions I didn't have which should also succeed.  I have also changed the example to a loop (just easier to test right?).</p>
",1143387,5154,03-05-2018 13:42,03-05-2018 13:47,0,5154,45,6,30,95,"{'badge_counts': {'bronze': 45, 'silver': 30, 'gold': 6}, 'account_id': 1164471, 'is_employee': False, 'last_modified_date': 1678211104, 'last_access_date': 1711033721, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5154, 'creation_date': 1326292531, 'user_type': 'registered', 'user_id': 1143387, 'accept_rate': 95, 'location': 'New England', 'website_url': '', 'link': 'https://stackoverflow.com/users/1143387/sniperd', 'profile_image': 'https://www.gravatar.com/avatar/0b9a9c460593d77b3561075ef1889150?s=256&d=identicon&r=PG', 'display_name': 'sniperd'}","I have code that works but I'm wondering if there is a more pythonic way to do this. I have a dictionary and I want to see if: a key exists that value isn't None (NULL from SQL in this case) that value isn't simply quote quote (blank?) that value doesn't solely consist of spaces So in my code the keys of ""a"", ""b"", and ""c"" would succeed, which is correct. What I have above works, but that seems like an awfully long set of conditions. Maybe this simply is the right solution but I'm wondering if there is a more pythonic ""exists and has stuff"" or better way to do this? UPDATE Thank you all for wonderful answers and thoughtful comments. With some of the points and tips, I've updated the question a little bit as there some conditions I didn't have which should also succeed. I have also changed the example to a loop (just easier to test right?).","import re

mydict = {
""a"":""alpha"",
""b"":0,
""c"":False,
""d"":None,
""e"":"""",
""g"":""   "",
}

#a,b,c should succeed
for k in mydict.keys():
    if k in mydict and mydict[k] is not None and not re.search(""^\s*$"", str(mydict[k])):
        print(k)
    else:
        print(""I am incomplete and sad"")
",16,34,0,0,
404,48645846,50633571,17911,"Python's Xgoost: ValueError('feature_names may not contain [, ] or <')",5,<python><pandas><numpy><scikit-learn><xgboost>,17,"<p>Python's implementation of XGBClassifier <a href=""https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/core.py#L667"" rel=""nofollow noreferrer"">does not accept</a> the characters <code>[, ] or &lt;'</code> as features names.</p>
<p>If that occurs, it raises the following:</p>
<blockquote>
<p>ValueError('feature_names may not contain [, ] or &lt;')</p>
</blockquote>
<p>It would seem that the obvious solution would be to pass the equivalent numpy arrays, and get rid of the column names altogether, but if they haven't done it that must be for a reason.</p>
<p>What use does XGBoost have for the feature names, and what is the downside of simply passing it Numpy Arrays instead of Pandas DataFrames?</p>
<p><em>Edit: this is not a question about workarounds (those are obvious and in the question), but about why it is implemented this way</em></p>
",2468113,6384,06-02-2018 14:56,31-05-2018 22:17,114,6384,59,12,46,52,"{'badge_counts': {'bronze': 59, 'silver': 46, 'gold': 12}, 'account_id': 2876074, 'is_employee': False, 'last_modified_date': 1644843000, 'last_access_date': 1696087148, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6384, 'creation_date': 1370775599, 'user_type': 'registered', 'user_id': 2468113, 'accept_rate': 52, 'location': 'Lisbon, Portugal', 'website_url': '', 'link': 'https://stackoverflow.com/users/2468113/sapo-cosmico', 'profile_image': 'https://www.gravatar.com/avatar/1552a75f9fd4f6850d3bedc930597b68?s=256&d=identicon&r=PG', 'display_name': 'sapo_cosmico'}","Python's implementation of XGBClassifier does not accept the characters as features names. If that occurs, it raises the following: ValueError('feature_names may not contain [, ] or &lt;') It would seem that the obvious solution would be to pass the equivalent numpy arrays, and get rid of the column names altogether, but if they haven't done it that must be for a reason. What use does XGBoost have for the feature names, and what is the downside of simply passing it Numpy Arrays instead of Pandas DataFrames? Edit: this is not a question about workarounds (those are obvious and in the question), but about why it is implemented this way","[, ] or &lt;'",-1,8,0,1,
405,49768770,49769015,174803,Not able to install Python packages [SSL: TLSV1_ALERT_PROTOCOL_VERSION],17,<python><python-2.7><pip><ssl-certificate>,177,"<p>I am trying to install a Python library using <code>pip</code>, getting an SSL error:</p>

<pre><code>~/projects/base  pre-master±  pip install xdict

Collecting xdict
  Could not fetch URL https://pypi.python.org/simple/xdict/: There was a problem confirming the ssl certificate: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:590) - skipping
  Could not find a version that satisfies the requirement xdict (from versions: )
No matching distribution found for xdict
</code></pre>

<p>pip version: pip 9.0.1</p>

<p>How do I fix this error?</p>
",1246764,8312,11-04-2018 07:15,11-04-2018 07:29,0,8312,81,13,60,80,"{'badge_counts': {'bronze': 81, 'silver': 60, 'gold': 13}, 'collectives': [{'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 1295617, 'is_employee': False, 'last_modified_date': 1653477301, 'last_access_date': 1711000795, 'reputation_change_year': 18, 'reputation_change_quarter': 18, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 8312, 'creation_date': 1330774402, 'user_type': 'registered', 'user_id': 1246764, 'accept_rate': 80, 'location': 'Aurangabad, Maharashtra, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/1246764/nishant-nawarkhede', 'profile_image': 'https://i.stack.imgur.com/FWtTl.png?s=256&g=1', 'display_name': 'Nishant Nawarkhede'}","I am trying to install a Python library using , getting an SSL error: pip version: pip 9.0.1 How do I fix this error?","pip ~/projects/base  pre-master±  pip install xdict

Collecting xdict
  Could not fetch URL https://pypi.python.org/simple/xdict/: There was a problem confirming the ssl certificate: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:590) - skipping
  Could not find a version that satisfies the requirement xdict (from versions: )
No matching distribution found for xdict
",4,13,0,0,
406,49693148,49693247,16176,Running celery worker + beat in the same container,4,<python><docker><flask><celery><celerybeat>,22,"<p>My flask app is comprised of four containers: web app, postgres, rabbitMQ and Celery.  Since I have celery tasks that run periodically, I am using celery beat.  I've configured my docker-compose file like this:</p>

<pre><code>version: '2'
services:
  rabbit:
    # ...      
  web:
    # ...
  rabbit:
    # ...
  celery:
    build:
        context: .
        dockerfile: Dockerfile.celery
</code></pre>

<p>And my Dockerfile.celery looks like this:</p>

<pre><code># ...code up here...
CMD [""celery"", ""-A"", ""app.tasks.celery"", ""worker"", ""-B"", ""-l"", ""INFO""]
</code></pre>

<p>While I read in the docs that I shouldn't go to production with the <code>-B</code> option, I hastily added it anyway (and forgot about changing it) and quickly learned that my scheduled tasks were running multiple times.  For those interested, if you do a <code>ps aux | grep celery</code> from within your celery container, you'll see multiple celery + beat processes running (but there should only be one beat process and however many worker processes).  I wasn't sure from the docs why you shouldn't run <code>-B</code> in production but now I know.</p>

<p>So then I changed my Dockerfile.celery to:</p>

<pre><code># ...code up here...
CMD [""celery"", ""-A"", ""app.tasks.celery"", ""worker"", ""-l"", ""INFO""]
CMD [""celery"", ""-A"", ""app.tasks.celery"", ""beat"", ""-l"", ""INFO""]
</code></pre>

<p>No when I start my app, the worker processes start but beat does not.  When I flip those commands around so that beat is called first, then beat starts but the worker processes do not.  So my question is: how do I run celery worker + beat together in my container?  I have combed through many articles/docs but I'm still unable to figure this out.</p>

<p><strong>EDITED</strong></p>

<p>I changed my Dockerfile.celery to the following:</p>

<pre><code>ENTRYPOINT [ ""/bin/sh"" ]
CMD [ ""./docker.celery.sh"" ]    
</code></pre>

<p>And my docker.celery.sh file looks like this:</p>

<pre><code>#!/bin/sh -ex
celery -A app.tasks.celery beat -l debug &amp;
celery -A app.tasks.celery worker -l info &amp;
</code></pre>

<p>However, I'm receiving the error <code>celery_1 exited with code 0</code></p>

<p>Edit #2</p>

<p>I added the following blocking command to the end of my docker.celery.sh file and all was fixed:</p>

<pre><code>tail -f /dev/null
</code></pre>
",1264589,1205,06-04-2018 12:45,06-04-2018 12:51,0,1205,28,1,13,60,"{'badge_counts': {'bronze': 28, 'silver': 13, 'gold': 1}, 'account_id': 1318164, 'is_employee': False, 'last_modified_date': 1703297100, 'last_access_date': 1709920641, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1205, 'creation_date': 1331569378, 'user_type': 'registered', 'user_id': 1264589, 'accept_rate': 60, 'link': 'https://stackoverflow.com/users/1264589/hugo', 'profile_image': 'https://www.gravatar.com/avatar/609d7fec37c8c26b7eeccf44f3806d65?s=256&d=identicon&r=PG', 'display_name': 'hugo'}","My flask app is comprised of four containers: web app, postgres, rabbitMQ and Celery. Since I have celery tasks that run periodically, I am using celery beat. I've configured my docker-compose file like this: And my Dockerfile.celery looks like this: While I read in the docs that I shouldn't go to production with the option, I hastily added it anyway (and forgot about changing it) and quickly learned that my scheduled tasks were running multiple times. For those interested, if you do a from within your celery container, you'll see multiple celery + beat processes running (but there should only be one beat process and however many worker processes). I wasn't sure from the docs why you shouldn't run in production but now I know. So then I changed my Dockerfile.celery to: No when I start my app, the worker processes start but beat does not. When I flip those commands around so that beat is called first, then beat starts but the worker processes do not. So my question is: how do I run celery worker + beat together in my container? I have combed through many articles/docs but I'm still unable to figure this out. EDITED I changed my Dockerfile.celery to the following: And my docker.celery.sh file looks like this: However, I'm receiving the error Edit #2 I added the following blocking command to the end of my docker.celery.sh file and all was fixed:","version: '2'
services:
  rabbit:
    # ...      
  web:
    # ...
  rabbit:
    # ...
  celery:
    build:
        context: .
        dockerfile: Dockerfile.celery
 # ...code up here...
CMD [""celery"", ""-A"", ""app.tasks.celery"", ""worker"", ""-B"", ""-l"", ""INFO""]
 -B ps aux | grep celery -B # ...code up here...
CMD [""celery"", ""-A"", ""app.tasks.celery"", ""worker"", ""-l"", ""INFO""]
CMD [""celery"", ""-A"", ""app.tasks.celery"", ""beat"", ""-l"", ""INFO""]
 ENTRYPOINT [ ""/bin/sh"" ]
CMD [ ""./docker.celery.sh"" ]    
 #!/bin/sh -ex
celery -A app.tasks.celery beat -l debug &amp;
celery -A app.tasks.celery worker -l info &amp;
 celery_1 exited with code 0 tail -f /dev/null
",13,56,0,0,
407,49887430,49888338,3756,Can I make type aliases for type constructors in python using the typing module?,1,<python>,18,"<p>Since python version 3.5, you can use type hints to indicate what type of arguments a function is expecting. I find these type hints extremely valuable for documentation purposes, so I try to use them as much as I can. They also help the linter and therefore regularly save me from bugs introduced by code changes.</p>

<p>For example, in my code I have a few functions which take a zero-argument function as an argument. E.g.:</p>

<pre><code>def onReady(f: Callable[[], Any]) -&gt; None:
    ...
</code></pre>

<p>Or</p>

<pre><code>def checkIfReady(f: Callable[[], Bool]) -&gt; None:
    ...
</code></pre>

<p>What I would like to do is make a type alias like so (the code below is not valid python):</p>

<pre><code>Action[A] = Callable[[], A]
</code></pre>

<p>And then I could shorten my types for the arguments above:</p>

<pre><code>def onReady(f: Action[Any]) -&gt; None:
    ...
</code></pre>

<p>I know I can make a type alias for a specific instance, e.g.:</p>

<pre><code>ActionBool = Callable[[], bool]
</code></pre>

<p>And I know of the existance of <code>NewType</code> in the <code>typing</code> module, but neither of these seem to generalize to higher order types.</p>
",2496293,2231,17-04-2018 21:01,17-04-2018 22:21,0,2251,34,1,25,67,"{'badge_counts': {'bronze': 34, 'silver': 25, 'gold': 1}, 'account_id': 2912360, 'is_employee': False, 'last_modified_date': 1709313000, 'last_access_date': 1711112289, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2251, 'creation_date': 1371543396, 'user_type': 'registered', 'user_id': 2496293, 'accept_rate': 67, 'website_url': '', 'link': 'https://stackoverflow.com/users/2496293/sam-de-meyer', 'profile_image': 'https://www.gravatar.com/avatar/00f8e804a4fc2e35e44f88aae73100d9?s=256&d=identicon&r=PG', 'display_name': 'Sam De Meyer'}","Since python version 3.5, you can use type hints to indicate what type of arguments a function is expecting. I find these type hints extremely valuable for documentation purposes, so I try to use them as much as I can. They also help the linter and therefore regularly save me from bugs introduced by code changes. For example, in my code I have a few functions which take a zero-argument function as an argument. E.g.: Or What I would like to do is make a type alias like so (the code below is not valid python): And then I could shorten my types for the arguments above: I know I can make a type alias for a specific instance, e.g.: And I know of the existance of in the module, but neither of these seem to generalize to higher order types.","def onReady(f: Callable[[], Any]) -&gt; None:
    ...
 def checkIfReady(f: Callable[[], Bool]) -&gt; None:
    ...
 Action[A] = Callable[[], A]
 def onReady(f: Action[Any]) -&gt; None:
    ...
 ActionBool = Callable[[], bool]
 NewType typing",1,31,0,0,
408,49280016,49280127,15871,How to make a generator callable?,4,<python>,16,"<p>I'm trying to create a dataset from a CSV file with 784-bit long rows. Here's my code:</p>

<pre><code>import tensorflow as tf

f = open(""test.csv"", ""r"")
csvreader = csv.reader(f)
gen = (row for row in csvreader)
ds = tf.data.Dataset()
ds.from_generator(gen, [tf.uint8]*28**2)
</code></pre>

<p>I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-22-4b244ea66c1d&gt; in &lt;module&gt;()
     12 gen = (row for row in csvreader_pat_trn)
     13 ds = tf.data.Dataset()
---&gt; 14 ds.from_generator(gen, [tf.uint8]*28**2)

~/Documents/Programming/ANN/labs/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in from_generator(generator, output_types, output_shapes)
    317     """"""
    318     if not callable(generator):
--&gt; 319       raise TypeError(""`generator` must be callable."")
    320     if output_shapes is None:
    321       output_shapes = nest.map_structure(

TypeError: `generator` must be callable.
</code></pre>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""noreferrer"">docs</a> said that I should have a generator passed to <code>from_generator()</code>, so that's what I did, <code>gen</code> is a generator. But now it's complaining that my generator isn't <strong>callable</strong>. How can I make the generator callable so I can get this to work?</p>

<p><strong>EDIT:</strong>
I'd like to add that I'm using python 3.6.4. Is this the reason for the error? </p>
",3128156,8208,14-03-2018 14:15,14-03-2018 14:20,0,8228,145,25,74,71,"{'badge_counts': {'bronze': 145, 'silver': 74, 'gold': 25}, 'account_id': 3764201, 'is_employee': False, 'last_modified_date': 1711158000, 'last_access_date': 1706098027, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 8228, 'creation_date': 1387758204, 'user_type': 'registered', 'user_id': 3128156, 'accept_rate': 71, 'location': 'Munich, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/3128156/sahand', 'profile_image': 'https://i.stack.imgur.com/h3FIS.jpg?s=256&g=1', 'display_name': 'Sahand'}","I'm trying to create a dataset from a CSV file with 784-bit long rows. Here's my code: I get the following error: The docs said that I should have a generator passed to , so that's what I did, is a generator. But now it's complaining that my generator isn't callable. How can I make the generator callable so I can get this to work? EDIT: I'd like to add that I'm using python 3.6.4. Is this the reason for the error?","import tensorflow as tf

f = open(""test.csv"", ""r"")
csvreader = csv.reader(f)
gen = (row for row in csvreader)
ds = tf.data.Dataset()
ds.from_generator(gen, [tf.uint8]*28**2)
 ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-22-4b244ea66c1d&gt; in &lt;module&gt;()
     12 gen = (row for row in csvreader_pat_trn)
     13 ds = tf.data.Dataset()
---&gt; 14 ds.from_generator(gen, [tf.uint8]*28**2)

~/Documents/Programming/ANN/labs/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py in from_generator(generator, output_types, output_shapes)
    317     """"""
    318     if not callable(generator):
--&gt; 319       raise TypeError(""`generator` must be callable."")
    320     if output_shapes is None:
    321       output_shapes = nest.map_structure(

TypeError: `generator` must be callable.
 from_generator() gen",18,34,0,1,
409,50274289,50274371,3080,adding static() to urlpatterns only work by appending to the list,1,<python><django><python-3.x><django-urls>,11,"<p>I'm pretty sure there is a duplicate lying around, but couldn't find it.</p>

<p>When declaring a <code>urlpatterns</code> in <strong>urls.py</strong> on dev, I use the following successfully:</p>

<pre><code>urlpatterns = [
    # some routes
]

urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)
</code></pre>

<p>Which understandably, works.</p>

<p>But if I try the following:</p>

<pre><code>urlpatterns = [
    # some routes,
    static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)
]
</code></pre>

<p>django server dies complaining:</p>

<blockquote>
  <p><code>?: (urls.E004) Your URL pattern [&lt;URLPattern '^static\/(?P&lt;path&gt;.*)$'&gt;] is invalid. Ensure that urlpatterns is a list of path() and/or re_path() instances.</code></p>
</blockquote>

<p>Why aren't the two definitions equivalent? The return of <code>static()</code> should be the same:</p>

<pre><code>return [
    re_path(r'^%s(?P&lt;path&gt;.*)$' % re.escape(prefix.lstrip('/')), view, kwargs=kwargs),
]
</code></pre>

<p>And thus valid, but only works if I concatenate the element to the list instead of defining it in the list directly.</p>

<p>Why one method works but not the other?</p>
",1426539,44400,10-05-2018 13:39,10-05-2018 13:43,0,44509,146,18,124,92,"{'badge_counts': {'bronze': 146, 'silver': 124, 'gold': 18}, 'account_id': 1528775, 'is_employee': False, 'last_modified_date': 1710088200, 'last_access_date': 1711141136, 'reputation_change_year': 924, 'reputation_change_quarter': 924, 'reputation_change_month': 244, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 44509, 'creation_date': 1338395268, 'user_type': 'registered', 'user_id': 1426539, 'accept_rate': 92, 'location': 'Madrid, Spain', 'website_url': 'https://weblog.yivoff.com', 'link': 'https://stackoverflow.com/users/1426539/yivi', 'profile_image': 'https://i.stack.imgur.com/rHdZU.png?s=256&g=1', 'display_name': 'yivi'}","I'm pretty sure there is a duplicate lying around, but couldn't find it. When declaring a in urls.py on dev, I use the following successfully: Which understandably, works. But if I try the following: django server dies complaining: Why aren't the two definitions equivalent? The return of should be the same: And thus valid, but only works if I concatenate the element to the list instead of defining it in the list directly. Why one method works but not the other?","urlpatterns urlpatterns = [
    # some routes
]

urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)
 urlpatterns = [
    # some routes,
    static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)
]
 ?: (urls.E004) Your URL pattern [&lt;URLPattern '^static\/(?P&lt;path&gt;.*)$'&gt;] is invalid. Ensure that urlpatterns is a list of path() and/or re_path() instances. static() return [
    re_path(r'^%s(?P&lt;path&gt;.*)$' % re.escape(prefix.lstrip('/')), view, kwargs=kwargs),
]
",6,37,0,0,
410,48791561,48791745,7828,Remove labels but keep legend in a pie chart,4,<python><matplotlib>,12,"<p>How can I remove the labels from a <a href=""https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pie.html"" rel=""noreferrer"">pie chart</a> but keep the legend ?</p>

<pre><code>import matplotlib.pyplot as plt

x = [15, 30, 55]
labels = [1, 2, 3]

plt.figure(figsize=(3, 3), dpi=72)
plt.pie(x, labels=labels)
plt.legend()
plt.savefig('pie_1.png')
</code></pre>

<p><a href=""https://i.stack.imgur.com/0YpnQ.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0YpnQ.jpg"" alt=""enter image description here""></a></p>
",1545917,13632,14-02-2018 16:10,14-02-2018 16:20,0,13692,103,10,59,55,"{'badge_counts': {'bronze': 103, 'silver': 59, 'gold': 10}, 'collectives': [{'collective': {'tags': ['continuous-integration', 'gitlab-ci-runner', 'github-actions', 'jenkins-groovy', 'continuous-delivery', 'jenkins', 'google-cloud-build', 'octopus-deploy', 'jenkins-plugins', 'argocd', 'teamcity', 'circleci', 'continuous-deployment', 'bitbucket-pipelines', 'tfsbuild', 'codemagic', 'hudson', 'cicd', 'azure-pipelines', 'continuous-testing', 'jenkins-pipeline', 'gitlab-ci'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where developers focused on continuous integration, delivery, and deployment can find, share, and learn about simultaneous development.', 'link': '/collectives/ci-cd', 'name': 'CI/CD', 'slug': 'ci-cd'}, 'role': 'member'}, {'collective': {'tags': ['azure-synapse-link', 'azure-management-groups', 'azure-deployment-slots', 'azure-data-lake-gen2', 'azure-ad-v2', 'azure-databoxfamily', 'azure-purview', 'azure-spring-boot', 'azure-table-storage', 'azure-blob-trigger', 'azure-application-registration', 'azure-calculator', 'azure-hub', 'azure-ad-b2c-custom-policy', 'azure-batch', 'azure-form-recognizer', 'azure-app-service-plans', 'azureshell', 'azure-notificationhub', 'azure-sql-database', 'azure-workflow-automation', 'azure-storage', 'azure-static-web-app', 'azure-nsg', 'azure-databricks', 'azure-app-api', 'azure-alerts', 'azure-virtual-machine', 'azure-cli', 'azure-git-deployment', 'azure-waf', 'azure-connect', 'azure-cosmosdb-cassandra-api', 'azure-database-mysql', 'azure-iot-suite', 'azure-ad-role', 'azure-dns', 'azure-emulator', 'azure-sas', 'azure-search-.net-sdk', 'azure-availability-zones', 'azure-bot-service', 'kql', 'azure-authentication', 'sql-azure-alerts', 'azure-service-fabric', 'azure-communication-services', 'azure-metrics-advisor', 'azure-iot-central', 'azure-cloud-services', 'azure-blob-storage', 'azure-regions', 'azure-store', 'azure-feature-manager', 'azure-cdn', 'azure-ad-graph-api', 'azure-industrial-iot', 'azure-ad-b2c', 'azure-synapse', 'azure-app-registration', 'azure-remote-rendering', 'azure-rtos', 'azure-storage-account', 'azure-iot-sdk', 'azure-ai-translator', 'azure-container-service', 'azure-text-translation', 'azure-fluent-api', 'azure-rest-api', 'azure-sphere', 'azure-gov', 'azure-hybrid-connections', 'azure-quantum', 'azure-digital-twins', 'azurekinect', 'azure-postgresql', 'azure-integration-account', 'microsoft-entra-private-access', 'azure-sql-server', 'azure-servicebus-topics', 'sql-server-azure', 'azure-log-analytics-workspace', 'azure-rbac', 'azure-policy', 'azure-mcd', 'azure-sdk-go', 'spark-bash-azure-databricks', 'azure-app-configuration', 'azure-iot-hub', 'azure-role-environment', 'azure-powershell', 'azure-identity', 'microsoft-entra-external-id', 'azure-deployment', 'azure-sdk-ruby', 'azure-free-services', 'azure-webjobs-triggered', 'azureadgraph-deprecation', 'azure-data-lake', 'azure-data-factory', 'azure-service-hooks', 'azure-image-builder', 'azure-backup-vault', 'azure-data-share', 'azure-data-catalog', 'azure-storage-files', 'azure-appservice', 'azure-files', 'azure-ad-domain-services', 'adal.js', 'azure-configuration', 'azure-qna-maker', 'spring-cloud-azure', 'azurerm-app-service', 'azure-sdk', 'azure-secrets', 'azure-affinity-group', 'kitchen-azurerm', 'azure-web-roles', 'azure-cli2', 'azure-log-analytics', 'azure-hdinsight', 'azure-application-gateway', 'azure-python-sdk', 'azure-sql-reporting', 'azure-disk', 'azure-devtest-labs', 'passport-azure-ad', 'azure-elasticpool', 'azure-function-app', 'azure-signalr', 'azure-webjobs-continuous', 'azure-compute-emulator', 'azure-speech', 'microsoft-entra-internet-access', 'azure-functions-runtime', 'azure-web-app-service', 'azure-front-door', 'azure-function-queue', 'azure-resource-group', 'azure-pack', 'azure-caching', 'azure-cosmosdb-mongoapi', 'azure-webjobs', 'azure-billing-api', 'azure-appfabric', 'azure-cloud-shell', 'azure-sdk-for-ruby', 'azure-virtual-network', 'azure-servicebus-subscriptions', 'azure-arc', 'azure-private-dns', 'azure-static-web-app-routing', 'azureservicebus', 'azure-defender', 'azure-data-studio', 'azure-iot-hub-device-management', 'azure-site-recovery', 'azure-application-settings', 'azure-cosmosdb-mongovcore', 'azure-security-center', 'azure-web-app-firewall', 'azure-queues', 'azure-functions', 'azure-management', 'azure-sdk-for-go', 'azure-blockchain-service', 'defaultazurecredential', 'azure-acs', 'azure-functions-core-tools', 'azure-servicebusrelay', 'pulumi-azure', 'azure-sql-edge', 'azure-subscription', 'azure-managed-database', 'azure-management-api', 'azure-bastion', 'azure-iot-dps', 'azure-vm-templates', 'azure-static-website-routing', 'azure-stack', 'azure-managed-app', 'azure-private-link', 'azure-ml-component', 'azure-function-async', 'azure-cosmosdb-tables', 'azure-timeseries-insights', 'azure-video-indexer', 'azure-ai', 'azure-elastic-scale', 'azure-clouddrive', 'azure-api-apps', 'azure-service-fabric-mesh', 'azure-functions-docker', 'microsoft-custom-vision', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-app-service-envrmnt', 'azure-sdk-js', 'azure-sdk-for-java', 'azure-advisor', 'azure-function-app-proxy', 'azure-functions-proxies', 'azure-sentinel', 'azure-anomaly-detector', 'azure-container-instances', 'azure-managed-disk', 'azure-active-directory', 'azureclicredential', 'azure-webapps', 'azure-ml-pipelines', 'azure-redis-cache', 'azure-http-trigger', 'azure-dsvm', 'azureportal', 'azure-servicebus-queues', 'azure-media-services', 'azure-ase', 'azure-node-sdk', 'azure-sql-managed-instance', 'sql-azure-federations', 'azure-debugger', 'azure-service-principal', 'azure-monitor-workbooks', 'azure-web-app-for-containers', 'azure-vm', 'azure-application-insights-profiler', 'azure-cost-calculation', 'azure-mobile-engagement', 'azure-file-copy', 'azure-diagnostics', 'azure-security', 'azure-analytics', 'azure-logic-app-standard', 'azure-vm-scale-set', 'azure-java-tools', 'azure-cognitive-services', 'django-pyodbc-azure', 'azure-application-proxy', 'azure-resource-graph', 'azure-ad-b2b', 'azure-compliance-policy', 'azure-durable-functions', 'azure-database-postgresql', 'azure-promptflow', 'azure-eventhub', 'azure-tablequery', 'azure-sdk-php', 'azure-storage-queues', 'azure-service-plan', 'azure-cosmosdb-emulator', 'azure-performancecounters', 'azure-scheduler', 'azure-availability-set', 'azure-dashboard', 'azure-mysql-database', 'azure-managed-grafana', 'azure-monitoring', 'azure-worker-roles', 'azure-service-runtime', 'azure-ddos', 'azure-data-sync', 'azure-machine-learning-service', 'azure-billing', 'azure-packaging', 'azure-container-apps', 'microsoft-entra-id', 'azure-sql', 'azure-bicep', 'azure-cosmosdb', 'azure-update-management-center', 'azure-mapping-data-flow', 'azure-lab-services', 'azure-custom-providers', 'azure-sdk-.net', 'azure-autoscaling-block', 'azure-data-explorer', 'azureml-python-sdk', 'azure-pipelines-release-pipeline', 'azure-load-balancer', 'azure-managed-identity', 'azure-ad-verifiable-credentials', 'azure-webjobssdk', 'azure-agent', 'azuremlsdk', 'azure-blueprints', 'azure-vpn', 'azure-automation', 'azure-blockchain-workbench', 'azure-api-management', 'azure-rm', 'azure-application-roles', 'azure-public-ip', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-sdk-python', 'terraform-provider-azure', 'azure-marketplace', 'azure-information-protection', 'azure-analysis-services', 'azure-zulu', 'azure-batch-account', 'azure-china', 'azure-android-sdk', 'azure-rm-template', 'azure-spring-cloud', 'azure-stream-analytics', 'azure-keyvault', 'azure-oms', 'azure-ad-msal', 'azure-webhooks', 'azure-adal-deprecation', 'azure-language-understanding', 'azure-container-registry', 'azure-web-pubsub', 'azure-maps', 'azure-migrate', 'azure-dev-spaces', 'fhir-server-for-azure', 'sitecore-azure', 'azure-cosmosdb-gremlinapi', 'azure-aks', 'azure.data.tables', 'azure-java-sdk', 'azure-static-website-hosting', 'azure-mobile-services', 'azure-triggers', 'azure-ad-powershell-v2', 'azure-adf', 'azure-in-role-cache', 'azure-iot-edge', 'azure-linux', 'azure-media-player', 'azure-storage-emulator', 'azure-eventgrid', 'azure-object-anchors', 'azure-management-portal', 'azure-notebooks', 'azure-custom-domain', 'azure-xplat-cli', 'azure-runbook', 'rebus-azureservicebus', 'azure-cognitive-search', 'azure-oauth', 'azure-application-insights', 'azure-traffic-manager', 'azure-anomaly-detection', 'azure-acr', 'adal', 'azure-storage-explorer', 'azure-private-dns-zone', 'azure', 'azure-auto-ml', 'azure-iot-hub-device-update', 'azure-logic-apps', 'azure-relay', 'azure-spatial-anchors', 'azure-monitor', 'azure-load-testing', 'azure-cosmosdb-changefeed', 'azure-function-http'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}, {'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}, {'collective': {'tags': ['amazon-elastic-beanstalk', 'aws-fargate', 'aws-sam', 'aws-codecommit', 'amazon-glacier', 'amazon-ami', 'aws-security-hub', 'alexa-sdk-nodejs', 'amazon-iam', 'amazon-guardduty', 'aws-glue', 'aws-sdk-ruby', 'amazon-cloudfront', 'aws-batch', 'aws-mediatailor', 'aws-global-accelerator', 'amazon-neptune', 'aws-sdk-go-v2', 'amazon-dynamodb-dax', 'aws-vpn', 'amazon-sumerian', 'aws-ssm', 'amazon-route53', 'aws-app-config', 'aws-sdk-ios', 'aws-cli', 'aws-app-mesh', 'aws-event-bridge', 'aws-directory-services', 'amazon-web-services', 'aws-copilot-cli', 'aws-transfer-family', 'aws-parameter-store', 'amazon-imagebuilder', 'amazon-sagemaker', 'amazon-workdocs', 'amazon-keyspaces', 'amazon-ecr', 'amazon-elb', 'aws-cloudformation', 'aws-config', 'aws-snowball', 'aws-sdk-mock', 'amazon-app-runner', 'aws-iot-analytics', 'amazon-sns', 'amazon-memory-db', 'aws-pinpoint', 'aws-deeplens', 'amazon-elasticache', 'aws-mediastore', 'amazon-bedrock', 'amazon-athena', 'amazon-gamelift', 'aws-codecatalyst', 'aws-graviton', 'aws-codeartifact', 'aws-elb', 'aws-cloudmap', 'aws-codeguru', 'amazon-translate', 'aws-sdk-java', 'aws-resource-group', 'amazon-data-pipeline', 'aws-sdk-js', 'amazon-ivs', 'aws-sdk-cpp', 'amazon-kinesis-analytics', 'amazon-cloudwatch', 'amazon-cloudhsm', 'aws-iot-sitewise', 'amazon-vpc', 'alexa-smart-home-skill', 'amazon-kendra', 'amazon-inspector', 'aws-datasync', 'aws-cloud9', 'amazon-ecs', 'amazon-rekognition', 'amazon-swf', 'aws-media-live', 'aws-sdk-js-v3', 'amazon-fsx', 'amazon-s3-select', 'aws-sdk-nodejs', 'aws-iam-identity-center', 'aws-chatbot', 'amazon-opensearch', 'aws-lambda', 'aws-lake-formation', 'aws-cdk', 'amazon-ses', 'aws-security-group', 'aws-mediapackage', 'amazon-connect', 'amazon-qldb', 'aws-iot-core', 'aws-sdk-rust', 'amazon-elastic-transcoder', 'aws-code-deploy', 'aws-serverless', 'amazon-honeycode', 'amazon-ec2', 'alexa-interaction-model', 'aws-xray', 'amazon-waf', 'aws-elemental', 'amazon-sqs', 'amazon-kms', 'aws-certificate-manager', 'aws-sam-cli', 'amazon-kinesis-video-streams', 'aws-lambda-powertools', 'amazon-ec2-spot-market', 'aws-documentdb', 'aws-control-tower', 'aws-service-catalog', 'aws-direct-connect', 'aws-billing', 'aws-iot', 'amazon-cloudwatchlogs', 'amazon-textract', 'alexa-sdk-python', 'aws-lambda-edge', 'amazon-dynamodb', 'amazon-rds', 'aws-iot-events', 'alexa-smapi', 'alexa-flash-briefing-skill', 'aws-dms', 'aws-mediaconnect', 'aws-organizations', 'amazon-macie', 'aws-sdk-comprehend', 'aws-device-farm', 'amazon-redshift-spectrum', 'aws-appsync', 'alexa-account-linking', 'amazon-transcribe', 'aws-acm', 'aws-opsworks', 'aws-step-functions', 'amazon-simpledb', 'amazon-lightsail', 'alexa-presentation-language', 'aws-amplify', 'amazon-workspaces', 'amazon-aurora', 'elastic-ip', 'aws-codepipeline', 'amazon-managed-blockchain', 'aws-application-load-balancer', 'amazon-forecast', 'aws-cloudshell', 'aws-mobilehub', 'aws-reserved-instances', 'amazon-efs', 'aws-sdk', 'aws-backup', 'amazon-timestream', 'amazon-cloudtrail', 'aws-sdk-go', 'amazon-appflow', 'amazon-emr', 'amazon-elasticsearch', 'aws-iot-greengrass', 'aws-sct', 'aws-private-link', 'amazon-quicksight', 'aws-fis', 'aws-sdk-net', 'alexa-skills-kit', 'amazon-kinesis-firehose', 'aws-sdk-java-2.0', 'amazon-ebs', 'aws-codestar', 'aws-sdk-android', 'aws-appstream', 'amazon-s3', 'amazon-lex', 'amazon-cloudsearch', 'aws-databrew', 'amazon-cognito', 'aws-elastictranscoder', 'amazon-workmail', 'amazon-comprehend', 'aws-auto-scaling', 'aws-codebuild', 'aws-api-gateway', 'aws-sso', 'amazon-eks', 'aws-storage-gateway', 'amazon-mq', 'aws-data-exchange', 'amazon-location-service', 'amazon-kinesis', 'amazon-sagemaker-compilers', 'aws-secrets-manager', 'aws-msk', 'amazon-personalize', 'aws-nlb', 'amazon-redshift', 'aws-copilot', 'aws-media-convert', 'amazon-polly'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 1681830, 'is_employee': False, 'last_modified_date': 1686920075, 'last_access_date': 1711162031, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 70, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 13692, 'creation_date': 1343048694, 'user_type': 'registered', 'user_id': 1545917, 'accept_rate': 55, 'location': 'Tizi Ouzou, Algeria', 'website_url': '', 'link': 'https://stackoverflow.com/users/1545917/ghilas-belhadj', 'profile_image': 'https://i.stack.imgur.com/o9dcW.png?s=256&g=1', 'display_name': 'Ghilas BELHADJ'}",How can I remove the labels from a pie chart but keep the legend ?,"import matplotlib.pyplot as plt

x = [15, 30, 55]
labels = [1, 2, 3]

plt.figure(figsize=(3, 3), dpi=72)
plt.pie(x, labels=labels)
plt.legend()
plt.savefig('pie_1.png')
",8,14,1,2,
411,48621360,48625510,12076,Does asyncio from python support coroutine-based API for UDP networking?,4,<python><python-asyncio>,16,"<p>I was browsing the python <code>asyncio</code> module documentation this night looking for some ideas for one of my course projects, but I soon find that there might be a lack of feature in python's standard <code>aysncio</code> module.</p>

<p>If you look through the documentation, you'll find that there's a callback based API and a coroutine based API. And the callback API could be used for building both UDP and TCP applications, while it looks that the coroutine API could only be used for building TCP application, as it utilizes the use of a stream-style API.</p>

<p>This quite causes a problem for me because I was looking for a coroutine-based API for UDP networking, although I did find that <code>asyncio</code> supports low-level coroutine based socket methods like <a href=""https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop.sock_recv"" rel=""noreferrer""><code>sock_recv</code></a> and <a href=""https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop.sock_sendall"" rel=""noreferrer""><code>sock_sendall</code></a>, but the crucial APIs for UDP networking, <code>recvfrom</code> and <code>sendto</code> are not there.</p>

<p>What I wish to do is to write some codes like:</p>

<pre><code>async def handle_income_packet(sock):
    await data, addr = sock.recvfrom(4096)
    # data handling here...
    await sock.sendto(addr, response)
</code></pre>

<p>I know that this could be equivalently implemented using a callback API, but the problem here is that callbacks are not coroutines but regular functions, so that in it you cannot yield control back to the event loop and preserve the function execution state. </p>

<p>Just look at the above code, if we need to do some blocking-IO operations in the data handling part, we won't have a problem in the coroutine version as long as our IO operations are done in coroutines as well: </p>

<pre><code>async def handle_income_packet(sock):
    await data, addr = sock.recvfrom(4096)
    async with aiohttp.ClientSession() as session:
        info = await session.get(...)
    response = generate_response_from_info(info)
    await sock.sendto(addr, response)
</code></pre>

<p>As long as we use <code>await</code> the event loop would take the control flow from that point to handle other things until that IO is done. But sadly these codes are <strong>not</strong> usable at this moment because we do not have a coroutined version of <code>socket.sendto</code> and <code>socket.recvfrom</code> in <code>asyncio</code>.</p>

<p>What we could implement this in is to use the transport-protocol callback API:</p>

<pre><code>class EchoServerClientProtocol(asyncio.Protocol):
    def connection_made(self, transport):
        peername = transport.get_extra_info('peername')
        self.transport = transport

    def data_received(self, data):
        info = requests.get(...)
        response = generate_response_from_info(info)
        self.transport.write(response)
        self.transport.close()
</code></pre>

<p>we cannot <code>await</code> a coroutine there because callbacks are not coroutines,  and using a blocking IO call like above would stall the control flow in the callback and prevent the loop to handle any other events until the IO is done</p>

<p>Another recommended implementation idea is to create a <code>Future</code> object in the <code>data_received</code> function, add it to the event loop, and store any needed state variable in the Protocol class, then explicitly return control to the loop. While this could work, it does create a lot of complex codes where in the coroutine version they're not needed in any way.</p>

<p>Also <a href=""https://www.pythonsheets.com/notes/python-asyncio.html#simple-asyncio-udp-echo-server"" rel=""noreferrer"">here</a> we have an example of using non-blocking socket and <code>add_reader</code> for handle UDP sockets. But the code still looks complex comparing to coroutine-version's a few lines.</p>

<p>The point I want to make is that coroutine is a really good design that could utilize the power of concurrency in one single thread while also has a really straightforward design pattern that could save both brainpower and unnecessary lines of codes, but the crucial part to get it work for UDP networking is really lacking in our <code>asyncio</code> standard library.</p>

<p>What do you guys think about this?</p>

<p>Also, if there's any other suggestions for 3rd party libraries supporting this kind of API for UDP networking, I would be extremely grateful for the sake of my course project. I found <a href=""https://github.com/sampsyo/bluelet"" rel=""noreferrer"">Bluelet</a> is quite like such a thing but it does not seem to be actively maintained.</p>

<p>edit:</p>

<p>It seems that this <a href=""https://github.com/python/asyncio/pull/321"" rel=""noreferrer"">PR</a> did implement this feature but was rejected by the <code>asyncio</code> developers. The developers claim that all functions could be implemented using <code>create_datagram_endpoint()</code>, the protocol-transfer API. But just as I have discussed above, coroutine API has the power of simpleness compared to using the callback API in many use cases, it is really unfortunate that we do not have these with UDP.</p>
",1548129,315,05-02-2018 11:15,05-02-2018 15:07,0,315,7,1,2,,"{'badge_counts': {'bronze': 7, 'silver': 2, 'gold': 1}, 'account_id': 1684598, 'is_employee': False, 'last_modified_date': 1682425009, 'last_access_date': 1701213446, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 315, 'creation_date': 1343120804, 'user_type': 'registered', 'user_id': 1548129, 'location': 'West Lafayette, IN, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/1548129/chaser-hkj', 'profile_image': 'https://www.gravatar.com/avatar/b28b88ed070c5f788fcd37872faac5c4?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Chaser hkj'}","I was browsing the python module documentation this night looking for some ideas for one of my course projects, but I soon find that there might be a lack of feature in python's standard module. If you look through the documentation, you'll find that there's a callback based API and a coroutine based API. And the callback API could be used for building both UDP and TCP applications, while it looks that the coroutine API could only be used for building TCP application, as it utilizes the use of a stream-style API. This quite causes a problem for me because I was looking for a coroutine-based API for UDP networking, although I did find that supports low-level coroutine based socket methods like and , but the crucial APIs for UDP networking, and are not there. What I wish to do is to write some codes like: I know that this could be equivalently implemented using a callback API, but the problem here is that callbacks are not coroutines but regular functions, so that in it you cannot yield control back to the event loop and preserve the function execution state. Just look at the above code, if we need to do some blocking-IO operations in the data handling part, we won't have a problem in the coroutine version as long as our IO operations are done in coroutines as well: As long as we use the event loop would take the control flow from that point to handle other things until that IO is done. But sadly these codes are not usable at this moment because we do not have a coroutined version of and in . What we could implement this in is to use the transport-protocol callback API: we cannot a coroutine there because callbacks are not coroutines, and using a blocking IO call like above would stall the control flow in the callback and prevent the loop to handle any other events until the IO is done Another recommended implementation idea is to create a object in the function, add it to the event loop, and store any needed state variable in the Protocol class, then explicitly return control to the loop. While this could work, it does create a lot of complex codes where in the coroutine version they're not needed in any way. Also here we have an example of using non-blocking socket and for handle UDP sockets. But the code still looks complex comparing to coroutine-version's a few lines. The point I want to make is that coroutine is a really good design that could utilize the power of concurrency in one single thread while also has a really straightforward design pattern that could save both brainpower and unnecessary lines of codes, but the crucial part to get it work for UDP networking is really lacking in our standard library. What do you guys think about this? Also, if there's any other suggestions for 3rd party libraries supporting this kind of API for UDP networking, I would be extremely grateful for the sake of my course project. I found Bluelet is quite like such a thing but it does not seem to be actively maintained. edit: It seems that this PR did implement this feature but was rejected by the developers. The developers claim that all functions could be implemented using , the protocol-transfer API. But just as I have discussed above, coroutine API has the power of simpleness compared to using the callback API in many use cases, it is really unfortunate that we do not have these with UDP.","asyncio aysncio asyncio sock_recv sock_sendall recvfrom sendto async def handle_income_packet(sock):
    await data, addr = sock.recvfrom(4096)
    # data handling here...
    await sock.sendto(addr, response)
 async def handle_income_packet(sock):
    await data, addr = sock.recvfrom(4096)
    async with aiohttp.ClientSession() as session:
        info = await session.get(...)
    response = generate_response_from_info(info)
    await sock.sendto(addr, response)
 await socket.sendto socket.recvfrom asyncio class EchoServerClientProtocol(asyncio.Protocol):
    def connection_made(self, transport):
        peername = transport.get_extra_info('peername')
        self.transport = transport

    def data_received(self, data):
        info = requests.get(...)
        response = generate_response_from_info(info)
        self.transport.write(response)
        self.transport.close()
 await Future data_received add_reader asyncio asyncio create_datagram_endpoint()",-1,57,0,5,
412,48201944,48202061,26753,How to use browsermob with python-selenium?,4,<python><selenium><browsermob>,24,"<p>I want to use browsermob to monitor the network connections when doing a GUI test with selenium. I have found some information and documentation <a href=""https://stackoverflow.com/questions/36744627/network-capturing-with-selenium-phantomjs"">here</a> and <a href=""https://github.com/AutomatedTester/browsermob-proxy-py"" rel=""noreferrer"">here</a> and <a href=""https://browsermob-proxy-py.readthedocs.io/en/stable/"" rel=""noreferrer"">here</a>, but its absolutely unclear how to really use it. </p>

<p>In the documentation it reads:</p>

<pre><code>server = Server(""path/to/browsermob-proxy"")
</code></pre>

<p>But what is that path? Where to find it?</p>

<p>Also I see</p>

<pre><code>java -jar browsermob.jar --port 9090
</code></pre>

<p>but no explanation at all as to what this jar file is, if it is part of the browsermob installation, or something unrelated. </p>

<p>I would appreciate if someone can provide a COMPLETE and WORKING example on how to use browsermob, and what ALL I need to install...</p>
",1581090,43038,11-01-2018 07:35,11-01-2018 07:44,0,43134,492,95,270,56,"{'badge_counts': {'bronze': 492, 'silver': 270, 'gold': 95}, 'collectives': [{'collective': {'tags': ['azure-synapse-link', 'azure-management-groups', 'azure-deployment-slots', 'azure-data-lake-gen2', 'azure-ad-v2', 'azure-databoxfamily', 'azure-purview', 'azure-spring-boot', 'azure-table-storage', 'azure-blob-trigger', 'azure-application-registration', 'azure-calculator', 'azure-hub', 'azure-ad-b2c-custom-policy', 'azure-batch', 'azure-form-recognizer', 'azure-app-service-plans', 'azureshell', 'azure-notificationhub', 'azure-sql-database', 'azure-workflow-automation', 'azure-storage', 'azure-static-web-app', 'azure-nsg', 'azure-databricks', 'azure-app-api', 'azure-alerts', 'azure-virtual-machine', 'azure-cli', 'azure-git-deployment', 'azure-waf', 'azure-connect', 'azure-cosmosdb-cassandra-api', 'azure-database-mysql', 'azure-iot-suite', 'azure-ad-role', 'azure-dns', 'azure-emulator', 'azure-sas', 'azure-search-.net-sdk', 'azure-availability-zones', 'azure-bot-service', 'kql', 'azure-authentication', 'sql-azure-alerts', 'azure-service-fabric', 'azure-communication-services', 'azure-metrics-advisor', 'azure-iot-central', 'azure-cloud-services', 'azure-blob-storage', 'azure-regions', 'azure-store', 'azure-feature-manager', 'azure-cdn', 'azure-ad-graph-api', 'azure-industrial-iot', 'azure-ad-b2c', 'azure-synapse', 'azure-app-registration', 'azure-remote-rendering', 'azure-rtos', 'azure-storage-account', 'azure-iot-sdk', 'azure-ai-translator', 'azure-container-service', 'azure-text-translation', 'azure-fluent-api', 'azure-rest-api', 'azure-sphere', 'azure-gov', 'azure-hybrid-connections', 'azure-quantum', 'azure-digital-twins', 'azurekinect', 'azure-postgresql', 'azure-integration-account', 'microsoft-entra-private-access', 'azure-sql-server', 'azure-servicebus-topics', 'sql-server-azure', 'azure-log-analytics-workspace', 'azure-rbac', 'azure-policy', 'azure-mcd', 'azure-sdk-go', 'spark-bash-azure-databricks', 'azure-app-configuration', 'azure-iot-hub', 'azure-role-environment', 'azure-powershell', 'azure-identity', 'microsoft-entra-external-id', 'azure-deployment', 'azure-sdk-ruby', 'azure-free-services', 'azure-webjobs-triggered', 'azureadgraph-deprecation', 'azure-data-lake', 'azure-data-factory', 'azure-service-hooks', 'azure-image-builder', 'azure-backup-vault', 'azure-data-share', 'azure-data-catalog', 'azure-storage-files', 'azure-appservice', 'azure-files', 'azure-ad-domain-services', 'adal.js', 'azure-configuration', 'azure-qna-maker', 'spring-cloud-azure', 'azurerm-app-service', 'azure-sdk', 'azure-secrets', 'azure-affinity-group', 'kitchen-azurerm', 'azure-web-roles', 'azure-cli2', 'azure-log-analytics', 'azure-hdinsight', 'azure-application-gateway', 'azure-python-sdk', 'azure-sql-reporting', 'azure-disk', 'azure-devtest-labs', 'passport-azure-ad', 'azure-elasticpool', 'azure-function-app', 'azure-signalr', 'azure-webjobs-continuous', 'azure-compute-emulator', 'azure-speech', 'microsoft-entra-internet-access', 'azure-functions-runtime', 'azure-web-app-service', 'azure-front-door', 'azure-function-queue', 'azure-resource-group', 'azure-pack', 'azure-caching', 'azure-cosmosdb-mongoapi', 'azure-webjobs', 'azure-billing-api', 'azure-appfabric', 'azure-cloud-shell', 'azure-sdk-for-ruby', 'azure-virtual-network', 'azure-servicebus-subscriptions', 'azure-arc', 'azure-private-dns', 'azure-static-web-app-routing', 'azureservicebus', 'azure-defender', 'azure-data-studio', 'azure-iot-hub-device-management', 'azure-site-recovery', 'azure-application-settings', 'azure-cosmosdb-mongovcore', 'azure-security-center', 'azure-web-app-firewall', 'azure-queues', 'azure-functions', 'azure-management', 'azure-sdk-for-go', 'azure-blockchain-service', 'defaultazurecredential', 'azure-acs', 'azure-functions-core-tools', 'azure-servicebusrelay', 'pulumi-azure', 'azure-sql-edge', 'azure-subscription', 'azure-managed-database', 'azure-management-api', 'azure-bastion', 'azure-iot-dps', 'azure-vm-templates', 'azure-static-website-routing', 'azure-stack', 'azure-managed-app', 'azure-private-link', 'azure-ml-component', 'azure-function-async', 'azure-cosmosdb-tables', 'azure-timeseries-insights', 'azure-video-indexer', 'azure-ai', 'azure-elastic-scale', 'azure-clouddrive', 'azure-api-apps', 'azure-service-fabric-mesh', 'azure-functions-docker', 'microsoft-custom-vision', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-app-service-envrmnt', 'azure-sdk-js', 'azure-sdk-for-java', 'azure-advisor', 'azure-function-app-proxy', 'azure-functions-proxies', 'azure-sentinel', 'azure-anomaly-detector', 'azure-container-instances', 'azure-managed-disk', 'azure-active-directory', 'azureclicredential', 'azure-webapps', 'azure-ml-pipelines', 'azure-redis-cache', 'azure-http-trigger', 'azure-dsvm', 'azureportal', 'azure-servicebus-queues', 'azure-media-services', 'azure-ase', 'azure-node-sdk', 'azure-sql-managed-instance', 'sql-azure-federations', 'azure-debugger', 'azure-service-principal', 'azure-monitor-workbooks', 'azure-web-app-for-containers', 'azure-vm', 'azure-application-insights-profiler', 'azure-cost-calculation', 'azure-mobile-engagement', 'azure-file-copy', 'azure-diagnostics', 'azure-security', 'azure-analytics', 'azure-logic-app-standard', 'azure-vm-scale-set', 'azure-java-tools', 'azure-cognitive-services', 'django-pyodbc-azure', 'azure-application-proxy', 'azure-resource-graph', 'azure-ad-b2b', 'azure-compliance-policy', 'azure-durable-functions', 'azure-database-postgresql', 'azure-promptflow', 'azure-eventhub', 'azure-tablequery', 'azure-sdk-php', 'azure-storage-queues', 'azure-service-plan', 'azure-cosmosdb-emulator', 'azure-performancecounters', 'azure-scheduler', 'azure-availability-set', 'azure-dashboard', 'azure-mysql-database', 'azure-managed-grafana', 'azure-monitoring', 'azure-worker-roles', 'azure-service-runtime', 'azure-ddos', 'azure-data-sync', 'azure-machine-learning-service', 'azure-billing', 'azure-packaging', 'azure-container-apps', 'microsoft-entra-id', 'azure-sql', 'azure-bicep', 'azure-cosmosdb', 'azure-update-management-center', 'azure-mapping-data-flow', 'azure-lab-services', 'azure-custom-providers', 'azure-sdk-.net', 'azure-autoscaling-block', 'azure-data-explorer', 'azureml-python-sdk', 'azure-pipelines-release-pipeline', 'azure-load-balancer', 'azure-managed-identity', 'azure-ad-verifiable-credentials', 'azure-webjobssdk', 'azure-agent', 'azuremlsdk', 'azure-blueprints', 'azure-vpn', 'azure-automation', 'azure-blockchain-workbench', 'azure-api-management', 'azure-rm', 'azure-application-roles', 'azure-public-ip', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-sdk-python', 'terraform-provider-azure', 'azure-marketplace', 'azure-information-protection', 'azure-analysis-services', 'azure-zulu', 'azure-batch-account', 'azure-china', 'azure-android-sdk', 'azure-rm-template', 'azure-spring-cloud', 'azure-stream-analytics', 'azure-keyvault', 'azure-oms', 'azure-ad-msal', 'azure-webhooks', 'azure-adal-deprecation', 'azure-language-understanding', 'azure-container-registry', 'azure-web-pubsub', 'azure-maps', 'azure-migrate', 'azure-dev-spaces', 'fhir-server-for-azure', 'sitecore-azure', 'azure-cosmosdb-gremlinapi', 'azure-aks', 'azure.data.tables', 'azure-java-sdk', 'azure-static-website-hosting', 'azure-mobile-services', 'azure-triggers', 'azure-ad-powershell-v2', 'azure-adf', 'azure-in-role-cache', 'azure-iot-edge', 'azure-linux', 'azure-media-player', 'azure-storage-emulator', 'azure-eventgrid', 'azure-object-anchors', 'azure-management-portal', 'azure-notebooks', 'azure-custom-domain', 'azure-xplat-cli', 'azure-runbook', 'rebus-azureservicebus', 'azure-cognitive-search', 'azure-oauth', 'azure-application-insights', 'azure-traffic-manager', 'azure-anomaly-detection', 'azure-acr', 'adal', 'azure-storage-explorer', 'azure-private-dns-zone', 'azure', 'azure-auto-ml', 'azure-iot-hub-device-update', 'azure-logic-apps', 'azure-relay', 'azure-spatial-anchors', 'azure-monitor', 'azure-load-testing', 'azure-cosmosdb-changefeed', 'azure-function-http'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}], 'account_id': 1726575, 'is_employee': False, 'last_modified_date': 1711039517, 'last_access_date': 1711036537, 'reputation_change_year': 690, 'reputation_change_quarter': 690, 'reputation_change_month': 146, 'reputation_change_week': 26, 'reputation_change_day': 0, 'reputation': 43134, 'creation_date': 1344321718, 'user_type': 'registered', 'user_id': 1581090, 'accept_rate': 56, 'link': 'https://stackoverflow.com/users/1581090/alex', 'profile_image': 'https://www.gravatar.com/avatar/e51877f9678542968a5b87c9d07c9000?s=256&d=identicon&r=PG', 'display_name': 'Alex'}","I want to use browsermob to monitor the network connections when doing a GUI test with selenium. I have found some information and documentation here and here and here, but its absolutely unclear how to really use it. In the documentation it reads: But what is that path? Where to find it? Also I see but no explanation at all as to what this jar file is, if it is part of the browsermob installation, or something unrelated. I would appreciate if someone can provide a COMPLETE and WORKING example on how to use browsermob, and what ALL I need to install...","server = Server(""path/to/browsermob-proxy"")
 java -jar browsermob.jar --port 9090
",0,17,0,3,
413,49863633,49864136,16026,numpy.product vs numpy.prod vs ndarray.prod,2,<python><numpy><numpy-ndarray>,16,"<p>I'm reading through the Numpy docs, and it appears that the functions <code>np.prod(...)</code>, <code>np.product(...)</code> and the <code>ndarray</code> method <code>a.prod(...)</code> are all equivalent. </p>

<p>Is there a preferred version to use, both in terms of style/readability and performance? Are there different situations where different versions are preferable? If not, why are there three separate but very similar ways to perform the same operation?</p>
",2593878,6930,16-04-2018 18:04,16-04-2018 18:37,0,6950,55,10,36,92,"{'badge_counts': {'bronze': 55, 'silver': 36, 'gold': 10}, 'account_id': 3061295, 'is_employee': False, 'last_modified_date': 1705990262, 'last_access_date': 1711152074, 'reputation_change_year': 168, 'reputation_change_quarter': 168, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 6950, 'creation_date': 1374119877, 'user_type': 'registered', 'user_id': 2593878, 'accept_rate': 92, 'website_url': '', 'link': 'https://stackoverflow.com/users/2593878/dkv', 'profile_image': 'https://i.stack.imgur.com/C01rI.png?s=256&g=1', 'display_name': 'dkv'}","I'm reading through the Numpy docs, and it appears that the functions , and the method are all equivalent. Is there a preferred version to use, both in terms of style/readability and performance? Are there different situations where different versions are preferable? If not, why are there three separate but very similar ways to perform the same operation?",np.prod(...) np.product(...) ndarray a.prod(...),-4,3,0,0,
414,49294222,49294278,4473,"In python, is there some kind of mapping to return the ""False value"" of a type?",5,<python><types><boolean>,42,"<p>I am looking for some kind of a mapping function <code>f()</code> that does something similar to this:</p>

<pre><code>f(str) = ''
f(complex) = 0j
f(list) = []
</code></pre>

<p>Meaning that it returns an object of type that evaluates to <code>False</code> when cast to <code>bool</code>.</p>

<p>Does such a function exist?</p>
",2594596,1711,15-03-2018 08:03,15-03-2018 08:07,0,1711,20,1,14,38,"{'badge_counts': {'bronze': 20, 'silver': 14, 'gold': 1}, 'account_id': 3062188, 'is_employee': False, 'last_modified_date': 1603036200, 'last_access_date': 1709825142, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1711, 'creation_date': 1374136105, 'user_type': 'registered', 'user_id': 2594596, 'accept_rate': 38, 'location': 'Eurasia', 'website_url': '', 'link': 'https://stackoverflow.com/users/2594596/matts', 'profile_image': 'https://www.gravatar.com/avatar/554c0bf151203f6abf7379e324da3351?s=256&d=identicon&r=PG', 'display_name': 'MattS'}",I am looking for some kind of a mapping function that does something similar to this: Meaning that it returns an object of type that evaluates to when cast to . Does such a function exist?,"f() f(str) = ''
f(complex) = 0j
f(list) = []
 False bool",-1,10,0,0,
415,48815341,48815706,9415,Why is Apache-Spark - Python so slow locally as compared to pandas?,1,<python><pandas><apache-spark><pyspark><apache-spark-sql>,34,"<p>A Spark newbie here.
I recently started playing around with Spark on my local machine on two cores by using the command:</p>

<pre><code>pyspark --master local[2]
</code></pre>

<p>I have a 393Mb text file which has almost a million rows. I wanted to perform some data manipulation operation. I am using the built-in dataframe functions of PySpark to perform simple operations like <code>groupBy</code>, <code>sum</code>, <code>max</code>, <code>stddev</code>.</p>

<p>However, when I do the exact same operations in pandas on the exact same dataset, pandas seems to defeat pyspark by a huge margin in terms of latency.</p>

<p>I was wondering what could be a possible reason for this. I have a couple of thoughts.</p>

<ol>
<li>Do built-in functions do the process of serialization/de-serialization inefficiently? If yes, what are the alternatives to them?</li>
<li>Is the data set too small that it cannot outrun the overhead cost of the underlying JVM on which spark runs?</li>
</ol>

<p>Thanks for looking. Much appreciated.</p>
",2019135,447,15-02-2018 20:01,15-02-2018 20:26,0,447,11,0,6,57,"{'badge_counts': {'bronze': 11, 'silver': 6, 'gold': 0}, 'account_id': 2298194, 'is_employee': False, 'last_modified_date': 1573681693, 'last_access_date': 1651233288, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 447, 'creation_date': 1359396264, 'user_type': 'registered', 'user_id': 2019135, 'accept_rate': 57, 'location': 'Bangalore, Karnataka, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/2019135/raj', 'profile_image': 'https://www.gravatar.com/avatar/955e38aa6a3b01f604d16ab5eff9efd4?s=256&d=identicon&r=PG', 'display_name': 'Raj'}","A Spark newbie here. I recently started playing around with Spark on my local machine on two cores by using the command: I have a 393Mb text file which has almost a million rows. I wanted to perform some data manipulation operation. I am using the built-in dataframe functions of PySpark to perform simple operations like , , , . However, when I do the exact same operations in pandas on the exact same dataset, pandas seems to defeat pyspark by a huge margin in terms of latency. I was wondering what could be a possible reason for this. I have a couple of thoughts. Do built-in functions do the process of serialization/de-serialization inefficiently? If yes, what are the alternatives to them? Is the data set too small that it cannot outrun the overhead cost of the underlying JVM on which spark runs? Thanks for looking. Much appreciated.","pyspark --master local[2]
 groupBy sum max stddev",-4,18,0,0,
416,48794268,48794353,14760,How do I make a Python function with mutually exclusive arguments?,7,<python><arguments><mutual-exclusion>,11,"<p>I have a Python class which needs to accept one of two mutually exclusive arguments. If the arguments are not exclusive, (ie: if both or neither are given), an error should be raised.</p>

<pre><code>class OrgLocation:
    __init__(self, location_num=None, location_path=None):
        """"""location_num &amp; location_path are mutually exclusive""""""
</code></pre>

<p>In most scenarios, the best option would be to make two separate classes. However, I am working with <a href=""https://stackoverflow.com/q/9812136/3357935"">an external API</a> which <em>requires</em> these two attributes to be mutually exclusive.</p>

<p>Request:</p>

<pre><code>&lt;OrgLocation LocationPathName=""ROOT/BU/DIV/SL/DEPT/JOB"" LocationNum=""1234""/&gt;
</code></pre>

<p>Response:</p>

<pre><code>&lt;Error Message=""Use either LocationNum or LocationPathName but not both."" ErrorCode=""1186""&gt;
</code></pre>

<hr>

<p>Similar questions seem to indicate that <a href=""https://docs.python.org/3/library/argparse.html"" rel=""noreferrer""><code>argparse</code></a> can be used for mutually exclusive arguments in command-line interfaces, but I'm unsure how to apply this to a class constructor</p>

<p>How can I create a Python function with mutually exclusive arguments?</p>
",3357935,25006,14-02-2018 18:52,14-02-2018 18:57,0,25082,230,29,128,82,"{'badge_counts': {'bronze': 230, 'silver': 128, 'gold': 29}, 'account_id': 4030991, 'is_employee': False, 'last_modified_date': 1703299500, 'last_access_date': 1711164785, 'reputation_change_year': 482, 'reputation_change_quarter': 482, 'reputation_change_month': 114, 'reputation_change_week': 26, 'reputation_change_day': 0, 'reputation': 25082, 'creation_date': 1393451089, 'user_type': 'registered', 'user_id': 3357935, 'accept_rate': 82, 'location': 'NY, USA', 'link': 'https://stackoverflow.com/users/3357935/stevoisiak', 'profile_image': 'https://i.stack.imgur.com/jaomO.png?s=256&g=1', 'display_name': 'Stevoisiak'}","I have a Python class which needs to accept one of two mutually exclusive arguments. If the arguments are not exclusive, (ie: if both or neither are given), an error should be raised. In most scenarios, the best option would be to make two separate classes. However, I am working with an external API which requires these two attributes to be mutually exclusive. Request: Response: Similar questions seem to indicate that can be used for mutually exclusive arguments in command-line interfaces, but I'm unsure how to apply this to a class constructor How can I create a Python function with mutually exclusive arguments?","class OrgLocation:
    __init__(self, location_num=None, location_path=None):
        """"""location_num &amp; location_path are mutually exclusive""""""
 &lt;OrgLocation LocationPathName=""ROOT/BU/DIV/SL/DEPT/JOB"" LocationNum=""1234""/&gt;
 &lt;Error Message=""Use either LocationNum or LocationPathName but not both."" ErrorCode=""1186""&gt;
 argparse",1,24,0,2,
417,48391146,48391281,13634,Change marker in the legend in matplotlib,3,<python><matplotlib>,11,"<p>Suppose that you plot a set of data:</p>

<pre><code>plt.plot(x,y, marker='.', label='something')
plt.legend()
</code></pre>

<p>On the display, you will obtain <code>.  something</code>, but how do you do to change it to <code>- something</code>, so that the marker that appears in the legend is a line a not a dot?</p>
",2820579,3361,22-01-2018 22:16,22-01-2018 22:27,0,3361,46,7,33,45,"{'badge_counts': {'bronze': 46, 'silver': 33, 'gold': 7}, 'account_id': 3358113, 'is_employee': False, 'last_modified_date': 1706320200, 'last_access_date': 1710283037, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3361, 'creation_date': 1380216908, 'user_type': 'registered', 'user_id': 2820579, 'accept_rate': 45, 'website_url': '', 'link': 'https://stackoverflow.com/users/2820579/user2820579', 'profile_image': 'https://www.gravatar.com/avatar/7f5a6dfa244ae99002635ef128962132?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user2820579'}","Suppose that you plot a set of data: On the display, you will obtain , but how do you do to change it to , so that the marker that appears in the legend is a line a not a dot?","plt.plot(x,y, marker='.', label='something')
plt.legend()
 .  something - something",-1,7,0,0,
418,48409128,48411543,50300,What is the difference between using loc and using just square brackets to filter for columns in Pandas/Python?,4,<python><pandas><dataframe>,175,"<p>I've noticed three methods of selecting a column in a Pandas DataFrame:</p>

<p><strong>First method of selecting a column using loc:</strong></p>

<pre><code>df_new = df.loc[:, 'col1']
</code></pre>

<p><strong>Second method - seems simpler and faster:</strong></p>

<pre><code>df_new = df['col1']
</code></pre>

<p><strong>Third method - most convenient:</strong></p>

<pre><code>df_new = df.col1
</code></pre>

<p>Is there a difference between these three methods? I don't think so, in which case I'd rather use the third method.</p>

<p>I'm mostly curious as to why there appear to be three methods for doing the same thing.</p>
",3385948,5148,23-01-2018 19:11,23-01-2018 22:00,0,5158,66,9,42,86,"{'badge_counts': {'bronze': 66, 'silver': 42, 'gold': 9}, 'account_id': 3951462, 'is_employee': False, 'last_modified_date': 1700273100, 'last_access_date': 1711124886, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5158, 'creation_date': 1394064325, 'user_type': 'registered', 'user_id': 3385948, 'accept_rate': 86, 'location': 'Calgary, Alberta, Canada', 'website_url': 'https://myijack.com', 'link': 'https://stackoverflow.com/users/3385948/sean-mccarthy', 'profile_image': 'https://i.stack.imgur.com/YJIjJ.png?s=256&g=1', 'display_name': 'Sean McCarthy'}","I've noticed three methods of selecting a column in a Pandas DataFrame: First method of selecting a column using loc: Second method - seems simpler and faster: Third method - most convenient: Is there a difference between these three methods? I don't think so, in which case I'd rather use the third method. I'm mostly curious as to why there appear to be three methods for doing the same thing.","df_new = df.loc[:, 'col1']
 df_new = df['col1']
 df_new = df.col1
",0,20,0,0,
419,49329569,49329633,30728,"How do you pop multiple columns off a Pandas dataframe, into a new dataframe?",4,<python><pandas>,37,"<p>Suppose I have the following:</p>

<pre><code>df = pd.DataFrame({'a':range(2), 'b':range(2), 'c':range(2), 'd':range(2)})
</code></pre>

<p>I'd like to ""pop"" two columns ('c' and 'd') off the dataframe, into a new dataframe, leaving 'a' and 'b' behind in the original df. The following does not work:</p>

<pre><code>df2 = df.pop(['c', 'd'])
</code></pre>

<p>Here's my error:</p>

<pre><code>TypeError: '['c', 'd']' is an invalid key
</code></pre>

<p>Does anyone know a quick, classy solution, besides doing the following?</p>

<pre><code>df2 = df[['c', 'd']]
df3 = df[['a', 'b']]
</code></pre>

<p>I know the above code is not <em>that</em> tedious to type, but this is why DataFrame.pop was invented--to save us a step when popping one column off a database.</p>
",3385948,5148,16-03-2018 21:11,16-03-2018 21:15,0,5158,66,9,42,86,"{'badge_counts': {'bronze': 66, 'silver': 42, 'gold': 9}, 'account_id': 3951462, 'is_employee': False, 'last_modified_date': 1700273100, 'last_access_date': 1711124886, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5158, 'creation_date': 1394064325, 'user_type': 'registered', 'user_id': 3385948, 'accept_rate': 86, 'location': 'Calgary, Alberta, Canada', 'website_url': 'https://myijack.com', 'link': 'https://stackoverflow.com/users/3385948/sean-mccarthy', 'profile_image': 'https://i.stack.imgur.com/YJIjJ.png?s=256&g=1', 'display_name': 'Sean McCarthy'}","Suppose I have the following: I'd like to ""pop"" two columns ('c' and 'd') off the dataframe, into a new dataframe, leaving 'a' and 'b' behind in the original df. The following does not work: Here's my error: Does anyone know a quick, classy solution, besides doing the following? I know the above code is not that tedious to type, but this is why DataFrame.pop was invented--to save us a step when popping one column off a database.","df = pd.DataFrame({'a':range(2), 'b':range(2), 'c':range(2), 'd':range(2)})
 df2 = df.pop(['c', 'd'])
 TypeError: '['c', 'd']' is an invalid key
 df2 = df[['c', 'd']]
df3 = df[['a', 'b']]
",1,22,0,0,
420,48519633,48519756,36492,SQLAlchemy cast boolean column to int,2,<python><postgresql><sqlalchemy>,18,"<p>I have a table of user records and I wish to see the average when some condition is met. the condition column is a boolean.
In Postgresql I could cast it easily:</p>

<pre><code>select id, avg(cond::INT) from table group by id;
</code></pre>

<p>But in SQLAlchemy I couldn't find anything equivalent to <code>::INT</code>.<br>
How do i deal with the type conversion?
I have</p>

<pre><code>orm_query(Table.id, func.avg(Table.cond))
</code></pre>

<p>which, of course, returns an error.</p>
",2821831,1901,30-01-2018 10:52,30-01-2018 10:58,0,1901,37,3,21,46,"{'badge_counts': {'bronze': 37, 'silver': 21, 'gold': 3}, 'account_id': 3359777, 'is_employee': False, 'last_modified_date': 1703312401, 'last_access_date': 1663497579, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1901, 'creation_date': 1380250469, 'user_type': 'registered', 'user_id': 2821831, 'accept_rate': 46, 'website_url': 'http://deanla.com', 'link': 'https://stackoverflow.com/users/2821831/deanla', 'profile_image': 'https://i.stack.imgur.com/PcLYv.jpg?s=256&g=1', 'display_name': 'DeanLa'}","I have a table of user records and I wish to see the average when some condition is met. the condition column is a boolean. In Postgresql I could cast it easily: But in SQLAlchemy I couldn't find anything equivalent to . How do i deal with the type conversion? I have which, of course, returns an error.","select id, avg(cond::INT) from table group by id;
 ::INT orm_query(Table.id, func.avg(Table.cond))
",-1,14,0,0,
421,50041919,50042039,12057,Django 'TestForm' object has no attribute 'fields',1,<python><django><forms><dropdownbox>,14,"<p>I'm using django:</p>

<p>I'm trying to pass a list of tuples from <code>views.py</code> to a dropdown box form but I get this attribute error</p>

<p><code>forms.py</code></p>

<pre><code>import logging                                                                   

from django import forms                                                         

log = logging.getLogger(__name__)                                                

class TestForm(forms.Form):                                                    

    def __init__(self, *args, **kwargs):                                         
        testlist = kwargs.pop('testlist',None)                               
        log.info(regionlist)                                                     
        self.fields['testlist'] = forms.ChoiceField(choices=testlist)        
        super(TestForm, self).__init__(*args, **kwargs) 
</code></pre>

<p><code>views.py</code></p>

<pre><code>form = forms.RegionForm(regionlist=data)     
</code></pre>

<p>Am I using the right method to pass variables between <code>views.py</code> and <code>forms.py</code>?</p>
",2166418,399,26-04-2018 11:32,26-04-2018 11:38,0,399,18,2,6,71,"{'badge_counts': {'bronze': 18, 'silver': 6, 'gold': 2}, 'account_id': 2488418, 'is_employee': False, 'last_modified_date': 1630114500, 'last_access_date': 1635850148, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 399, 'creation_date': 1363192262, 'user_type': 'registered', 'user_id': 2166418, 'accept_rate': 71, 'location': 'West Yorkshire', 'website_url': 'http://Alcock', 'link': 'https://stackoverflow.com/users/2166418/jalcock501', 'profile_image': 'https://www.gravatar.com/avatar/971e200fcf7b018d3aeda0cc4669347a?s=256&d=identicon&r=PG', 'display_name': 'Jalcock501'}",I'm using django: I'm trying to pass a list of tuples from to a dropdown box form but I get this attribute error Am I using the right method to pass variables between and ?,"views.py forms.py import logging                                                                   

from django import forms                                                         

log = logging.getLogger(__name__)                                                

class TestForm(forms.Form):                                                    

    def __init__(self, *args, **kwargs):                                         
        testlist = kwargs.pop('testlist',None)                               
        log.info(regionlist)                                                     
        self.fields['testlist'] = forms.ChoiceField(choices=testlist)        
        super(TestForm, self).__init__(*args, **kwargs) 
 views.py form = forms.RegionForm(regionlist=data)     
 views.py forms.py",7,27,0,0,
422,48840025,49025400,8334,Heroku: deploying Deep Learning model,7,<python><tensorflow><heroku><keras><deep-learning>,11,"<p>I have developed a rest API using Flask to expose a Python Keras Deep Learning model (CNN for text classification). I have a very simple script that loads the model into memory and outputs class probabilities for a given text input. The API works perfectly locally.</p>

<p>However, when I <code>git push heroku master</code>, I get <code>Compiled slug size: 588.2M is too large (max is 500M)</code>. The model is 83MB in size, which is quite small for a Deep Learning model. Notable dependencies include Keras and its tensorflow backend. </p>

<p>I know that you can use GBs of RAM and disk space on Heroku. But the bottleneck seems to be the slug size. Is there a way to circumvent this? Or is Heroku just not the right tool for deploying Deep Learning models? </p>
",2835597,1698,17-02-2018 09:52,28-02-2018 08:51,11,1699,52,4,23,83,"{'badge_counts': {'bronze': 52, 'silver': 23, 'gold': 4}, 'account_id': 3377795, 'is_employee': False, 'last_modified_date': 1710350400, 'last_access_date': 1711027628, 'reputation_change_year': 11, 'reputation_change_quarter': 11, 'reputation_change_month': 1, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1699, 'creation_date': 1380642842, 'user_type': 'registered', 'user_id': 2835597, 'accept_rate': 83, 'location': 'Paris, France', 'website_url': 'https://tixierae.github.io./', 'link': 'https://stackoverflow.com/users/2835597/antoine', 'profile_image': 'https://i.stack.imgur.com/W0o1L.jpg?s=256&g=1', 'display_name': 'Antoine'}","I have developed a rest API using Flask to expose a Python Keras Deep Learning model (CNN for text classification). I have a very simple script that loads the model into memory and outputs class probabilities for a given text input. The API works perfectly locally. However, when I , I get . The model is 83MB in size, which is quite small for a Deep Learning model. Notable dependencies include Keras and its tensorflow backend. I know that you can use GBs of RAM and disk space on Heroku. But the bottleneck seems to be the slug size. Is there a way to circumvent this? Or is Heroku just not the right tool for deploying Deep Learning models?",git push heroku master Compiled slug size: 588.2M is too large (max is 500M),-2,5,0,0,
423,49658802,49659358,21113,How can I use tf.data Datasets in eager execution mode?,2,<python><tensorflow>,14,"<p>In the <a href=""https://youtu.be/uIcqeP7MFH0?t=654"" rel=""noreferrer"">tf.data talk</a> at the TensorFlow Dev Summit 2018, Derek Murray presented a way to combine the <a href=""https://www.tensorflow.org/api_docs/python/tf/data"" rel=""noreferrer""><code>tf.data</code></a> API with TensorFlow's eager execution mode (at 10:54). I tried out a simplified version of the code shown there:</p>

<pre><code>import tensorflow as tf
tf.enable_eager_execution()

dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([50, 10]))
dataset = dataset.batch(5)
for batch in dataset:
    print(batch)
</code></pre>

<p>causing </p>

<pre><code>TypeError: 'BatchDataset' object is not iterable
</code></pre>

<p>I also tried using <code>dataset.make_one_shot_iterator()</code> and <code>dataset.make_initializable_iterator()</code> to iterate over the dataset, but they result in </p>

<pre><code>RuntimeError: dataset.make_one_shot_iterator is not supported when eager execution is enabled.
</code></pre>

<p>and</p>

<pre><code>RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled.
</code></pre>

<p><em>TensorFlow version: 1.7.0, Python version: 3.6</em></p>

<p>How can you use the <code>tf.data</code> API with eager execution? </p>
",2305095,6868,04-04-2018 19:18,04-04-2018 19:54,0,6878,51,4,38,,"{'badge_counts': {'bronze': 51, 'silver': 38, 'gold': 4}, 'account_id': 2666850, 'is_employee': False, 'last_modified_date': 1691566494, 'last_access_date': 1702820595, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 6878, 'creation_date': 1366571248, 'user_type': 'registered', 'user_id': 2305095, 'website_url': 'https://www.kilians.net', 'link': 'https://stackoverflow.com/users/2305095/kilian-obermeier', 'profile_image': 'https://i.stack.imgur.com/jmMgf.jpg?s=256&g=1', 'display_name': 'Kilian Obermeier'}","In the tf.data talk at the TensorFlow Dev Summit 2018, Derek Murray presented a way to combine the API with TensorFlow's eager execution mode (at 10:54). I tried out a simplified version of the code shown there: causing I also tried using and to iterate over the dataset, but they result in and TensorFlow version: 1.7.0, Python version: 3.6 How can you use the API with eager execution?","tf.data import tensorflow as tf
tf.enable_eager_execution()

dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([50, 10]))
dataset = dataset.batch(5)
for batch in dataset:
    print(batch)
 TypeError: 'BatchDataset' object is not iterable
 dataset.make_one_shot_iterator() dataset.make_initializable_iterator() RuntimeError: dataset.make_one_shot_iterator is not supported when eager execution is enabled.
 RuntimeError: dataset.make_initializable_iterator is not supported when eager execution is enabled.
 tf.data",2,29,0,2,
424,48164206,48164615,64199,PySpark - Adding a Column from a list of values,7,<python><list><apache-spark><pyspark><apache-spark-sql>,18,"<p>I have to add column to a PySpark dataframe based on a list of values. </p>

<pre><code>a= spark.createDataFrame([(""Dog"", ""Cat""), (""Cat"", ""Dog""), (""Mouse"", ""Cat"")],[""Animal"", ""Enemy""])
</code></pre>

<p>I have a list called rating, which is a rating of each pet. </p>

<pre><code>rating = [5,4,1]
</code></pre>

<p>I need to append the dataframe with a column called Rating, such that </p>

<pre class=""lang-none prettyprint-override""><code>+------+-----+------+
|Animal|Enemy|Rating|
+------+-----+------+
|   Dog|  Cat|     5|
|   Cat|  Dog|     4|
| Mouse|  Cat|     1|
+------+-----+------+
</code></pre>

<p>I have done the following however it is returning only the first value in the list in the Rating Column </p>

<pre><code>def add_labels():
    return rating.pop(0)

labels_udf = udf(add_labels, IntegerType())

new_df = a.withColumn('Rating', labels_udf()).cache()
</code></pre>

<p>out:</p>

<pre class=""lang-none prettyprint-override""><code>+------+-----+------+
|Animal|Enemy|Rating|
+------+-----+------+
|   Dog|  Cat|     5|
|   Cat|  Dog|     5|
| Mouse|  Cat|     5|
+------+-----+------+
</code></pre>
",2402501,3197,09-01-2018 08:39,09-01-2018 09:06,0,3197,72,10,43,80,"{'badge_counts': {'bronze': 72, 'silver': 43, 'gold': 10}, 'collectives': [{'collective': {'tags': ['azure-synapse-link', 'azure-management-groups', 'azure-deployment-slots', 'azure-data-lake-gen2', 'azure-ad-v2', 'azure-databoxfamily', 'azure-purview', 'azure-spring-boot', 'azure-table-storage', 'azure-blob-trigger', 'azure-application-registration', 'azure-calculator', 'azure-hub', 'azure-ad-b2c-custom-policy', 'azure-batch', 'azure-form-recognizer', 'azure-app-service-plans', 'azureshell', 'azure-notificationhub', 'azure-sql-database', 'azure-workflow-automation', 'azure-storage', 'azure-static-web-app', 'azure-nsg', 'azure-databricks', 'azure-app-api', 'azure-alerts', 'azure-virtual-machine', 'azure-cli', 'azure-git-deployment', 'azure-waf', 'azure-connect', 'azure-cosmosdb-cassandra-api', 'azure-database-mysql', 'azure-iot-suite', 'azure-ad-role', 'azure-dns', 'azure-emulator', 'azure-sas', 'azure-search-.net-sdk', 'azure-availability-zones', 'azure-bot-service', 'kql', 'azure-authentication', 'sql-azure-alerts', 'azure-service-fabric', 'azure-communication-services', 'azure-metrics-advisor', 'azure-iot-central', 'azure-cloud-services', 'azure-blob-storage', 'azure-regions', 'azure-store', 'azure-feature-manager', 'azure-cdn', 'azure-ad-graph-api', 'azure-industrial-iot', 'azure-ad-b2c', 'azure-synapse', 'azure-app-registration', 'azure-remote-rendering', 'azure-rtos', 'azure-storage-account', 'azure-iot-sdk', 'azure-ai-translator', 'azure-container-service', 'azure-text-translation', 'azure-fluent-api', 'azure-rest-api', 'azure-sphere', 'azure-gov', 'azure-hybrid-connections', 'azure-quantum', 'azure-digital-twins', 'azurekinect', 'azure-postgresql', 'azure-integration-account', 'microsoft-entra-private-access', 'azure-sql-server', 'azure-servicebus-topics', 'sql-server-azure', 'azure-log-analytics-workspace', 'azure-rbac', 'azure-policy', 'azure-mcd', 'azure-sdk-go', 'spark-bash-azure-databricks', 'azure-app-configuration', 'azure-iot-hub', 'azure-role-environment', 'azure-powershell', 'azure-identity', 'microsoft-entra-external-id', 'azure-deployment', 'azure-sdk-ruby', 'azure-free-services', 'azure-webjobs-triggered', 'azureadgraph-deprecation', 'azure-data-lake', 'azure-data-factory', 'azure-service-hooks', 'azure-image-builder', 'azure-backup-vault', 'azure-data-share', 'azure-data-catalog', 'azure-storage-files', 'azure-appservice', 'azure-files', 'azure-ad-domain-services', 'adal.js', 'azure-configuration', 'azure-qna-maker', 'spring-cloud-azure', 'azurerm-app-service', 'azure-sdk', 'azure-secrets', 'azure-affinity-group', 'kitchen-azurerm', 'azure-web-roles', 'azure-cli2', 'azure-log-analytics', 'azure-hdinsight', 'azure-application-gateway', 'azure-python-sdk', 'azure-sql-reporting', 'azure-disk', 'azure-devtest-labs', 'passport-azure-ad', 'azure-elasticpool', 'azure-function-app', 'azure-signalr', 'azure-webjobs-continuous', 'azure-compute-emulator', 'azure-speech', 'microsoft-entra-internet-access', 'azure-functions-runtime', 'azure-web-app-service', 'azure-front-door', 'azure-function-queue', 'azure-resource-group', 'azure-pack', 'azure-caching', 'azure-cosmosdb-mongoapi', 'azure-webjobs', 'azure-billing-api', 'azure-appfabric', 'azure-cloud-shell', 'azure-sdk-for-ruby', 'azure-virtual-network', 'azure-servicebus-subscriptions', 'azure-arc', 'azure-private-dns', 'azure-static-web-app-routing', 'azureservicebus', 'azure-defender', 'azure-data-studio', 'azure-iot-hub-device-management', 'azure-site-recovery', 'azure-application-settings', 'azure-cosmosdb-mongovcore', 'azure-security-center', 'azure-web-app-firewall', 'azure-queues', 'azure-functions', 'azure-management', 'azure-sdk-for-go', 'azure-blockchain-service', 'defaultazurecredential', 'azure-acs', 'azure-functions-core-tools', 'azure-servicebusrelay', 'pulumi-azure', 'azure-sql-edge', 'azure-subscription', 'azure-managed-database', 'azure-management-api', 'azure-bastion', 'azure-iot-dps', 'azure-vm-templates', 'azure-static-website-routing', 'azure-stack', 'azure-managed-app', 'azure-private-link', 'azure-ml-component', 'azure-function-async', 'azure-cosmosdb-tables', 'azure-timeseries-insights', 'azure-video-indexer', 'azure-ai', 'azure-elastic-scale', 'azure-clouddrive', 'azure-api-apps', 'azure-service-fabric-mesh', 'azure-functions-docker', 'microsoft-custom-vision', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-app-service-envrmnt', 'azure-sdk-js', 'azure-sdk-for-java', 'azure-advisor', 'azure-function-app-proxy', 'azure-functions-proxies', 'azure-sentinel', 'azure-anomaly-detector', 'azure-container-instances', 'azure-managed-disk', 'azure-active-directory', 'azureclicredential', 'azure-webapps', 'azure-ml-pipelines', 'azure-redis-cache', 'azure-http-trigger', 'azure-dsvm', 'azureportal', 'azure-servicebus-queues', 'azure-media-services', 'azure-ase', 'azure-node-sdk', 'azure-sql-managed-instance', 'sql-azure-federations', 'azure-debugger', 'azure-service-principal', 'azure-monitor-workbooks', 'azure-web-app-for-containers', 'azure-vm', 'azure-application-insights-profiler', 'azure-cost-calculation', 'azure-mobile-engagement', 'azure-file-copy', 'azure-diagnostics', 'azure-security', 'azure-analytics', 'azure-logic-app-standard', 'azure-vm-scale-set', 'azure-java-tools', 'azure-cognitive-services', 'django-pyodbc-azure', 'azure-application-proxy', 'azure-resource-graph', 'azure-ad-b2b', 'azure-compliance-policy', 'azure-durable-functions', 'azure-database-postgresql', 'azure-promptflow', 'azure-eventhub', 'azure-tablequery', 'azure-sdk-php', 'azure-storage-queues', 'azure-service-plan', 'azure-cosmosdb-emulator', 'azure-performancecounters', 'azure-scheduler', 'azure-availability-set', 'azure-dashboard', 'azure-mysql-database', 'azure-managed-grafana', 'azure-monitoring', 'azure-worker-roles', 'azure-service-runtime', 'azure-ddos', 'azure-data-sync', 'azure-machine-learning-service', 'azure-billing', 'azure-packaging', 'azure-container-apps', 'microsoft-entra-id', 'azure-sql', 'azure-bicep', 'azure-cosmosdb', 'azure-update-management-center', 'azure-mapping-data-flow', 'azure-lab-services', 'azure-custom-providers', 'azure-sdk-.net', 'azure-autoscaling-block', 'azure-data-explorer', 'azureml-python-sdk', 'azure-pipelines-release-pipeline', 'azure-load-balancer', 'azure-managed-identity', 'azure-ad-verifiable-credentials', 'azure-webjobssdk', 'azure-agent', 'azuremlsdk', 'azure-blueprints', 'azure-vpn', 'azure-automation', 'azure-blockchain-workbench', 'azure-api-management', 'azure-rm', 'azure-application-roles', 'azure-public-ip', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-sdk-python', 'terraform-provider-azure', 'azure-marketplace', 'azure-information-protection', 'azure-analysis-services', 'azure-zulu', 'azure-batch-account', 'azure-china', 'azure-android-sdk', 'azure-rm-template', 'azure-spring-cloud', 'azure-stream-analytics', 'azure-keyvault', 'azure-oms', 'azure-ad-msal', 'azure-webhooks', 'azure-adal-deprecation', 'azure-language-understanding', 'azure-container-registry', 'azure-web-pubsub', 'azure-maps', 'azure-migrate', 'azure-dev-spaces', 'fhir-server-for-azure', 'sitecore-azure', 'azure-cosmosdb-gremlinapi', 'azure-aks', 'azure.data.tables', 'azure-java-sdk', 'azure-static-website-hosting', 'azure-mobile-services', 'azure-triggers', 'azure-ad-powershell-v2', 'azure-adf', 'azure-in-role-cache', 'azure-iot-edge', 'azure-linux', 'azure-media-player', 'azure-storage-emulator', 'azure-eventgrid', 'azure-object-anchors', 'azure-management-portal', 'azure-notebooks', 'azure-custom-domain', 'azure-xplat-cli', 'azure-runbook', 'rebus-azureservicebus', 'azure-cognitive-search', 'azure-oauth', 'azure-application-insights', 'azure-traffic-manager', 'azure-anomaly-detection', 'azure-acr', 'adal', 'azure-storage-explorer', 'azure-private-dns-zone', 'azure', 'azure-auto-ml', 'azure-iot-hub-device-update', 'azure-logic-apps', 'azure-relay', 'azure-spatial-anchors', 'azure-monitor', 'azure-load-testing', 'azure-cosmosdb-changefeed', 'azure-function-http'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}], 'account_id': 2792546, 'is_employee': False, 'last_modified_date': 1679238600, 'last_access_date': 1708658129, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3197, 'creation_date': 1369068264, 'user_type': 'registered', 'user_id': 2402501, 'accept_rate': 80, 'location': 'South Africa', 'website_url': '', 'link': 'https://stackoverflow.com/users/2402501/bryce-ramgovind', 'profile_image': 'https://i.stack.imgur.com/zOXsA.jpg?s=256&g=1', 'display_name': 'Bryce Ramgovind'}","I have to add column to a PySpark dataframe based on a list of values. I have a list called rating, which is a rating of each pet. I need to append the dataframe with a column called Rating, such that I have done the following however it is returning only the first value in the list in the Rating Column out:","a= spark.createDataFrame([(""Dog"", ""Cat""), (""Cat"", ""Dog""), (""Mouse"", ""Cat"")],[""Animal"", ""Enemy""])
 rating = [5,4,1]
 +------+-----+------+
|Animal|Enemy|Rating|
+------+-----+------+
|   Dog|  Cat|     5|
|   Cat|  Dog|     4|
| Mouse|  Cat|     1|
+------+-----+------+
 def add_labels():
    return rating.pop(0)

labels_udf = udf(add_labels, IntegerType())

new_df = a.withColumn('Rating', labels_udf()).cache()
 +------+-----+------+
|Animal|Enemy|Rating|
+------+-----+------+
|   Dog|  Cat|     5|
|   Cat|  Dog|     5|
| Mouse|  Cat|     5|
+------+-----+------+
",17,41,0,0,
425,48700710,48700903,13678,Python - Flatten the list of dictionaries,4,<python><list><dictionary><join><list-comprehension>,33,"<p>List of dictionaries:</p>

<pre><code>data = [{
         'a':{'l':'Apple',
                'b':'Milk',
                'd':'Meatball'},
         'b':{'favourite':'coke',
              'dislike':'juice'}
         },
         {
         'a':{'l':'Apple1',
                'b':'Milk1',
                'd':'Meatball2'},
         'b':{'favourite':'coke2',
              'dislike':'juice3'}
         }, ...
]
</code></pre>

<p>I need to join all nested dictionaries to reach at the expected output:</p>

<pre><code> [{'d': 'Meatball', 'b': 'Milk', 'l': 'Apple', 'dislike': 'juice', 'favourite': 'coke'},
  {'d': 'Meatball2', 'b': 'Milk1', 'l': 'Apple1', 'dislike': 'juice3', 'favourite': 'coke2'}]
</code></pre>

<p>I try nested list comprehension, but cannot join dict together:</p>

<pre><code>L = [y for x in data for y in x.values()]
print (L)

[{'d': 'Meatball', 'b': 'Milk', 'l': 'Apple'}, 
 {'dislike': 'juice', 'favourite': 'coke'}, 
{'d': 'Meatball2', 'b': 'Milk1', 'l': 'Apple1'}, 
 {'dislike': 'juice3', 'favourite': 'coke2'}]
</code></pre>

<p>I am looking for the fastest solution.</p>
",2901002,843331,09-02-2018 07:27,09-02-2018 07:40,0,844511,1288,100,1382,97,"{'badge_counts': {'bronze': 1288, 'silver': 1382, 'gold': 100}, 'account_id': 3465245, 'is_employee': False, 'last_modified_date': 1711116304, 'last_access_date': 1711133404, 'reputation_change_year': 7956, 'reputation_change_quarter': 7956, 'reputation_change_month': 2141, 'reputation_change_week': 640, 'reputation_change_day': 0, 'reputation': 844511, 'creation_date': 1382300846, 'user_type': 'registered', 'user_id': 2901002, 'accept_rate': 97, 'location': 'Bratislava, Slovakia', 'website_url': '', 'link': 'https://stackoverflow.com/users/2901002/jezrael', 'profile_image': 'https://i.stack.imgur.com/hMDvl.jpg?s=256&g=1', 'display_name': 'jezrael'}","List of dictionaries: I need to join all nested dictionaries to reach at the expected output: I try nested list comprehension, but cannot join dict together: I am looking for the fastest solution.","data = [{
         'a':{'l':'Apple',
                'b':'Milk',
                'd':'Meatball'},
         'b':{'favourite':'coke',
              'dislike':'juice'}
         },
         {
         'a':{'l':'Apple1',
                'b':'Milk1',
                'd':'Meatball2'},
         'b':{'favourite':'coke2',
              'dislike':'juice3'}
         }, ...
]
  [{'d': 'Meatball', 'b': 'Milk', 'l': 'Apple', 'dislike': 'juice', 'favourite': 'coke'},
  {'d': 'Meatball2', 'b': 'Milk1', 'l': 'Apple1', 'dislike': 'juice3', 'favourite': 'coke2'}]
 L = [y for x in data for y in x.values()]
print (L)

[{'d': 'Meatball', 'b': 'Milk', 'l': 'Apple'}, 
 {'dislike': 'juice', 'favourite': 'coke'}, 
{'d': 'Meatball2', 'b': 'Milk1', 'l': 'Apple1'}, 
 {'dislike': 'juice3', 'favourite': 'coke2'}]
",21,37,0,0,
426,48498139,48501009,19455,How to fix issues with E402?,1,<python><pep8><flake8><pycodestyle>,21,"<p>We are trying to fix issues with PEP8 E402.</p>

<p>Mostly our code is broken on:</p>

<pre><code>import os
os.environ['LIB_CAN_THROW_ERROR_ON_IMPORT'] = 2
import lib
os.environ['LIB_CAN_THROW_ERROR_ON_IMPORT'] = 0 # back
</code></pre>

<p>-</p>

<pre><code>if sys.version_info[0] &gt; 2:
    import python3lib
else:
    import python2lib
</code></pre>

<p>-</p>

<pre><code>try:
    import lib
except:
    print('lib is required')
    sys.exit(1)
</code></pre>

<p>How to solve these violations?</p>
",2420520,1704,29-01-2018 09:34,29-01-2018 12:13,0,1704,42,3,22,91,"{'badge_counts': {'bronze': 42, 'silver': 22, 'gold': 3}, 'account_id': 2815256, 'is_employee': False, 'last_modified_date': 1641670800, 'last_access_date': 1710017481, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1704, 'creation_date': 1369493791, 'user_type': 'registered', 'user_id': 2420520, 'accept_rate': 91, 'website_url': '', 'link': 'https://stackoverflow.com/users/2420520/aleskva', 'profile_image': 'https://i.stack.imgur.com/mTWj0.png?s=256&g=1', 'display_name': 'aleskva'}",We are trying to fix issues with PEP8 E402. Mostly our code is broken on: - - How to solve these violations?,"import os
os.environ['LIB_CAN_THROW_ERROR_ON_IMPORT'] = 2
import lib
os.environ['LIB_CAN_THROW_ERROR_ON_IMPORT'] = 0 # back
 if sys.version_info[0] &gt; 2:
    import python3lib
else:
    import python2lib
 try:
    import lib
except:
    print('lib is required')
    sys.exit(1)
",10,28,0,0,
427,50180735,50180784,37427,How can dataclasses be made to work better with __slots__?,6,<python><python-3.x><slots><python-dataclasses>,45,"<p>It <a href=""https://github.com/ericvsmith/dataclasses/issues/28"" rel=""noreferrer"">was decided</a> to remove direct support for <code>__slots__</code> from dataclasses for Python 3.7.</p>
<p>Despite this, <code>__slots__</code> can still be used with dataclasses:</p>
<pre><code>from dataclasses import dataclass

@dataclass
class C():
    __slots__ = &quot;x&quot;
    x: int
</code></pre>
<p>However, because of the way <code>__slots__</code> works it isn't possible to assign a default value to a dataclass field:</p>
<pre><code>from dataclasses import dataclass

@dataclass
class C():
    __slots__ = &quot;x&quot;
    x: int = 1
</code></pre>
<p>This results in an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ValueError: 'x' in __slots__ conflicts with class variable
</code></pre>
<p>How can <code>__slots__</code> and default <code>dataclass</code> fields be made to work together?</p>
",2437514,43837,04-05-2018 18:02,04-05-2018 18:04,0,43917,118,15,77,69,"{'badge_counts': {'bronze': 118, 'silver': 77, 'gold': 15}, 'account_id': 2837118, 'is_employee': False, 'last_modified_date': 1705929000, 'last_access_date': 1711137070, 'reputation_change_year': 360, 'reputation_change_quarter': 360, 'reputation_change_month': 110, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 43917, 'creation_date': 1369932661, 'user_type': 'registered', 'user_id': 2437514, 'accept_rate': 69, 'location': 'Dayton, OH, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/2437514/rick', 'profile_image': 'https://i.stack.imgur.com/8QN0K.jpg?s=256&g=1', 'display_name': 'Rick'}","It was decided to remove direct support for from dataclasses for Python 3.7. Despite this, can still be used with dataclasses: However, because of the way works it isn't possible to assign a default value to a dataclass field: This results in an error: How can and default fields be made to work together?","__slots__ __slots__ from dataclasses import dataclass

@dataclass
class C():
    __slots__ = &quot;x&quot;
    x: int
 __slots__ from dataclasses import dataclass

@dataclass
class C():
    __slots__ = &quot;x&quot;
    x: int = 1
 Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ValueError: 'x' in __slots__ conflicts with class variable
 __slots__ dataclass",7,23,0,1,
428,48183160,52481267,7661,How to pip install *.whl on Windows (using a wildcard),1,<python><pip>,13,"<p>For some reason, I cannot <code>pip install %CD%\*.whl</code> as I will then get:</p>

<pre><code>Requirement 'C:\\Users\fredrik\\Downloads\\*.whl' looks like a filename, but the file does not exist
`*.whl is not a valid wheel filename.
</code></pre>

<p>On macOS (and I believe on Linux), I can do this without issues:</p>

<pre><code>pip install *.whl
Processing ./certifi-2017.11.5-py2.py3-none-any.whl
Processing ./chardet-3.0.4-py2.py3-none-any.whl
Processing ./idna-2.6-py2.py3-none-any.whl
Processing ./requests-2.18.4-py2.py3-none-any.whl
Processing ./urllib3-1.22-py2.py3-none-any.whl
...
</code></pre>

<hr>

<ol>
<li>Why is there a difference in this behavior between the platforms?</li>
<li>Is there a preferred way to make this (<code>pip install *.whl</code>) work on Windows?</li>
</ol>
",2448495,9841,10-01-2018 08:29,24-09-2018 14:11,257,9861,139,19,77,87,"{'badge_counts': {'bronze': 139, 'silver': 77, 'gold': 19}, 'account_id': 2851341, 'is_employee': False, 'last_modified_date': 1707980653, 'last_access_date': 1710482069, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 9861, 'creation_date': 1370273090, 'user_type': 'registered', 'user_id': 2448495, 'accept_rate': 87, 'location': 'Sweden', 'website_url': 'https://fredrikaverpil.github.io', 'link': 'https://stackoverflow.com/users/2448495/fredrik', 'profile_image': 'https://www.gravatar.com/avatar/229080fa24540bf44c84ed194ae69792?s=256&d=identicon&r=PG', 'display_name': 'fredrik'}","For some reason, I cannot as I will then get: On macOS (and I believe on Linux), I can do this without issues: Why is there a difference in this behavior between the platforms? Is there a preferred way to make this () work on Windows?","pip install %CD%\*.whl Requirement 'C:\\Users\fredrik\\Downloads\\*.whl' looks like a filename, but the file does not exist
`*.whl is not a valid wheel filename.
 pip install *.whl
Processing ./certifi-2017.11.5-py2.py3-none-any.whl
Processing ./chardet-3.0.4-py2.py3-none-any.whl
Processing ./idna-2.6-py2.py3-none-any.whl
Processing ./requests-2.18.4-py2.py3-none-any.whl
Processing ./urllib3-1.22-py2.py3-none-any.whl
...
 pip install *.whl",5,23,0,0,
429,49622924,49632779,12615,Wait for timeout or event being set for asyncio.Event,2,<python><events><python-asyncio>,15,"<p>I have a class with a method that looks like this:</p>

<pre><code># self.stop_event -&gt; threading.Event
def run(self):
    while not self.stop_event.wait(3):  # i.e. repeat every 3 sec
        pass  # do stuff
</code></pre>

<p>The idea is that several of these are run in their own thread and at some point one thread does <code>stop_event.set()</code>, which naturally stops all others. I want to switch to asyncio for this, because the tasks in <code>run</code> are mostly sleeping and doing IO. Thus, I got to:</p>

<pre><code># self.stop_event -&gt; asyncio.Event
async def run(self):
    while not self.stop_event.is_set():
        await asyncio.sleep(3)
        pass  # do stuff
</code></pre>

<p>The problem is that the <code>asyncio.Event</code> cannot be waited on, so when it is set, there are at most 3 seconds to wait before the method completes. This is a problem, because the sleep time may be minutes. Currently, I am working around this by wrapping the <code>run</code> in an <code>asyncio.Task</code> and then cancelling it like <code>event_loop.call_soon(the_task.cancel)</code>.</p>

<p>I want to ask if there is a better way to achieve the above? Is there a way I can wait on an <code>asyncio.Event</code> with a timeout somehow, similar to the <code>threading.Event</code>?</p>
",2946546,345,03-04-2018 05:35,03-04-2018 14:36,0,345,9,1,4,,"{'badge_counts': {'bronze': 9, 'silver': 4, 'gold': 1}, 'account_id': 3525239, 'is_employee': False, 'last_modified_date': 1676748000, 'last_access_date': 1706465559, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 345, 'creation_date': 1383342836, 'user_type': 'registered', 'user_id': 2946546, 'link': 'https://stackoverflow.com/users/2946546/ivan-kalchev', 'profile_image': 'https://www.gravatar.com/avatar/97b43eac19a3ba6eb000668405855fb9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ivan Kalchev'}","I have a class with a method that looks like this: The idea is that several of these are run in their own thread and at some point one thread does , which naturally stops all others. I want to switch to asyncio for this, because the tasks in are mostly sleeping and doing IO. Thus, I got to: The problem is that the cannot be waited on, so when it is set, there are at most 3 seconds to wait before the method completes. This is a problem, because the sleep time may be minutes. Currently, I am working around this by wrapping the in an and then cancelling it like . I want to ask if there is a better way to achieve the above? Is there a way I can wait on an with a timeout somehow, similar to the ?","# self.stop_event -&gt; threading.Event
def run(self):
    while not self.stop_event.wait(3):  # i.e. repeat every 3 sec
        pass  # do stuff
 stop_event.set() run # self.stop_event -&gt; asyncio.Event
async def run(self):
    while not self.stop_event.is_set():
        await asyncio.sleep(3)
        pass  # do stuff
 asyncio.Event run asyncio.Task event_loop.call_soon(the_task.cancel) asyncio.Event threading.Event",-1,20,0,0,
430,48855400,48856442,36477,What does offset mean in a pandas rolling window?,2,<python><python-3.x><pandas><dataframe><datetime>,22,"<p>The rolling window function <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html"" rel=""nofollow noreferrer""><code>pandas.DataFrame.rolling</code></a> takes a <code>window</code> argument that is described as follows:</p>
<blockquote>
<p><strong>window</strong> : int, or offset</p>
<p>Size of the moving window. This is the number of observations used for
calculating the statistic. Each window will be a fixed size.</p>
<p>If its an offset then this will be the time period of each window.
Each window will be a variable sized based on the observations
included in the time-period. This is only valid for datetimelike
indexes. This is new in 0.19.0</p>
</blockquote>
<p>What actually is an <em>offset</em> in this context?</p>
",3104974,5906,18-02-2018 18:57,18-02-2018 20:51,0,5936,69,12,49,100,"{'badge_counts': {'bronze': 69, 'silver': 49, 'gold': 12}, 'collectives': [{'collective': {'tags': ['azure-synapse-link', 'azure-management-groups', 'azure-deployment-slots', 'azure-data-lake-gen2', 'azure-ad-v2', 'azure-databoxfamily', 'azure-purview', 'azure-spring-boot', 'azure-table-storage', 'azure-blob-trigger', 'azure-application-registration', 'azure-calculator', 'azure-hub', 'azure-ad-b2c-custom-policy', 'azure-batch', 'azure-form-recognizer', 'azure-app-service-plans', 'azureshell', 'azure-notificationhub', 'azure-sql-database', 'azure-workflow-automation', 'azure-storage', 'azure-static-web-app', 'azure-nsg', 'azure-databricks', 'azure-app-api', 'azure-alerts', 'azure-virtual-machine', 'azure-cli', 'azure-git-deployment', 'azure-waf', 'azure-connect', 'azure-cosmosdb-cassandra-api', 'azure-database-mysql', 'azure-iot-suite', 'azure-ad-role', 'azure-dns', 'azure-emulator', 'azure-sas', 'azure-search-.net-sdk', 'azure-availability-zones', 'azure-bot-service', 'kql', 'azure-authentication', 'sql-azure-alerts', 'azure-service-fabric', 'azure-communication-services', 'azure-metrics-advisor', 'azure-iot-central', 'azure-cloud-services', 'azure-blob-storage', 'azure-regions', 'azure-store', 'azure-feature-manager', 'azure-cdn', 'azure-ad-graph-api', 'azure-industrial-iot', 'azure-ad-b2c', 'azure-synapse', 'azure-app-registration', 'azure-remote-rendering', 'azure-rtos', 'azure-storage-account', 'azure-iot-sdk', 'azure-ai-translator', 'azure-container-service', 'azure-text-translation', 'azure-fluent-api', 'azure-rest-api', 'azure-sphere', 'azure-gov', 'azure-hybrid-connections', 'azure-quantum', 'azure-digital-twins', 'azurekinect', 'azure-postgresql', 'azure-integration-account', 'microsoft-entra-private-access', 'azure-sql-server', 'azure-servicebus-topics', 'sql-server-azure', 'azure-log-analytics-workspace', 'azure-rbac', 'azure-policy', 'azure-mcd', 'azure-sdk-go', 'spark-bash-azure-databricks', 'azure-app-configuration', 'azure-iot-hub', 'azure-role-environment', 'azure-powershell', 'azure-identity', 'microsoft-entra-external-id', 'azure-deployment', 'azure-sdk-ruby', 'azure-free-services', 'azure-webjobs-triggered', 'azureadgraph-deprecation', 'azure-data-lake', 'azure-data-factory', 'azure-service-hooks', 'azure-image-builder', 'azure-backup-vault', 'azure-data-share', 'azure-data-catalog', 'azure-storage-files', 'azure-appservice', 'azure-files', 'azure-ad-domain-services', 'adal.js', 'azure-configuration', 'azure-qna-maker', 'spring-cloud-azure', 'azurerm-app-service', 'azure-sdk', 'azure-secrets', 'azure-affinity-group', 'kitchen-azurerm', 'azure-web-roles', 'azure-cli2', 'azure-log-analytics', 'azure-hdinsight', 'azure-application-gateway', 'azure-python-sdk', 'azure-sql-reporting', 'azure-disk', 'azure-devtest-labs', 'passport-azure-ad', 'azure-elasticpool', 'azure-function-app', 'azure-signalr', 'azure-webjobs-continuous', 'azure-compute-emulator', 'azure-speech', 'microsoft-entra-internet-access', 'azure-functions-runtime', 'azure-web-app-service', 'azure-front-door', 'azure-function-queue', 'azure-resource-group', 'azure-pack', 'azure-caching', 'azure-cosmosdb-mongoapi', 'azure-webjobs', 'azure-billing-api', 'azure-appfabric', 'azure-cloud-shell', 'azure-sdk-for-ruby', 'azure-virtual-network', 'azure-servicebus-subscriptions', 'azure-arc', 'azure-private-dns', 'azure-static-web-app-routing', 'azureservicebus', 'azure-defender', 'azure-data-studio', 'azure-iot-hub-device-management', 'azure-site-recovery', 'azure-application-settings', 'azure-cosmosdb-mongovcore', 'azure-security-center', 'azure-web-app-firewall', 'azure-queues', 'azure-functions', 'azure-management', 'azure-sdk-for-go', 'azure-blockchain-service', 'defaultazurecredential', 'azure-acs', 'azure-functions-core-tools', 'azure-servicebusrelay', 'pulumi-azure', 'azure-sql-edge', 'azure-subscription', 'azure-managed-database', 'azure-management-api', 'azure-bastion', 'azure-iot-dps', 'azure-vm-templates', 'azure-static-website-routing', 'azure-stack', 'azure-managed-app', 'azure-private-link', 'azure-ml-component', 'azure-function-async', 'azure-cosmosdb-tables', 'azure-timeseries-insights', 'azure-video-indexer', 'azure-ai', 'azure-elastic-scale', 'azure-clouddrive', 'azure-api-apps', 'azure-service-fabric-mesh', 'azure-functions-docker', 'microsoft-custom-vision', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-app-service-envrmnt', 'azure-sdk-js', 'azure-sdk-for-java', 'azure-advisor', 'azure-function-app-proxy', 'azure-functions-proxies', 'azure-sentinel', 'azure-anomaly-detector', 'azure-container-instances', 'azure-managed-disk', 'azure-active-directory', 'azureclicredential', 'azure-webapps', 'azure-ml-pipelines', 'azure-redis-cache', 'azure-http-trigger', 'azure-dsvm', 'azureportal', 'azure-servicebus-queues', 'azure-media-services', 'azure-ase', 'azure-node-sdk', 'azure-sql-managed-instance', 'sql-azure-federations', 'azure-debugger', 'azure-service-principal', 'azure-monitor-workbooks', 'azure-web-app-for-containers', 'azure-vm', 'azure-application-insights-profiler', 'azure-cost-calculation', 'azure-mobile-engagement', 'azure-file-copy', 'azure-diagnostics', 'azure-security', 'azure-analytics', 'azure-logic-app-standard', 'azure-vm-scale-set', 'azure-java-tools', 'azure-cognitive-services', 'django-pyodbc-azure', 'azure-application-proxy', 'azure-resource-graph', 'azure-ad-b2b', 'azure-compliance-policy', 'azure-durable-functions', 'azure-database-postgresql', 'azure-promptflow', 'azure-eventhub', 'azure-tablequery', 'azure-sdk-php', 'azure-storage-queues', 'azure-service-plan', 'azure-cosmosdb-emulator', 'azure-performancecounters', 'azure-scheduler', 'azure-availability-set', 'azure-dashboard', 'azure-mysql-database', 'azure-managed-grafana', 'azure-monitoring', 'azure-worker-roles', 'azure-service-runtime', 'azure-ddos', 'azure-data-sync', 'azure-machine-learning-service', 'azure-billing', 'azure-packaging', 'azure-container-apps', 'microsoft-entra-id', 'azure-sql', 'azure-bicep', 'azure-cosmosdb', 'azure-update-management-center', 'azure-mapping-data-flow', 'azure-lab-services', 'azure-custom-providers', 'azure-sdk-.net', 'azure-autoscaling-block', 'azure-data-explorer', 'azureml-python-sdk', 'azure-pipelines-release-pipeline', 'azure-load-balancer', 'azure-managed-identity', 'azure-ad-verifiable-credentials', 'azure-webjobssdk', 'azure-agent', 'azuremlsdk', 'azure-blueprints', 'azure-vpn', 'azure-automation', 'azure-blockchain-workbench', 'azure-api-management', 'azure-rm', 'azure-application-roles', 'azure-public-ip', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-sdk-python', 'terraform-provider-azure', 'azure-marketplace', 'azure-information-protection', 'azure-analysis-services', 'azure-zulu', 'azure-batch-account', 'azure-china', 'azure-android-sdk', 'azure-rm-template', 'azure-spring-cloud', 'azure-stream-analytics', 'azure-keyvault', 'azure-oms', 'azure-ad-msal', 'azure-webhooks', 'azure-adal-deprecation', 'azure-language-understanding', 'azure-container-registry', 'azure-web-pubsub', 'azure-maps', 'azure-migrate', 'azure-dev-spaces', 'fhir-server-for-azure', 'sitecore-azure', 'azure-cosmosdb-gremlinapi', 'azure-aks', 'azure.data.tables', 'azure-java-sdk', 'azure-static-website-hosting', 'azure-mobile-services', 'azure-triggers', 'azure-ad-powershell-v2', 'azure-adf', 'azure-in-role-cache', 'azure-iot-edge', 'azure-linux', 'azure-media-player', 'azure-storage-emulator', 'azure-eventgrid', 'azure-object-anchors', 'azure-management-portal', 'azure-notebooks', 'azure-custom-domain', 'azure-xplat-cli', 'azure-runbook', 'rebus-azureservicebus', 'azure-cognitive-search', 'azure-oauth', 'azure-application-insights', 'azure-traffic-manager', 'azure-anomaly-detection', 'azure-acr', 'adal', 'azure-storage-explorer', 'azure-private-dns-zone', 'azure', 'azure-auto-ml', 'azure-iot-hub-device-update', 'azure-logic-apps', 'azure-relay', 'azure-spatial-anchors', 'azure-monitor', 'azure-load-testing', 'azure-cosmosdb-changefeed', 'azure-function-http'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}], 'account_id': 3733298, 'is_employee': False, 'last_modified_date': 1678021202, 'last_access_date': 1710709734, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5936, 'creation_date': 1387128810, 'user_type': 'registered', 'user_id': 3104974, 'accept_rate': 100, 'location': 'Stuttgart, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/3104974/ascripter', 'profile_image': 'https://www.gravatar.com/avatar/f1f930f18633137ee58fa9232928e678?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ascripter'}","The rolling window function takes a argument that is described as follows: window : int, or offset Size of the moving window. This is the number of observations used for calculating the statistic. Each window will be a fixed size. If its an offset then this will be the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. This is new in 0.19.0 What actually is an offset in this context?",pandas.DataFrame.rolling window,-2,11,0,1,
431,48491737,48506964,16551,Understanding Keras LSTMs: Role of Batch-size and Statefulness,1,<python><keras><lstm><recurrent-neural-network>,38,"<h2>Sources</h2>
<p>There are several sources out there explaining stateful / stateless LSTMs and the role of batch_size which I've read already. I'll refer to them later in my post:</p>
<p>[<a href=""https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">1</a>] <a href=""https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/</a></p>
<p>[<a href=""https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/"" rel=""noreferrer"">2</a>] <a href=""https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/"" rel=""noreferrer"">https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/</a></p>
<p>[<a href=""http://philipperemy.github.io/keras-stateful-lstm/"" rel=""noreferrer"">3</a>] <a href=""http://philipperemy.github.io/keras-stateful-lstm/"" rel=""noreferrer"">http://philipperemy.github.io/keras-stateful-lstm/</a></p>
<p>[<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>] <a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/</a></p>
<p>And also other SO threads like <a href=""https://stackoverflow.com/questions/38714959/understanding-keras-lstms"">Understanding Keras LSTMs</a> and <a href=""https://stackoverflow.com/questions/39681046/keras-stateful-vs-stateless-lstms"">Keras - stateful vs stateless LSTMs</a> which didn't fully explain what I'm looking for however.</p>
<hr />
<h2>My Problem</h2>
<p>I am still not sure what is the correct approach for my task regarding statefulness and determining batch_size.</p>
<p>I have about 1000 independent time series (<code>samples</code>) that have a length of about 600 days (<code>timesteps</code>) each (actually variable length, but I thought about trimming the data to a constant timeframe) with 8 features (or <code>input_dim</code>) for each timestep (some of the features are identical to every sample, some individual per sample).</p>
<p><code>Input shape = (1000, 600, 8)</code></p>
<p>One of the features is the one I want to predict, while the others are (supposed to be) supportive for the prediction of this one “master feature”. I will do that for each of the 1000 time series. What would be the best strategy to model this problem?</p>
<p><code>Output shape = (1000, 600, 1)</code></p>
<hr />
<h2>What is a Batch?</h2>
<p>From [<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>]:</p>
<blockquote>
<p>Keras uses fast symbolic mathematical libraries as a backend, such as TensorFlow and Theano.</p>
<p>A downside of using these libraries is that the shape and size of your data must be defined once up front and held constant regardless of whether you are training your network or making predictions.</p>
<p>[…]</p>
<p>This does become a problem when you wish to make fewer predictions than the batch size. For example, you may get the best results with a large batch size, but are required to make predictions for one observation at a time on something like a time series or sequence problem.</p>
</blockquote>
<p>This sounds to me like a “batch” would be splitting the data along the <code>timesteps</code>-dimension.</p>
<p>However, [<a href=""http://philipperemy.github.io/keras-stateful-lstm/"" rel=""noreferrer"">3</a>] states that:</p>
<blockquote>
<p>Said differently, whenever you train or test your LSTM, you first have to build your input matrix <code>X</code> of shape <code>nb_samples, timesteps, input_dim</code> where your batch size divides <code>nb_samples</code>. For instance, if <code>nb_samples=1024</code> and <code>batch_size=64</code>, it means that your model will receive blocks of 64 samples, compute each output (whatever the number of timesteps is for every sample), average the gradients and propagate it to update the parameters vector.</p>
</blockquote>
<p>When looking deeper into the examples of [<a href=""https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">1</a>] and [<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>], Jason is always splitting his time series to several samples that only contain 1 timestep (the predecessor that in his example fully determines the next element in the sequence). So I think the batches are really split along the <code>samples</code>-axis. (However his approach of time series splitting doesn’t make sense to me for a long-term dependency problem.)</p>
<p><strong>Conclusion</strong></p>
<p>So let’s say I pick <code>batch_size=10</code>, that means during one epoch the weights are updated 1000 / 10 = 100 times with 10 randomly picked, complete time series containing 600 x 8 values, and when I later want to make predictions with the model, I’ll always have to feed it batches of 10 complete time series (or use <em>solution 3</em> from [<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>], copying the weights to a new model with different batch_size).</p>
<p>Principles of batch_size understood – <em>however still not knowing what would be a <strong>good value for batch_size.</strong> and how to determine it</em></p>
<hr />
<h2>Statefulness</h2>
<p>The <a href=""https://keras.io/layers/recurrent/#rnn"" rel=""noreferrer"">KERAS documentation</a> tells us</p>
<blockquote>
<p>You can set RNN layers to be 'stateful', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch.</p>
</blockquote>
<p><em><strong>If I’m splitting my time series</strong> into several <code>samples</code> (like in the examples of [<a href=""https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">1</a>] and [<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>]) so that the dependencies I’d like to model span across several batches, <strong>or the batch-spanning samples are otherwise correlated</strong> with each other, I may <strong>need a stateful net</strong>, otherwise not. Is that a correct and complete conclusion?</em></p>
<p>So for my problem I suppose I won’t need a stateful net. I’d build my training data as a 3D array of the shape <code>(samples, timesteps, features)</code> and then call <code>model.fit</code> with a batch_size yet to determine. Sample code could look like:</p>
<pre><code>model = Sequential()
model.add(LSTM(32, input_shape=(600, 8)))   # (timesteps, features)
model.add(LSTM(32))
model.add(LSTM(32))
model.add(LSTM(32))
model.add(Dense(1, activation='linear'))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)
</code></pre>
",3104974,5906,28-01-2018 21:53,29-01-2018 17:37,1,5936,69,12,49,100,"{'badge_counts': {'bronze': 69, 'silver': 49, 'gold': 12}, 'collectives': [{'collective': {'tags': ['azure-synapse-link', 'azure-management-groups', 'azure-deployment-slots', 'azure-data-lake-gen2', 'azure-ad-v2', 'azure-databoxfamily', 'azure-purview', 'azure-spring-boot', 'azure-table-storage', 'azure-blob-trigger', 'azure-application-registration', 'azure-calculator', 'azure-hub', 'azure-ad-b2c-custom-policy', 'azure-batch', 'azure-form-recognizer', 'azure-app-service-plans', 'azureshell', 'azure-notificationhub', 'azure-sql-database', 'azure-workflow-automation', 'azure-storage', 'azure-static-web-app', 'azure-nsg', 'azure-databricks', 'azure-app-api', 'azure-alerts', 'azure-virtual-machine', 'azure-cli', 'azure-git-deployment', 'azure-waf', 'azure-connect', 'azure-cosmosdb-cassandra-api', 'azure-database-mysql', 'azure-iot-suite', 'azure-ad-role', 'azure-dns', 'azure-emulator', 'azure-sas', 'azure-search-.net-sdk', 'azure-availability-zones', 'azure-bot-service', 'kql', 'azure-authentication', 'sql-azure-alerts', 'azure-service-fabric', 'azure-communication-services', 'azure-metrics-advisor', 'azure-iot-central', 'azure-cloud-services', 'azure-blob-storage', 'azure-regions', 'azure-store', 'azure-feature-manager', 'azure-cdn', 'azure-ad-graph-api', 'azure-industrial-iot', 'azure-ad-b2c', 'azure-synapse', 'azure-app-registration', 'azure-remote-rendering', 'azure-rtos', 'azure-storage-account', 'azure-iot-sdk', 'azure-ai-translator', 'azure-container-service', 'azure-text-translation', 'azure-fluent-api', 'azure-rest-api', 'azure-sphere', 'azure-gov', 'azure-hybrid-connections', 'azure-quantum', 'azure-digital-twins', 'azurekinect', 'azure-postgresql', 'azure-integration-account', 'microsoft-entra-private-access', 'azure-sql-server', 'azure-servicebus-topics', 'sql-server-azure', 'azure-log-analytics-workspace', 'azure-rbac', 'azure-policy', 'azure-mcd', 'azure-sdk-go', 'spark-bash-azure-databricks', 'azure-app-configuration', 'azure-iot-hub', 'azure-role-environment', 'azure-powershell', 'azure-identity', 'microsoft-entra-external-id', 'azure-deployment', 'azure-sdk-ruby', 'azure-free-services', 'azure-webjobs-triggered', 'azureadgraph-deprecation', 'azure-data-lake', 'azure-data-factory', 'azure-service-hooks', 'azure-image-builder', 'azure-backup-vault', 'azure-data-share', 'azure-data-catalog', 'azure-storage-files', 'azure-appservice', 'azure-files', 'azure-ad-domain-services', 'adal.js', 'azure-configuration', 'azure-qna-maker', 'spring-cloud-azure', 'azurerm-app-service', 'azure-sdk', 'azure-secrets', 'azure-affinity-group', 'kitchen-azurerm', 'azure-web-roles', 'azure-cli2', 'azure-log-analytics', 'azure-hdinsight', 'azure-application-gateway', 'azure-python-sdk', 'azure-sql-reporting', 'azure-disk', 'azure-devtest-labs', 'passport-azure-ad', 'azure-elasticpool', 'azure-function-app', 'azure-signalr', 'azure-webjobs-continuous', 'azure-compute-emulator', 'azure-speech', 'microsoft-entra-internet-access', 'azure-functions-runtime', 'azure-web-app-service', 'azure-front-door', 'azure-function-queue', 'azure-resource-group', 'azure-pack', 'azure-caching', 'azure-cosmosdb-mongoapi', 'azure-webjobs', 'azure-billing-api', 'azure-appfabric', 'azure-cloud-shell', 'azure-sdk-for-ruby', 'azure-virtual-network', 'azure-servicebus-subscriptions', 'azure-arc', 'azure-private-dns', 'azure-static-web-app-routing', 'azureservicebus', 'azure-defender', 'azure-data-studio', 'azure-iot-hub-device-management', 'azure-site-recovery', 'azure-application-settings', 'azure-cosmosdb-mongovcore', 'azure-security-center', 'azure-web-app-firewall', 'azure-queues', 'azure-functions', 'azure-management', 'azure-sdk-for-go', 'azure-blockchain-service', 'defaultazurecredential', 'azure-acs', 'azure-functions-core-tools', 'azure-servicebusrelay', 'pulumi-azure', 'azure-sql-edge', 'azure-subscription', 'azure-managed-database', 'azure-management-api', 'azure-bastion', 'azure-iot-dps', 'azure-vm-templates', 'azure-static-website-routing', 'azure-stack', 'azure-managed-app', 'azure-private-link', 'azure-ml-component', 'azure-function-async', 'azure-cosmosdb-tables', 'azure-timeseries-insights', 'azure-video-indexer', 'azure-ai', 'azure-elastic-scale', 'azure-clouddrive', 'azure-api-apps', 'azure-service-fabric-mesh', 'azure-functions-docker', 'microsoft-custom-vision', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-app-service-envrmnt', 'azure-sdk-js', 'azure-sdk-for-java', 'azure-advisor', 'azure-function-app-proxy', 'azure-functions-proxies', 'azure-sentinel', 'azure-anomaly-detector', 'azure-container-instances', 'azure-managed-disk', 'azure-active-directory', 'azureclicredential', 'azure-webapps', 'azure-ml-pipelines', 'azure-redis-cache', 'azure-http-trigger', 'azure-dsvm', 'azureportal', 'azure-servicebus-queues', 'azure-media-services', 'azure-ase', 'azure-node-sdk', 'azure-sql-managed-instance', 'sql-azure-federations', 'azure-debugger', 'azure-service-principal', 'azure-monitor-workbooks', 'azure-web-app-for-containers', 'azure-vm', 'azure-application-insights-profiler', 'azure-cost-calculation', 'azure-mobile-engagement', 'azure-file-copy', 'azure-diagnostics', 'azure-security', 'azure-analytics', 'azure-logic-app-standard', 'azure-vm-scale-set', 'azure-java-tools', 'azure-cognitive-services', 'django-pyodbc-azure', 'azure-application-proxy', 'azure-resource-graph', 'azure-ad-b2b', 'azure-compliance-policy', 'azure-durable-functions', 'azure-database-postgresql', 'azure-promptflow', 'azure-eventhub', 'azure-tablequery', 'azure-sdk-php', 'azure-storage-queues', 'azure-service-plan', 'azure-cosmosdb-emulator', 'azure-performancecounters', 'azure-scheduler', 'azure-availability-set', 'azure-dashboard', 'azure-mysql-database', 'azure-managed-grafana', 'azure-monitoring', 'azure-worker-roles', 'azure-service-runtime', 'azure-ddos', 'azure-data-sync', 'azure-machine-learning-service', 'azure-billing', 'azure-packaging', 'azure-container-apps', 'microsoft-entra-id', 'azure-sql', 'azure-bicep', 'azure-cosmosdb', 'azure-update-management-center', 'azure-mapping-data-flow', 'azure-lab-services', 'azure-custom-providers', 'azure-sdk-.net', 'azure-autoscaling-block', 'azure-data-explorer', 'azureml-python-sdk', 'azure-pipelines-release-pipeline', 'azure-load-balancer', 'azure-managed-identity', 'azure-ad-verifiable-credentials', 'azure-webjobssdk', 'azure-agent', 'azuremlsdk', 'azure-blueprints', 'azure-vpn', 'azure-automation', 'azure-blockchain-workbench', 'azure-api-management', 'azure-rm', 'azure-application-roles', 'azure-public-ip', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-sdk-python', 'terraform-provider-azure', 'azure-marketplace', 'azure-information-protection', 'azure-analysis-services', 'azure-zulu', 'azure-batch-account', 'azure-china', 'azure-android-sdk', 'azure-rm-template', 'azure-spring-cloud', 'azure-stream-analytics', 'azure-keyvault', 'azure-oms', 'azure-ad-msal', 'azure-webhooks', 'azure-adal-deprecation', 'azure-language-understanding', 'azure-container-registry', 'azure-web-pubsub', 'azure-maps', 'azure-migrate', 'azure-dev-spaces', 'fhir-server-for-azure', 'sitecore-azure', 'azure-cosmosdb-gremlinapi', 'azure-aks', 'azure.data.tables', 'azure-java-sdk', 'azure-static-website-hosting', 'azure-mobile-services', 'azure-triggers', 'azure-ad-powershell-v2', 'azure-adf', 'azure-in-role-cache', 'azure-iot-edge', 'azure-linux', 'azure-media-player', 'azure-storage-emulator', 'azure-eventgrid', 'azure-object-anchors', 'azure-management-portal', 'azure-notebooks', 'azure-custom-domain', 'azure-xplat-cli', 'azure-runbook', 'rebus-azureservicebus', 'azure-cognitive-search', 'azure-oauth', 'azure-application-insights', 'azure-traffic-manager', 'azure-anomaly-detection', 'azure-acr', 'adal', 'azure-storage-explorer', 'azure-private-dns-zone', 'azure', 'azure-auto-ml', 'azure-iot-hub-device-update', 'azure-logic-apps', 'azure-relay', 'azure-spatial-anchors', 'azure-monitor', 'azure-load-testing', 'azure-cosmosdb-changefeed', 'azure-function-http'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}], 'account_id': 3733298, 'is_employee': False, 'last_modified_date': 1678021202, 'last_access_date': 1710709734, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5936, 'creation_date': 1387128810, 'user_type': 'registered', 'user_id': 3104974, 'accept_rate': 100, 'location': 'Stuttgart, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/3104974/ascripter', 'profile_image': 'https://www.gravatar.com/avatar/f1f930f18633137ee58fa9232928e678?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ascripter'}","Sources There are several sources out there explaining stateful / stateless LSTMs and the role of batch_size which I've read already. I'll refer to them later in my post: [1] https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/ [2] https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/ [3] http://philipperemy.github.io/keras-stateful-lstm/ [4] https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/ And also other SO threads like Understanding Keras LSTMs and Keras - stateful vs stateless LSTMs which didn't fully explain what I'm looking for however. My Problem I am still not sure what is the correct approach for my task regarding statefulness and determining batch_size. I have about 1000 independent time series () that have a length of about 600 days () each (actually variable length, but I thought about trimming the data to a constant timeframe) with 8 features (or ) for each timestep (some of the features are identical to every sample, some individual per sample). One of the features is the one I want to predict, while the others are (supposed to be) supportive for the prediction of this one “master feature”. I will do that for each of the 1000 time series. What would be the best strategy to model this problem? What is a Batch? From [4]: Keras uses fast symbolic mathematical libraries as a backend, such as TensorFlow and Theano. A downside of using these libraries is that the shape and size of your data must be defined once up front and held constant regardless of whether you are training your network or making predictions. […] This does become a problem when you wish to make fewer predictions than the batch size. For example, you may get the best results with a large batch size, but are required to make predictions for one observation at a time on something like a time series or sequence problem. This sounds to me like a “batch” would be splitting the data along the -dimension. However, [3] states that: Said differently, whenever you train or test your LSTM, you first have to build your input matrix of shape where your batch size divides . For instance, if and , it means that your model will receive blocks of 64 samples, compute each output (whatever the number of timesteps is for every sample), average the gradients and propagate it to update the parameters vector. When looking deeper into the examples of [1] and [4], Jason is always splitting his time series to several samples that only contain 1 timestep (the predecessor that in his example fully determines the next element in the sequence). So I think the batches are really split along the -axis. (However his approach of time series splitting doesn’t make sense to me for a long-term dependency problem.) Conclusion So let’s say I pick , that means during one epoch the weights are updated 1000 / 10 = 100 times with 10 randomly picked, complete time series containing 600 x 8 values, and when I later want to make predictions with the model, I’ll always have to feed it batches of 10 complete time series (or use solution 3 from [4], copying the weights to a new model with different batch_size). Principles of batch_size understood – however still not knowing what would be a good value for batch_size. and how to determine it Statefulness The KERAS documentation tells us You can set RNN layers to be 'stateful', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch. If I’m splitting my time series into several (like in the examples of [1] and [4]) so that the dependencies I’d like to model span across several batches, or the batch-spanning samples are otherwise correlated with each other, I may need a stateful net, otherwise not. Is that a correct and complete conclusion? So for my problem I suppose I won’t need a stateful net. I’d build my training data as a 3D array of the shape and then call with a batch_size yet to determine. Sample code could look like:","samples timesteps input_dim Input shape = (1000, 600, 8) Output shape = (1000, 600, 1) timesteps X nb_samples, timesteps, input_dim nb_samples nb_samples=1024 batch_size=64 samples batch_size=10 samples (samples, timesteps, features) model.fit model = Sequential()
model.add(LSTM(32, input_shape=(600, 8)))   # (timesteps, features)
model.add(LSTM(32))
model.add(LSTM(32))
model.add(LSTM(32))
model.add(Dense(1, activation='linear'))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)
",-9,49,0,18,
432,48792664,48827581,33683,ImportError: No module named google.oauth2,7,<python><google-app-engine><google-cloud-platform><google-cloud-sdk>,18,"<p>I wanted to add a google.cloud.storage dependency to my project so I tried to install this dependency with</p>

<p><code>pip install --upgrade google-cloud-storage</code></p>

<p>Running my app again with dev_appserver, it shows me that my gcloud components needed to be updated. Ok so, <code>gcloud components update</code></p>

<p>And in my <code>src/__init__.py</code> file, I got the code that tells gcloud in which folder to look for dependencies like this:</p>

<pre><code>from google.appengine.ext import vendor

vendor.add('src/libs')
</code></pre>

<p>All the dependencies are installed correctly, except that I'm getting the error <code>ImportError: No module named google.oauth2</code></p>

<p>PS: My app is using OAuth2 to secure access to the API. And it was working correctly before I do a components update, now even if I rollback code, remove the libs folder and install again dependencies, I still got the No module error, and it seems like dev_appserver is not looking for that dependency inside the libs folder !</p>

<p>Here's the result of <code>gcloud --version</code>:</p>

<pre><code>Google Cloud SDK 188.0.1
app-engine-python 1.9.66
app-engine-python-extras 1.9.63
bq 2.0.28
core 2018.02.08
gsutil 4.28
</code></pre>

<p>And here's the Traceback:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/login/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 240, in Handle
    handler = _config_handle.add_wsgi_middleware(self._LoadHandler())
  File ""/home/login/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 299, in _LoadHandler
    handler, path, err = LoadObject(self._handler)
  File ""/home/login/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 96, in LoadObject
    __import__(cumulative_path)
  File ""/home/headless/Documents/Projects/meterFleet/app-backend/src/main.py"", line 5, in &lt;module&gt;
    from src.app.user.api import UserApi
  File ""/home/headless/Documents/Projects/meterFleet/app-backend/src/app/user/api.py"", line 7, in &lt;module&gt;
    from src.googleapis.iam import getIamPolicy, addIapUser, deleteIapUser
  File ""/home/headless/Documents/Projects/meterFleet/app-backend/src/googleapis/iam.py"", line 5, in &lt;module&gt;
    from src.common.authentication import OAuth
  File ""/home/headless/Documents/Projects/meterFleet/app-backend/src/common/authentication.py"", line 3, in &lt;module&gt;
    from google.oauth2 import service_account
  File ""/home/login/google-cloud-sdk/platform/google_appengine/google/appengine/tools/devappserver2/python/runtime/sandbox.py"", line 1147, in load_module
    raise ImportError('No module named %s' % fullname)
ImportError: No module named google.oauth2
</code></pre>
",2511384,3778,14-02-2018 17:07,16-02-2018 13:13,2,3778,46,3,23,67,"{'badge_counts': {'bronze': 46, 'silver': 23, 'gold': 3}, 'collectives': [{'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 2931211, 'is_employee': False, 'last_modified_date': 1707246601, 'last_access_date': 1709903962, 'reputation_change_year': 34, 'reputation_change_quarter': 34, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3778, 'creation_date': 1371895415, 'user_type': 'registered', 'user_id': 2511384, 'accept_rate': 67, 'location': 'Paris, France', 'website_url': 'https://medium.com/@benmohamehdi', 'link': 'https://stackoverflow.com/users/2511384/mehdi-benmoha', 'profile_image': 'https://i.stack.imgur.com/4imlQ.png?s=256&g=1', 'display_name': 'Mehdi Benmoha'}","I wanted to add a google.cloud.storage dependency to my project so I tried to install this dependency with Running my app again with dev_appserver, it shows me that my gcloud components needed to be updated. Ok so, And in my file, I got the code that tells gcloud in which folder to look for dependencies like this: All the dependencies are installed correctly, except that I'm getting the error PS: My app is using OAuth2 to secure access to the API. And it was working correctly before I do a components update, now even if I rollback code, remove the libs folder and install again dependencies, I still got the No module error, and it seems like dev_appserver is not looking for that dependency inside the libs folder ! Here's the result of : And here's the Traceback:","pip install --upgrade google-cloud-storage gcloud components update src/__init__.py from google.appengine.ext import vendor

vendor.add('src/libs')
 ImportError: No module named google.oauth2 gcloud --version Google Cloud SDK 188.0.1
app-engine-python 1.9.66
app-engine-python-extras 1.9.63
bq 2.0.28
core 2018.02.08
gsutil 4.28
 Traceback (most recent call last):
  File ""/home/login/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 240, in Handle
    handler = _config_handle.add_wsgi_middleware(self._LoadHandler())
  File ""/home/login/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 299, in _LoadHandler
    handler, path, err = LoadObject(self._handler)
  File ""/home/login/google-cloud-sdk/platform/google_appengine/google/appengine/runtime/wsgi.py"", line 96, in LoadObject
    __import__(cumulative_path)
  File ""/home/headless/Documents/Projects/meterFleet/app-backend/src/main.py"", line 5, in &lt;module&gt;
    from src.app.user.api import UserApi
  File ""/home/headless/Documents/Projects/meterFleet/app-backend/src/app/user/api.py"", line 7, in &lt;module&gt;
    from src.googleapis.iam import getIamPolicy, addIapUser, deleteIapUser
  File ""/home/headless/Documents/Projects/meterFleet/app-backend/src/googleapis/iam.py"", line 5, in &lt;module&gt;
    from src.common.authentication import OAuth
  File ""/home/headless/Documents/Projects/meterFleet/app-backend/src/common/authentication.py"", line 3, in &lt;module&gt;
    from google.oauth2 import service_account
  File ""/home/login/google-cloud-sdk/platform/google_appengine/google/appengine/tools/devappserver2/python/runtime/sandbox.py"", line 1147, in load_module
    raise ImportError('No module named %s' % fullname)
ImportError: No module named google.oauth2
",19,48,0,0,
433,48971121,48971158,16047,What is the difference between .Semaphore() and .BoundedSemaphore()?,3,<python><multithreading><mutex><semaphore><python-multithreading>,28,"<p>I know that <code>threading.Lock()</code> is equal to <code>threading.Semaphore(1)</code>.</p>
<p>Is also <code>threading.Lock()</code> equal to <code>threading.BoundedSemaphore(1)</code> ?</p>
<p>And newly I saw <code>threading.BoundedSemaphore()</code>, what is the difference between them? For example in the following code snippet (applying limitation on threads):</p>
<pre><code>import threading

sem = threading.Semaphore(5)
sem = threading.BoundedSemaphore(5)
</code></pre>
",3702377,30139,25-02-2018 07:08,25-02-2018 07:12,0,30296,163,28,147,90,"{'badge_counts': {'bronze': 163, 'silver': 147, 'gold': 28}, 'collectives': [{'collective': {'tags': ['sentiment-analysis', 'topic-modeling', 'opennlp', 'named-entity-recognition', 'word-embedding', 'nlp', 'tf-idf', 'gensim', 'spacy', 'word2vec', 'bert-language-model', 'nlp-question-answering', 'nltk', 'huggingface-transformers', 'stanford-nlp', 'spacy-3'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective focused on NLP (natural language processing), the transformation or extraction of useful information from natural language data.', 'link': '/collectives/nlp', 'name': 'NLP', 'slug': 'nlp'}, 'role': 'member'}, {'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}, {'collective': {'tags': ['amazon-elastic-beanstalk', 'aws-fargate', 'aws-sam', 'aws-codecommit', 'amazon-glacier', 'amazon-ami', 'aws-security-hub', 'alexa-sdk-nodejs', 'amazon-iam', 'amazon-guardduty', 'aws-glue', 'aws-sdk-ruby', 'amazon-cloudfront', 'aws-batch', 'aws-mediatailor', 'aws-global-accelerator', 'amazon-neptune', 'aws-sdk-go-v2', 'amazon-dynamodb-dax', 'aws-vpn', 'amazon-sumerian', 'aws-ssm', 'amazon-route53', 'aws-app-config', 'aws-sdk-ios', 'aws-cli', 'aws-app-mesh', 'aws-event-bridge', 'aws-directory-services', 'amazon-web-services', 'aws-copilot-cli', 'aws-transfer-family', 'aws-parameter-store', 'amazon-imagebuilder', 'amazon-sagemaker', 'amazon-workdocs', 'amazon-keyspaces', 'amazon-ecr', 'amazon-elb', 'aws-cloudformation', 'aws-config', 'aws-snowball', 'aws-sdk-mock', 'amazon-app-runner', 'aws-iot-analytics', 'amazon-sns', 'amazon-memory-db', 'aws-pinpoint', 'aws-deeplens', 'amazon-elasticache', 'aws-mediastore', 'amazon-bedrock', 'amazon-athena', 'amazon-gamelift', 'aws-codecatalyst', 'aws-graviton', 'aws-codeartifact', 'aws-elb', 'aws-cloudmap', 'aws-codeguru', 'amazon-translate', 'aws-sdk-java', 'aws-resource-group', 'amazon-data-pipeline', 'aws-sdk-js', 'amazon-ivs', 'aws-sdk-cpp', 'amazon-kinesis-analytics', 'amazon-cloudwatch', 'amazon-cloudhsm', 'aws-iot-sitewise', 'amazon-vpc', 'alexa-smart-home-skill', 'amazon-kendra', 'amazon-inspector', 'aws-datasync', 'aws-cloud9', 'amazon-ecs', 'amazon-rekognition', 'amazon-swf', 'aws-media-live', 'aws-sdk-js-v3', 'amazon-fsx', 'amazon-s3-select', 'aws-sdk-nodejs', 'aws-iam-identity-center', 'aws-chatbot', 'amazon-opensearch', 'aws-lambda', 'aws-lake-formation', 'aws-cdk', 'amazon-ses', 'aws-security-group', 'aws-mediapackage', 'amazon-connect', 'amazon-qldb', 'aws-iot-core', 'aws-sdk-rust', 'amazon-elastic-transcoder', 'aws-code-deploy', 'aws-serverless', 'amazon-honeycode', 'amazon-ec2', 'alexa-interaction-model', 'aws-xray', 'amazon-waf', 'aws-elemental', 'amazon-sqs', 'amazon-kms', 'aws-certificate-manager', 'aws-sam-cli', 'amazon-kinesis-video-streams', 'aws-lambda-powertools', 'amazon-ec2-spot-market', 'aws-documentdb', 'aws-control-tower', 'aws-service-catalog', 'aws-direct-connect', 'aws-billing', 'aws-iot', 'amazon-cloudwatchlogs', 'amazon-textract', 'alexa-sdk-python', 'aws-lambda-edge', 'amazon-dynamodb', 'amazon-rds', 'aws-iot-events', 'alexa-smapi', 'alexa-flash-briefing-skill', 'aws-dms', 'aws-mediaconnect', 'aws-organizations', 'amazon-macie', 'aws-sdk-comprehend', 'aws-device-farm', 'amazon-redshift-spectrum', 'aws-appsync', 'alexa-account-linking', 'amazon-transcribe', 'aws-acm', 'aws-opsworks', 'aws-step-functions', 'amazon-simpledb', 'amazon-lightsail', 'alexa-presentation-language', 'aws-amplify', 'amazon-workspaces', 'amazon-aurora', 'elastic-ip', 'aws-codepipeline', 'amazon-managed-blockchain', 'aws-application-load-balancer', 'amazon-forecast', 'aws-cloudshell', 'aws-mobilehub', 'aws-reserved-instances', 'amazon-efs', 'aws-sdk', 'aws-backup', 'amazon-timestream', 'amazon-cloudtrail', 'aws-sdk-go', 'amazon-appflow', 'amazon-emr', 'amazon-elasticsearch', 'aws-iot-greengrass', 'aws-sct', 'aws-private-link', 'amazon-quicksight', 'aws-fis', 'aws-sdk-net', 'alexa-skills-kit', 'amazon-kinesis-firehose', 'aws-sdk-java-2.0', 'amazon-ebs', 'aws-codestar', 'aws-sdk-android', 'aws-appstream', 'amazon-s3', 'amazon-lex', 'amazon-cloudsearch', 'aws-databrew', 'amazon-cognito', 'aws-elastictranscoder', 'amazon-workmail', 'amazon-comprehend', 'aws-auto-scaling', 'aws-codebuild', 'aws-api-gateway', 'aws-sso', 'amazon-eks', 'aws-storage-gateway', 'amazon-mq', 'aws-data-exchange', 'amazon-location-service', 'amazon-kinesis', 'amazon-sagemaker-compilers', 'aws-secrets-manager', 'aws-msk', 'amazon-personalize', 'aws-nlb', 'amazon-redshift', 'aws-copilot', 'aws-media-convert', 'amazon-polly'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 4559679, 'is_employee': False, 'last_modified_date': 1708981816, 'last_access_date': 1711123879, 'reputation_change_year': 923, 'reputation_change_quarter': 923, 'reputation_change_month': 287, 'reputation_change_week': 99, 'reputation_change_day': 0, 'reputation': 30296, 'creation_date': 1401786328, 'user_type': 'registered', 'user_id': 3702377, 'accept_rate': 90, 'location': 'Tehran Province, Iran', 'website_url': 'https://www.linkedin.com/in/benyaminjmf', 'link': 'https://stackoverflow.com/users/3702377/benyamin-jafari', 'profile_image': 'https://i.stack.imgur.com/HWz1T.jpg?s=256&g=1', 'display_name': 'Benyamin Jafari'}","I know that is equal to . Is also equal to ? And newly I saw , what is the difference between them? For example in the following code snippet (applying limitation on threads):","threading.Lock() threading.Semaphore(1) threading.Lock() threading.BoundedSemaphore(1) threading.BoundedSemaphore() import threading

sem = threading.Semaphore(5)
sem = threading.BoundedSemaphore(5)
",-2,8,0,0,
434,48464565,48590509,7524,Share a dictionary of pandas dataframe across multiprocessing python,1,<python><pandas><dictionary><multiprocessing>,18,"<p>I have a dictionary of python pandas dataframes. The total size of this dictionary is about 2GB. However, when I share it across 16 multiprocessing (in the subprocesses I only read the data of the dict  without modifying it), it takes 32GB ram. So I would like to ask if it is possible for me to share this dictionary across multiprocessing without copying it. I tried to convert it to manager.dict(). But it seems it takes too long. What would be the most standard way to achieve this? Thank you.</p>
",3707564,1842,26-01-2018 15:36,02-02-2018 20:35,7,1842,51,8,30,79,"{'badge_counts': {'bronze': 51, 'silver': 30, 'gold': 8}, 'account_id': 3382713, 'is_employee': False, 'last_modified_date': 1707529500, 'last_access_date': 1710464516, 'reputation_change_year': 4, 'reputation_change_quarter': 4, 'reputation_change_month': -50, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1842, 'creation_date': 1401893262, 'user_type': 'registered', 'user_id': 3707564, 'accept_rate': 79, 'link': 'https://stackoverflow.com/users/3707564/user40780', 'profile_image': 'https://www.gravatar.com/avatar/6789da87cc115cbd19413540aad9efac?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user40780'}","I have a dictionary of python pandas dataframes. The total size of this dictionary is about 2GB. However, when I share it across 16 multiprocessing (in the subprocesses I only read the data of the dict without modifying it), it takes 32GB ram. So I would like to ask if it is possible for me to share this dictionary across multiprocessing without copying it. I tried to convert it to manager.dict(). But it seems it takes too long. What would be the most standard way to achieve this? Thank you.",,0,1,0,0,
435,49017178,49017309,25836,Pandas DataFrame.groupby() to dictionary with multiple columns for value,3,<python><pandas><dictionary><dataframe><jupyter>,23,"<pre><code>type(Table)
pandas.core.frame.DataFrame

Table
======= ======= =======
Column1 Column2 Column3
0       23      1
1       5       2
1       2       3
1       19      5
2       56      1
2       22      2
3       2       4
3       14      5
4       59      1
5       44      1
5       1       2
5       87      3
</code></pre>

<p>For anyone familliar with pandas how would I build a multivalue dictionary with the <code>.groupby()</code> method?</p>

<p>I would like an output to resemble this format: </p>

<pre><code>{
    0: [(23,1)]
    1: [(5,  2), (2, 3), (19, 5)]
    # etc...
    }
</code></pre>

<p>where <code>Col1</code> values are represented as keys and the corresponding <code>Col2</code> and <code>Col3</code> are tuples packed into an array for each <code>Col1</code> key. </p>

<p>My syntax works for pooling only one column into the <code>.groupby()</code>:</p>

<pre><code>Table.groupby('Column1')['Column2'].apply(list).to_dict()
# Result as expected
{
    0: [23], 
    1: [5, 2, 19], 
    2: [56, 22], 
    3: [2, 14], 
    4: [59], 
    5: [44, 1, 87]
}
</code></pre>

<p>However specifying multiple values for the indices results in returning column names for the value :</p>

<pre><code>Table.groupby('Column1')[('Column2', 'Column3')].apply(list).to_dict()
# Result has column namespace as array value
{
    0: ['Column2', 'Column3'],
    1: ['Column2', 'Column3'],
    2: ['Column2', 'Column3'],
    3: ['Column2', 'Column3'],
    4: ['Column2', 'Column3'],
    5: ['Column2', 'Column3']
 }
</code></pre>

<p>How would I return a list of tuples in the value array?</p>
",3830766,466,27-02-2018 20:13,27-02-2018 20:23,0,466,14,1,4,89,"{'badge_counts': {'bronze': 14, 'silver': 4, 'gold': 1}, 'account_id': 4736528, 'is_employee': False, 'last_modified_date': 1638281100, 'last_access_date': 1711141327, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 466, 'creation_date': 1405107516, 'user_type': 'registered', 'user_id': 3830766, 'accept_rate': 89, 'location': 'Austin, TX, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/3830766/micks-ketches', 'profile_image': 'https://i.stack.imgur.com/AKwJg.jpg?s=256&g=1', 'display_name': 'Micks Ketches'}",For anyone familliar with pandas how would I build a multivalue dictionary with the method? I would like an output to resemble this format: where values are represented as keys and the corresponding and are tuples packed into an array for each key. My syntax works for pooling only one column into the : However specifying multiple values for the indices results in returning column names for the value : How would I return a list of tuples in the value array?,"type(Table)
pandas.core.frame.DataFrame

Table
======= ======= =======
Column1 Column2 Column3
0       23      1
1       5       2
1       2       3
1       19      5
2       56      1
2       22      2
3       2       4
3       14      5
4       59      1
5       44      1
5       1       2
5       87      3
 .groupby() {
    0: [(23,1)]
    1: [(5,  2), (2, 3), (19, 5)]
    # etc...
    }
 Col1 Col2 Col3 Col1 .groupby() Table.groupby('Column1')['Column2'].apply(list).to_dict()
# Result as expected
{
    0: [23], 
    1: [5, 2, 19], 
    2: [56, 22], 
    3: [2, 14], 
    4: [59], 
    5: [44, 1, 87]
}
 Table.groupby('Column1')[('Column2', 'Column3')].apply(list).to_dict()
# Result has column namespace as array value
{
    0: ['Column2', 'Column3'],
    1: ['Column2', 'Column3'],
    2: ['Column2', 'Column3'],
    3: ['Column2', 'Column3'],
    4: ['Column2', 'Column3'],
    5: ['Column2', 'Column3']
 }
",33,62,0,0,
436,49501538,49502288,21396,Custom weight initialization tensorflow tf.layers.dense,3,<python><python-3.x><tensorflow><deep-learning>,13,"<p>I'm trying to set up custom initializer to <code>tf.layers.dense</code> where I initialize <code>kernel_initializer</code> with a weight matrix I already have.</p>

<pre><code>u_1 = tf.placeholder(tf.float32, [784, 784])
first_layer_u = tf.layers.dense(X_, n_params, activation=None, 
                              kernel_initializer=u_1,
                              bias_initializer=tf.keras.initializers.he_normal())
</code></pre>

<p>This is throwing error saying <code>ValueError: If initializer is a constant, do not specify shape.</code></p>

<p>Is it a problem to assign placeholder to <code>kernel_initializer</code> or am I missing something?</p>
",2706419,4255,26-03-2018 22:19,26-03-2018 23:41,0,4265,47,7,25,50,"{'badge_counts': {'bronze': 47, 'silver': 25, 'gold': 7}, 'account_id': 3205273, 'is_employee': False, 'last_modified_date': 1678205700, 'last_access_date': 1711152000, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4265, 'creation_date': 1377158205, 'user_type': 'registered', 'user_id': 2706419, 'accept_rate': 50, 'location': 'San Francisco, CA, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/2706419/tourist', 'profile_image': 'https://www.gravatar.com/avatar/2af78d75102f7905372a542e1e62a4a3?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'tourist'}",I'm trying to set up custom initializer to where I initialize with a weight matrix I already have. This is throwing error saying Is it a problem to assign placeholder to or am I missing something?,"tf.layers.dense kernel_initializer u_1 = tf.placeholder(tf.float32, [784, 784])
first_layer_u = tf.layers.dense(X_, n_params, activation=None, 
                              kernel_initializer=u_1,
                              bias_initializer=tf.keras.initializers.he_normal())
 ValueError: If initializer is a constant, do not specify shape. kernel_initializer",-1,11,0,0,
437,49451568,49451953,2197,Initialise a NumPy array based on its index,3,<python><python-3.x><numpy><matrix><python-itertools>,11,"<p>I am creating a couple of multi dimensional arrays using NumPy and inititalising them based on the index as follows:</p>

<pre><code>pos_data = []
# Some typical values
d = 2  # Could also be 3
vol_ext = (1000, 500)  # If d = 3, this will have another entry
ratio = [5.0, 8.0]  # Again, if d = 3, it will have another entry

for i in range(d):
    pos_data.append(np.zeros(vol_ext))

if d == 2:
    for y in range(vol_ext[1]):
        for x in range(vol_ext[0]):
            pos_data[0][x, y] = (x - 1.0) * ratio[0]
            pos_data[1][x, y] = (y - 1.0) * ratio[1]
elif d == 3:
    for z in range(vol_ext[2]):
        for y in range(vol_ext[1]):
            for x in range(vol_ext[0]):
                pos_data[0][x, y, z] = (x - 1.0) * ratio[0]
                pos_data[1][x, y, z] = (y - 1.0) * ratio[1]
                pos_data[2][x, y, z] = (z - 1.0) * ratio[2]
</code></pre>

<p>The looping is a bit ugly and slow as well. Additionally, if I have a 3-dimensional object then I have to have another nested loop.</p>

<p>I was wondering if there is a Pythonic way to generate these values as they are just based on the x, y and z indices. I tried to use the combinatorics bit from <code>itertools</code>, but I could not make it work.</p>
",2713740,10700,23-03-2018 14:09,23-03-2018 14:28,0,10730,252,25,114,85,"{'badge_counts': {'bronze': 252, 'silver': 114, 'gold': 25}, 'account_id': 3214634, 'is_employee': False, 'last_modified_date': 1706925301, 'last_access_date': 1710793209, 'reputation_change_year': 152, 'reputation_change_quarter': 152, 'reputation_change_month': 60, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 10730, 'creation_date': 1377353333, 'user_type': 'registered', 'user_id': 2713740, 'accept_rate': 85, 'website_url': '', 'link': 'https://stackoverflow.com/users/2713740/luca', 'profile_image': 'https://www.gravatar.com/avatar/d07d4aaa7850d639f2115ecfd145b6f7?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Luca'}","I am creating a couple of multi dimensional arrays using NumPy and inititalising them based on the index as follows: The looping is a bit ugly and slow as well. Additionally, if I have a 3-dimensional object then I have to have another nested loop. I was wondering if there is a Pythonic way to generate these values as they are just based on the x, y and z indices. I tried to use the combinatorics bit from , but I could not make it work.","pos_data = []
# Some typical values
d = 2  # Could also be 3
vol_ext = (1000, 500)  # If d = 3, this will have another entry
ratio = [5.0, 8.0]  # Again, if d = 3, it will have another entry

for i in range(d):
    pos_data.append(np.zeros(vol_ext))

if d == 2:
    for y in range(vol_ext[1]):
        for x in range(vol_ext[0]):
            pos_data[0][x, y] = (x - 1.0) * ratio[0]
            pos_data[1][x, y] = (y - 1.0) * ratio[1]
elif d == 3:
    for z in range(vol_ext[2]):
        for y in range(vol_ext[1]):
            for x in range(vol_ext[0]):
                pos_data[0][x, y, z] = (x - 1.0) * ratio[0]
                pos_data[1][x, y, z] = (y - 1.0) * ratio[1]
                pos_data[2][x, y, z] = (z - 1.0) * ratio[2]
 itertools",19,28,0,0,
438,49653205,50113786,12048,Pybind11 or Boost.Python or neither-,2,<python><c++><pybind11>,20,"<p>I'm curious what the most flexible, most efficient, and most seamless method is for getting C++ and Python to talk to each other. 
The contenders seem to be Pybind11, Boost.Python, and neither (simply writing functions and wrappers as below).</p>

<pre><code>using namespace boost::algorithm;
static PyObject* strtest(PyObject* self, PyObject* args)
{
    std::string s = ""Boost C++ Libraries"";
    to_upper(s);
    PyObject * python_val = Py_BuildValue(""s"", s.c_str());
    return python_val;
}


PyMODINIT_FUNC initmath_demo(void)
{
    static PyMethodDef methods[] = {
        ""Test boost libraries"" },
        { NULL, NULL, 0, NULL }

    };

    PyObject *m = Py_InitModule(""math_demo"", methods);
}
</code></pre>
",2723494,1178,04-04-2018 14:07,01-05-2018 08:57,27,1178,26,3,15,86,"{'badge_counts': {'bronze': 26, 'silver': 15, 'gold': 3}, 'account_id': 3227504, 'is_employee': False, 'last_modified_date': 1607614712, 'last_access_date': 1710868859, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1178, 'creation_date': 1377648870, 'user_type': 'registered', 'user_id': 2723494, 'accept_rate': 86, 'link': 'https://stackoverflow.com/users/2723494/user2723494', 'profile_image': 'https://www.gravatar.com/avatar/83db0d39a256b6d50fc728ce40d066c9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user2723494'}","I'm curious what the most flexible, most efficient, and most seamless method is for getting C++ and Python to talk to each other. The contenders seem to be Pybind11, Boost.Python, and neither (simply writing functions and wrappers as below).","using namespace boost::algorithm;
static PyObject* strtest(PyObject* self, PyObject* args)
{
    std::string s = ""Boost C++ Libraries"";
    to_upper(s);
    PyObject * python_val = Py_BuildValue(""s"", s.c_str());
    return python_val;
}


PyMODINIT_FUNC initmath_demo(void)
{
    static PyMethodDef methods[] = {
        ""Test boost libraries"" },
        { NULL, NULL, 0, NULL }

    };

    PyObject *m = Py_InitModule(""math_demo"", methods);
}
",19,24,0,0,
439,48560227,48568878,6527,How to take the average of the weights of two networks?,1,<python><neural-network><deep-learning><pytorch>,13,"<p>Suppose in PyTorch I have <code>model1</code> and <code>model2</code> which have the same architecture. They were further trained on same data or one model is an earlier version of the othter, but it is not technically relevant for the question. Now I want to set the weights of <code>model</code> to be the average of the weights of <code>model1</code> and <code>model2</code>. How would I do that in PyTorch?</p>
",3990607,18103,01-02-2018 10:15,01-02-2018 17:50,0,18143,135,13,94,77,"{'badge_counts': {'bronze': 135, 'silver': 94, 'gold': 13}, 'account_id': 4958744, 'is_employee': False, 'last_modified_date': 1652904600, 'last_access_date': 1710448393, 'reputation_change_year': 190, 'reputation_change_quarter': 190, 'reputation_change_month': 50, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 18143, 'creation_date': 1409324095, 'user_type': 'registered', 'user_id': 3990607, 'accept_rate': 77, 'location': 'Laniakea Supercluster : Milky Way : Orion Arm : Sun System : 3rd planet', 'website_url': '', 'link': 'https://stackoverflow.com/users/3990607/patapouf-ai', 'profile_image': 'https://i.stack.imgur.com/0ZRtB.png?s=256&g=1', 'display_name': 'patapouf_ai'}","Suppose in PyTorch I have and which have the same architecture. They were further trained on same data or one model is an earlier version of the othter, but it is not technically relevant for the question. Now I want to set the weights of to be the average of the weights of and . How would I do that in PyTorch?",model1 model2 model model1 model2,-5,1,0,0,
440,48483348,48484593,86751,How to limit concurrency with Python asyncio?,10,<python><python-3.x><asynchronous><concurrency><python-asyncio>,139,"<p>Let's assume we have a bunch of links to download and each of the link may take a different amount of time to download. And I'm allowed to download using utmost 3 connections only. Now, I want to ensure that I do this efficiently using asyncio.</p>
<p>Here's what I'm trying to achieve: At any point in time, try to ensure that I have atleast 3 downloads running.</p>
<pre><code>Connection 1: 1---------7---9---
Connection 2: 2---4----6-----
Connection 3: 3-----5---8-----
</code></pre>
<p>The numbers represent the download links, while hyphens represent Waiting for download.</p>
<p>Here is the code that I'm using right now</p>
<pre><code>from random import randint
import asyncio

count = 0


async def download(code, permit_download, no_concurrent, downloading_event):
    global count
    downloading_event.set()
    wait_time = randint(1, 3)
    print('downloading {} will take {} second(s)'.format(code, wait_time))
    await asyncio.sleep(wait_time)  # I/O, context will switch to main function
    print('downloaded {}'.format(code))
    count -= 1
    if count &lt; no_concurrent and not permit_download.is_set():
        permit_download.set()


async def main(loop):
    global count
    permit_download = asyncio.Event()
    permit_download.set()
    downloading_event = asyncio.Event()
    no_concurrent = 3
    i = 0
    while i &lt; 9:
        if permit_download.is_set():
            count += 1
            if count &gt;= no_concurrent:
                permit_download.clear()
            loop.create_task(download(i, permit_download, no_concurrent, downloading_event))
            await downloading_event.wait()  # To force context to switch to download function
            downloading_event.clear()
            i += 1
        else:
            await permit_download.wait()
    await asyncio.sleep(9)

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    try:
        loop.run_until_complete(main(loop))
    finally:
        loop.close()
</code></pre>
<p>And the output is as expected:</p>
<pre><code>downloading 0 will take 2 second(s)
downloading 1 will take 3 second(s)
downloading 2 will take 1 second(s)
downloaded 2
downloading 3 will take 2 second(s)
downloaded 0
downloading 4 will take 3 second(s)
downloaded 1
downloaded 3
downloading 5 will take 2 second(s)
downloading 6 will take 2 second(s)
downloaded 5
downloaded 6
downloaded 4
downloading 7 will take 1 second(s)
downloading 8 will take 1 second(s)
downloaded 7
downloaded 8
</code></pre>
<p>But here are my questions:</p>
<ol>
<li><p>At the moment, I'm simply waiting for 9 seconds to keep the main function running till the downloads are complete. Is there an efficient way of waiting for the last download to complete before exiting the <code>main</code> function? (I know there's <code>asyncio.wait</code>, but I'll need to store all the task references for it to work)</p>
</li>
<li><p>What's a good library that does this kind of task? I know javascript has a lot of async libraries, but what about Python?</p>
</li>
</ol>
<p>Edit:
2. What's a good library that takes care of common async patterns? (Something like <a href=""https://www.npmjs.com/package/async"" rel=""noreferrer"">async</a>)</p>
",4001713,1447,28-01-2018 05:08,28-01-2018 08:42,0,1457,9,2,10,,"{'badge_counts': {'bronze': 9, 'silver': 10, 'gold': 2}, 'account_id': 4975101, 'is_employee': False, 'last_modified_date': 1607614609, 'last_access_date': 1710802984, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1457, 'creation_date': 1409688018, 'user_type': 'registered', 'user_id': 4001713, 'location': 'Bangalore, Karnataka, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/4001713/shridharshan', 'profile_image': 'https://www.gravatar.com/avatar/77027cab19da935d511d0148c2e50548?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Shridharshan'}","Let's assume we have a bunch of links to download and each of the link may take a different amount of time to download. And I'm allowed to download using utmost 3 connections only. Now, I want to ensure that I do this efficiently using asyncio. Here's what I'm trying to achieve: At any point in time, try to ensure that I have atleast 3 downloads running. The numbers represent the download links, while hyphens represent Waiting for download. Here is the code that I'm using right now And the output is as expected: But here are my questions: At the moment, I'm simply waiting for 9 seconds to keep the main function running till the downloads are complete. Is there an efficient way of waiting for the last download to complete before exiting the function? (I know there's , but I'll need to store all the task references for it to work) What's a good library that does this kind of task? I know javascript has a lot of async libraries, but what about Python? Edit: 2. What's a good library that takes care of common async patterns? (Something like async)","Connection 1: 1---------7---9---
Connection 2: 2---4----6-----
Connection 3: 3-----5---8-----
 from random import randint
import asyncio

count = 0


async def download(code, permit_download, no_concurrent, downloading_event):
    global count
    downloading_event.set()
    wait_time = randint(1, 3)
    print('downloading {} will take {} second(s)'.format(code, wait_time))
    await asyncio.sleep(wait_time)  # I/O, context will switch to main function
    print('downloaded {}'.format(code))
    count -= 1
    if count &lt; no_concurrent and not permit_download.is_set():
        permit_download.set()


async def main(loop):
    global count
    permit_download = asyncio.Event()
    permit_download.set()
    downloading_event = asyncio.Event()
    no_concurrent = 3
    i = 0
    while i &lt; 9:
        if permit_download.is_set():
            count += 1
            if count &gt;= no_concurrent:
                permit_download.clear()
            loop.create_task(download(i, permit_download, no_concurrent, downloading_event))
            await downloading_event.wait()  # To force context to switch to download function
            downloading_event.clear()
            i += 1
        else:
            await permit_download.wait()
    await asyncio.sleep(9)

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    try:
        loop.run_until_complete(main(loop))
    finally:
        loop.close()
 downloading 0 will take 2 second(s)
downloading 1 will take 3 second(s)
downloading 2 will take 1 second(s)
downloaded 2
downloading 3 will take 2 second(s)
downloaded 0
downloading 4 will take 3 second(s)
downloaded 1
downloaded 3
downloading 5 will take 2 second(s)
downloading 6 will take 2 second(s)
downloaded 5
downloaded 6
downloaded 4
downloading 7 will take 1 second(s)
downloading 8 will take 1 second(s)
downloaded 7
downloaded 8
 main asyncio.wait",60,82,0,1,
441,48580465,48581361,3889,django RequestFactory loses url kwargs,1,<python><django>,14,"<p>I am trying to switch from using Django Test Client to RequestFactory to speed up my tests. However, requests generated by RequestFactory do not supply proper <code>kwargs</code> to views. </p>

<p>Example: this is my view </p>

<pre><code>class SomeView(View):
    def get(self, request, *args, **kwargs):
        return JsonResponse({'your kwargs': str(kwargs)})
</code></pre>

<p>with urlconf</p>

<pre><code>url(r'^some_view/(?P&lt;some_kwarg&gt;[\-0-9a-fA-F]+)/$',
    views.SomeView.as_view(),
    name='some_view'),
</code></pre>

<p>and two tests:</p>

<pre><code>def test_different_kwargs():

    c = Client()
    response = c.get(
        reverse('bots:some_view',
                kwargs={'some_kwarg': '12345'}),
    )
    print('\n\nResponse for TestClient: ', response.content.decode())

    rf = RequestFactory()
    request = rf.get(
        reverse('bots:some_view',
                kwargs={'some_kwarg': '12345'}),
    )
    response = SomeView.as_view()(request)
    print('\n\nResponse for RequestFactory: ', response.content.decode())
</code></pre>

<p>What they produce is:</p>

<pre><code>Response for TestClient:  {""your kwargs"": ""{'some_kwarg': '12345'}""}
Response for RequestFactory:  {""your kwargs"": ""{}""}
</code></pre>

<p>So, what's the point of <code>RequestFactory</code> if it loses url kwargs? Or is there a way to put them into the view somehow?</p>
",4057053,8360,02-02-2018 10:16,02-02-2018 11:05,0,8390,98,14,57,54,"{'badge_counts': {'bronze': 98, 'silver': 57, 'gold': 14}, 'account_id': 5053094, 'is_employee': False, 'last_modified_date': 1711157700, 'last_access_date': 1683877401, 'reputation_change_year': 110, 'reputation_change_quarter': 110, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 8390, 'creation_date': 1411104087, 'user_type': 'registered', 'user_id': 4057053, 'accept_rate': 54, 'website_url': '', 'link': 'https://stackoverflow.com/users/4057053/kurtgn', 'profile_image': 'https://www.gravatar.com/avatar/2fb7bd99985b273390a73fc0260f1587?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'kurtgn'}","I am trying to switch from using Django Test Client to RequestFactory to speed up my tests. However, requests generated by RequestFactory do not supply proper to views. Example: this is my view with urlconf and two tests: What they produce is: So, what's the point of if it loses url kwargs? Or is there a way to put them into the view somehow?","kwargs class SomeView(View):
    def get(self, request, *args, **kwargs):
        return JsonResponse({'your kwargs': str(kwargs)})
 url(r'^some_view/(?P&lt;some_kwarg&gt;[\-0-9a-fA-F]+)/$',
    views.SomeView.as_view(),
    name='some_view'),
 def test_different_kwargs():

    c = Client()
    response = c.get(
        reverse('bots:some_view',
                kwargs={'some_kwarg': '12345'}),
    )
    print('\n\nResponse for TestClient: ', response.content.decode())

    rf = RequestFactory()
    request = rf.get(
        reverse('bots:some_view',
                kwargs={'some_kwarg': '12345'}),
    )
    response = SomeView.as_view()(request)
    print('\n\nResponse for RequestFactory: ', response.content.decode())
 Response for TestClient:  {""your kwargs"": ""{'some_kwarg': '12345'}""}
Response for RequestFactory:  {""your kwargs"": ""{}""}
 RequestFactory",18,43,0,0,
442,48879920,48912833,16200,Airflow Worker Configuration,4,<python><python-3.x><airflow><airflow-scheduler>,12,"<p>I am a newbie to Airflow. I'm trying to setup Distributed Mode of Airflow Using Celery Executor by Refering this article <a href=""https://stlong0521.github.io/20161023%20-%20Airflow.html"" rel=""noreferrer"">https://stlong0521.github.io/20161023%20-%20Airflow.html</a></p>

<p>Before getting into detail about the specification I would like to confirm that <strong>I've installed PostgreSQL on a seperate instance</strong>.</p>

<p>The specification of the setup is detailed below:</p>

<p><strong>Airflow core/server computer</strong></p>

<ul>
<li>Python 3.5

<ul>
<li>airflow (AIRFLOW_HOME = ~/airflow)</li>
<li>celery</li>
<li>psycogp2</li>
</ul></li>
<li>RabbitMQ</li>
</ul>

<p><strong>Configurations made in airflow.cfg</strong>:</p>

<pre><code>sql_alchemy_conn = postgresql+psycopg2://username:password@192.168.2.12:5432/airflow
executor = CeleryExecutor
broker_url = amqp://username:password@192.168.1.12:5672//
celery_result_backend = db+postgresql://username:password@192.168.2.12:5432/airflow
</code></pre>

<p>Tests performed:</p>

<pre><code>RabbitMQ is running
Can connect to PostgreSQL and have confirmed that Airflow has created tables
Can start and view the webserver (including custom dags)
</code></pre>

<p><strong>Airflow worker computer</strong></p>

<p>Has the following installed:</p>

<ul>
<li>Python 3.5 with

<ul>
<li>airflow (AIRFLOW_HOME = ~/airflow)</li>
<li>celery</li>
</ul></li>
<li>psycogp2</li>
</ul>

<p>Configurations made in airflow.cfg are exactly the same as in the server:</p>

<pre><code>sql_alchemy_conn = postgresql+psycopg2://username:password@192.168.2.12:5432/airflow
executor = CeleryExecutor
broker_url = amqp://username:password@192.168.1.12:5672//
celery_result_backend = db+postgresql://username:password@192.168.2.12:5432/airflow
</code></pre>

<p>Output from commands run on the worker machine:</p>

<p>When running airflow flower:</p>

<pre><code>[2018-02-19 14:58:14,276] {__init__.py:57} INFO - Using executor CeleryExecutor
[2018-02-19 14:58:14,360] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt
[2018-02-19 14:58:14,384] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt
[I 180219 14:58:15 command:139] Visit me at http://0.0.0.0:5555
[I 180219 14:58:15 command:144] Broker: amqp://username:password@192.168.1.12:5672//
[I 180219 14:58:15 command:147] Registered tasks: 
    ['celery.accumulate',
     'celery.backend_cleanup',
     'celery.chain',
     'celery.chord',
     'celery.chord_unlock',
     'celery.chunks',
     'celery.group',
     'celery.map',
     'celery.starmap']
[I 180219 14:58:15 mixins:224] Connected to amqp://username:password@192.168.1.12:5672//
</code></pre>

<p>I am passing the dag in the <strong>Airflow Core machine</strong> and also I have copied the sample data(Excel sheets) which the dag will process to the same core machine.</p>

<p>My worker log 
<code>raise CalledProcessError(retcode, cmd)
    subprocess.CalledProcessError: Command 'airflow run dag_name_x task_name_xx 2018-02-19T10:15:41.657243 --local -sd /home/Distributedici/airflow/dags/sample_data_xx.py' returned non-zero exit status 1</code></p>

<p><strong>Now my query is</strong></p>

<p>1) Should I copy the dag folder to the worker computer also</p>

<p>2) Right now, I have not copied the dag folder on the worker computer and I'm not able to see the worker process pick up the task.</p>

<p>Please point me where I am making a mistake and how to make the worker process pick up the process.</p>
",4066422,133,20-02-2018 07:30,21-02-2018 18:16,1,133,6,1,1,,"{'badge_counts': {'bronze': 6, 'silver': 1, 'gold': 1}, 'account_id': 5066434, 'is_employee': False, 'last_modified_date': 1573680070, 'last_access_date': 1525625519, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 133, 'creation_date': 1411387731, 'user_type': 'registered', 'user_id': 4066422, 'link': 'https://stackoverflow.com/users/4066422/soundar-raj', 'profile_image': 'https://www.gravatar.com/avatar/d1a76d52f7d6caaf0ea24c1f7b28f551?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Soundar Raj'}","I am a newbie to Airflow. I'm trying to setup Distributed Mode of Airflow Using Celery Executor by Refering this article https://stlong0521.github.io/20161023%20-%20Airflow.html Before getting into detail about the specification I would like to confirm that I've installed PostgreSQL on a seperate instance. The specification of the setup is detailed below: Airflow core/server computer Python 3.5 airflow (AIRFLOW_HOME = ~/airflow) celery psycogp2 RabbitMQ Configurations made in airflow.cfg: Tests performed: Airflow worker computer Has the following installed: Python 3.5 with airflow (AIRFLOW_HOME = ~/airflow) celery psycogp2 Configurations made in airflow.cfg are exactly the same as in the server: Output from commands run on the worker machine: When running airflow flower: I am passing the dag in the Airflow Core machine and also I have copied the sample data(Excel sheets) which the dag will process to the same core machine. My worker log Now my query is 1) Should I copy the dag folder to the worker computer also 2) Right now, I have not copied the dag folder on the worker computer and I'm not able to see the worker process pick up the task. Please point me where I am making a mistake and how to make the worker process pick up the process.","sql_alchemy_conn = postgresql+psycopg2://username:password@192.168.2.12:5432/airflow
executor = CeleryExecutor
broker_url = amqp://username:password@192.168.1.12:5672//
celery_result_backend = db+postgresql://username:password@192.168.2.12:5432/airflow
 RabbitMQ is running
Can connect to PostgreSQL and have confirmed that Airflow has created tables
Can start and view the webserver (including custom dags)
 sql_alchemy_conn = postgresql+psycopg2://username:password@192.168.2.12:5432/airflow
executor = CeleryExecutor
broker_url = amqp://username:password@192.168.1.12:5672//
celery_result_backend = db+postgresql://username:password@192.168.2.12:5432/airflow
 [2018-02-19 14:58:14,276] {__init__.py:57} INFO - Using executor CeleryExecutor
[2018-02-19 14:58:14,360] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt
[2018-02-19 14:58:14,384] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt
[I 180219 14:58:15 command:139] Visit me at http://0.0.0.0:5555
[I 180219 14:58:15 command:144] Broker: amqp://username:password@192.168.1.12:5672//
[I 180219 14:58:15 command:147] Registered tasks: 
    ['celery.accumulate',
     'celery.backend_cleanup',
     'celery.chain',
     'celery.chord',
     'celery.chord_unlock',
     'celery.chunks',
     'celery.group',
     'celery.map',
     'celery.starmap']
[I 180219 14:58:15 mixins:224] Connected to amqp://username:password@192.168.1.12:5672//
 raise CalledProcessError(retcode, cmd)
    subprocess.CalledProcessError: Command 'airflow run dag_name_x task_name_xx 2018-02-19T10:15:41.657243 --local -sd /home/Distributedici/airflow/dags/sample_data_xx.py' returned non-zero exit status 1",23,91,0,1,
443,49836676,49836753,443807,Error after upgrading pip: cannot import name 'main',32,<python><pip>,506,"<p>Whenever I am trying to install any package using pip, I am getting this import error:</p>

<pre><code>guru@guru-notebook:~$ pip3 install numpy
Traceback (most recent call last):
  File ""/usr/bin/pip3"", line 9, in &lt;module&gt;
    from pip import main
ImportError: cannot import name 'main'
</code></pre>

<p><br></p>

<pre><code>guru@guru-notebook:~$ cat `which pip3`
#!/usr/bin/python3
# GENERATED BY DEBIAN

import sys

# Run the main entry point, similarly to how setuptools does it, but because
# we didn't install the actual entry point from setup.py, don't use the
# pkg_resources API.
from pip import main
if __name__ == '__main__':
    sys.exit(main())
</code></pre>

<p>It was working fine earlier, I am not sure why it is throwing this error.
I have searched about this error, but can't find anything to fix it.</p>

<p>Please let me know if you need any further detail, I will update my question.</p>
",2743206,5509,14-04-2018 22:12,14-04-2018 22:24,0,5509,23,3,20,38,"{'badge_counts': {'bronze': 23, 'silver': 20, 'gold': 3}, 'account_id': 2590606, 'is_employee': False, 'last_modified_date': 1698409800, 'last_access_date': 1708018901, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5509, 'creation_date': 1378214847, 'user_type': 'registered', 'user_id': 2743206, 'accept_rate': 38, 'location': 'India', 'link': 'https://stackoverflow.com/users/2743206/g-p', 'profile_image': 'https://www.gravatar.com/avatar/12cd7e03f7d31d7af5b33040f7c8954d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'g_p'}","Whenever I am trying to install any package using pip, I am getting this import error: It was working fine earlier, I am not sure why it is throwing this error. I have searched about this error, but can't find anything to fix it. Please let me know if you need any further detail, I will update my question.","guru@guru-notebook:~$ pip3 install numpy
Traceback (most recent call last):
  File ""/usr/bin/pip3"", line 9, in &lt;module&gt;
    from pip import main
ImportError: cannot import name 'main'
 guru@guru-notebook:~$ cat `which pip3`
#!/usr/bin/python3
# GENERATED BY DEBIAN

import sys

# Run the main entry point, similarly to how setuptools does it, but because
# we didn't install the actual entry point from setup.py, don't use the
# pkg_resources API.
from pip import main
if __name__ == '__main__':
    sys.exit(main())
",15,29,0,0,
444,49295992,49296412,24489,ImportError: No module... After python setup.py install,2,<python><installation><python-import><setuptools><importerror>,19,"<p>I'm having trouble installing one of my python scripts. It has the following structure:</p>

<pre><code>myproject
  setup.py
  src
    myproject
      otherfolders
      main.py
      __init__.py
</code></pre>

<p>And my <code>setup.py</code>creates an entry point like this:</p>

<pre><code>from setuptools import setup, find_packages

setup(name='mypackage',
version='2.4.0',
author='me',
author_email='...',
package_dir={'':'src'},
packages=find_packages('myproject'),
install_requires=[
    ""networkx"",
    ""geopy"",
    ""pyyaml""
],
zip_safe=False,
entry_points={
    'console_scripts': [
        'myproject=myproject.main:main',
    ],
},
)
</code></pre>

<p>Now, after installing this successfully with <code>sudo python setup.py install</code>, I run <code>mypackage</code> and get an import error: <code>No module named mypackage.main</code>.</p>

<p>I am aware that there are lots of similar questions and I tried most/all solutions suggested <a href=""https://stackoverflow.com/questions/338768/python-error-importerror-no-module-named"">here</a>, e.g., checking the <code>__init__.py</code> and setting <code>PYTHONPATH</code>, but the problem still exists.
I'm running this on two different Ubuntu 16.04 machines.</p>

<p>I'm pretty sure this worked before, but even when I go back to an earlier commit it doesn't work now.</p>

<p>I noticed the installation works with <code>develop</code> but still fails with <code>install</code>. Does that make sense to anyone?</p>
",2745116,5792,15-03-2018 09:42,15-03-2018 10:00,0,5822,94,8,57,68,"{'badge_counts': {'bronze': 94, 'silver': 57, 'gold': 8}, 'account_id': 3259215, 'is_employee': False, 'last_modified_date': 1702731618, 'last_access_date': 1711114730, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 50, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5822, 'creation_date': 1378263844, 'user_type': 'registered', 'user_id': 2745116, 'accept_rate': 68, 'website_url': 'https://stefanbschneider.github.io/', 'link': 'https://stackoverflow.com/users/2745116/stefanbschneider', 'profile_image': 'https://www.gravatar.com/avatar/42da9dbfd6e66ca2f90e5d515928d27d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'stefanbschneider'}","I'm having trouble installing one of my python scripts. It has the following structure: And my creates an entry point like this: Now, after installing this successfully with , I run and get an import error: . I am aware that there are lots of similar questions and I tried most/all solutions suggested here, e.g., checking the and setting , but the problem still exists. I'm running this on two different Ubuntu 16.04 machines. I'm pretty sure this worked before, but even when I go back to an earlier commit it doesn't work now. I noticed the installation works with but still fails with . Does that make sense to anyone?","myproject
  setup.py
  src
    myproject
      otherfolders
      main.py
      __init__.py
 setup.py from setuptools import setup, find_packages

setup(name='mypackage',
version='2.4.0',
author='me',
author_email='...',
package_dir={'':'src'},
packages=find_packages('myproject'),
install_requires=[
    ""networkx"",
    ""geopy"",
    ""pyyaml""
],
zip_safe=False,
entry_points={
    'console_scripts': [
        'myproject=myproject.main:main',
    ],
},
)
 sudo python setup.py install mypackage No module named mypackage.main __init__.py PYTHONPATH develop install",17,43,0,1,
445,48143233,48143549,30530,how to plot multiple time series in chartjs where each time series has different times,2,<javascript><python><chart.js>,18,"<p>I have two time series for example:</p>

<pre><code>s1:
  2017-01-06 18:39:30    100
  2017-01-07 18:39:28    101
</code></pre>

<p>and</p>

<pre><code>s2:
2017-01-07 18:00:00     90
2017-01-08 18:00:00    105
</code></pre>

<p>I want to plot these in a Chartjs chart, however it seems that Chartjs only takes one x-axis array (or label in Chartjs terminology). </p>

<p><strong>So my question is what is the best way to plot both of these?</strong></p>

<p>My approach was to write a function (in python, although the language doesn't really matter for this part) that iterates through both time series and creates 3 new arrays which is in the format apparently Chartjs needs based off the 1st example here: <a href=""https://www.sitepoint.com/introduction-chart-js-2-0-six-examples/"" rel=""noreferrer"">https://www.sitepoint.com/introduction-chart-js-2-0-six-examples/</a> </p>

<p>The algorithm (in sudo code) goes like:</p>

<pre><code>    # inputs are initial time series s1 and s2
    y1 = [] # to hold new s1 data values
    y2 = [] # to hold new s2 data values
    x  = [] # to hold times

    # iterate through longest series s1 or s2
    if s1[idx].time &gt; s2[idx].time
      x.append(s1[idx].time)
      y1.append(s1[idx].data)
      # y2 appends the linear interpolation of 
      # of closest y2 points

    if (s1[idx].time &lt; s2[idx].time)
      x.append(s2[idx].time)
      # opposite of above. ie. swap y1&lt;-&gt;y2, s1-&gt;s2

    else # they have the same time
      x.append(s1[idx].time)
      y1.append(s1[idx].data)
      y2.append(s2[idx].data) 
</code></pre>

<p>There are a couple other conditional checks for when data runs out of the shorter series but that is the main logic. After which I have 3 arrays that I can now add to chart js via one time/label array/x-axis and two data arrays. However this seems WAY more complicated than it should be considering how common I assume this use case is. Any help or advice is greatly appreciated.</p>
",4158001,401,08-01-2018 01:34,08-01-2018 02:31,0,401,10,1,3,,"{'badge_counts': {'bronze': 10, 'silver': 3, 'gold': 1}, 'account_id': 5197199, 'is_employee': False, 'last_modified_date': 1607614599, 'last_access_date': 1703350189, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 401, 'creation_date': 1413685050, 'user_type': 'registered', 'user_id': 4158001, 'link': 'https://stackoverflow.com/users/4158001/achyrd', 'profile_image': 'https://www.gravatar.com/avatar/bf2953536030f4f32a89514355073ca5?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'achyrd'}","I have two time series for example: and I want to plot these in a Chartjs chart, however it seems that Chartjs only takes one x-axis array (or label in Chartjs terminology). So my question is what is the best way to plot both of these? My approach was to write a function (in python, although the language doesn't really matter for this part) that iterates through both time series and creates 3 new arrays which is in the format apparently Chartjs needs based off the 1st example here: https://www.sitepoint.com/introduction-chart-js-2-0-six-examples/ The algorithm (in sudo code) goes like: There are a couple other conditional checks for when data runs out of the shorter series but that is the main logic. After which I have 3 arrays that I can now add to chart js via one time/label array/x-axis and two data arrays. However this seems WAY more complicated than it should be considering how common I assume this use case is. Any help or advice is greatly appreciated.","s1:
  2017-01-06 18:39:30    100
  2017-01-07 18:39:28    101
 s2:
2017-01-07 18:00:00     90
2017-01-08 18:00:00    105
     # inputs are initial time series s1 and s2
    y1 = [] # to hold new s1 data values
    y2 = [] # to hold new s2 data values
    x  = [] # to hold times

    # iterate through longest series s1 or s2
    if s1[idx].time &gt; s2[idx].time
      x.append(s1[idx].time)
      y1.append(s1[idx].data)
      # y2 appends the linear interpolation of 
      # of closest y2 points

    if (s1[idx].time &lt; s2[idx].time)
      x.append(s2[idx].time)
      # opposite of above. ie. swap y1&lt;-&gt;y2, s1-&gt;s2

    else # they have the same time
      x.append(s1[idx].time)
      y1.append(s1[idx].data)
      y2.append(s2[idx].data) 
",23,45,0,1,
446,49465891,49470884,38858,Building an SVM with Tensorflow,2,<python><tensorflow><machine-learning><svm>,19,"<p>I currently have two numpy arrays:</p>

<ul>
<li><code>X</code> - (157, 128) - 157 sets of 128 features</li>
<li><code>Y</code> - (157) - classifications of the feature sets</li>
</ul>

<p>This is the code I have written to attempt to build a linear classification model of these features.</p>

<p>First of all I adapted the arrays to a Tensorflow dataset:</p>

<pre><code>train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": X},
    y=Y,
    num_epochs=None,
    shuffle=True)
</code></pre>

<p>I then tried to <code>fit</code> an SVM model:</p>

<pre><code>svm = tf.contrib.learn.SVM(
    example_id_column='example_id', # not sure why this is necessary
    feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(X), # create feature columns (not sure why this is necessary) 
    l2_regularization=0.1)

svm.fit(input_fn=train_input_fn, steps=10)
</code></pre>

<p>But this just returns the error:</p>

<pre><code>WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpf1mwlR
WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)
Traceback (most recent call last):
  File ""/var/www/idmy.team/python/train/classifier.py"", line 59, in &lt;module&gt;
    svm.fit(input_fn=train_input_fn, steps=10)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 480, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 985, in _train_model
    model_fn_ops = self._get_train_ops(features, labels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1201, in _get_train_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1165, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py"", line 244, in sdca_model_fn
    features.update(layers.transform_features(features, feature_columns))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py"", line 656, in transform_features
    transformer.transform(column)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py"", line 847, in transform
    feature_column.insert_transformed_feature(self._columns_to_tensors)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column.py"", line 1816, in insert_transformed_feature
    input_tensor = self._normalized_input_tensor(columns_to_tensors[self.name])
KeyError: ''
</code></pre>

<p>What am I doing wrong?</p>
",2768038,4093,24-03-2018 14:32,24-03-2018 23:06,0,4103,99,9,50,72,"{'badge_counts': {'bronze': 99, 'silver': 50, 'gold': 9}, 'account_id': 3289385, 'is_employee': False, 'last_modified_date': 1690181700, 'last_access_date': 1711026531, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4103, 'creation_date': 1378891501, 'user_type': 'registered', 'user_id': 2768038, 'accept_rate': 72, 'location': 'London, UK', 'website_url': 'https://max.me.uk', 'link': 'https://stackoverflow.com/users/2768038/maxisme', 'profile_image': 'https://i.stack.imgur.com/hxk9G.png?s=256&g=1', 'display_name': 'maxisme'}","I currently have two numpy arrays: - (157, 128) - 157 sets of 128 features - (157) - classifications of the feature sets This is the code I have written to attempt to build a linear classification model of these features. First of all I adapted the arrays to a Tensorflow dataset: I then tried to an SVM model: But this just returns the error: What am I doing wrong?","X Y train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": X},
    y=Y,
    num_epochs=None,
    shuffle=True)
 fit svm = tf.contrib.learn.SVM(
    example_id_column='example_id', # not sure why this is necessary
    feature_columns=tf.contrib.learn.infer_real_valued_columns_from_input(X), # create feature columns (not sure why this is necessary) 
    l2_regularization=0.1)

svm.fit(input_fn=train_input_fn, steps=10)
 WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpf1mwlR
WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)
Traceback (most recent call last):
  File ""/var/www/idmy.team/python/train/classifier.py"", line 59, in &lt;module&gt;
    svm.fit(input_fn=train_input_fn, steps=10)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 480, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 985, in _train_model
    model_fn_ops = self._get_train_ops(features, labels)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1201, in _get_train_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1165, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/linear.py"", line 244, in sdca_model_fn
    features.update(layers.transform_features(features, feature_columns))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py"", line 656, in transform_features
    transformer.transform(column)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.py"", line 847, in transform
    feature_column.insert_transformed_feature(self._columns_to_tensors)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column.py"", line 1816, in insert_transformed_feature
    input_tensor = self._normalized_input_tensor(columns_to_tensors[self.name])
KeyError: ''
",30,58,0,0,
447,48451228,48451275,137869,How to spread a python array,3,<python>,122,"<p>In JS I can do this</p>

<pre><code>const a = [1,2,3,4]
const b = [10, ...a]
console.log(b) // [10,1,2,3,4]
</code></pre>

<p>Is there a similar way in python?</p>
",2782583,11181,25-01-2018 20:19,25-01-2018 20:22,0,11191,64,11,53,67,"{'badge_counts': {'bronze': 64, 'silver': 53, 'gold': 11}, 'account_id': 3308573, 'is_employee': False, 'last_modified_date': 1658838977, 'last_access_date': 1710336903, 'reputation_change_year': 230, 'reputation_change_quarter': 230, 'reputation_change_month': 40, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 11191, 'creation_date': 1379304091, 'user_type': 'registered', 'user_id': 2782583, 'accept_rate': 67, 'location': 'San Francisco, CA, United States', 'website_url': 'http://alexcory.com', 'link': 'https://stackoverflow.com/users/2782583/alex-cory', 'profile_image': 'https://www.gravatar.com/avatar/35e1b9b60444f89ad8c4990a3043f103?s=256&d=identicon&r=PG', 'display_name': 'Alex Cory'}",In JS I can do this Is there a similar way in python?,"const a = [1,2,3,4]
const b = [10, ...a]
console.log(b) // [10,1,2,3,4]
",2,8,0,0,
448,49883236,49887868,11082,How to generate a Hash or checksum value on Python Dataframe (created from a fixed width file)?,4,<python><pandas><hash>,18,"<p>I have 2 fixed width files like below (only change is Date value starting at position 14).</p>
<p><strong>sample_hash1.txt</strong></p>
<pre><code>GOKULKRISHNA 04/17/2018
ABCDEFGHIJKL 04/17/2018
111111111111 04/17/2018
</code></pre>
<p><strong>sample_hash2.txt</strong></p>
<pre><code>GOKULKRISHNA 04/16/2018
ABCDEFGHIJKL 04/16/2018
111111111111 04/16/2018
</code></pre>
<p>Using pandas read_fwf I am reading this file and creating a Dataframe by excluding the Date value and loading only the first 13 characters. My dataframe looks like this</p>
<pre><code>import pandas as pd
df1 = pd.read_fwf(&quot;sample_hash1.txt&quot;, colspecs=[(0,13)])
df2 = pd.read_fwf(&quot;sample_hash2.txt&quot;, colspecs=[(0,13)])
</code></pre>
<p><strong>df1</strong></p>
<pre><code>   GOKULKRISHNA
0  ABCDEFGHIJKL
1  111111111111
...
</code></pre>
<p><strong>df2</strong></p>
<pre><code>   GOKULKRISHNA
0  ABCDEFGHIJKL
1  111111111111
...
</code></pre>
<p>Now I am trying to generate a hash value on each Dataframe, but the hash is different for df1 and df2. I'm not sure what's wrong with this. Can someone throw some light on this please? I have to identify if there is any change in data between the files (excluding the Date columns).</p>
<pre><code>print(hash(df1.values.tostring()))
-3571422965125408226

print(hash(df2.values.tostring()))
5039867957859242153
</code></pre>
<p>I am loading these files into a table (each full file is around 2 GB size). Every time we are receiving full files from source. Sometimes there is no change in the data (excluding the last column, Date). My idea is to reject such files. If I can generate a hash on the file and store it somewhere (in a table) next time I can compare the new file hash value with the stored hash. I thought this is the right approach but I got stuck with hash generation.</p>
<p>I checked this post
<a href=""https://stackoverflow.com/questions/16589791/most-efficient-property-to-hash-for-numpy-array"">Most efficient property to hash for numpy array</a>
but that is not what I am looking for.</p>
",2799214,1206,17-04-2018 16:31,17-04-2018 21:37,0,1206,37,5,19,75,"{'badge_counts': {'bronze': 37, 'silver': 19, 'gold': 5}, 'account_id': 3330070, 'is_employee': False, 'last_modified_date': 1671207491, 'last_access_date': 1686098913, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1206, 'creation_date': 1379681350, 'user_type': 'registered', 'user_id': 2799214, 'accept_rate': 75, 'location': 'Tampa, FL, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/2799214/goks', 'profile_image': 'https://www.gravatar.com/avatar/9f094d883b851459af33409a5262285e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'goks'}","I have 2 fixed width files like below (only change is Date value starting at position 14). sample_hash1.txt sample_hash2.txt Using pandas read_fwf I am reading this file and creating a Dataframe by excluding the Date value and loading only the first 13 characters. My dataframe looks like this df1 df2 Now I am trying to generate a hash value on each Dataframe, but the hash is different for df1 and df2. I'm not sure what's wrong with this. Can someone throw some light on this please? I have to identify if there is any change in data between the files (excluding the Date columns). I am loading these files into a table (each full file is around 2 GB size). Every time we are receiving full files from source. Sometimes there is no change in the data (excluding the last column, Date). My idea is to reject such files. If I can generate a hash on the file and store it somewhere (in a table) next time I can compare the new file hash value with the stored hash. I thought this is the right approach but I got stuck with hash generation. I checked this post Most efficient property to hash for numpy array but that is not what I am looking for.","GOKULKRISHNA 04/17/2018
ABCDEFGHIJKL 04/17/2018
111111111111 04/17/2018
 GOKULKRISHNA 04/16/2018
ABCDEFGHIJKL 04/16/2018
111111111111 04/16/2018
 import pandas as pd
df1 = pd.read_fwf(&quot;sample_hash1.txt&quot;, colspecs=[(0,13)])
df2 = pd.read_fwf(&quot;sample_hash2.txt&quot;, colspecs=[(0,13)])
    GOKULKRISHNA
0  ABCDEFGHIJKL
1  111111111111
...
    GOKULKRISHNA
0  ABCDEFGHIJKL
1  111111111111
...
 print(hash(df1.values.tostring()))
-3571422965125408226

print(hash(df2.values.tostring()))
5039867957859242153
",16,39,0,1,
449,48294332,48295933,19741,Plot datetime.timedelta,3,<python><matplotlib><datetime><timedelta>,11,"<p>I am working on a task, where I need to calculate time spent on each day and then represent that time using a bar plot, so for this task I used python and able to get time spent on each day, and stored it in a list &quot;time_list&quot;, now I don't understand how to plot this using matplotlib function.</p>
<p>The problem is that, this list contains datetime.timedelta class values.
Example:</p>
<pre><code>time_list
[datetime.timedelta(0, 23820), datetime.timedelta(0, 27480), datetime.timedelta(0, 28500), datetime.timedelta(0, 24180), datetime.timedelta(0, 27540), datetime.timedelta(0, 28920), datetime.timedelta(0, 28800), datetime.timedelta(0, 29100), datetime.timedelta(0, 29100), datetime.timedelta(0, 24480), datetime.timedelta(0, 27000)]
</code></pre>
<p>And these values meaning is as follow:</p>
<pre><code>Total Time Spent on  2  is  6:37:00
Total Time Spent on  3  is  7:38:00
Total Time Spent on  4  is  7:55:00
Total Time Spent on  5  is  6:43:00
Total Time Spent on  8  is  7:39:00
Total Time Spent on  9  is  8:02:00
Total Time Spent on  10  is  8:00:00
Total Time Spent on  11  is  8:05:00
Total Time Spent on  12  is  8:05:00
Total Time Spent on  15  is  6:48:00
Total Time Spent on  16  is  7:30:00
</code></pre>
",2809861,377,17-01-2018 05:46,17-01-2018 07:42,0,377,20,1,4,100,"{'badge_counts': {'bronze': 20, 'silver': 4, 'gold': 1}, 'account_id': 3276396, 'is_employee': False, 'last_modified_date': 1689168002, 'last_access_date': 1710512842, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 377, 'creation_date': 1380005362, 'user_type': 'registered', 'user_id': 2809861, 'accept_rate': 100, 'location': 'India', 'link': 'https://stackoverflow.com/users/2809861/xpress-embedo', 'profile_image': 'https://www.gravatar.com/avatar/049be581a4a8c551f9b135df60f76928?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'xpress_embedo'}","I am working on a task, where I need to calculate time spent on each day and then represent that time using a bar plot, so for this task I used python and able to get time spent on each day, and stored it in a list &quot;time_list&quot;, now I don't understand how to plot this using matplotlib function. The problem is that, this list contains datetime.timedelta class values. Example: And these values meaning is as follow:","time_list
[datetime.timedelta(0, 23820), datetime.timedelta(0, 27480), datetime.timedelta(0, 28500), datetime.timedelta(0, 24180), datetime.timedelta(0, 27540), datetime.timedelta(0, 28920), datetime.timedelta(0, 28800), datetime.timedelta(0, 29100), datetime.timedelta(0, 29100), datetime.timedelta(0, 24480), datetime.timedelta(0, 27000)]
 Total Time Spent on  2  is  6:37:00
Total Time Spent on  3  is  7:38:00
Total Time Spent on  4  is  7:55:00
Total Time Spent on  5  is  6:43:00
Total Time Spent on  8  is  7:39:00
Total Time Spent on  9  is  8:02:00
Total Time Spent on  10  is  8:00:00
Total Time Spent on  11  is  8:05:00
Total Time Spent on  12  is  8:05:00
Total Time Spent on  15  is  6:48:00
Total Time Spent on  16  is  7:30:00
",11,19,0,0,
450,48283886,48302646,13895,"Python type-hinting, indexable object",3,<python><type-hinting>,19,"<p>My function needs to accept an object, from which data can be extracted by index, viz. a <code>List</code> or an instance with defined <code>__getitem__</code> method.</p>

<p>Which type can I use for type hinting this argument?</p>

<p>Update:
As I understand there are presently no such type, I tried to make one myself:</p>

<pre class=""lang-py prettyprint-override""><code>class IndexableContainer(Generic[int, ReturnType]):
    def __getitem__(self, key: int) -&gt; ReturnType:
        ...
</code></pre>

<p>But I get the following error:</p>

<pre class=""lang-sh prettyprint-override""><code>  File ""indexable_container.py"", line 22, in IndexableContainer
    class IndexableContainer(Generic[int, ReturnType]):
  File "".../lib/python3.6/typing.py"", line 682, in inner
    return func(*args, **kwds)
  File "".../lib/python3.6/typing.py"", line 1112, in __getitem__
    ""Parameters to Generic[...] must all be type variables"")
TypeError: Parameters to Generic[...] must all be type variables
</code></pre>

<p>How should I do it?</p>
",2059584,804,16-01-2018 14:47,17-01-2018 13:46,1,804,35,1,13,83,"{'badge_counts': {'bronze': 35, 'silver': 13, 'gold': 1}, 'account_id': 2350644, 'is_employee': False, 'last_modified_date': 1696567799, 'last_access_date': 1711110933, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 804, 'creation_date': 1360528114, 'user_type': 'registered', 'user_id': 2059584, 'accept_rate': 83, 'location': 'United Kingdom', 'website_url': 'http://rizhiy.com', 'link': 'https://stackoverflow.com/users/2059584/rizhiy', 'profile_image': 'https://i.stack.imgur.com/r1tOa.jpg?s=256&g=1', 'display_name': 'Rizhiy'}","My function needs to accept an object, from which data can be extracted by index, viz. a or an instance with defined method. Which type can I use for type hinting this argument? Update: As I understand there are presently no such type, I tried to make one myself: But I get the following error: How should I do it?","List __getitem__ class IndexableContainer(Generic[int, ReturnType]):
    def __getitem__(self, key: int) -&gt; ReturnType:
        ...
   File ""indexable_container.py"", line 22, in IndexableContainer
    class IndexableContainer(Generic[int, ReturnType]):
  File "".../lib/python3.6/typing.py"", line 682, in inner
    return func(*args, **kwds)
  File "".../lib/python3.6/typing.py"", line 1112, in __getitem__
    ""Parameters to Generic[...] must all be type variables"")
TypeError: Parameters to Generic[...] must all be type variables
",6,24,0,0,
451,48258466,48258476,7173,Pandas select columns with regex and divide by value,3,<python><regex><pandas>,11,"<p>I want to divide all values in certain columns matching a regex expression by some value and still have the complete dataframe.</p>

<p>As can be found here: <a href=""https://stackoverflow.com/questions/30808430/how-to-select-columns-from-dataframe-by-regex"">How to select columns from dataframe by regex</a> , e.g. all columns starting with d can be selected with:</p>

<pre><code>df.filter(regex=(""d.*""))
</code></pre>

<p>Now I have the columns selected I need, I want e.g. divide the values by 2. Which is possible with the following code:</p>

<pre><code>df.filter(regex=(""d.*"")).divide(2)
</code></pre>

<p>However if I try to update my dataframe like this, it gives a <code>can't assign to function call</code>:</p>

<pre><code>df.filter(regex=(""d.*"")) = df.filter(regex=(""d.*"")).divide(2)
</code></pre>

<p>How to properly update my existing df?</p>
",3399066,6042,15-01-2018 07:19,15-01-2018 07:19,0,6052,78,7,43,84,"{'badge_counts': {'bronze': 78, 'silver': 43, 'gold': 7}, 'account_id': 4145068, 'is_employee': False, 'last_modified_date': 1668606600, 'last_access_date': 1707199598, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6052, 'creation_date': 1394386212, 'user_type': 'registered', 'user_id': 3399066, 'accept_rate': 84, 'website_url': '', 'link': 'https://stackoverflow.com/users/3399066/numessanguis', 'profile_image': 'https://www.gravatar.com/avatar/d172044b860e517fd6dba7df2d1841aa?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'NumesSanguis'}","I want to divide all values in certain columns matching a regex expression by some value and still have the complete dataframe. As can be found here: How to select columns from dataframe by regex , e.g. all columns starting with d can be selected with: Now I have the columns selected I need, I want e.g. divide the values by 2. Which is possible with the following code: However if I try to update my dataframe like this, it gives a : How to properly update my existing df?","df.filter(regex=(""d.*""))
 df.filter(regex=(""d.*"")).divide(2)
 can't assign to function call df.filter(regex=(""d.*"")) = df.filter(regex=(""d.*"")).divide(2)
",-1,18,0,1,
452,48752152,48752179,8776,How do I pass a string into subprocess.run using stdin?,2,<python><python-3.x><subprocess>,18,"<p>How do I pass a string <code>&quot;foo&quot;</code> to a program expecting it on stdin if I'm using Python's <code>subprocess.run()</code>?</p>
",4360746,5575,12-02-2018 17:27,12-02-2018 17:29,0,5605,69,4,35,53,"{'badge_counts': {'bronze': 69, 'silver': 35, 'gold': 4}, 'account_id': 5486002, 'is_employee': False, 'last_modified_date': 1668700805, 'last_access_date': 1695806972, 'reputation_change_year': 140, 'reputation_change_quarter': 140, 'reputation_change_month': 80, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 5605, 'creation_date': 1418606642, 'user_type': 'registered', 'user_id': 4360746, 'accept_rate': 53, 'location': 'Boston, MA', 'website_url': 'http://www.raydanielmystery.com', 'link': 'https://stackoverflow.com/users/4360746/ray-salemi', 'profile_image': 'https://www.gravatar.com/avatar/1daf5b4cd599a49ba6464490ee2a7dd1?s=256&d=identicon&r=PG', 'display_name': 'Ray Salemi'}",How do I pass a string to a program expecting it on stdin if I'm using Python's ?,&quot;foo&quot; subprocess.run(),-2,1,0,0,
453,48919761,48919962,38614,How to run .so files using through python script,3,<python><cython><shared-objects>,11,"<p>I have a c program(.c file). I am converting that to a shared object(.so). How can i call and run the shared object from my python code? If possible, please suggest me a list of libraries that can help me to do this task.</p>
",3415910,450,22-02-2018 04:36,22-02-2018 05:00,0,450,19,3,5,75,"{'badge_counts': {'bronze': 19, 'silver': 5, 'gold': 3}, 'account_id': 4167184, 'is_employee': False, 'last_modified_date': 1618271404, 'last_access_date': 1710324543, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 450, 'creation_date': 1394721524, 'user_type': 'registered', 'user_id': 3415910, 'accept_rate': 75, 'link': 'https://stackoverflow.com/users/3415910/user3415910', 'profile_image': 'https://www.gravatar.com/avatar/86ad2600c8f89a1c0c1288963f5563d3?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user3415910'}","I have a c program(.c file). I am converting that to a shared object(.so). How can i call and run the shared object from my python code? If possible, please suggest me a list of libraries that can help me to do this task.",,0,1,0,0,
454,48731746,51180882,13473,"How to set a Tkinter widget to a monospaced, platform independent font?",1,<python><tkinter><fonts><ttk><monospace>,15,"<p>It's said <a href=""http://www.tkdocs.com/tutorial/fonts.html"" rel=""noreferrer"">here</a> in the section  <em>Standard Fonts</em>:</p>

<blockquote>
  <p>Particularly for more-or-less standard user interface elements, each
  platform defines specific fonts that should be used. Tk encapsulates
  many of these into a standard set of fonts that are always available,
  and of course the standard widgets use these fonts. This helps
  abstract away platform differences.</p>
</blockquote>

<p>And then in the predefined fonts list there is:</p>

<blockquote>
  <p><strong><code>TkFixedFont</code></strong> <code>A standard fixed-width font.</code></p>
</blockquote>

<p>This also corresponds with what I could find here on the standard ways of choosing a monospaced, platform independent font in <code>Tkinter</code> like for example stated in <a href=""https://stackoverflow.com/a/42332342/4465708"">this answer</a>.</p>

<p>Alas, when I try to do this on my own, like with the simple code below:</p>

<pre><code>import tkinter as tk
from tkinter import ttk

root = tk.Tk()
frame = ttk.Frame(root)
style = ttk.Style()
style.configure(""Fixed.TButton"", font=(""TkFixedFont"", 16))

button = ttk.Button(text=""Some monospaced text, hopefully"", style=""Fixed.TButton"")
frame.grid()
button.grid(sticky=""news"")
button.configure(text=""I don't quite like this font."")
</code></pre>

<p>what I get is this:</p>

<p><a href=""https://i.stack.imgur.com/M8wQp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M8wQp.png"" alt=""TkFixedFont, size 16""></a></p>

<p>That doesn't look like monospaced to me, so I check what exactly <code>Tkinter</code> translates <code>TkFixedFont</code> into on my platform with:</p>

<pre><code>from tkinter import font
font.nametofont(""TkFixedFont"").actual()
</code></pre>

<p>and the answer is:</p>

<pre><code>{'family': 'DejaVu Sans Mono', 'size': 9, 'weight': 'normal', 'slant': 'roman', 'underline': 0, 'overstrike': 0}
</code></pre>

<p>So how does <code>DejaVu Sans Mono</code> look like?</p>

<p><a href=""https://i.stack.imgur.com/840Pz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/840Pz.png"" alt=""DejaVu Sans Mono, size 16""></a></p>

<p>The <em>Tkdocs.com tutorial</em> quoted above has also a section on <em>Named Fonts</em> and there it says:</p>

<blockquote>
  <p>the names <strong><code>Courier</code></strong>, <code>Times</code>, and <code>Helvetica</code> are guaranteed to be supported
  (and mapped to an appropriate <strong>monospaced</strong>, serif, or sans-serif font)</p>
</blockquote>

<p>So I try with:</p>

<pre><code>style.configure(""Courier.TButton"", font=(""Courier"", 16))
button.configure(style=""Courier.TButton"")
</code></pre>

<p>and now finally I get a monospaced result:</p>

<p><a href=""https://i.stack.imgur.com/utKwB.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/utKwB.png"" alt=""enter image description here""></a></p>

<p>Admittedly, it's <code>Courier New</code> and not <code>DejaVu Sans Mono</code> that my platform chooses as the standard monospaced font, but that's at least something, right? But shouldn't <code>TkFixedFont</code> just work?</p>
",4465708,3447,11-02-2018 13:01,04-07-2018 21:21,143,3446,41,6,25,100,"{'badge_counts': {'bronze': 41, 'silver': 25, 'gold': 6}, 'account_id': 3368581, 'is_employee': False, 'last_modified_date': 1677348600, 'last_access_date': 1710955845, 'reputation_change_year': 47, 'reputation_change_quarter': 47, 'reputation_change_month': 9, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3446, 'creation_date': 1421529174, 'user_type': 'registered', 'user_id': 4465708, 'accept_rate': 100, 'location': 'Poland', 'website_url': '', 'link': 'https://stackoverflow.com/users/4465708/z33k', 'profile_image': 'https://i.stack.imgur.com/xSEVw.jpg?s=256&g=1', 'display_name': 'z33k'}","It's said here in the section Standard Fonts: Particularly for more-or-less standard user interface elements, each platform defines specific fonts that should be used. Tk encapsulates many of these into a standard set of fonts that are always available, and of course the standard widgets use these fonts. This helps abstract away platform differences. And then in the predefined fonts list there is: This also corresponds with what I could find here on the standard ways of choosing a monospaced, platform independent font in like for example stated in this answer. Alas, when I try to do this on my own, like with the simple code below: what I get is this: That doesn't look like monospaced to me, so I check what exactly translates into on my platform with: and the answer is: So how does look like? The Tkdocs.com tutorial quoted above has also a section on Named Fonts and there it says: the names , , and are guaranteed to be supported (and mapped to an appropriate monospaced, serif, or sans-serif font) So I try with: and now finally I get a monospaced result: Admittedly, it's and not that my platform chooses as the standard monospaced font, but that's at least something, right? But shouldn't just work?","TkFixedFont A standard fixed-width font. Tkinter import tkinter as tk
from tkinter import ttk

root = tk.Tk()
frame = ttk.Frame(root)
style = ttk.Style()
style.configure(""Fixed.TButton"", font=(""TkFixedFont"", 16))

button = ttk.Button(text=""Some monospaced text, hopefully"", style=""Fixed.TButton"")
frame.grid()
button.grid(sticky=""news"")
button.configure(text=""I don't quite like this font."")
 Tkinter TkFixedFont from tkinter import font
font.nametofont(""TkFixedFont"").actual()
 {'family': 'DejaVu Sans Mono', 'size': 9, 'weight': 'normal', 'slant': 'roman', 'underline': 0, 'overstrike': 0}
 DejaVu Sans Mono Courier Times Helvetica style.configure(""Courier.TButton"", font=(""Courier"", 16))
button.configure(style=""Courier.TButton"")
 Courier New DejaVu Sans Mono TkFixedFont",1,71,3,5,
455,50375985,50376135,112216,Pandas add column with value based on condition based on other columns,2,<python><pandas><dataframe><performance><conditional-statements>,48,"<p>I have the following pandas dataframe:</p>
<p><a href=""https://i.stack.imgur.com/zMiCD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zMiCD.png"" alt=""df"" /></a></p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np

d = {'age' : [21, 45, 45, 5],
     'salary' : [20, 40, 10, 100]}

df = pd.DataFrame(d)
</code></pre>
<p>and would like to add an extra column called &quot;is_rich&quot; which captures if a person is rich depending on his/her salary. I found multiple ways to accomplish this:</p>
<pre class=""lang-py prettyprint-override""><code># method 1
df['is_rich_method1'] = np.where(df['salary']&gt;=50, 'yes', 'no')

# method 2
df['is_rich_method2'] = ['yes' if x &gt;= 50 else 'no' for x in df['salary']]

# method 3
df['is_rich_method3'] = 'no'
df.loc[df['salary'] &gt; 50,'is_rich_method3'] = 'yes'
</code></pre>
<p>resulting in:</p>
<p><a href=""https://i.stack.imgur.com/kodEo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kodEo.png"" alt=""df2"" /></a></p>
<p>However I don't understand what the preferred way is. Are all methods equally good depending on your application?</p>
",3429605,4193,16-05-2018 16:36,16-05-2018 16:46,0,4223,46,3,36,33,"{'badge_counts': {'bronze': 46, 'silver': 36, 'gold': 3}, 'account_id': 3976255, 'is_employee': False, 'last_modified_date': 1648514700, 'last_access_date': 1698052976, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4223, 'creation_date': 1395071820, 'user_type': 'registered', 'user_id': 3429605, 'accept_rate': 33, 'website_url': '', 'link': 'https://stackoverflow.com/users/3429605/rutger-hofste', 'profile_image': 'https://www.gravatar.com/avatar/de10992453ae7431ca7313869d840c0c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Rutger Hofste'}",I have the following pandas dataframe: and would like to add an extra column called &quot;is_rich&quot; which captures if a person is rich depending on his/her salary. I found multiple ways to accomplish this: resulting in: However I don't understand what the preferred way is. Are all methods equally good depending on your application?,"import pandas as pd
import numpy as np

d = {'age' : [21, 45, 45, 5],
     'salary' : [20, 40, 10, 100]}

df = pd.DataFrame(d)
 # method 1
df['is_rich_method1'] = np.where(df['salary']&gt;=50, 'yes', 'no')

# method 2
df['is_rich_method2'] = ['yes' if x &gt;= 50 else 'no' for x in df['salary']]

# method 3
df['is_rich_method3'] = 'no'
df.loc[df['salary'] &gt; 50,'is_rich_method3'] = 'yes'
",14,24,2,2,
456,50307483,50307485,1289,How many local variables can a Python (CPython implementation) function possibly hold?,2,<python><python-3.x><function><namespaces><python-internals>,18,"<p>We already know that Function arguments used to have the <a href=""https://stackoverflow.com/questions/714475/what-is-a-maximum-number-of-arguments-in-a-python-function?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa"">limit of 255 explicitly passed arguments</a>. However, this behaviour is changed now and since Python-3.7 there's no limit except <code>sys.maxsize</code> which is actually the limit of python's containers. But what about the local variables?</p>

<p>We basically cannot add local variables to a function in a dynamic manner and/or changing the <code>locals()</code> dictionary is not permitted directly so that one can even test this in a brute force way. But the problem is that even if you change the <code>locals()</code> using <code>compile</code> module or <code>exec</code> function it doesn't affect the <code>function.__code__.co_varnames</code>, hence, you cannot access the variables explicitly inside the function. </p>

<pre><code>In [142]: def bar():
     ...:     exec('k=10')
     ...:     print(f""locals: {locals()}"")
     ...:     print(k)
     ...:     g = 100
     ...:     
     ...:     

In [143]: bar()
locals: {'k': 10}
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-143-226d01f48125&gt; in &lt;module&gt;()
----&gt; 1 bar()

&lt;ipython-input-142-69d0ec0a7b24&gt; in bar()
      2     exec('k=10')
      3     print(f""locals: {locals()}"")
----&gt; 4     print(k)
      5     g = 100
      6 

NameError: name 'k' is not defined

In [144]: bar.__code__.co_varnames
Out[144]: ('g',)
</code></pre>

<p>This means that even if you use a <code>for</code> loop like:</p>

<pre><code>for i in range(2**17):
    exec(f'var_{i} = {i}')
</code></pre>

<p>The <code>locals()</code> will be contain 2**17 variables but you cannot do something like <code>print(var_100)</code> inside the function. </p>

<p>We know that basically there is no need to dynamically add a variable to the function while you can use a dictionary or in other words a custom namespace. But what's the proper way to test the limit of the maximum number of local variables in a function?</p>
",2867928,106057,12-05-2018 14:48,12-05-2018 14:48,0,106117,190,19,160,90,"{'badge_counts': {'bronze': 190, 'silver': 160, 'gold': 19}, 'account_id': 2444936, 'is_employee': False, 'last_modified_date': 1705925700, 'last_access_date': 1710972153, 'reputation_change_year': 528, 'reputation_change_quarter': 528, 'reputation_change_month': 110, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 106117, 'creation_date': 1381421431, 'user_type': 'registered', 'user_id': 2867928, 'accept_rate': 90, 'website_url': '', 'link': 'https://stackoverflow.com/users/2867928/mazdak', 'profile_image': 'https://i.stack.imgur.com/5qfHZ.png?s=256&g=1', 'display_name': 'Mazdak'}","We already know that Function arguments used to have the limit of 255 explicitly passed arguments. However, this behaviour is changed now and since Python-3.7 there's no limit except which is actually the limit of python's containers. But what about the local variables? We basically cannot add local variables to a function in a dynamic manner and/or changing the dictionary is not permitted directly so that one can even test this in a brute force way. But the problem is that even if you change the using module or function it doesn't affect the , hence, you cannot access the variables explicitly inside the function. This means that even if you use a loop like: The will be contain 2**17 variables but you cannot do something like inside the function. We know that basically there is no need to dynamically add a variable to the function while you can use a dictionary or in other words a custom namespace. But what's the proper way to test the limit of the maximum number of local variables in a function?","sys.maxsize locals() locals() compile exec function.__code__.co_varnames In [142]: def bar():
     ...:     exec('k=10')
     ...:     print(f""locals: {locals()}"")
     ...:     print(k)
     ...:     g = 100
     ...:     
     ...:     

In [143]: bar()
locals: {'k': 10}
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-143-226d01f48125&gt; in &lt;module&gt;()
----&gt; 1 bar()

&lt;ipython-input-142-69d0ec0a7b24&gt; in bar()
      2     exec('k=10')
      3     print(f""locals: {locals()}"")
----&gt; 4     print(k)
      5     g = 100
      6 

NameError: name 'k' is not defined

In [144]: bar.__code__.co_varnames
Out[144]: ('g',)
 for for i in range(2**17):
    exec(f'var_{i} = {i}')
 locals() print(var_100)",17,41,0,1,
457,48548878,48549498,20174,"psycopg2.DataError: invalid input syntax for integer: ""test"" Getting error when moving code to test server",6,<python><django>,11,"<p>I'm running Django 1.11 with Python 3.4 on Ubuntu 14.04.5</p>

<p>Moving my development code to the test server and running into some strange errors.  Can anyone see what is wrong from the traceback?</p>

<p>I'm very new to linux and have made the mistake of developing on a Windows machine on this first go around.  I have since created a virtualbox copy of the test and production servers to develop on, but I'm hoping I can salvage what's up on the test server now.</p>

<p>I think my app is looking in the correct directory for this environment, but I am a Django, Python and linux noob.</p>

<p>Any direction would be very helpful.</p>

<p>**UPDATE: I added models.py and migration for relevant app. Also, I was using sqlite on dev machine and am using postgreSQL on test server (like a fool).</p>

<p>Thanks! 
staff_manager/models.py</p>

<pre><code>    # -*- coding: utf-8 -*-
from __future__ import unicode_literals

# Create your models here.

from django.db import models
from django.utils.encoding import python_2_unicode_compatible
from smrt.settings import DATE_INPUT_FORMATS


class OrganizationTitle(models.Model):
    def __str__(self):
        return ""{}"".format(self.organization_title_name)
    organization_title_name = models.CharField(max_length=150, unique=True)


class ClassificationTitle(models.Model):
    def __str__(self):
        return ""{}"".format(self.classification_title_name)
    classification_title_name = models.CharField(max_length=150, unique=True)


class WorkingTitle(models.Model):
    def __str__(self):
        return ""{}"".format(self.working_title_name)
    working_title_name = models.CharField(max_length=150, unique=True)


class Category(models.Model):
    def __str__(self):
        return ""{}"".format(self.category_name)
    category_name = models.CharField(max_length=150, unique=True)


class Department(models.Model):
    def __str__(self):
        return ""{}"".format(self.department_name)
    department_name = models.CharField(max_length=150, unique=True)


class Employee(models.Model):
    first_name = models.CharField(max_length=150)
    last_name = models.CharField(max_length=150)
    org_title = models.ForeignKey(OrganizationTitle, blank=True, null=True, on_delete=models.SET_NULL)
    manager = models.ForeignKey('self', blank=True, null=True, on_delete=models.SET_NULL)
    manager_email = models.EmailField(max_length=50, blank=True, null=True)
    hire_date = models.DateField(blank=True, null=True)
    classification_title = models.ForeignKey(ClassificationTitle, blank=True, null=True, on_delete=models.SET_NULL)
    working_title = models.ForeignKey(WorkingTitle, blank=True, null=True, on_delete=models.SET_NULL)
    email_address = models.EmailField(max_length=250, blank=False, unique=True,
                                      error_messages={'unique': 'An account with this email exist.',
                                                      'required': 'Please provide an email address.'})
    category = models.ForeignKey(Category, blank=True, null=True, on_delete=models.SET_NULL)
    is_substitute = models.BooleanField(default=False)
    department = models.ForeignKey(Department, blank=True, null=True, on_delete=models.SET_NULL)
    is_active = models.BooleanField(default=True)
    is_manager = models.BooleanField(default=False)

    class Meta:
        ordering = ('is_active', 'last_name',)

    def __str__(self):
        return ""{}"".format(self.first_name + ' ' + self.last_name)

    def __iter__(self):
        return iter([
                     self.email_address,
                     self.last_name,
                     self.first_name,
                     self.org_title,
                     self.manager,
                     self.manager.email_address,
                     self.hire_date,
                     self.classification_title,
                     self.working_title,
                     self.email_address,
                     self.category,
                     self.is_substitute,
                     self.department
                     ])

    def save(self, *args, **kwargs):
        for field_name in ['first_name', 'last_name']:
            val = getattr(self, field_name, False)
            if val:
                setattr(self, field_name, val.capitalize())
        super(Employee, self).save(*args, **kwargs)
</code></pre>

<p>MIGRATION staff_manager.0003_auto_20180131_1756: </p>

<pre><code># -*- coding: utf-8 -*-
# Generated by Django 1.11.7 on 2018-01-31 17:56
from __future__ import unicode_literals

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('staff_manager', '0002_auto_20171127_2244'),
    ]

    operations = [
        migrations.CreateModel(
            name='Category',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('category_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='ClassificationTitle',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('classification_title_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='Department',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('department_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='OrganizationTitle',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('organization_title_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='WorkingTitle',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('working_title_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.AlterField(
            model_name='employee',
            name='category',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.Category'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='classification_title',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.ClassificationTitle'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='department',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.Department'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='email_address',
            field=models.EmailField(error_messages={'required': 'Please provide an email address.', 'unique': 'An account with this email exist.'}, max_length=250, unique=True),
        ),
        migrations.AlterField(
            model_name='employee',
            name='first_name',
            field=models.CharField(max_length=150),
        ),
        migrations.AlterField(
            model_name='employee',
            name='hire_date',
            field=models.DateField(blank=True, null=True),
        ),
        migrations.AlterField(
            model_name='employee',
            name='last_name',
            field=models.CharField(max_length=150),
        ),
        migrations.AlterField(
            model_name='employee',
            name='manager',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.Employee'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='manager_email',
            field=models.EmailField(blank=True, max_length=50, null=True),
        ),
        migrations.AlterField(
            model_name='employee',
            name='org_title',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.OrganizationTitle'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='working_title',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.WorkingTitle'),
        ),
    ]
</code></pre>

<p>TRACEBACK:</p>

<pre><code>Operations to perform:
      Apply all migrations: admin, auth, contenttypes, csvimport, sessions, staff_manager
    Running migrations:
      Applying staff_manager.0003_auto_20180131_1756...Traceback (most recent call last):
      File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/utils.py"", line 65, in execute
        return self.cursor.execute(sql, params)
    psycopg2.DataError: invalid input syntax for integer: ""test""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""manage.py"", line 22, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/__init__.py"", line 364, in execute_from_command_line
    utility.execute()
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/__init__.py"", line 356, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/base.py"", line 283, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/base.py"", line 330, in execute
    output = self.handle(*args, **options)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/commands/migrate.py"", line 204, in handle
    fake_initial=fake_initial,
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/executor.py"", line 115, in migrate
    state = self._migrate_all_forwards(state, plan, full_plan, fake=fake, fake_initial=fake_initial)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/executor.py"", line 145, in _migrate_all_forwards
    state = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/executor.py"", line 244, in apply_migration
    state = migration.apply(state, schema_editor)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/migration.py"", line 129, in apply
    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/operations/fields.py"", line 216, in database_forwards
    schema_editor.alter_field(from_model, from_field, to_field)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/base/schema.py"", line 515, in alter_field
    old_db_params, new_db_params, strict)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/postgresql/schema.py"", line 112, in _alter_field
    new_db_params, strict,
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/base/schema.py"", line 684, in _alter_field
    params,
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/base/schema.py"", line 120, in execute
    cursor.execute(sql, params)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/utils.py"", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/utils.py"", line 65, in execute
    return self.cursor.execute(sql, params)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/utils.py"", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/utils/six.py"", line 685, in reraise
    raise value.with_traceback(tb)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/utils.py"", line 65, in execute
    return self.cursor.execute(sql, params)
django.db.utils.DataError: invalid input syntax for integer: ""test""

(django_env_1) www-root@Server:~/envs/django_env_1/smrt$
^C
(django_env_1) www-root@Server:~/envs/django_env_1/smrt$ django.db.utils.DataError: invalid input syntax for integer: ""test""django.db.utils.DataError: invalid input syntax for integer: ""test""
</code></pre>
",3541909,733,31-01-2018 18:15,31-01-2018 18:55,0,733,16,2,7,100,"{'badge_counts': {'bronze': 16, 'silver': 7, 'gold': 2}, 'account_id': 4337842, 'is_employee': False, 'last_modified_date': 1685373002, 'last_access_date': 1680796635, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 733, 'creation_date': 1397662510, 'user_type': 'registered', 'user_id': 3541909, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/3541909/totaltotals', 'profile_image': 'https://www.gravatar.com/avatar/c8b08ddf8a52a2f9f7e2d430f240e800?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'totaltotals'}","I'm running Django 1.11 with Python 3.4 on Ubuntu 14.04.5 Moving my development code to the test server and running into some strange errors. Can anyone see what is wrong from the traceback? I'm very new to linux and have made the mistake of developing on a Windows machine on this first go around. I have since created a virtualbox copy of the test and production servers to develop on, but I'm hoping I can salvage what's up on the test server now. I think my app is looking in the correct directory for this environment, but I am a Django, Python and linux noob. Any direction would be very helpful. **UPDATE: I added models.py and migration for relevant app. Also, I was using sqlite on dev machine and am using postgreSQL on test server (like a fool). Thanks! staff_manager/models.py MIGRATION staff_manager.0003_auto_20180131_1756: TRACEBACK:","    # -*- coding: utf-8 -*-
from __future__ import unicode_literals

# Create your models here.

from django.db import models
from django.utils.encoding import python_2_unicode_compatible
from smrt.settings import DATE_INPUT_FORMATS


class OrganizationTitle(models.Model):
    def __str__(self):
        return ""{}"".format(self.organization_title_name)
    organization_title_name = models.CharField(max_length=150, unique=True)


class ClassificationTitle(models.Model):
    def __str__(self):
        return ""{}"".format(self.classification_title_name)
    classification_title_name = models.CharField(max_length=150, unique=True)


class WorkingTitle(models.Model):
    def __str__(self):
        return ""{}"".format(self.working_title_name)
    working_title_name = models.CharField(max_length=150, unique=True)


class Category(models.Model):
    def __str__(self):
        return ""{}"".format(self.category_name)
    category_name = models.CharField(max_length=150, unique=True)


class Department(models.Model):
    def __str__(self):
        return ""{}"".format(self.department_name)
    department_name = models.CharField(max_length=150, unique=True)


class Employee(models.Model):
    first_name = models.CharField(max_length=150)
    last_name = models.CharField(max_length=150)
    org_title = models.ForeignKey(OrganizationTitle, blank=True, null=True, on_delete=models.SET_NULL)
    manager = models.ForeignKey('self', blank=True, null=True, on_delete=models.SET_NULL)
    manager_email = models.EmailField(max_length=50, blank=True, null=True)
    hire_date = models.DateField(blank=True, null=True)
    classification_title = models.ForeignKey(ClassificationTitle, blank=True, null=True, on_delete=models.SET_NULL)
    working_title = models.ForeignKey(WorkingTitle, blank=True, null=True, on_delete=models.SET_NULL)
    email_address = models.EmailField(max_length=250, blank=False, unique=True,
                                      error_messages={'unique': 'An account with this email exist.',
                                                      'required': 'Please provide an email address.'})
    category = models.ForeignKey(Category, blank=True, null=True, on_delete=models.SET_NULL)
    is_substitute = models.BooleanField(default=False)
    department = models.ForeignKey(Department, blank=True, null=True, on_delete=models.SET_NULL)
    is_active = models.BooleanField(default=True)
    is_manager = models.BooleanField(default=False)

    class Meta:
        ordering = ('is_active', 'last_name',)

    def __str__(self):
        return ""{}"".format(self.first_name + ' ' + self.last_name)

    def __iter__(self):
        return iter([
                     self.email_address,
                     self.last_name,
                     self.first_name,
                     self.org_title,
                     self.manager,
                     self.manager.email_address,
                     self.hire_date,
                     self.classification_title,
                     self.working_title,
                     self.email_address,
                     self.category,
                     self.is_substitute,
                     self.department
                     ])

    def save(self, *args, **kwargs):
        for field_name in ['first_name', 'last_name']:
            val = getattr(self, field_name, False)
            if val:
                setattr(self, field_name, val.capitalize())
        super(Employee, self).save(*args, **kwargs)
 # -*- coding: utf-8 -*-
# Generated by Django 1.11.7 on 2018-01-31 17:56
from __future__ import unicode_literals

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('staff_manager', '0002_auto_20171127_2244'),
    ]

    operations = [
        migrations.CreateModel(
            name='Category',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('category_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='ClassificationTitle',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('classification_title_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='Department',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('department_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='OrganizationTitle',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('organization_title_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.CreateModel(
            name='WorkingTitle',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('working_title_name', models.CharField(max_length=150, unique=True)),
            ],
        ),
        migrations.AlterField(
            model_name='employee',
            name='category',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.Category'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='classification_title',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.ClassificationTitle'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='department',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.Department'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='email_address',
            field=models.EmailField(error_messages={'required': 'Please provide an email address.', 'unique': 'An account with this email exist.'}, max_length=250, unique=True),
        ),
        migrations.AlterField(
            model_name='employee',
            name='first_name',
            field=models.CharField(max_length=150),
        ),
        migrations.AlterField(
            model_name='employee',
            name='hire_date',
            field=models.DateField(blank=True, null=True),
        ),
        migrations.AlterField(
            model_name='employee',
            name='last_name',
            field=models.CharField(max_length=150),
        ),
        migrations.AlterField(
            model_name='employee',
            name='manager',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.Employee'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='manager_email',
            field=models.EmailField(blank=True, max_length=50, null=True),
        ),
        migrations.AlterField(
            model_name='employee',
            name='org_title',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.OrganizationTitle'),
        ),
        migrations.AlterField(
            model_name='employee',
            name='working_title',
            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.SET_NULL, to='staff_manager.WorkingTitle'),
        ),
    ]
 Operations to perform:
      Apply all migrations: admin, auth, contenttypes, csvimport, sessions, staff_manager
    Running migrations:
      Applying staff_manager.0003_auto_20180131_1756...Traceback (most recent call last):
      File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/utils.py"", line 65, in execute
        return self.cursor.execute(sql, params)
    psycopg2.DataError: invalid input syntax for integer: ""test""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""manage.py"", line 22, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/__init__.py"", line 364, in execute_from_command_line
    utility.execute()
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/__init__.py"", line 356, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/base.py"", line 283, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/base.py"", line 330, in execute
    output = self.handle(*args, **options)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/core/management/commands/migrate.py"", line 204, in handle
    fake_initial=fake_initial,
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/executor.py"", line 115, in migrate
    state = self._migrate_all_forwards(state, plan, full_plan, fake=fake, fake_initial=fake_initial)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/executor.py"", line 145, in _migrate_all_forwards
    state = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/executor.py"", line 244, in apply_migration
    state = migration.apply(state, schema_editor)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/migration.py"", line 129, in apply
    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/migrations/operations/fields.py"", line 216, in database_forwards
    schema_editor.alter_field(from_model, from_field, to_field)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/base/schema.py"", line 515, in alter_field
    old_db_params, new_db_params, strict)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/postgresql/schema.py"", line 112, in _alter_field
    new_db_params, strict,
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/base/schema.py"", line 684, in _alter_field
    params,
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/base/schema.py"", line 120, in execute
    cursor.execute(sql, params)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/utils.py"", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/utils.py"", line 65, in execute
    return self.cursor.execute(sql, params)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/utils.py"", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/utils/six.py"", line 685, in reraise
    raise value.with_traceback(tb)
  File ""/home/www-root/envs/django_env_1/lib/python3.4/site-packages/django/db/backends/utils.py"", line 65, in execute
    return self.cursor.execute(sql, params)
django.db.utils.DataError: invalid input syntax for integer: ""test""

(django_env_1) www-root@Server:~/envs/django_env_1/smrt$
^C
(django_env_1) www-root@Server:~/envs/django_env_1/smrt$ django.db.utils.DataError: invalid input syntax for integer: ""test""django.db.utils.DataError: invalid input syntax for integer: ""test""
",246,273,0,0,
458,48779429,60510920,2586,PyCharm not inserting docstring stub for class?,1,<python><pycharm><stub><docstring>,12,"<p>I have the following piece of code:</p>

<pre><code>class Note:
    def __init__(self, note=None, duration=None, start_time=None):
        self.note = note
        self.duration = duration
        self.start_time = start_time
</code></pre>

<p>I wanted to generate a docstring for the class according to the NumPy docstring style, but it doesn't autocomplete. I have chosen the NumPy format in the settings under <code>File | Settings | Tools | Python Integrated Tools</code></p>

<p>The auto complete works for <code>def __init__()</code>. When I start a new line after it and type <code>'''</code> it automatically inserts this:</p>

<pre><code>'''

Parameters
----------
note :
duration :
start_time :
'''
</code></pre>

<p>But when I do the same thing under <code>class Note:</code> it doesn't do that. I'm using PyCharm 2017.3.3</p>
",3620725,4879,14-02-2018 03:58,03-03-2020 15:48,748,4888,80,5,40,86,"{'badge_counts': {'bronze': 80, 'silver': 40, 'gold': 5}, 'account_id': 4447926, 'is_employee': False, 'last_modified_date': 1698152702, 'last_access_date': 1711041323, 'reputation_change_year': 119, 'reputation_change_quarter': 119, 'reputation_change_month': 19, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4888, 'creation_date': 1399644703, 'user_type': 'registered', 'user_id': 3620725, 'accept_rate': 86, 'website_url': '', 'link': 'https://stackoverflow.com/users/3620725/pyjamas', 'profile_image': 'https://i.stack.imgur.com/WxhEq.png?s=256&g=1', 'display_name': 'pyjamas'}","I have the following piece of code: I wanted to generate a docstring for the class according to the NumPy docstring style, but it doesn't autocomplete. I have chosen the NumPy format in the settings under The auto complete works for . When I start a new line after it and type it automatically inserts this: But when I do the same thing under it doesn't do that. I'm using PyCharm 2017.3.3","class Note:
    def __init__(self, note=None, duration=None, start_time=None):
        self.note = note
        self.duration = duration
        self.start_time = start_time
 File | Settings | Tools | Python Integrated Tools def __init__() ''' '''

Parameters
----------
note :
duration :
start_time :
'''
 class Note:",7,24,0,0,
459,49785536,49785582,53567,Get learning rate of keras model,7,<python><machine-learning><neural-network><keras>,41,"<p>I cannot seem to get the value of learning rate. What I get is below. </p>

<p>I've tried the model for 200 epochs and want to see/change the learning rate. Is this not the correct way?</p>

<pre><code>&gt;&gt;&gt; print(ig_cnn_model.optimizer.lr)
&lt;tf.Variable 'lr_6:0' shape=() dtype=float32_ref&gt;
</code></pre>
",3657151,2184,11-04-2018 22:56,11-04-2018 23:00,0,2184,47,3,26,80,"{'badge_counts': {'bronze': 47, 'silver': 26, 'gold': 3}, 'account_id': 4497174, 'is_employee': False, 'last_modified_date': 1644867300, 'last_access_date': 1650504602, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2184, 'creation_date': 1400597212, 'user_type': 'registered', 'user_id': 3657151, 'accept_rate': 80, 'location': 'UK', 'link': 'https://stackoverflow.com/users/3657151/user14492', 'profile_image': 'https://www.gravatar.com/avatar/7b8b2a26937f08d487a57f2b97f7fbb8?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user14492'}",I cannot seem to get the value of learning rate. What I get is below. I've tried the model for 200 epochs and want to see/change the learning rate. Is this not the correct way?,"&gt;&gt;&gt; print(ig_cnn_model.optimizer.lr)
&lt;tf.Variable 'lr_6:0' shape=() dtype=float32_ref&gt;
",1,7,0,0,
460,48140576,48140926,12266,matplotlib toolbar in a pyqt5 application,1,<python><matplotlib><pyqt><pyqt5>,14,"<p>This is a very simple application done with python and qt, where there is an embedded matplotlib plot. I would like to include the standard toolbar (zoom, home, etc.) over the plot. Is it posible? Above there is an example of the code. </p>

<pre><code>import sys
from PyQt5 import QtCore
from PyQt5.QtWidgets import QApplication, QMainWindow, QMenu, QVBoxLayout, QSizePolicy, QMessageBox, QWidget, QPushButton, QAction, QLineEdit, QLabel
from PyQt5.QtGui import QIcon
from PyQt5 import QtWidgets
from PyQt5.QtCore import QSize

from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
import matplotlib.pyplot as plt
import random

from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar


class MainWindow(QMainWindow):
    def __init__(self):

        QMainWindow.__init__(self)

        self.title = 'test'
        self.left = 10
        self.top = 10
        self.width = 1920
        self.height = 1080

        self.setWindowTitle(self.title)
        self.setGeometry(self.left, self.top, self.width, self.height)

        m = PlotCanvas(self, width=10, height=8)
        m.move(0,100)

        self.statusBar().showMessage('Ready')

        mainMenu = self.menuBar()
        mainMenu.setNativeMenuBar(False)
        fileMenu = mainMenu.addMenu('File')
        helpMenu = mainMenu.addMenu('Help')

        exitButton = QAction(QIcon('exit24.png'), 'Exit', self)
        exitButton.setShortcut('Ctrl+Q')
        exitButton.setStatusTip('Exit application')
        exitButton.triggered.connect(self.close)
        fileMenu.addAction(exitButton)


        self.nameLabel = QLabel(self)
        self.nameLabel.setText('Name:')
        self.line = QLineEdit(self)

        self.nameLabel2 = QLabel(self)
        self.nameLabel2.setText('Result')
        #self.line2 = QLineEdit(self)

        self.line.move(80, 20)
        self.line.resize(200, 32)
        self.nameLabel.move(20, 20)

        #self.line2.move(500, 20)
        #self.line2.resize(500, 32)
        self.nameLabel2.move(500, 20)

        pybutton = QPushButton('Click me', self)
        pybutton.clicked.connect(self.clickMethod)
        pybutton.resize(100,32)
        pybutton.move(50, 50)

    def clickMethod(self):
        print('Clicked Pyqt button.')
        if((self.line.text() == '')):
            self.statusBar().showMessage('Not a Number')
        else:
            print('Number: ' + str(float(self.line.text())*2))
            self.statusBar().showMessage('Introduction of a number')
            self.nameLabel2.setText(str(float(self.line.text())*2))


class PlotCanvas(FigureCanvas):

    def __init__(self, parent=None, width=10, height=8, dpi=100):
        fig = Figure(figsize=(width, height), dpi=dpi)
        self.axes = fig.add_subplot(111)

        FigureCanvas.__init__(self, fig)
        self.setParent(parent)
        #self.addWidget(toolbar)


        FigureCanvas.setSizePolicy(self, QSizePolicy.Expanding, QSizePolicy.Expanding)
        FigureCanvas.updateGeometry(self)
        self.plot()

    def plot(self):
        data = [random.random() for i in range(250)]
        ax = self.figure.add_subplot(111)
        ax.plot(data, 'r-', linewidth = 0.5)
        ax.set_title('PyQt Matplotlib Example')
        self.draw()

if __name__ == ""__main__"":
    app = QtWidgets.QApplication(sys.argv)
    mainWin = MainWindow()
    mainWin.show()
    sys.exit( app.exec_())
</code></pre>

<p>The code has a button, a statusbar, a matplotlib plot and a QlineEdit.</p>
",3152047,2685,07-01-2018 19:02,07-01-2018 19:48,0,2705,50,4,28,97,"{'badge_counts': {'bronze': 50, 'silver': 28, 'gold': 4}, 'account_id': 3797494, 'is_employee': False, 'last_modified_date': 1668726264, 'last_access_date': 1710430814, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 2705, 'creation_date': 1388615511, 'user_type': 'registered', 'user_id': 3152047, 'accept_rate': 97, 'location': 'Porto, Portugal', 'website_url': '', 'link': 'https://stackoverflow.com/users/3152047/nunodsousa', 'profile_image': 'https://www.gravatar.com/avatar/7e42ff413d2f4929b98d1f6c42e36bdb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'nunodsousa'}","This is a very simple application done with python and qt, where there is an embedded matplotlib plot. I would like to include the standard toolbar (zoom, home, etc.) over the plot. Is it posible? Above there is an example of the code. The code has a button, a statusbar, a matplotlib plot and a QlineEdit.","import sys
from PyQt5 import QtCore
from PyQt5.QtWidgets import QApplication, QMainWindow, QMenu, QVBoxLayout, QSizePolicy, QMessageBox, QWidget, QPushButton, QAction, QLineEdit, QLabel
from PyQt5.QtGui import QIcon
from PyQt5 import QtWidgets
from PyQt5.QtCore import QSize

from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
import matplotlib.pyplot as plt
import random

from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar


class MainWindow(QMainWindow):
    def __init__(self):

        QMainWindow.__init__(self)

        self.title = 'test'
        self.left = 10
        self.top = 10
        self.width = 1920
        self.height = 1080

        self.setWindowTitle(self.title)
        self.setGeometry(self.left, self.top, self.width, self.height)

        m = PlotCanvas(self, width=10, height=8)
        m.move(0,100)

        self.statusBar().showMessage('Ready')

        mainMenu = self.menuBar()
        mainMenu.setNativeMenuBar(False)
        fileMenu = mainMenu.addMenu('File')
        helpMenu = mainMenu.addMenu('Help')

        exitButton = QAction(QIcon('exit24.png'), 'Exit', self)
        exitButton.setShortcut('Ctrl+Q')
        exitButton.setStatusTip('Exit application')
        exitButton.triggered.connect(self.close)
        fileMenu.addAction(exitButton)


        self.nameLabel = QLabel(self)
        self.nameLabel.setText('Name:')
        self.line = QLineEdit(self)

        self.nameLabel2 = QLabel(self)
        self.nameLabel2.setText('Result')
        #self.line2 = QLineEdit(self)

        self.line.move(80, 20)
        self.line.resize(200, 32)
        self.nameLabel.move(20, 20)

        #self.line2.move(500, 20)
        #self.line2.resize(500, 32)
        self.nameLabel2.move(500, 20)

        pybutton = QPushButton('Click me', self)
        pybutton.clicked.connect(self.clickMethod)
        pybutton.resize(100,32)
        pybutton.move(50, 50)

    def clickMethod(self):
        print('Clicked Pyqt button.')
        if((self.line.text() == '')):
            self.statusBar().showMessage('Not a Number')
        else:
            print('Number: ' + str(float(self.line.text())*2))
            self.statusBar().showMessage('Introduction of a number')
            self.nameLabel2.setText(str(float(self.line.text())*2))


class PlotCanvas(FigureCanvas):

    def __init__(self, parent=None, width=10, height=8, dpi=100):
        fig = Figure(figsize=(width, height), dpi=dpi)
        self.axes = fig.add_subplot(111)

        FigureCanvas.__init__(self, fig)
        self.setParent(parent)
        #self.addWidget(toolbar)


        FigureCanvas.setSizePolicy(self, QSizePolicy.Expanding, QSizePolicy.Expanding)
        FigureCanvas.updateGeometry(self)
        self.plot()

    def plot(self):
        data = [random.random() for i in range(250)]
        ax = self.figure.add_subplot(111)
        ax.plot(data, 'r-', linewidth = 0.5)
        ax.set_title('PyQt Matplotlib Example')
        self.draw()

if __name__ == ""__main__"":
    app = QtWidgets.QApplication(sys.argv)
    mainWin = MainWindow()
    mainWin.show()
    sys.exit( app.exec_())
",104,110,0,0,
461,49695990,49696936,52175,Authenticate from Linux to Windows SQL Server with pyodbc,7,<python><sql><linux><windows><pyodbc>,29,"<p>I am trying to connect from a linux machine to a windows SQL Server with pyodbc.</p>

<p>I do have a couple of constraints:</p>

<ul>
<li>Need to log on with a windows domain account</li>
<li>Need to use python3</li>
<li>Need to do it from Linux to Windows</li>
<li>Need to connect to a specific instance</li>
</ul>

<p>I set up the environment as described by microsoft and have it working (I can import pyodbc and use the configured mussel driver).</p>

<p>I am not familiar with Windows domain authentication and what not, so there is where my problem is.</p>

<p>My connection string:</p>

<pre><code>DRIVER={ODBC Driver 17 for SQL Server};SERVER=myserver.mydomain.com;PORT=1433;DATABASE=MyDatabase;Domain=MyCompanyDomain;Instance=MyInstance;UID=myDomainUser;PWD=XXXXXXXX;Trusted_Connection=yes;Integrated_Security=SSPI
</code></pre>

<p>Supposedly one should use ""Trusted_Connection"" to use the Windows domain authentication instead of directly authenticating with the SQL server.</p>

<p>The error I get when running <strong><em>pyodbc.connect(connString)</em></strong>:</p>

<pre><code>pyodbc.Error: ('HY000', '[HY000] [unixODBC][Microsoft][ODBC Driver 17 for SQL Server]SSPI Provider: No Kerberos credentials available (851968) (SQLDriverConnect)')
</code></pre>

<p>From other sources I read this should work on Windows as this code would use the credentials of the currently logged in user.</p>

<p>My question is how can I connect to a Windows SQL Server instance from Linux using Windows Domain credentials.</p>
",3182879,820,06-04-2018 15:16,06-04-2018 16:09,0,820,19,1,9,60,"{'badge_counts': {'bronze': 19, 'silver': 9, 'gold': 1}, 'account_id': 3839319, 'is_employee': False, 'last_modified_date': 1701665700, 'last_access_date': 1705613497, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 820, 'creation_date': 1389375522, 'user_type': 'registered', 'user_id': 3182879, 'accept_rate': 60, 'website_url': '', 'link': 'https://stackoverflow.com/users/3182879/esser420', 'profile_image': 'https://graph.facebook.com/603989666/picture?type=large', 'display_name': 'Esser420'}","I am trying to connect from a linux machine to a windows SQL Server with pyodbc. I do have a couple of constraints: Need to log on with a windows domain account Need to use python3 Need to do it from Linux to Windows Need to connect to a specific instance I set up the environment as described by microsoft and have it working (I can import pyodbc and use the configured mussel driver). I am not familiar with Windows domain authentication and what not, so there is where my problem is. My connection string: Supposedly one should use ""Trusted_Connection"" to use the Windows domain authentication instead of directly authenticating with the SQL server. The error I get when running pyodbc.connect(connString): From other sources I read this should work on Windows as this code would use the credentials of the currently logged in user. My question is how can I connect to a Windows SQL Server instance from Linux using Windows Domain credentials.","DRIVER={ODBC Driver 17 for SQL Server};SERVER=myserver.mydomain.com;PORT=1433;DATABASE=MyDatabase;Domain=MyCompanyDomain;Instance=MyInstance;UID=myDomainUser;PWD=XXXXXXXX;Trusted_Connection=yes;Integrated_Security=SSPI
 pyodbc.Error: ('HY000', '[HY000] [unixODBC][Microsoft][ODBC Driver 17 for SQL Server]SSPI Provider: No Kerberos credentials available (851968) (SQLDriverConnect)')
",0,30,0,0,
462,48145992,48164310,30363,Showing json field in Django admin,4,<python><json><django>,26,"<p>I have an Django model as follows:</p>

<pre><code>class BodyHeight(models.Model):
    seats = models.ForeignKey(to=Seats)
    name = models.CharField(max_length=127, null=True, blank=True)
    key = models.CharField(max_length=100, null=True)
    data = models.TextField(null=True, blank=True)

    class Meta:
        verbose_name_plural = ""Body heights""

    def __str__(self):
        return self.name
</code></pre>

<p>And in the data field I store the json data, as follows:</p>

<pre><code>{""url"": ""https://some_url=/BE?category=COMMERCIAL, ""images"": [""url_to_some_image""]}
</code></pre>

<p>And I want to show in the Django admin panel only the url from that field.</p>

<p>Now I have:</p>

<pre><code>class BodyHeightAdmin(admin.ModelAdmin):
    search_fields = ('name', )
    list_display = ('id', 'name', 'key', )
    list_display_links = ('id', 'name', 'key', )

admin.site.register(BodyHeight, BodyHeightAdmin)
</code></pre>

<p>That is without the data field. If I add the <code>data</code> field in <code>list_display</code> it shows than whole json (ugly format), but I want only the <code>url</code>.</p>

<p>Any idea how to do that?</p>
",3895259,11834,08-01-2018 07:36,09-01-2018 08:46,1,11854,168,29,98,61,"{'badge_counts': {'bronze': 168, 'silver': 98, 'gold': 29}, 'account_id': 4825724, 'is_employee': False, 'last_modified_date': 1708740600, 'last_access_date': 1711110917, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 50, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 11854, 'creation_date': 1406803482, 'user_type': 'registered', 'user_id': 3895259, 'accept_rate': 61, 'location': 'Belgium', 'website_url': '', 'link': 'https://stackoverflow.com/users/3895259/boky', 'profile_image': 'https://i.stack.imgur.com/xYA9X.png?s=256&g=1', 'display_name': 'Boky'}","I have an Django model as follows: And in the data field I store the json data, as follows: And I want to show in the Django admin panel only the url from that field. Now I have: That is without the data field. If I add the field in it shows than whole json (ugly format), but I want only the . Any idea how to do that?","class BodyHeight(models.Model):
    seats = models.ForeignKey(to=Seats)
    name = models.CharField(max_length=127, null=True, blank=True)
    key = models.CharField(max_length=100, null=True)
    data = models.TextField(null=True, blank=True)

    class Meta:
        verbose_name_plural = ""Body heights""

    def __str__(self):
        return self.name
 {""url"": ""https://some_url=/BE?category=COMMERCIAL, ""images"": [""url_to_some_image""]}
 class BodyHeightAdmin(admin.ModelAdmin):
    search_fields = ('name', )
    list_display = ('id', 'name', 'key', )
    list_display_links = ('id', 'name', 'key', )

admin.site.register(BodyHeight, BodyHeightAdmin)
 data list_display url",12,35,0,0,
463,50194637,50194820,49811,Colaboratory: How to install and use on local machine?,3,<python><pip><google-colaboratory><productivity-power-tools>,28,"<p>Google Colab is awesome to work with, but I wish I can run Colab Notebooks completely locally and offline, just like Jupyter notebooks served from the local?</p>
<p>How do I do this? Is there a <code>Colab</code> package which I can install?</p>
<hr />
<p><strong>EDIT</strong>: Some previous answers to the question seem to give methods to access Colab hosted by Google. But that's not what I'm looking for.</p>
<p>My question is how do I <code>pip install colab</code> so I can run it locally like jupyter after <code>pip install jupyter</code>. Colab package doesn't seem to exist, so if I want it, what do I do to install it from the source?</p>
",3211422,8651,05-05-2018 22:40,05-05-2018 23:10,0,8671,106,12,55,,"{'badge_counts': {'bronze': 106, 'silver': 55, 'gold': 12}, 'account_id': 1546053, 'is_employee': False, 'last_modified_date': 1679924400, 'last_access_date': 1711073967, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 8671, 'creation_date': 1387621362, 'user_type': 'registered', 'user_id': 3211422, 'website_url': 'https://www.linkedin.com/in/saravanabalagi/', 'link': 'https://stackoverflow.com/users/3211422/saravanabalagi-ramachandran', 'profile_image': 'https://www.gravatar.com/avatar/1d02c9b6dabf9af7f3b35be308cc0d0b?s=256&d=identicon&r=PG', 'display_name': 'Saravanabalagi Ramachandran'}","Google Colab is awesome to work with, but I wish I can run Colab Notebooks completely locally and offline, just like Jupyter notebooks served from the local? How do I do this? Is there a package which I can install? EDIT: Some previous answers to the question seem to give methods to access Colab hosted by Google. But that's not what I'm looking for. My question is how do I so I can run it locally like jupyter after . Colab package doesn't seem to exist, so if I want it, what do I do to install it from the source?",Colab pip install colab pip install jupyter,-3,5,0,0,
464,48247921,48251801,19331,"Tensorflow Object Detection API on Windows - error ""ModuleNotFoundError: No module named 'utils'""",4,<python><tensorflow><object-detection><object-detection-api>,12,"<p>I'm attempting to get the TensorFlow Object Detection API</p>

<p><a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""noreferrer"">https://github.com/tensorflow/models/tree/master/research/object_detection</a></p>

<p>working on Windows by following the install instructions</p>

<p><a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""noreferrer"">https://github.com/tensorflow/models/tree/master/research/object_detection</a></p>

<p>Which seem to be for Linux/Mac.  I can only get this to work if I put a script in the directory I cloned the above repo to.  If I put the script in any other directory I get this error:</p>

<pre><code>ModuleNotFoundError: No module named 'utils'
</code></pre>

<p>I suspect that the cause is not properly doing the Windows equivalent of this command listed on the install instructions above:</p>

<pre><code># From tensorflow/models/research/
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
</code></pre>

<p>I'm using Windows 10, Python 3.6, and TensorFlow 1.4.0 if that matters.  Of course, I've Googled on this concern and found various links, for example, this:</p>

<p><a href=""https://github.com/tensorflow/models/issues/1747"" rel=""noreferrer"">https://github.com/tensorflow/models/issues/1747</a></p>

<p>But this has not resolved the concern.  Any suggestions on how to resolve this?</p>

<p>Here are the steps I've done so far specifically:</p>

<hr>

<h2>EDIT: these steps work now after updating to incorporate RecencyEffect's answer</h2>

<p>1) Install TensorFlow and related tools via pip3</p>

<p>2) From an administrative command prompt, run the following:</p>

<pre><code>pip3 install pillow
pip3 install lxml
pip3 install jupyter
pip3 install matplotlib
</code></pre>

<p>3) Clone the TensorFlow ""models"" repository to the Documents folder, in my case</p>

<pre><code>C:\Users\cdahms\Documents\models
</code></pre>

<p>4) Downloaded Google Protobuf <a href=""https://github.com/google/protobuf"" rel=""noreferrer"">https://github.com/google/protobuf</a> Windows v3.4.0 release ""protoc-3.4.0-win32.zip"" (I tried the most current 3.5.1 and got errors on the subsequent steps, so I tried 3.4.0 per this vid <a href=""https://www.youtube.com/watch?v=COlbP62-B-U&amp;list=PLQVvvaa0QuDcNK5GeCQnxYnSSaar2tpku&amp;index=1"" rel=""noreferrer"">https://www.youtube.com/watch?v=COlbP62-B-U&amp;list=PLQVvvaa0QuDcNK5GeCQnxYnSSaar2tpku&amp;index=1</a> and the protobuf compile worked)</p>

<p>5) Extract the Protobuf download to Program Files, specifically</p>

<pre><code>""C:\Program Files\protoc-3.4.0-win32""
</code></pre>

<p>6) CD into the models\research directory, specifically</p>

<pre><code>cd C:\Users\cdahms\Documents\models\research
</code></pre>

<p>7) Executed the protobuf compile, specifically</p>

<pre><code>“C:\Program Files\protoc-3.4.0-win32\bin\protoc.exe” object_detection/protos/*.proto --python_out=.
</code></pre>

<p>Navigate to:</p>

<pre><code>C:\Users\cdahms\Documents\models\research\object_detection\protos
</code></pre>

<p>and verify the .py files were created successfully as a result of the compile (only the .proto files were there to begin with)</p>

<p>8) cd to the object_detection directory, ex:</p>

<pre><code>cd C:\Users\cdahms\Documents\models\research\object_detection
</code></pre>

<p>then enter the following at a command prompt to start the object_detection_tutorial.ipynb Jupyter Notebook</p>

<pre><code>jupyter notebook
</code></pre>

<p>9) In the Jupyter Notebook, choose ""object_detection_tutorial.ipynb"" -> Cell -> Run all, the example should run within the notebook</p>

<p>10) In the Jupyter Notebook, choose “File” -> “Download As” -> “Python”, and save the .py version of the notebook to the same directory, i.e.</p>

<pre><code>C:\Users\cdahms\Documents\models\research\object_detection\object_detection_tutorial.py
</code></pre>

<p>You can now open the script in your chosen Python editor (ex. PyCharm) and run it.</p>

<hr>

<h2>EDIT per RecencyEffect's answer below, if you follow these additional steps you will be able to run the object_detection_tutorial.py script from any directory</h2>

<p>11) Move the script to any other directory, then attempt to run it and you will find you will get the error:</p>

<pre><code>ModuleNotFoundError: No module named 'utils'
</code></pre>

<p>because we have not yet informed Python how to find the utils directory that these lines use:</p>

<pre><code>from utils import label_map_util
from utils import visualization_utils as vis_util
</code></pre>

<p>To resolve this . . .</p>

<p>12) Go to System -> Advanced system settings -> Environment Variables . . . -> New, and add a variable with the name PYTHONPATH and these values:</p>

<p><a href=""https://i.stack.imgur.com/vL13P.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vL13P.png"" alt=""enter image description here""></a></p>

<p>13) Also under Environment Variables, edit PATH and add %PYTHONPATH% like so:</p>

<p><a href=""https://i.stack.imgur.com/jdpY3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jdpY3.png"" alt=""enter image description here""></a></p>

<p>14) Reboot to make sure these path changes take effect</p>

<p>15) Pull up a command prompt and run the command ""set"", verify PYTHONPATH is there and PYTHONPATH and PATH contained the values from the previous steps.</p>

<p>16) Now you can copy the ""object_detection_tutorial.py"" to any other directory and it will run</p>
",4835204,3520,14-01-2018 08:36,14-01-2018 16:42,0,3530,79,11,51,58,"{'badge_counts': {'bronze': 79, 'silver': 51, 'gold': 11}, 'account_id': 6207885, 'is_employee': False, 'last_modified_date': 1706924701, 'last_access_date': 1711169027, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 3530, 'creation_date': 1430074212, 'user_type': 'registered', 'user_id': 4835204, 'accept_rate': 58, 'location': 'Holland, MI, United States', 'website_url': 'https://www.youtube.com/user/18F4550videos/videos', 'link': 'https://stackoverflow.com/users/4835204/cdahms', 'profile_image': 'https://www.gravatar.com/avatar/dd0d3a82e9d6d42eb572adba44b1bc62?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'cdahms'}","I'm attempting to get the TensorFlow Object Detection API https://github.com/tensorflow/models/tree/master/research/object_detection working on Windows by following the install instructions https://github.com/tensorflow/models/tree/master/research/object_detection Which seem to be for Linux/Mac. I can only get this to work if I put a script in the directory I cloned the above repo to. If I put the script in any other directory I get this error: I suspect that the cause is not properly doing the Windows equivalent of this command listed on the install instructions above: I'm using Windows 10, Python 3.6, and TensorFlow 1.4.0 if that matters. Of course, I've Googled on this concern and found various links, for example, this: https://github.com/tensorflow/models/issues/1747 But this has not resolved the concern. Any suggestions on how to resolve this? Here are the steps I've done so far specifically: EDIT: these steps work now after updating to incorporate RecencyEffect's answer 1) Install TensorFlow and related tools via pip3 2) From an administrative command prompt, run the following: 3) Clone the TensorFlow ""models"" repository to the Documents folder, in my case 4) Downloaded Google Protobuf https://github.com/google/protobuf Windows v3.4.0 release ""protoc-3.4.0-win32.zip"" (I tried the most current 3.5.1 and got errors on the subsequent steps, so I tried 3.4.0 per this vid https://www.youtube.com/watch?v=COlbP62-B-U&amp;list=PLQVvvaa0QuDcNK5GeCQnxYnSSaar2tpku&amp;index=1 and the protobuf compile worked) 5) Extract the Protobuf download to Program Files, specifically 6) CD into the models\research directory, specifically 7) Executed the protobuf compile, specifically Navigate to: and verify the .py files were created successfully as a result of the compile (only the .proto files were there to begin with) 8) cd to the object_detection directory, ex: then enter the following at a command prompt to start the object_detection_tutorial.ipynb Jupyter Notebook 9) In the Jupyter Notebook, choose ""object_detection_tutorial.ipynb"" -> Cell -> Run all, the example should run within the notebook 10) In the Jupyter Notebook, choose “File” -> “Download As” -> “Python”, and save the .py version of the notebook to the same directory, i.e. You can now open the script in your chosen Python editor (ex. PyCharm) and run it. EDIT per RecencyEffect's answer below, if you follow these additional steps you will be able to run the object_detection_tutorial.py script from any directory 11) Move the script to any other directory, then attempt to run it and you will find you will get the error: because we have not yet informed Python how to find the utils directory that these lines use: To resolve this . . . 12) Go to System -> Advanced system settings -> Environment Variables . . . -> New, and add a variable with the name PYTHONPATH and these values: 13) Also under Environment Variables, edit PATH and add %PYTHONPATH% like so: 14) Reboot to make sure these path changes take effect 15) Pull up a command prompt and run the command ""set"", verify PYTHONPATH is there and PYTHONPATH and PATH contained the values from the previous steps. 16) Now you can copy the ""object_detection_tutorial.py"" to any other directory and it will run","ModuleNotFoundError: No module named 'utils'
 # From tensorflow/models/research/
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
 pip3 install pillow
pip3 install lxml
pip3 install jupyter
pip3 install matplotlib
 C:\Users\cdahms\Documents\models
 ""C:\Program Files\protoc-3.4.0-win32""
 cd C:\Users\cdahms\Documents\models\research
 “C:\Program Files\protoc-3.4.0-win32\bin\protoc.exe” object_detection/protos/*.proto --python_out=.
 C:\Users\cdahms\Documents\models\research\object_detection\protos
 cd C:\Users\cdahms\Documents\models\research\object_detection
 jupyter notebook
 C:\Users\cdahms\Documents\models\research\object_detection\object_detection_tutorial.py
 ModuleNotFoundError: No module named 'utils'
 from utils import label_map_util
from utils import visualization_utils as vis_util
",5,119,2,7,
465,49480716,49480815,5669,type hints for method annotated with @property,2,<python><python-3.x>,11,"<p>How do you access type hints for methods annotated with the @property decorator?</p>

<p>Normally, this is very straightforward:</p>

<pre><code>&gt;&gt;&gt; class Foo:
...     def bar(self) -&gt; str:
...             pass
... 
&gt;&gt;&gt; import typing
&gt;&gt;&gt; typing.get_type_hints(Foo.bar)
{'return': &lt;class 'str'&gt;}
</code></pre>

<p>But once <code>bar</code> is annotated with <code>@property</code> and made into a property object, it's not obvious:</p>

<pre><code>&gt;&gt;&gt; class Foo:
...     @property
...     def bar(self) -&gt; str:
...             pass
... 
&gt;&gt;&gt; import typing
&gt;&gt;&gt; typing.get_type_hints(Foo.bar)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/typing.py"", line 1527, in get_type_hints
    'or function.'.format(obj))
TypeError: &lt;property object at 0x1050fcc28&gt; is not a module, class, method, or function.
&gt;&gt;&gt; typing.get_type_hints(Foo.bar.__get__)
{}
</code></pre>
",3213235,221,25-03-2018 20:40,25-03-2018 20:52,0,221,7,0,2,,"{'badge_counts': {'bronze': 7, 'silver': 2, 'gold': 0}, 'account_id': 3880605, 'is_employee': False, 'last_modified_date': 1573680792, 'last_access_date': 1523668762, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 221, 'creation_date': 1390169931, 'user_type': 'registered', 'user_id': 3213235, 'location': 'Palo Alto, CA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/3213235/asp', 'profile_image': 'https://www.gravatar.com/avatar/43be0829b14c2db47ef241425b3844d1?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'asp'}","How do you access type hints for methods annotated with the @property decorator? Normally, this is very straightforward: But once is annotated with and made into a property object, it's not obvious:","&gt;&gt;&gt; class Foo:
...     def bar(self) -&gt; str:
...             pass
... 
&gt;&gt;&gt; import typing
&gt;&gt;&gt; typing.get_type_hints(Foo.bar)
{'return': &lt;class 'str'&gt;}
 bar @property &gt;&gt;&gt; class Foo:
...     @property
...     def bar(self) -&gt; str:
...             pass
... 
&gt;&gt;&gt; import typing
&gt;&gt;&gt; typing.get_type_hints(Foo.bar)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/typing.py"", line 1527, in get_type_hints
    'or function.'.format(obj))
TypeError: &lt;property object at 0x1050fcc28&gt; is not a module, class, method, or function.
&gt;&gt;&gt; typing.get_type_hints(Foo.bar.__get__)
{}
",17,30,0,0,
466,49491511,49493313,4613,Pandas DataFrame.from_dict() poor performance when generating from a lengthy dict of dicts,3,<python><pandas><dictionary><dataframe><sparse-matrix>,12,"<p>In my Python application, I find it handy to use a dictionary of dictionaries as the source data for constructing a sparse pandas DataFrame, which I then use to train a model in sklearn.</p>

<p>The structure of the dictionary is like this:</p>

<p><code>data = {""X"": {'a': 1, 'b': 2, 'c': 3}, ""Y"": {'d': 4, 'e': 5, 'f': 6}, ""Z"": {'g': 7, 'h': 8, 'i': 9}}</code></p>

<p>Ideally, I'd like turn it into a dataframe like this:</p>

<p><code>df = pandas.DataFrame.from_dict(data, orient=""index"").fillna(0).astype(int)</code></p>

<p>Which generates this:</p>

<p><code>
   e  d  f  a  c  b  i  h  g
X  0  0  0  1  3  2  0  0  0
Y  5  4  6  0  0  0  0  0  0
Z  0  0  0  0  0  0  9  8  7
</code></p>

<p>Now, here's my problem. My data has a number of rows in the hundreds of thousands (ie, the number of keys in the outer dictionary). Each one of these has only a handful of columns associated with it (ie, the number of keys in each inner dictionary), but the total number of columns numbers in the thousands. I've found DataFrame generation using from_dict to be very slow, on the order of 2.5-3 minutes for 200,000 rows and 6,000 columns. </p>

<p>Furthermore, in the case when the row index is a MultiIndex (ie, instead of X, Y and Z the keys of the outer directionary are tuples), from_dict is even slower, on the order of 7+ minutes for 200,000 rows. I've found that this overhead can be avoided if instead of a dictionary of dictionaries, one uses a list of dictionaries and then adds the MultiIndex back to the resulting DataFrame using set_index.</p>

<p>In summary, how would you suggest I deal with this? Performance with the MultiIndex can clearly be improved by the library developers, but am I using the wrong tool for the job here? If written to disk, the DataFrame is around 2.5GB in size. Reading a 2.5GB file from disk in around 2 or so minutes seems about right, but the sparsity of my data in memory should theoretically allow this to be much faster.</p>
",3245319,231,26-03-2018 12:28,26-03-2018 14:00,0,231,11,0,3,,"{'badge_counts': {'bronze': 11, 'silver': 3, 'gold': 0}, 'account_id': 3924128, 'is_employee': False, 'last_modified_date': 1676081700, 'last_access_date': 1707145178, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 231, 'creation_date': 1390925238, 'user_type': 'registered', 'user_id': 3245319, 'website_url': '', 'link': 'https://stackoverflow.com/users/3245319/tovi-almozlino', 'profile_image': 'https://www.gravatar.com/avatar/62afe59dfd256aa0e36c651398c0ab8f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Tovi Almozlino'}","In my Python application, I find it handy to use a dictionary of dictionaries as the source data for constructing a sparse pandas DataFrame, which I then use to train a model in sklearn. The structure of the dictionary is like this: Ideally, I'd like turn it into a dataframe like this: Which generates this: Now, here's my problem. My data has a number of rows in the hundreds of thousands (ie, the number of keys in the outer dictionary). Each one of these has only a handful of columns associated with it (ie, the number of keys in each inner dictionary), but the total number of columns numbers in the thousands. I've found DataFrame generation using from_dict to be very slow, on the order of 2.5-3 minutes for 200,000 rows and 6,000 columns. Furthermore, in the case when the row index is a MultiIndex (ie, instead of X, Y and Z the keys of the outer directionary are tuples), from_dict is even slower, on the order of 7+ minutes for 200,000 rows. I've found that this overhead can be avoided if instead of a dictionary of dictionaries, one uses a list of dictionaries and then adds the MultiIndex back to the resulting DataFrame using set_index. In summary, how would you suggest I deal with this? Performance with the MultiIndex can clearly be improved by the library developers, but am I using the wrong tool for the job here? If written to disk, the DataFrame is around 2.5GB in size. Reading a 2.5GB file from disk in around 2 or so minutes seems about right, but the sparsity of my data in memory should theoretically allow this to be much faster.","data = {""X"": {'a': 1, 'b': 2, 'c': 3}, ""Y"": {'d': 4, 'e': 5, 'f': 6}, ""Z"": {'g': 7, 'h': 8, 'i': 9}} df = pandas.DataFrame.from_dict(data, orient=""index"").fillna(0).astype(int) 
   e  d  f  a  c  b  i  h  g
X  0  0  0  1  3  2  0  0  0
Y  5  4  6  0  0  0  0  0  0
Z  0  0  0  0  0  0  9  8  7
",2,24,0,0,
467,49006699,49007720,21410,"Plot two pandas data frames side by side, each in subplot style",1,<python><pandas><matplotlib>,15,"<p>I want to plot two pandas dataframes side by side, each plot should be in subplot form. I am using following lines:</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# create dummy pandas dataframes
pd1 = pd.DataFrame({'a':np.random.random(22),'b':np.random.random(22),'c':np.random.random(22)})
pd2 = pd.DataFrame({'J':np.random.random(22),'K':np.random.random(22),'P':np.random.random(22)})
#create subplot figure with having two side by side plots
fig, axes = plt.subplots(nrows=1,ncols=2,figsize=(12,6))
# plot first pandas frame in subplot style
pd1.plot(ax = axes[0],subplots=True) 
# plot second pandas frame in subplot style
pd2.plot(ax = axes[1],subplots=True)
</code></pre>

<p>Without subplot option, side by side plots are drawn but I want in subplot style. Is there any other option to get it done?</p>

<p>Precisely, I want to plot pandas in the following manner:
<a href=""https://i.stack.imgur.com/BotzSm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BotzSm.png"" alt=""enter image description here""></a></p>
",3317829,2887,27-02-2018 10:42,27-02-2018 11:34,0,2887,67,5,31,82,"{'badge_counts': {'bronze': 67, 'silver': 31, 'gold': 5}, 'account_id': 4032140, 'is_employee': False, 'last_modified_date': 1700877300, 'last_access_date': 1657182162, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2887, 'creation_date': 1392615374, 'user_type': 'registered', 'user_id': 3317829, 'accept_rate': 82, 'location': 'India', 'website_url': 'https://www.iiitd.edu.in/~haroonr/', 'link': 'https://stackoverflow.com/users/3317829/haroon-lone', 'profile_image': 'https://www.gravatar.com/avatar/2209006ef8f0ab2073ccb99f4b5c3ccb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Haroon Lone'}","I want to plot two pandas dataframes side by side, each plot should be in subplot form. I am using following lines: Without subplot option, side by side plots are drawn but I want in subplot style. Is there any other option to get it done? Precisely, I want to plot pandas in the following manner:","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# create dummy pandas dataframes
pd1 = pd.DataFrame({'a':np.random.random(22),'b':np.random.random(22),'c':np.random.random(22)})
pd2 = pd.DataFrame({'J':np.random.random(22),'K':np.random.random(22),'P':np.random.random(22)})
#create subplot figure with having two side by side plots
fig, axes = plt.subplots(nrows=1,ncols=2,figsize=(12,6))
# plot first pandas frame in subplot style
pd1.plot(ax = axes[0],subplots=True) 
# plot second pandas frame in subplot style
pd2.plot(ax = axes[1],subplots=True)
",11,20,1,1,
468,48822463,48824768,85785,How to use PyTorch multiprocessing?,2,<python><computer-vision><multiprocessing><pytorch>,36,"<p>I'm trying to use python's multiprocessing <code>Pool</code> method in <code>pytorch</code> to process a image. Here's the code:</p>

<pre><code>from multiprocessing import Process, Pool
from torch.autograd import Variable
import numpy as np
from scipy.ndimage import zoom

def get_pred(args):

  img = args[0]
  scale = args[1]
  scales = args[2]
  img_scale = zoom(img.numpy(),
                     (1., 1., scale, scale),
                     order=1,
                     prefilter=False,
                     mode='nearest')

  # feed input data
  input_img = Variable(torch.from_numpy(img_scale),
                     volatile=True).cuda()
  return input_img

scales = [1,2,3,4,5]
scale_list = []
for scale in scales: 
    scale_list.append([img,scale,scales])
multi_pool = Pool(processes=5)
predictions = multi_pool.map(get_pred,scale_list)
multi_pool.close() 
multi_pool.join()
</code></pre>

<p>I'm getting this error:</p>

<pre><code>`RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
</code></pre>

<p>`
In this line:</p>

<pre><code>predictions = multi_pool.map(get_pred,scale_list)
</code></pre>

<p>Can anyone tell me what I'm doing wrong ?</p>
",3336062,3258,16-02-2018 08:05,16-02-2018 10:29,0,3278,70,8,40,67,"{'badge_counts': {'bronze': 70, 'silver': 40, 'gold': 8}, 'account_id': 4059055, 'is_employee': False, 'last_modified_date': 1618649414, 'last_access_date': 1711116556, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 10, 'reputation': 3278, 'creation_date': 1392965830, 'user_type': 'registered', 'user_id': 3336062, 'accept_rate': 67, 'location': 'Ranchi, India', 'website_url': 'https://in.linkedin.com/in/rahulranjan7', 'link': 'https://stackoverflow.com/users/3336062/rahul', 'profile_image': 'https://i.stack.imgur.com/S7T0N.jpg?s=256&g=1', 'display_name': 'Rahul'}",I'm trying to use python's multiprocessing method in to process a image. Here's the code: I'm getting this error: ` In this line: Can anyone tell me what I'm doing wrong ?,"Pool pytorch from multiprocessing import Process, Pool
from torch.autograd import Variable
import numpy as np
from scipy.ndimage import zoom

def get_pred(args):

  img = args[0]
  scale = args[1]
  scales = args[2]
  img_scale = zoom(img.numpy(),
                     (1., 1., scale, scale),
                     order=1,
                     prefilter=False,
                     mode='nearest')

  # feed input data
  input_img = Variable(torch.from_numpy(img_scale),
                     volatile=True).cuda()
  return input_img

scales = [1,2,3,4,5]
scale_list = []
for scale in scales: 
    scale_list.append([img,scale,scales])
multi_pool = Pool(processes=5)
predictions = multi_pool.map(get_pred,scale_list)
multi_pool.close() 
multi_pool.join()
 `RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
 predictions = multi_pool.map(get_pred,scale_list)
",26,45,0,0,
469,49783178,49783218,72363,Keep other columns when using sum() with groupby,4,<python><pandas>,51,"<p>I have a pandas dataframe below:</p>
<pre><code>    df

    name    value1    value2  otherstuff1 otherstuff2 
0   Jack       1         1       1.19        2.39     
1   Jack       1         2       1.19        2.39
2   Luke       0         1       1.08        1.08  
3   Mark       0         1       3.45        3.45
4   Luke       1         0       1.08        1.08
</code></pre>
<p>Same <code>name</code> will have the same value for <code>otherstuff1</code> and <code>otherstuff2</code>.</p>
<p>I'm trying to group by column <code>name</code> and sum both columns <code>value1</code> and <code>value2</code>. (Not sum <code>value1</code> with <code>value2</code>!!! But sum them individually in each column.)</p>
<p>Expecting to get result below:</p>
<pre><code>    newdf

    name    value1    value2  otherstuff1 otherstuff2 
0   Jack       2         3       1.19        2.39     
1   Luke       1         1       1.08        1.08  
2   Mark       0         1       3.45        3.45
</code></pre>
<p>I've tried</p>
<pre><code>newdf = df.groupby(['name'], as_index=False).sum()
</code></pre>
<p>which groups by <code>name</code> and sums up both <code>value1</code> and <code>value2</code> columns correctly, but ends up dropping columns <code>otherstuff1</code> and <code>otherstuff2</code>.</p>
",3914955,797,11-04-2018 19:36,11-04-2018 19:39,0,797,16,1,9,,"{'badge_counts': {'bronze': 16, 'silver': 9, 'gold': 1}, 'account_id': 4853175, 'is_employee': False, 'last_modified_date': 1589958923, 'last_access_date': 1710135952, 'reputation_change_year': 18, 'reputation_change_quarter': 18, 'reputation_change_month': 8, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 797, 'creation_date': 1407337920, 'user_type': 'registered', 'user_id': 3914955, 'location': 'Indianapolis, IN, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/3914955/swagz', 'profile_image': 'https://www.gravatar.com/avatar/6a72334e3ca935447b22396bb3a53157?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'SwagZ'}","I have a pandas dataframe below: Same will have the same value for and . I'm trying to group by column and sum both columns and . (Not sum with !!! But sum them individually in each column.) Expecting to get result below: I've tried which groups by and sums up both and columns correctly, but ends up dropping columns and .","    df

    name    value1    value2  otherstuff1 otherstuff2 
0   Jack       1         1       1.19        2.39     
1   Jack       1         2       1.19        2.39
2   Luke       0         1       1.08        1.08  
3   Mark       0         1       3.45        3.45
4   Luke       1         0       1.08        1.08
 name otherstuff1 otherstuff2 name value1 value2 value1 value2     newdf

    name    value1    value2  otherstuff1 otherstuff2 
0   Jack       2         3       1.19        2.39     
1   Luke       1         1       1.08        1.08  
2   Mark       0         1       3.45        3.45
 newdf = df.groupby(['name'], as_index=False).sum()
 name value1 value2 otherstuff1 otherstuff2",-1,24,0,0,
470,50217968,50218184,34487,Pandas: split list in column into multiple rows,4,<python><pandas>,16,"<p>I have a question regarding splitting a list in a dataframe column into multiple rows.</p>

<p>Let's say I have this dataframe:</p>

<pre><code>  Job position   Job type  id
0          [6]        [1]   3
1       [2, 6]  [3, 6, 5]   4
2          [1]        [9]  43
</code></pre>

<p>I would like every single combination of numbers, so the final result would be:</p>

<pre><code>   id    Job position  Job type
0   3         6.0       1.0
1   4         2.0       3.0
2   4         2.0       6.0
3   4         2.0       5.0
4   4         6.0       3.0
5   4         6.0       6.0
6   4         6.0       5.0
7  43         1.0       9.0
</code></pre>

<p>Because right now I get this result:</p>

<pre><code>   id    Job position  Job type
0   3         6.0       1.0
1   4         2.0       3.0
2   4         6.0       6.0
3   4         NaN       5.0
4  43         1.0       9.0
</code></pre>

<p>In order to get the result above, I did:</p>

<pre><code>df = df.set_index(['id'])
(df.apply(lambda x: pd.DataFrame(x.tolist(),index=x.index)
                        .stack()
                        .rename(x.name)).reset_index())
</code></pre>
",3925143,752,07-05-2018 15:45,07-05-2018 15:58,0,752,24,1,9,62,"{'badge_counts': {'bronze': 24, 'silver': 9, 'gold': 1}, 'account_id': 4867392, 'is_employee': False, 'last_modified_date': 1587774900, 'last_access_date': 1632406832, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 752, 'creation_date': 1407591926, 'user_type': 'registered', 'user_id': 3925143, 'accept_rate': 62, 'website_url': '', 'link': 'https://stackoverflow.com/users/3925143/mathias-lund', 'profile_image': 'https://www.gravatar.com/avatar/1209770c87f20411aabaf572d0f38716?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Mathias Lund'}","I have a question regarding splitting a list in a dataframe column into multiple rows. Let's say I have this dataframe: I would like every single combination of numbers, so the final result would be: Because right now I get this result: In order to get the result above, I did:","  Job position   Job type  id
0          [6]        [1]   3
1       [2, 6]  [3, 6, 5]   4
2          [1]        [9]  43
    id    Job position  Job type
0   3         6.0       1.0
1   4         2.0       3.0
2   4         2.0       6.0
3   4         2.0       5.0
4   4         6.0       3.0
5   4         6.0       6.0
6   4         6.0       5.0
7  43         1.0       9.0
    id    Job position  Job type
0   3         6.0       1.0
1   4         2.0       3.0
2   4         6.0       6.0
3   4         NaN       5.0
4  43         1.0       9.0
 df = df.set_index(['id'])
(df.apply(lambda x: pd.DataFrame(x.tolist(),index=x.index)
                        .stack()
                        .rename(x.name)).reset_index())
",19,40,0,0,
471,49962417,49962706,13641,Why does .loc have inclusive behavior for slices?,1,<python><pandas><dataframe><slice>,45,"<p>For some reason, the following 2 calls to <code>iloc</code> / <code>loc</code> produce different behavior:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame(dict(A=range(3), B=range(3)))
&gt;&gt;&gt; df.iloc[:1]
   A  B
0  0  0
&gt;&gt;&gt; df.loc[:1]
   A  B
0  0  0
1  1  1
</code></pre>

<p>I understand that <code>loc</code> considers the row labels, while <code>iloc</code> considers the integer-based indices of the rows. But why is the upper bound for the <code>loc</code> call considered inclusive, while the <code>iloc</code> bound is considered exclusive?</p>
",4077294,33286,22-04-2018 04:10,22-04-2018 05:12,0,33386,246,32,138,74,"{'badge_counts': {'bronze': 246, 'silver': 138, 'gold': 32}, 'account_id': 5081290, 'is_employee': False, 'last_modified_date': 1702476000, 'last_access_date': 1710880793, 'reputation_change_year': 500, 'reputation_change_quarter': 500, 'reputation_change_month': 110, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 33386, 'creation_date': 1411608822, 'user_type': 'registered', 'user_id': 4077294, 'accept_rate': 74, 'location': 'New York, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/4077294/james-ko', 'profile_image': 'https://lh3.googleusercontent.com/-IS-STMhd89w/AAAAAAAAAAI/AAAAAAAABIk/_I8CGPoI1BE/photo.jpg?sz=256', 'display_name': 'James Ko'}","For some reason, the following 2 calls to / produce different behavior: I understand that considers the row labels, while considers the integer-based indices of the rows. But why is the upper bound for the call considered inclusive, while the bound is considered exclusive?","iloc loc &gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; df = pd.DataFrame(dict(A=range(3), B=range(3)))
&gt;&gt;&gt; df.iloc[:1]
   A  B
0  0  0
&gt;&gt;&gt; df.loc[:1]
   A  B
0  0  0
1  1  1
 loc iloc loc iloc",2,14,0,0,
472,49851280,52064387,22964,Showing a simple matplotlib plot in plotly Dash,3,<python><matplotlib><plotly><plotly-dash>,22,"<p>Is it possible to show a simple matplotlib plot (the kind usually generated by <code>plt.show()</code>) in plotly's Dash framework? Or just plotly-like graphs with plotly's Scatters and Data traces?</p>

<p>Specifically I guess I need a different component than <code>Graph</code> (see below) and a way to return the simple plot in the <code>update_figure</code> function.</p>

<p>Example:</p>

<pre><code>import dash
import dash_core_components as dcc
import dash_html_components as html
import numpy as np
import matplotlib.pyplot as plt

app = dash.Dash()

app.layout = html.Div(children=[
    html.H1(children='Hello Dash'),

    dcc.Slider(
        id='n_points',
        min=10,
        max=100,
        step=1,
        value=50,
    ),

    dcc.Graph(id='example') # or something other than Graph?...
])

@app.callback(
    dash.dependencies.Output('example', 'figure'),
    [dash.dependencies.Input('n_points', 'value')]
)

def update_figure(n_points):
    #create some matplotlib graph
    x = np.random.rand(n_points)
    y = np.random.rand(n_points)
    plt.scatter(x, y)
    # plt.show()
    return None # return what, I don't know exactly, `plt`?

if __name__ == '__main__':
    app.run_server(debug=True)
</code></pre>
",4095235,3577,16-04-2018 07:00,28-08-2018 18:40,134,3589,75,3,36,93,"{'badge_counts': {'bronze': 75, 'silver': 36, 'gold': 3}, 'account_id': 5107577, 'is_employee': False, 'last_modified_date': 1710342300, 'last_access_date': 1711041454, 'reputation_change_year': 52, 'reputation_change_quarter': 52, 'reputation_change_month': 12, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3589, 'creation_date': 1412084436, 'user_type': 'registered', 'user_id': 4095235, 'accept_rate': 93, 'website_url': 'http://giorasimchoni.com/', 'link': 'https://stackoverflow.com/users/4095235/giora-simchoni', 'profile_image': 'https://i.stack.imgur.com/H7ocx.jpg?s=256&g=1', 'display_name': 'Giora Simchoni'}",Is it possible to show a simple matplotlib plot (the kind usually generated by ) in plotly's Dash framework? Or just plotly-like graphs with plotly's Scatters and Data traces? Specifically I guess I need a different component than (see below) and a way to return the simple plot in the function. Example:,"plt.show() Graph update_figure import dash
import dash_core_components as dcc
import dash_html_components as html
import numpy as np
import matplotlib.pyplot as plt

app = dash.Dash()

app.layout = html.Div(children=[
    html.H1(children='Hello Dash'),

    dcc.Slider(
        id='n_points',
        min=10,
        max=100,
        step=1,
        value=50,
    ),

    dcc.Graph(id='example') # or something other than Graph?...
])

@app.callback(
    dash.dependencies.Output('example', 'figure'),
    [dash.dependencies.Input('n_points', 'value')]
)

def update_figure(n_points):
    #create some matplotlib graph
    x = np.random.rand(n_points)
    y = np.random.rand(n_points)
    plt.scatter(x, y)
    # plt.show()
    return None # return what, I don't know exactly, `plt`?

if __name__ == '__main__':
    app.run_server(debug=True)
",33,44,0,0,
473,50275646,50275738,18272,How do I run background job in Flask without threading or task-queue,2,<python><multithreading><flask><queue><celery>,14,"<p>I am building REST API with Flask-restplus. One of my endpoints takes a file uploaded from client and run some analysis. The job uses up to 30 seconds. I don't want the job to block the main process. So the endpoint will return a response with 200 or 201 right away, the job can still be running. Results will be saved to database which will be retrieved later.</p>

<p>It seems I have two options for long-running jobs. </p>

<ol>
<li>Threading</li>
<li>Task-queue</li>
</ol>

<p>Threading is relatively simpler. But problem is, there is a limit of thread numbers for Flask app. In a standalone Python app, I could use a queue for the threads. But this is REST api, each request call is independent. I don't know if there is a way to maintain a global queue for that. So if the requests exceed the thread limit, it won't be able to take more requests.</p>

<p>Task-queue with Celery and Redis is probably better option. But this is just a proof of concept thing, and time line is kind of tight. Setting up Celery, Redis with Flask is not easy, I am having lots of trouble on my dev machine which is a Windows. It will be deployed on AWS which is kind of complex. </p>

<p>I wonder if there is a third option for this case?</p>
",3358927,4833,10-05-2018 14:46,10-05-2018 14:51,0,4843,129,15,76,79,"{'badge_counts': {'bronze': 129, 'silver': 76, 'gold': 15}, 'account_id': 4090948, 'is_employee': False, 'last_modified_date': 1684578000, 'last_access_date': 1709158734, 'reputation_change_year': 78, 'reputation_change_quarter': 78, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4843, 'creation_date': 1393477703, 'user_type': 'registered', 'user_id': 3358927, 'accept_rate': 79, 'website_url': '', 'link': 'https://stackoverflow.com/users/3358927/ddd', 'profile_image': 'https://www.gravatar.com/avatar/96fe219228f204c205bff346e4f2267c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ddd'}","I am building REST API with Flask-restplus. One of my endpoints takes a file uploaded from client and run some analysis. The job uses up to 30 seconds. I don't want the job to block the main process. So the endpoint will return a response with 200 or 201 right away, the job can still be running. Results will be saved to database which will be retrieved later. It seems I have two options for long-running jobs. Threading Task-queue Threading is relatively simpler. But problem is, there is a limit of thread numbers for Flask app. In a standalone Python app, I could use a queue for the threads. But this is REST api, each request call is independent. I don't know if there is a way to maintain a global queue for that. So if the requests exceed the thread limit, it won't be able to take more requests. Task-queue with Celery and Redis is probably better option. But this is just a proof of concept thing, and time line is kind of tight. Setting up Celery, Redis with Flask is not easy, I am having lots of trouble on my dev machine which is a Windows. It will be deployed on AWS which is kind of complex. I wonder if there is a third option for this case?",,0,14,0,0,
474,50117377,50498945,9891,Selenium with chromedriver doesn't start via cron,5,<python><selenium><cron><selenium-chromedriver><centos7>,12,"<p>Python script with Selenium and Chromedriver in <strong>headless mode</strong> on CentOS7 runs fine when called manually.</p>

<pre><code>options = webdriver.ChromeOptions()
options.add_argument('headless')
options.add_argument('no-sandbox')
self.driver = webdriver.Chrome(chrome_options=options)
</code></pre>

<p>When starting script with crontab however it throws this exception at line 4 (above). Full traceback at bottom.</p>

<pre><code>selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: exited abnormally (Driver info: chromedriver=2.38.552522
</code></pre>

<p>Cron is setup with crontab -e</p>

<pre><code>* * * * * cd /to/path &amp;&amp; /to/path/.virtualenvs/selenium/bin/python /to/path/script.py -t arg1 arg2 &gt; /to/path/log.txt 2&gt;&amp;1
</code></pre>

<p>This produced errors like chromedriver couldn't be found. I then added following to crontab -e. <br>
1) Use bash instead of sh, although starting python script manually from sh works fine<br>
2) Specify path to chromedriver</p>

<pre><code>SHELL=/bin/bash
PATH=/usr/local/bin/
</code></pre>

<p>I tried different suggestions found on the web like adding --no-sandbox options to chromedriver in my script. All didn't help. Please note that I am using chrome in headless mode, so I think I don't need this export DISPLAY=:0 stuff in cron, or Xvfb libs as it used to be.</p>

<p>Python 3.6.1<br>
Selenium 3.4.3<br>
Chromedriver 2.38.552522<br>
google-chrome-stable 65.0.3325.181<br></p>

<p>Full traceback</p>

<pre><code>Exception in thread &lt;name&gt;:
Traceback (most recent call last):
  File ""/usr/lib64/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib64/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/path/to/script.py"", line 53, in start
    self.site_scrape(test_run)
  File ""/path/to/script.py"", line 65, in site
    self.driver = webdriver.Chrome(chrome_options=options)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/chrome/webdriver.py"", line 69, in __init__
    desired_capabilities=desired_capabilities)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 98, in __init__
    self.start_session(desired_capabilities, browser_profile)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 188, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 256, in execute
    self.error_handler.check_response(response)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py"", line 194, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: exited abnormally
  (Driver info: chromedriver=2.38.552522 (437e6fbedfa8762dec75e2c5b3ddb86763dc9dcb),platform=Linux 4.14.12-x86_64-linode92 x86_64)
</code></pre>
",4417912,661,01-05-2018 13:28,23-05-2018 23:42,22,661,18,0,5,75,"{'badge_counts': {'bronze': 18, 'silver': 5, 'gold': 0}, 'account_id': 5573632, 'is_employee': False, 'last_modified_date': 1694439300, 'last_access_date': 1702057022, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 661, 'creation_date': 1420396397, 'user_type': 'registered', 'user_id': 4417912, 'accept_rate': 75, 'website_url': '', 'link': 'https://stackoverflow.com/users/4417912/jim-b', 'profile_image': 'https://www.gravatar.com/avatar/35fd65d9a605ea6310533d40150b4dfb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jim B'}","Python script with Selenium and Chromedriver in headless mode on CentOS7 runs fine when called manually. When starting script with crontab however it throws this exception at line 4 (above). Full traceback at bottom. Cron is setup with crontab -e This produced errors like chromedriver couldn't be found. I then added following to crontab -e. 1) Use bash instead of sh, although starting python script manually from sh works fine 2) Specify path to chromedriver I tried different suggestions found on the web like adding --no-sandbox options to chromedriver in my script. All didn't help. Please note that I am using chrome in headless mode, so I think I don't need this export DISPLAY=:0 stuff in cron, or Xvfb libs as it used to be. Python 3.6.1 Selenium 3.4.3 Chromedriver 2.38.552522 google-chrome-stable 65.0.3325.181 Full traceback","options = webdriver.ChromeOptions()
options.add_argument('headless')
options.add_argument('no-sandbox')
self.driver = webdriver.Chrome(chrome_options=options)
 selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: exited abnormally (Driver info: chromedriver=2.38.552522
 * * * * * cd /to/path &amp;&amp; /to/path/.virtualenvs/selenium/bin/python /to/path/script.py -t arg1 arg2 &gt; /to/path/log.txt 2&gt;&amp;1
 SHELL=/bin/bash
PATH=/usr/local/bin/
 Exception in thread &lt;name&gt;:
Traceback (most recent call last):
  File ""/usr/lib64/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/lib64/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/path/to/script.py"", line 53, in start
    self.site_scrape(test_run)
  File ""/path/to/script.py"", line 65, in site
    self.driver = webdriver.Chrome(chrome_options=options)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/chrome/webdriver.py"", line 69, in __init__
    desired_capabilities=desired_capabilities)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 98, in __init__
    self.start_session(desired_capabilities, browser_profile)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 188, in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 256, in execute
    self.error_handler.check_response(response)
  File ""/home/&lt;user&gt;/.virtualenvs/selenium/lib64/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py"", line 194, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: exited abnormally
  (Driver info: chromedriver=2.38.552522 (437e6fbedfa8762dec75e2c5b3ddb86763dc9dcb),platform=Linux 4.14.12-x86_64-linode92 x86_64)
",25,58,0,0,
475,48097742,48105955,38294,geopandas point in polygon,1,<python><pandas><geopandas><point-in-polygon>,34,"<p>I have a GeoDataFrame of polygons (~30) and a GeoDataFrame of Points (~10k)</p>

<p>I'm looking to create 30 new columns (with appropriate polygon names) in my GeoDataFrame of Points with a simple boolean True/False if the point is present in the polygon.</p>

<p>As an example, the GeoDataFrame of Polygons is this:</p>

<pre><code>id  geometry
foo POLYGON ((-0.18353,51.51022, -0.18421,51.50767, -0.18253,51.50744, -0.1794,51.50914))
bar POLYGON ((-0.17003,51.50739, -0.16904,51.50604, -0.16488,51.50615, -0.1613,51.5091))
</code></pre>

<p>The GeoDataFrame of Points is like this:</p>

<pre><code>counter     points
   1     ((-0.17987,51.50974))
   2     ((-0.16507,51.50925))
</code></pre>

<p>Expected output:</p>

<pre><code>counter          points        foo    bar
   1    ((-0.17987,51.50974))  False  False
   1    ((-0.16507,51.50925))  False  False
</code></pre>

<p>I can do this manually by:</p>

<pre><code>foo = df_poly.loc[df_poly.id=='foo']
df_points['foo'] = df_points['points'].map(lambda x: True if foo.contains(x).any()==True else False
</code></pre>

<p>But given that I have 30 polygons, I was wondering if there is a better way.
Appreciate any help!</p>
",5236124,1371,04-01-2018 14:48,05-01-2018 01:27,1,1371,33,7,21,91,"{'badge_counts': {'bronze': 33, 'silver': 21, 'gold': 7}, 'account_id': 6803578, 'is_employee': False, 'last_modified_date': 1652469900, 'last_access_date': 1692085756, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1371, 'creation_date': 1439829628, 'user_type': 'registered', 'user_id': 5236124, 'accept_rate': 91, 'link': 'https://stackoverflow.com/users/5236124/kvothe', 'profile_image': 'https://www.gravatar.com/avatar/95b4fb7a5b98355f4866cbc4334b2071?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Kvothe'}","I have a GeoDataFrame of polygons (~30) and a GeoDataFrame of Points (~10k) I'm looking to create 30 new columns (with appropriate polygon names) in my GeoDataFrame of Points with a simple boolean True/False if the point is present in the polygon. As an example, the GeoDataFrame of Polygons is this: The GeoDataFrame of Points is like this: Expected output: I can do this manually by: But given that I have 30 polygons, I was wondering if there is a better way. Appreciate any help!","id  geometry
foo POLYGON ((-0.18353,51.51022, -0.18421,51.50767, -0.18253,51.50744, -0.1794,51.50914))
bar POLYGON ((-0.17003,51.50739, -0.16904,51.50604, -0.16488,51.50615, -0.1613,51.5091))
 counter     points
   1     ((-0.17987,51.50974))
   2     ((-0.16507,51.50925))
 counter          points        foo    bar
   1    ((-0.17987,51.50974))  False  False
   1    ((-0.16507,51.50925))  False  False
 foo = df_poly.loc[df_poly.id=='foo']
df_points['foo'] = df_points['points'].map(lambda x: True if foo.contains(x).any()==True else False
",7,33,0,0,
476,48608894,48609128,109065,ImproperlyConfiguredError about app_name when using namespace in include(),7,<python><django><django-urls>,162,"<p>I am currently trying out Django. I use the <code>namespace</code> argument in one of my <code>include()</code>s in urls.py. When I run the server and try to browse,
I get this error.</p>
<pre><code>File &quot;C:\Users\User\AppData\Local\Programs\Python\Python36-32\lib\site-packages\django\urls\conf.py&quot;, line 39, in include
    'Specifying a namespace in include() without providing an app_name '
django.core.exceptions.ImproperlyConfigured: Specifying a namespace in include() without providing an app_name is not supported. Set the app_name attribute in the included module, or pass a 2-tuple containing the list of patterns and app_name instead.
</code></pre>
<p>These are my urls.py files:</p>
<pre><code>#project/urls.py

from django.conf.urls import include, url
from django.contrib import admin

urlpatterns = [
    url(r'^reviews/', include('reviews.urls', namespace='reviews')),
    url(r'^admin/', include(admin.site.urls)),
]
</code></pre>
<p>and</p>
<pre><code>#app/urls.py

from django.conf.urls import url

from . import views

urlpatterns = [
    # ex: /
    url(r'^$', views.review_list, name='review_list'),
    # ex: /review/5/
    url(r'^review/(?P&lt;review_id&gt;[0-9]+)/$', views.review_detail, name='review_detail'),
    # ex: /wine/
    url(r'^wine$', views.wine_list, name='wine_list'),
    # ex: /wine/5/
    url(r'^wine/(?P&lt;wine_id&gt;[0-9]+)/$', views.wine_detail, name='wine_detail'),
]
</code></pre>
<p>What do I pass the <code>app_name</code> as stated in the error message?</p>
",4469914,1708,04-02-2018 14:07,04-02-2018 14:29,0,1708,19,4,13,,"{'badge_counts': {'bronze': 19, 'silver': 13, 'gold': 4}, 'account_id': 5648577, 'is_employee': False, 'last_modified_date': 1624513731, 'last_access_date': 1683605387, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1708, 'creation_date': 1421669137, 'user_type': 'registered', 'user_id': 4469914, 'location': 'Uganda', 'website_url': 'https://nelsonmurungi.vercel.app/resume', 'link': 'https://stackoverflow.com/users/4469914/nelson-m', 'profile_image': 'https://www.gravatar.com/avatar/1483f4f8cea41296fd5a0ac057c104b7?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nelson M'}","I am currently trying out Django. I use the argument in one of my s in urls.py. When I run the server and try to browse, I get this error. These are my urls.py files: and What do I pass the as stated in the error message?","namespace include() File &quot;C:\Users\User\AppData\Local\Programs\Python\Python36-32\lib\site-packages\django\urls\conf.py&quot;, line 39, in include
    'Specifying a namespace in include() without providing an app_name '
django.core.exceptions.ImproperlyConfigured: Specifying a namespace in include() without providing an app_name is not supported. Set the app_name attribute in the included module, or pass a 2-tuple containing the list of patterns and app_name instead.
 #project/urls.py

from django.conf.urls import include, url
from django.contrib import admin

urlpatterns = [
    url(r'^reviews/', include('reviews.urls', namespace='reviews')),
    url(r'^admin/', include(admin.site.urls)),
]
 #app/urls.py

from django.conf.urls import url

from . import views

urlpatterns = [
    # ex: /
    url(r'^$', views.review_list, name='review_list'),
    # ex: /review/5/
    url(r'^review/(?P&lt;review_id&gt;[0-9]+)/$', views.review_detail, name='review_detail'),
    # ex: /wine/
    url(r'^wine$', views.wine_list, name='wine_list'),
    # ex: /wine/5/
    url(r'^wine/(?P&lt;wine_id&gt;[0-9]+)/$', views.wine_detail, name='wine_detail'),
]
 app_name",22,36,0,0,
477,49734744,49841953,68192,AWS Lambda - unable to import module 'lambda_function',6,<python><amazon-web-services><aws-lambda>,18,"<p>Like many others before me, I'm trying to run an AWS Lambda function and when I try to test it, I get </p>

<blockquote>
  <p>""errorMessage"": ""Unable to import module 'lambda_function'""</p>
</blockquote>

<p>My Handler is set to lambda_function.lambda_handler, and I indeed have a file named lambda_function.py which contains a function called lambda_handler. Here's a screenshot as proof: 
<a href=""https://i.stack.imgur.com/DtvIN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DtvIN.png"" alt=""enter image description here""></a></p>

<p>Everything was working fine when I was writing snippets of code inline in the included IDE, but when I zipped my full program with all of its dependencies and uploaded it, I got the above error.</p>

<p>I'm using the Numpy and Scipy packages, which are quite large. My zipped directory is 34 MB, and my unzipped directory 122 MB. I think this should be fine since the limit is 50 MB for a zipped directory. It appears to be uploading fine, since I see the message: </p>

<blockquote>
  <p>The deployment package of your Lambda function ""one-shot-image-classification"" is too large to enable inline code editing. However, you can still invoke your function right now.</p>
</blockquote>

<p>I've seen that some posts solve this by using virtualenv, but I'm not familiar with that technology and I'm not sure how to use it properly. </p>

<p>I've also seen some posts saying that sometimes dependencies have dependencies and I may need to include those, but I'm not sure how to find this out. </p>

<p>Here's the top portion of lambda_function.py, which should be enough to see the libraries I'm using and that I do indeed have a lambda_handler function: </p>

<pre><code>import os
import boto3
import numpy as np
from scipy.ndimage import imread
from scipy.spatial.distance import cdist

def lambda_handler(event, context):

    s3 = boto3.resource('s3')
</code></pre>

<p>Here a screenshot of the unzipped version of the directory I'm uploading: 
<a href=""https://i.stack.imgur.com/52o4W.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/52o4W.png"" alt=""enter image description here""></a></p>

<p>I can also post the policy role that my Lambda is using if that could be an issue. </p>

<p>Any insight is much appreciated!</p>

<p><strong>UPDATE:</strong> </p>

<p>Here's one solution I tried: 
1. <code>git clone https://github.com/Miserlou/lambda-packages</code>
2. create a folder in Documents called new_lambda
3. copy my lambda_function.py and the numpy folder from the lambda-packages into new_lambda, along with the scipy library that I compiled using Docker for AWS as per the article: <a href=""https://serverlesscode.com/post/scikitlearn-with-amazon-linux-container/"" rel=""noreferrer"">https://serverlesscode.com/post/scikitlearn-with-amazon-linux-container/</a>
4. Zip the new_lambda folder by right-clicking it and selecting 'compress'</p>

<p>My results: </p>

<blockquote>
  <p>Unable to import module 'lambda_function': No module named
  'lambda_function'</p>
</blockquote>

<p>To reiterate, my file is named lambda_function.py and contains a function called lambda_handler, which accepts two arguments (as seen above). This information matches that seen in Handler, also seen above. </p>

<p>I am using a Mac computer, if that matters. </p>

<p><strong>UPDATE 2</strong></p>

<p>If I follow the above steps but instead zip the files by directly selecting the files that I want to compress and then right clicking and selecting 'compress', I instead get the error </p>

<blockquote>
  <p>Unable to import module 'lambda_function': cannot import name 'show_config'</p>
</blockquote>

<p>Also, the precompiled lambda-packages says that they are compiled for ""at least Python 2.7"", but my lambda runtime is 3.6. Could this be an issue?</p>
",4487368,1104,09-04-2018 13:56,15-04-2018 12:34,6,1124,27,3,15,83,"{'badge_counts': {'bronze': 27, 'silver': 15, 'gold': 3}, 'account_id': 5673340, 'is_employee': False, 'last_modified_date': 1688779500, 'last_access_date': 1711134072, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1124, 'creation_date': 1422029066, 'user_type': 'registered', 'user_id': 4487368, 'accept_rate': 83, 'website_url': '', 'link': 'https://stackoverflow.com/users/4487368/spencer-goff', 'profile_image': 'https://i.stack.imgur.com/5AOW9.jpg?s=256&g=1', 'display_name': 'Spencer Goff'}","Like many others before me, I'm trying to run an AWS Lambda function and when I try to test it, I get ""errorMessage"": ""Unable to import module 'lambda_function'"" My Handler is set to lambda_function.lambda_handler, and I indeed have a file named lambda_function.py which contains a function called lambda_handler. Here's a screenshot as proof: Everything was working fine when I was writing snippets of code inline in the included IDE, but when I zipped my full program with all of its dependencies and uploaded it, I got the above error. I'm using the Numpy and Scipy packages, which are quite large. My zipped directory is 34 MB, and my unzipped directory 122 MB. I think this should be fine since the limit is 50 MB for a zipped directory. It appears to be uploading fine, since I see the message: The deployment package of your Lambda function ""one-shot-image-classification"" is too large to enable inline code editing. However, you can still invoke your function right now. I've seen that some posts solve this by using virtualenv, but I'm not familiar with that technology and I'm not sure how to use it properly. I've also seen some posts saying that sometimes dependencies have dependencies and I may need to include those, but I'm not sure how to find this out. Here's the top portion of lambda_function.py, which should be enough to see the libraries I'm using and that I do indeed have a lambda_handler function: Here a screenshot of the unzipped version of the directory I'm uploading: I can also post the policy role that my Lambda is using if that could be an issue. Any insight is much appreciated! UPDATE: Here's one solution I tried: 1. 2. create a folder in Documents called new_lambda 3. copy my lambda_function.py and the numpy folder from the lambda-packages into new_lambda, along with the scipy library that I compiled using Docker for AWS as per the article: https://serverlesscode.com/post/scikitlearn-with-amazon-linux-container/ 4. Zip the new_lambda folder by right-clicking it and selecting 'compress' My results: Unable to import module 'lambda_function': No module named 'lambda_function' To reiterate, my file is named lambda_function.py and contains a function called lambda_handler, which accepts two arguments (as seen above). This information matches that seen in Handler, also seen above. I am using a Mac computer, if that matters. UPDATE 2 If I follow the above steps but instead zip the files by directly selecting the files that I want to compress and then right clicking and selecting 'compress', I instead get the error Unable to import module 'lambda_function': cannot import name 'show_config' Also, the precompiled lambda-packages says that they are compiled for ""at least Python 2.7"", but my lambda runtime is 3.6. Could this be an issue?","import os
import boto3
import numpy as np
from scipy.ndimage import imread
from scipy.spatial.distance import cdist

def lambda_handler(event, context):

    s3 = boto3.resource('s3')
 git clone https://github.com/Miserlou/lambda-packages",7,69,2,3,
478,49051638,49051757,4215,python - abstract method in normal class,1,<python><abstract-class><abstract-methods>,15,"<p>I was reading official python <a href=""https://docs.python.org/3/library/abc.html#abc.abstractmethod"" rel=""noreferrer"">documentation</a>.</p>

<p>In the mentioned link, the second line states that:</p>

<blockquote>
  <p>Using this decorator requires that the class’s metaclass is ABCMeta or
  is derived from it.</p>
</blockquote>

<p>But, I was successfully able to define the below given class.</p>

<pre><code>from abc import abstractmethod

class A(object):
    def __init__(self):
        self.a = 5
    @abstractmethod
    def f(self):
        return self.a

a = A()
a.f()
</code></pre>

<p>So, the code above worked fine.
And also, I was able to create a subclass</p>

<pre><code>class B(A):
    def __init__(self):
        super(B, self).__init__() 

b = B()
b.f()
</code></pre>

<p>Without overriding the abstract method defined above.</p>

<p>So, basically does this mean that if my base class's <code>metaclass</code> is not <code>ABCMeta</code>(or derived from it), the class does not behave like an abstract class even though I have an abstract method in it?</p>

<p>That means, the documentation needs more clarity?</p>

<p>Or, is this behaviour useful somehow and I'm missing the point.</p>
",3444956,1092,01-03-2018 14:17,01-03-2018 14:23,0,1092,24,0,9,75,"{'badge_counts': {'bronze': 24, 'silver': 9, 'gold': 0}, 'account_id': 4206108, 'is_employee': False, 'last_modified_date': 1632415800, 'last_access_date': 1710852805, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1092, 'creation_date': 1395377358, 'user_type': 'registered', 'user_id': 3444956, 'accept_rate': 75, 'website_url': '', 'link': 'https://stackoverflow.com/users/3444956/harmands', 'profile_image': 'https://www.gravatar.com/avatar/3ce4e888a13977a364ebedeb45009e50?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'harmands'}","I was reading official python documentation. In the mentioned link, the second line states that: Using this decorator requires that the class’s metaclass is ABCMeta or is derived from it. But, I was successfully able to define the below given class. So, the code above worked fine. And also, I was able to create a subclass Without overriding the abstract method defined above. So, basically does this mean that if my base class's is not (or derived from it), the class does not behave like an abstract class even though I have an abstract method in it? That means, the documentation needs more clarity? Or, is this behaviour useful somehow and I'm missing the point.","from abc import abstractmethod

class A(object):
    def __init__(self):
        self.a = 5
    @abstractmethod
    def f(self):
        return self.a

a = A()
a.f()
 class B(A):
    def __init__(self):
        super(B, self).__init__() 

b = B()
b.f()
 metaclass ABCMeta",13,42,0,1,
479,49797786,49805176,61911,How to get cursor in SQLAlchemy,4,<python><flask><sqlalchemy><flask-sqlalchemy><database-cursor>,31,"<p>I am newbie in Python Flask. In my project we are creating db object using below code.    </p>

<pre><code>    app = Flask(__name__)  
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'  
    db = SQLAlchemy(app)   
</code></pre>

<p>I want to get cursor object from db. Can someone please help me on it. 
I know using connection object we can get cursor. But can we get cursor from db object which is created by above way? Thanks. </p>
",3462649,1579,12-04-2018 13:25,12-04-2018 20:15,0,1589,19,3,13,,"{'badge_counts': {'bronze': 19, 'silver': 13, 'gold': 3}, 'account_id': 4230168, 'is_employee': False, 'last_modified_date': 1614502847, 'last_access_date': 1687405856, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1589, 'creation_date': 1395810337, 'user_type': 'registered', 'user_id': 3462649, 'link': 'https://stackoverflow.com/users/3462649/user3462649', 'profile_image': 'https://www.gravatar.com/avatar/43e57f2ef7bad32fc2519399e7c0c528?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user3462649'}",I am newbie in Python Flask. In my project we are creating db object using below code. I want to get cursor object from db. Can someone please help me on it. I know using connection object we can get cursor. But can we get cursor from db object which is created by above way? Thanks.,"    app = Flask(__name__)  
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db'  
    db = SQLAlchemy(app)   
",2,9,0,0,
480,50132703,50135020,72000,Pytest fixture for a class through self not as method argument,3,<python><unit-testing><pytest>,77,"<p>Often I'll write a test class that uses a pytest fixture in every method. Here's an example. I'd like to be able to avoid having to write the fixture name in the signature of every method. It's not DRY. How can this be done? </p>

<p>I would like to be able to access the fixture by giving the fixture as an attribute of the test class. In this example, I would like to see the google fixture as an attribute of TestGoogle. Is this possible?</p>

<pre><code>from bs4 import BeautifulSoup
import pytest
import requests

@pytest.fixture()
def google():
    return requests.get(""https://www.google.com"")


class TestGoogle:
    def test_alive(self, google):
        assert google.status_code == 200

    def test_html_title(self, google):
        soup = BeautifulSoup(google.content, ""html.parser"")
        assert soup.title.text.upper() == ""GOOGLE""
</code></pre>
",4498470,9180,02-05-2018 10:42,02-05-2018 12:47,0,9210,21,3,18,,"{'badge_counts': {'bronze': 21, 'silver': 18, 'gold': 3}, 'account_id': 5690173, 'is_employee': False, 'last_modified_date': 1664453400, 'last_access_date': 1711031422, 'reputation_change_year': 410, 'reputation_change_quarter': 410, 'reputation_change_month': 100, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 9210, 'creation_date': 1422357895, 'user_type': 'registered', 'user_id': 4498470, 'website_url': '', 'link': 'https://stackoverflow.com/users/4498470/donal', 'profile_image': 'https://www.gravatar.com/avatar/2c1d54a295b9712c18b20e393ba89946?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Donal'}","Often I'll write a test class that uses a pytest fixture in every method. Here's an example. I'd like to be able to avoid having to write the fixture name in the signature of every method. It's not DRY. How can this be done? I would like to be able to access the fixture by giving the fixture as an attribute of the test class. In this example, I would like to see the google fixture as an attribute of TestGoogle. Is this possible?","from bs4 import BeautifulSoup
import pytest
import requests

@pytest.fixture()
def google():
    return requests.get(""https://www.google.com"")


class TestGoogle:
    def test_alive(self, google):
        assert google.status_code == 200

    def test_html_title(self, google):
        soup = BeautifulSoup(google.content, ""html.parser"")
        assert soup.title.text.upper() == ""GOOGLE""
",15,21,0,0,
481,48389157,48389207,20574,Python typing for module type,1,<python><type-hinting><python-typing>,76,"<p>I am dynamically loading a Python module using <code>importlib.import_module</code> as follows</p>

<pre><code>    def load_module(mod_name: str) -&gt; ???:
        return importlib.import_module(mod_name)
</code></pre>

<p>Can somebody tell me what is the correct type annotation for a module type. The <code>typing</code> module does not contain one and I could not find an answer elsewhere.</p>
",3512469,879,22-01-2018 19:45,22-01-2018 19:49,0,879,6,2,8,,"{'badge_counts': {'bronze': 6, 'silver': 8, 'gold': 2}, 'account_id': 4297952, 'is_employee': False, 'last_modified_date': 1615111800, 'last_access_date': 1697286001, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 879, 'creation_date': 1396984056, 'user_type': 'registered', 'user_id': 3512469, 'website_url': '', 'link': 'https://stackoverflow.com/users/3512469/andrekupka', 'profile_image': 'https://www.gravatar.com/avatar/a9aa42ae578cd5ef1526c93770b50b63?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'andrekupka'}",I am dynamically loading a Python module using as follows Can somebody tell me what is the correct type annotation for a module type. The module does not contain one and I could not find an answer elsewhere.,"importlib.import_module     def load_module(mod_name: str) -&gt; ???:
        return importlib.import_module(mod_name)
 typing",-1,7,0,0,
482,50021282,50021346,26804,Python Argparse - How can I add text to the default help message?,2,<python><argparse>,43,"<p>I'm using python's argparse to handle parsing of arguments.
I get a default help message structured like so:</p>

<pre><code>usage: ProgramName [-h] ...

Description

positional arguments:
  ...

optional arguments:
  -h, --help            show this help message and exit
  ...
</code></pre>

<p>What I want is to add an entire new section to this message, for example:</p>

<pre><code>usage: ProgramName [-h] ...

Description

positional arguments:
  ...

optional arguments:
  -h, --help            show this help message and exit
  ...

additional information:
  This will show additional information relevant to the user.
  ....
</code></pre>

<p>Is there a way to achieve this behavior?
A solution that is supported by both python 2.7 and 3.x is preferred.</p>

<p>Edit:
I would also rather have a solution that will add the new section / sections at the bottom of the help message.</p>
",3531416,519,25-04-2018 11:35,25-04-2018 11:38,0,519,16,1,8,18,"{'badge_counts': {'bronze': 16, 'silver': 8, 'gold': 1}, 'account_id': 4324146, 'is_employee': False, 'last_modified_date': 1607614647, 'last_access_date': 1711009239, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 519, 'creation_date': 1397468842, 'user_type': 'registered', 'user_id': 3531416, 'accept_rate': 18, 'website_url': '', 'link': 'https://stackoverflow.com/users/3531416/yoavhayun', 'profile_image': 'https://www.gravatar.com/avatar/f8037446c0b6beb7ce22d41e526913fb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Yoavhayun'}","I'm using python's argparse to handle parsing of arguments. I get a default help message structured like so: What I want is to add an entire new section to this message, for example: Is there a way to achieve this behavior? A solution that is supported by both python 2.7 and 3.x is preferred. Edit: I would also rather have a solution that will add the new section / sections at the bottom of the help message.","usage: ProgramName [-h] ...

Description

positional arguments:
  ...

optional arguments:
  -h, --help            show this help message and exit
  ...
 usage: ProgramName [-h] ...

Description

positional arguments:
  ...

optional arguments:
  -h, --help            show this help message and exit
  ...

additional information:
  This will show additional information relevant to the user.
  ....
",22,38,0,0,
483,50355697,50463785,616,I have a OneToOne relationship between two objects of the same class in a Django app. Is it possible to enforce the uniqueness of this relationship?,2,<python><django><database><database-design><django-models>,14,"<p>I have the following in my app:</p>

<pre><code>class University(models.Model):
    ...
    sister_university = models.OneToOneField('self', related_name = 
                        'university_sister_university', 
                        blank=True, null=True, 
                        on_delete=models.SET_NULL)
</code></pre>

<p>I only want a university to be related to one other university in both directions of that relationship.</p>

<p>For example, in the database, if I select university A as the sister_university of university B, I only want to be allowed to select university B as the sister_university under university A also. However, as it is, that second relationship is not enforced. </p>

<p>For example: Right now, under the Django Admin site, if I first select university A as the sister university of university B, I am still able to select any other university as the sister university of the university A object. I’m not constrained to only selecting university B.</p>

<p>Is it possible to enforce that uniqueness at the database level? Is there a better way of accomplishing what I’m trying to do?</p>
",4589892,163,15-05-2018 17:01,22-05-2018 09:05,7,163,10,0,0,,"{'badge_counts': {'bronze': 10, 'silver': 0, 'gold': 0}, 'account_id': 5822211, 'is_employee': False, 'last_modified_date': 1703300100, 'last_access_date': 1601100871, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 163, 'creation_date': 1424471556, 'user_type': 'registered', 'user_id': 4589892, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/4589892/joe', 'profile_image': 'https://i.stack.imgur.com/6AnkT.jpg?s=256&g=1', 'display_name': 'Joe'}","I have the following in my app: I only want a university to be related to one other university in both directions of that relationship. For example, in the database, if I select university A as the sister_university of university B, I only want to be allowed to select university B as the sister_university under university A also. However, as it is, that second relationship is not enforced. For example: Right now, under the Django Admin site, if I first select university A as the sister university of university B, I am still able to select any other university as the sister university of the university A object. I’m not constrained to only selecting university B. Is it possible to enforce that uniqueness at the database level? Is there a better way of accomplishing what I’m trying to do?","class University(models.Model):
    ...
    sister_university = models.OneToOneField('self', related_name = 
                        'university_sister_university', 
                        blank=True, null=True, 
                        on_delete=models.SET_NULL)
",5,17,0,0,
484,48840378,48841071,57631,Python - Attempt to decode JSON with unexpected mimetype:,2,<python><json><python-3.x><python-asyncio><aiohttp>,39,"<p>I recently swapped over from requests to aiohttp because I couldn't use it in asyncio loops.</p>

<p>The swap went perfectly and everything goes well except for one thing. My console is full of </p>

<pre><code>Attempt to decode JSON with unexpected mimetype:
</code></pre>

<p>and</p>

<pre><code>Attempt to decode JSON with unexpected mimetype: txt/html; charset=utf-8
</code></pre>

<p>My code has a list of sites it goes too and grabs JSON from, Each site is different but my loop is basically the same for each of them, Ive simplified it here:</p>

<pre><code>PoolName = ""http://website.com""
endpoint = ""/api/stats""
headers = ""headers = {'content-type': 'text/html'}"" #Ive tried ""application/json"" and no headers
async with aiohttp.get(url=PoolName+endpoint, headers=headers) as hashrate:
                hashrate = await hashrate.json()
endVariable = hashrate['GLRC']['HASH']
</code></pre>

<p>It works perfectly, connects to the site grabs the json and sets endVariable correctly. but for some reason </p>

<pre><code>Attempt to decode JSON with unexpected mimetype:
</code></pre>

<p>prints every time it goes through the loop. Which is annoying because it prints stats to the console and they get lost in the the errors each time it grabs a sites json</p>

<p>Is there a way to fix this error or to hide it?</p>
",3673022,910,17-02-2018 10:33,17-02-2018 12:05,0,910,32,1,13,100,"{'badge_counts': {'bronze': 32, 'silver': 13, 'gold': 1}, 'account_id': 4519200, 'is_employee': False, 'last_modified_date': 1601937000, 'last_access_date': 1704920274, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 910, 'creation_date': 1400993651, 'user_type': 'registered', 'user_id': 3673022, 'accept_rate': 100, 'location': 'Australia', 'website_url': 'http://pfy.ch', 'link': 'https://stackoverflow.com/users/3673022/pfych', 'profile_image': 'https://i.stack.imgur.com/QcJAy.jpg?s=256&g=1', 'display_name': 'pfych'}","I recently swapped over from requests to aiohttp because I couldn't use it in asyncio loops. The swap went perfectly and everything goes well except for one thing. My console is full of and My code has a list of sites it goes too and grabs JSON from, Each site is different but my loop is basically the same for each of them, Ive simplified it here: It works perfectly, connects to the site grabs the json and sets endVariable correctly. but for some reason prints every time it goes through the loop. Which is annoying because it prints stats to the console and they get lost in the the errors each time it grabs a sites json Is there a way to fix this error or to hide it?","Attempt to decode JSON with unexpected mimetype:
 Attempt to decode JSON with unexpected mimetype: txt/html; charset=utf-8
 PoolName = ""http://website.com""
endpoint = ""/api/stats""
headers = ""headers = {'content-type': 'text/html'}"" #Ive tried ""application/json"" and no headers
async with aiohttp.get(url=PoolName+endpoint, headers=headers) as hashrate:
                hashrate = await hashrate.json()
endVariable = hashrate['GLRC']['HASH']
 Attempt to decode JSON with unexpected mimetype:
",5,30,0,0,
485,48393253,48451104,12544,How to parse table with rowspan and colspan,3,<python><html><html-table>,14,"<p>First, I have read <a href=""https://stackoverflow.com/questions/9978445/parsing-a-table-with-rowspan-and-colspan/46538366"">Parsing a table with rowspan and colspan</a>. I even answered the question. Please read before you mark this as duplicate.</p>

<pre class=""lang-html prettyprint-override""><code>&lt;table border=""1""&gt;
  &lt;tr&gt;
    &lt;th&gt;A&lt;/th&gt;
    &lt;th&gt;B&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=""2""&gt;C&lt;/td&gt;
    &lt;td rowspan=""1""&gt;D&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;E&lt;/td&gt;
    &lt;td&gt;F&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;G&lt;/td&gt;
    &lt;td&gt;H&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</code></pre>

<p>It will render like</p>

<pre><code>+---+---+---+
| A | B |   |
+---+---+   |
|   | D |   |
+ C +---+---+
|   | E | F |
+---+---+---+
| G | H |   |
+---+---+---+
</code></pre>

<pre class=""lang-html prettyprint-override""><code>&lt;table border=""1""&gt;
  &lt;tr&gt;
    &lt;th&gt;A&lt;/th&gt;
    &lt;th&gt;B&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=""2""&gt;C&lt;/td&gt;
    &lt;td rowspan=""2""&gt;D&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;E&lt;/td&gt;
    &lt;td&gt;F&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;G&lt;/td&gt;
    &lt;td&gt;H&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</code></pre>

<p>However, this will render like this.</p>

<pre><code>+---+---+-------+
| A | B |       |
+---+---+-------+
|   |   |       |
| C | D +---+---+
|   |   | E | F |
+---+---+---+---+
| G | H |       |
+---+---+---+---+
</code></pre>

<p>My code from previous answer can only parse table which has all the columns defined in the first row.</p>

<pre><code>def table_to_2d(table_tag):
    rows = table_tag(""tr"")
    cols = rows[0]([""td"", ""th""])
    table = [[None] * len(cols) for _ in range(len(rows))]
    for row_i, row in enumerate(rows):
        for col_i, col in enumerate(row([""td"", ""th""])):
            insert(table, row_i, col_i, col)
    return table


def insert(table, row, col, element):
    if row &gt;= len(table) or col &gt;= len(table[row]):
        return
    if table[row][col] is None:
        value = element.get_text()
        table[row][col] = value
        if element.has_attr(""colspan""):
            span = int(element[""colspan""])
            for i in range(1, span):
                table[row][col+i] = value
        if element.has_attr(""rowspan""):
            span = int(element[""rowspan""])
            for i in range(1, span):
                table[row+i][col] = value
    else:
        insert(table, row, col + 1, element)

soup = BeautifulSoup('''
    &lt;table&gt;
        &lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;th&gt;2&lt;/th&gt;&lt;th&gt;5&lt;/th&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td rowspan=""2""&gt;3&lt;/td&gt;&lt;td colspan=""2""&gt;4&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
    &lt;/table&gt;''', 'html.parser')
print(table_to_2d(soup.table))
</code></pre>

<p>My question is how to parse a table into a 2D array which represent <strong>exactly</strong> how it render in browser. Or someone can explain how the browser renders the table is also fine.</p>
",3673259,6061,23-01-2018 02:25,25-01-2018 20:09,2,6061,54,2,32,94,"{'badge_counts': {'bronze': 54, 'silver': 32, 'gold': 2}, 'account_id': 4519550, 'is_employee': False, 'last_modified_date': 1673281800, 'last_access_date': 1708475535, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6061, 'creation_date': 1401005154, 'user_type': 'registered', 'user_id': 3673259, 'accept_rate': 94, 'website_url': 'https://joshuaavalon.io', 'link': 'https://stackoverflow.com/users/3673259/joshua', 'profile_image': 'https://i.stack.imgur.com/DPHO1.png?s=256&g=1', 'display_name': 'Joshua'}","First, I have read Parsing a table with rowspan and colspan. I even answered the question. Please read before you mark this as duplicate. It will render like However, this will render like this. My code from previous answer can only parse table which has all the columns defined in the first row. My question is how to parse a table into a 2D array which represent exactly how it render in browser. Or someone can explain how the browser renders the table is also fine.","&lt;table border=""1""&gt;
  &lt;tr&gt;
    &lt;th&gt;A&lt;/th&gt;
    &lt;th&gt;B&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=""2""&gt;C&lt;/td&gt;
    &lt;td rowspan=""1""&gt;D&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;E&lt;/td&gt;
    &lt;td&gt;F&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;G&lt;/td&gt;
    &lt;td&gt;H&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
 +---+---+---+
| A | B |   |
+---+---+   |
|   | D |   |
+ C +---+---+
|   | E | F |
+---+---+---+
| G | H |   |
+---+---+---+
 &lt;table border=""1""&gt;
  &lt;tr&gt;
    &lt;th&gt;A&lt;/th&gt;
    &lt;th&gt;B&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td rowspan=""2""&gt;C&lt;/td&gt;
    &lt;td rowspan=""2""&gt;D&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;E&lt;/td&gt;
    &lt;td&gt;F&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;G&lt;/td&gt;
    &lt;td&gt;H&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
 +---+---+-------+
| A | B |       |
+---+---+-------+
|   |   |       |
| C | D +---+---+
|   |   | E | F |
+---+---+---+---+
| G | H |       |
+---+---+---+---+
 def table_to_2d(table_tag):
    rows = table_tag(""tr"")
    cols = rows[0]([""td"", ""th""])
    table = [[None] * len(cols) for _ in range(len(rows))]
    for row_i, row in enumerate(rows):
        for col_i, col in enumerate(row([""td"", ""th""])):
            insert(table, row_i, col_i, col)
    return table


def insert(table, row, col, element):
    if row &gt;= len(table) or col &gt;= len(table[row]):
        return
    if table[row][col] is None:
        value = element.get_text()
        table[row][col] = value
        if element.has_attr(""colspan""):
            span = int(element[""colspan""])
            for i in range(1, span):
                table[row][col+i] = value
        if element.has_attr(""rowspan""):
            span = int(element[""rowspan""])
            for i in range(1, span):
                table[row+i][col] = value
    else:
        insert(table, row, col + 1, element)

soup = BeautifulSoup('''
    &lt;table&gt;
        &lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;th&gt;2&lt;/th&gt;&lt;th&gt;5&lt;/th&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td rowspan=""2""&gt;3&lt;/td&gt;&lt;td colspan=""2""&gt;4&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
    &lt;/table&gt;''', 'html.parser')
print(table_to_2d(soup.table))
",83,107,0,1,
486,48304927,48304953,33775,Cleanly combine year and month columns to single date column with pandas,1,<python><pandas><date><datetime><dataframe>,21,"<p>I have data that looks like this:</p>

<pre><code>+----+------+-------+
| ID | YEAR | MONTH |
+----+------+-------+
| A  | 2017 |     1 |
| B  | 2017 |     2 |
| C  | 2017 |     3 |
| D  | 2017 |     4 |
| E  | 2017 |     5 |
| F  | 2017 |     6 |
+----+------+-------+
</code></pre>

<p>I want to add a new column called <code>DATE</code> which store the a new column made up of a date object of the <code>YEAR</code> and <code>MONTH</code> columns.  Something like this:</p>

<pre><code>+----+------+-------+------------+
| ID | YEAR | MONTH |    DATE    |
+----+------+-------+------------+
| A  | 2017 |     1 | 2017-01-01 |
| B  | 2017 |     2 | 2017-02-01 |
| C  | 2017 |     3 | 2017-03-01 |
| D  | 2017 |     4 | 2017-04-01 |
| E  | 2017 |     5 | 2017-05-01 |
| F  | 2017 |     6 | 2017-06-01 |
+----+------+-------+------------+
</code></pre>

<p>I used the following code to create the column, but was wondering if there's a cleaner 'Pythonic' one-liner. Something along the lines of <code>df['DATE']=date(df.year, df.month, 1)</code>.</p>

<pre><code>import pandas as pd
from datetime import date


ID  = ['A', 'B', 'C', 'D', 'E', 'F']
YEAR = [2017, 2017, 2017, 2017, 2017, 2017]
MONTH = [1, 2, 3, 4, 5, 6]


df = pd.DataFrame({'ID': ID, 'YEAR': YEAR, 'MONTH': MONTH})


DATE = []
for y, m in zip(df.YEAR, df.MONTH):
    DATE.append(date(y, m, 1))


df['DATE'] = DATE
</code></pre>
",3699823,581,17-01-2018 15:46,17-01-2018 15:48,0,581,10,1,3,,"{'badge_counts': {'bronze': 10, 'silver': 3, 'gold': 1}, 'account_id': 2977989, 'is_employee': False, 'last_modified_date': 1573680358, 'last_access_date': 1617112162, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 581, 'creation_date': 1401720279, 'user_type': 'registered', 'user_id': 3699823, 'location': 'New Jersey, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/3699823/sam', 'profile_image': 'https://www.gravatar.com/avatar/3fc9d64baa3134adc85c2a4f158541e7?s=256&d=identicon&r=PG', 'display_name': 'Sam'}","I have data that looks like this: I want to add a new column called which store the a new column made up of a date object of the and columns. Something like this: I used the following code to create the column, but was wondering if there's a cleaner 'Pythonic' one-liner. Something along the lines of .","+----+------+-------+
| ID | YEAR | MONTH |
+----+------+-------+
| A  | 2017 |     1 |
| B  | 2017 |     2 |
| C  | 2017 |     3 |
| D  | 2017 |     4 |
| E  | 2017 |     5 |
| F  | 2017 |     6 |
+----+------+-------+
 DATE YEAR MONTH +----+------+-------+------------+
| ID | YEAR | MONTH |    DATE    |
+----+------+-------+------------+
| A  | 2017 |     1 | 2017-01-01 |
| B  | 2017 |     2 | 2017-02-01 |
| C  | 2017 |     3 | 2017-03-01 |
| D  | 2017 |     4 | 2017-04-01 |
| E  | 2017 |     5 | 2017-05-01 |
| F  | 2017 |     6 | 2017-06-01 |
+----+------+-------+------------+
 df['DATE']=date(df.year, df.month, 1) import pandas as pd
from datetime import date


ID  = ['A', 'B', 'C', 'D', 'E', 'F']
YEAR = [2017, 2017, 2017, 2017, 2017, 2017]
MONTH = [1, 2, 3, 4, 5, 6]


df = pd.DataFrame({'ID': ID, 'YEAR': YEAR, 'MONTH': MONTH})


DATE = []
for y, m in zip(df.YEAR, df.MONTH):
    DATE.append(date(y, m, 1))


df['DATE'] = DATE
",31,49,0,0,
487,48098829,49490415,17523,Why do we need Signatures in Celery?,2,<python><django><celery><signatures>,27,"<p>I've started using Celery 4.1 in my Django Python project and have come across Signatures.</p>
<p>In the <a href=""http://docs.celeryproject.org/en/latest/userguide/canvas.html#signatures"" rel=""noreferrer"">documentation</a> it says the following:</p>
<blockquote>
<p>You just learned how to call a task using the tasks delay method in the calling guide, and this is often all you need, but sometimes you may want to pass the signature of a task invocation to another process or as an argument to another function.</p>
<p>A signature() wraps the arguments, keyword arguments, and execution options of a single task invocation in a way such that it can be passed to functions or even serialized and sent across the wire.</p>
</blockquote>
<p>Although I see them used in some of the examples I don't really know when and why to use them, as well as which problems they solve.
Can someone explain this to a layman?</p>
",5568461,371,04-01-2018 15:49,26-03-2018 11:31,81,371,10,2,4,,"{'badge_counts': {'bronze': 10, 'silver': 4, 'gold': 2}, 'account_id': 6400013, 'is_employee': False, 'last_modified_date': 1688640432, 'last_access_date': 1711014302, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 371, 'creation_date': 1447691441, 'user_type': 'registered', 'user_id': 5568461, 'location': 'Karlsruhe, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/5568461/monoclecat', 'profile_image': 'https://i.stack.imgur.com/7idKM.png?s=256&g=1', 'display_name': 'Monoclecat'}","I've started using Celery 4.1 in my Django Python project and have come across Signatures. In the documentation it says the following: You just learned how to call a task using the tasks delay method in the calling guide, and this is often all you need, but sometimes you may want to pass the signature of a task invocation to another process or as an argument to another function. A signature() wraps the arguments, keyword arguments, and execution options of a single task invocation in a way such that it can be passed to functions or even serialized and sent across the wire. Although I see them used in some of the examples I don't really know when and why to use them, as well as which problems they solve. Can someone explain this to a layman?",,0,8,0,1,
488,48327054,48327127,26879,Where does pip download .whl files?,2,<python>,19,"<p>I'd like to install a certain python package with pip but because of the proxy I am sitting behind pip cannot connect to the internet.</p>

<p>So my question is: Where does pip look for .whl files in order to download them? Can't I just use my browser (which can connect to the internet just fine) to download the .whl file? <a href=""https://stackoverflow.com/questions/27885397/how-do-i-install-a-python-package-with-a-whl-file"">Installing the package with the downloaded .whl file</a> would be not a problem then.</p>
",5572543,2256,18-01-2018 17:25,18-01-2018 17:29,0,2256,27,1,17,55,"{'badge_counts': {'bronze': 27, 'silver': 17, 'gold': 1}, 'account_id': 7314382, 'is_employee': False, 'last_modified_date': 1573679339, 'last_access_date': 1711108228, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2256, 'creation_date': 1447771750, 'user_type': 'registered', 'user_id': 5572543, 'accept_rate': 55, 'website_url': '', 'link': 'https://stackoverflow.com/users/5572543/elzell', 'profile_image': 'https://www.gravatar.com/avatar/6f73140eedc8165647dc849dd8ae7cdb?s=256&d=identicon&r=PG', 'display_name': 'elzell'}",I'd like to install a certain python package with pip but because of the proxy I am sitting behind pip cannot connect to the internet. So my question is: Where does pip look for .whl files in order to download them? Can't I just use my browser (which can connect to the internet just fine) to download the .whl file? Installing the package with the downloaded .whl file would be not a problem then.,,0,3,0,1,
489,48274929,48278089,76858,"Pytorch - RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed",3,<python><deep-learning><pytorch><recurrent-neural-network><autograd>,56,"<p>I keep running into this error:</p>
<blockquote>
<p>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</p>
</blockquote>
<p>I had searched in Pytorch forum, but still can’t find out what I have done wrong in my custom loss function. My model is nn.GRU, and here is my custom loss function:</p>
<pre><code>def _loss(outputs, session, items):  # `items` is a dict() contains embedding of all items
    def f(output, target):
        pos = torch.from_numpy(np.array([items[target[&quot;click&quot;]]])).float()
        neg = torch.from_numpy(np.array([items[idx] for idx in target[&quot;suggest_list&quot;] if idx != target[&quot;click&quot;]])).float()
        if USE_CUDA:
            pos, neg = pos.cuda(), neg.cuda()
        pos, neg = Variable(pos), Variable(neg)

        pos = F.cosine_similarity(output, pos)
        if neg.size()[0] == 0:
            return torch.mean(F.logsigmoid(pos))
        neg = F.cosine_similarity(output.expand_as(neg), neg)

        return torch.mean(F.logsigmoid(pos - neg))

    loss = map(f, outputs, session)
    return -torch.mean(torch.cat(loss))
</code></pre>
<p>Training code:</p>
<pre><code>    # zero the parameter gradients
    model.zero_grad()

    # forward + backward + optimize
    outputs, hidden = model(inputs, hidden)
    loss = _loss(outputs, session, items)
    acc_loss += loss.data[0]

    loss.backward()
    # Add parameters' gradients to their values, multiplied by learning rate
    for p in model.parameters():
        p.data.add_(-learning_rate, p.grad.data)
</code></pre>
",4728650,2039,16-01-2018 05:57,16-01-2018 09:35,0,2039,41,3,23,91,"{'badge_counts': {'bronze': 41, 'silver': 23, 'gold': 3}, 'account_id': 6021794, 'is_employee': False, 'last_modified_date': 1687957200, 'last_access_date': 1706388263, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2039, 'creation_date': 1427700606, 'user_type': 'registered', 'user_id': 4728650, 'accept_rate': 91, 'location': 'Hanoi, Vietnam', 'website_url': 'https://dogy.io', 'link': 'https://stackoverflow.com/users/4728650/viet-phan', 'profile_image': 'https://graph.facebook.com/1642705992615708/picture?type=large', 'display_name': 'Viet Phan'}","I keep running into this error: RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time. I had searched in Pytorch forum, but still can’t find out what I have done wrong in my custom loss function. My model is nn.GRU, and here is my custom loss function: Training code:","def _loss(outputs, session, items):  # `items` is a dict() contains embedding of all items
    def f(output, target):
        pos = torch.from_numpy(np.array([items[target[&quot;click&quot;]]])).float()
        neg = torch.from_numpy(np.array([items[idx] for idx in target[&quot;suggest_list&quot;] if idx != target[&quot;click&quot;]])).float()
        if USE_CUDA:
            pos, neg = pos.cuda(), neg.cuda()
        pos, neg = Variable(pos), Variable(neg)

        pos = F.cosine_similarity(output, pos)
        if neg.size()[0] == 0:
            return torch.mean(F.logsigmoid(pos))
        neg = F.cosine_similarity(output.expand_as(neg), neg)

        return torch.mean(F.logsigmoid(pos - neg))

    loss = map(f, outputs, session)
    return -torch.mean(torch.cat(loss))
     # zero the parameter gradients
    model.zero_grad()

    # forward + backward + optimize
    outputs, hidden = model(inputs, hidden)
    loss = _loss(outputs, session, items)
    acc_loss += loss.data[0]

    loss.backward()
    # Add parameters' gradients to their values, multiplied by learning rate
    for p in model.parameters():
        p.data.add_(-learning_rate, p.grad.data)
",27,37,0,0,
490,48340392,48486720,57285,FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated,11,<python><numpy><tensorflow>,49,"<p>After updating my <code>Numpy</code> and <code>Tensorflow</code> I am getting these kind of warnings. I had already tried <a href=""https://github.com/scikit-learn/scikit-learn/issues/9673"" rel=""noreferrer"">these</a>, but nothing works, every suggestion will be appreciated.</p>

<pre><code>FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-01-19 17:11:38.695932: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
</code></pre>
",4786793,2773,19-01-2018 11:46,28-01-2018 13:11,9,2773,46,5,31,75,"{'badge_counts': {'bronze': 46, 'silver': 31, 'gold': 5}, 'collectives': [{'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 6138760, 'is_employee': False, 'last_modified_date': 1694421300, 'last_access_date': 1691327857, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2773, 'creation_date': 1429006470, 'user_type': 'registered', 'user_id': 4786793, 'accept_rate': 75, 'location': 'Indore, Madhya Pradesh, India', 'website_url': 'https://github.com/shubhamsharmacs', 'link': 'https://stackoverflow.com/users/4786793/shubham-sharma', 'profile_image': 'https://i.stack.imgur.com/JauRI.jpg?s=256&g=1', 'display_name': 'Shubham Sharma'}","After updating my and I am getting these kind of warnings. I had already tried these, but nothing works, every suggestion will be appreciated.","Numpy Tensorflow FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
2018-01-19 17:11:38.695932: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
",0,6,0,1,
491,48097941,48098534,24181,Strided convolution of 2D in numpy,6,<python><numpy><convolution>,16,"<p>I tried to implement strided convolution of a 2D array using for loop i.e </p>

<pre><code>arr = np.array([[2,3,7,4,6,2,9],
                [6,6,9,8,7,4,3],
                [3,4,8,3,8,9,7],
                [7,8,3,6,6,3,4],
                [4,2,1,8,3,4,6],
                [3,2,4,1,9,8,3],
                [0,1,3,9,2,1,4]])

arr2 = np.array([[3,4,4],
                 [1,0,2],
                 [-1,0,3]])

def stride_conv(arr1,arr2,s,p):
    beg = 0
    end = arr2.shape[0]
    final = []
    for i in range(0,arr1.shape[0]-1,s):
        k = []
        for j in range(0,arr1.shape[0]-1,s):
            k.append(np.sum(arr1[beg+i : end+i, beg+j:end+j] * (arr2)))
        final.append(k)

    return np.array(final)

stride_conv(arr,arr2,2,0)
</code></pre>

<p>This results in 3*3 array: </p>

<pre><code>array([[ 91, 100,  88],
       [ 69,  91, 117],
       [ 44,  72,  74]])
</code></pre>

<p>Is there a numpy function or scipy function to do the same? My approach is not that good. How can I vectorize this? </p>
",4800652,30325,04-01-2018 14:59,04-01-2018 15:33,0,30325,110,6,61,92,"{'badge_counts': {'bronze': 110, 'silver': 61, 'gold': 6}, 'account_id': 6008029, 'is_employee': False, 'last_modified_date': 1706785959, 'last_access_date': 1711097756, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 30325, 'creation_date': 1429266807, 'user_type': 'registered', 'user_id': 4800652, 'accept_rate': 92, 'location': 'Earth, neighbour Mars', 'website_url': 'https://bharathm.dev', 'link': 'https://stackoverflow.com/users/4800652/bharath-m-shetty', 'profile_image': 'https://i.stack.imgur.com/UGQEb.png?s=256&g=1', 'display_name': 'Bharath M Shetty'}",I tried to implement strided convolution of a 2D array using for loop i.e This results in 3*3 array: Is there a numpy function or scipy function to do the same? My approach is not that good. How can I vectorize this?,"arr = np.array([[2,3,7,4,6,2,9],
                [6,6,9,8,7,4,3],
                [3,4,8,3,8,9,7],
                [7,8,3,6,6,3,4],
                [4,2,1,8,3,4,6],
                [3,2,4,1,9,8,3],
                [0,1,3,9,2,1,4]])

arr2 = np.array([[3,4,4],
                 [1,0,2],
                 [-1,0,3]])

def stride_conv(arr1,arr2,s,p):
    beg = 0
    end = arr2.shape[0]
    final = []
    for i in range(0,arr1.shape[0]-1,s):
        k = []
        for j in range(0,arr1.shape[0]-1,s):
            k.append(np.sum(arr1[beg+i : end+i, beg+j:end+j] * (arr2)))
        final.append(k)

    return np.array(final)

stride_conv(arr,arr2,2,0)
 array([[ 91, 100,  88],
       [ 69,  91, 117],
       [ 44,  72,  74]])
",26,37,0,0,
492,48230706,49099457,36255,How to remove password for Jupyter Notebooks and set token again,6,<python><pycharm><ipython><jupyter-notebook><jupyter>,24,"<p>I need to do this for Pycharm.</p>

<p>Here are the steps that I did which I'm not able to undo.<br></p>

<ol>
<li><p>I <strong>added a password</strong> for authentication using:<br></p>

<blockquote>
  <p>$ jupyter notebook password</p>
</blockquote></li>
<li><p>I then used the below command to <strong>comment all the code</strong> in jupyter_notebook_config.py</p>

<blockquote>
  <p>$ jupyter notebook --generate-config</p>
</blockquote></li>
<li><p>Then I <strong>removed the hashed password</strong> generated in the jupyter_notebook_config.json which now looks like this</p>

<blockquote>
  <p>{
  ""NotebookApp"": {
  ""password"": """"
  }
  }</p>
</blockquote></li>
<li><p>I then did the following <strong>changes</strong> in jupyter_notebook_config.py file</p>

<blockquote>
  <p>c.NotebookApp.password = ''<br>
  c.NotebookApp.token = '&lt; generated>'<br></p>
</blockquote></li>
<li><p>Now, There is no token getting generated and there is no password as well when I start the Jupyter notebook.</p>

<blockquote>
  <p>Pycharm git:(master) ✗ jupyter notebook<br>
  [I 21:53:35.158 NotebookApp] Serving notebooks from local directory: /Users/...<br>
  [I 21:53:35.158 NotebookApp] 0 active kernels<br>
  [I 21:53:35.158 NotebookApp] The Jupyter Notebook is running at:<br>
  [I 21:53:35.158 NotebookApp] <a href=""http://localhost:8888/?token=%3Cgenerated%3E"" rel=""noreferrer"">http://localhost:8888/?token=%3Cgenerated%3E</a><br><br>
  Copy/paste this URL into your browser when you connect for the first time,<br>
  to login with a token:<br>
      <a href=""http://localhost:8888/?token=%3Cgenerated%3E"" rel=""noreferrer"">http://localhost:8888/?token=%3Cgenerated%3E</a><br></p>
</blockquote></li>
</ol>

<p><strong>Now, how do I make it like the way it was or how do I get the token back??</strong></p>

<p>PS - I even tried <em>jupyter notebook list</em>,  but still the same URL is coming. Also, I'm doing this on a mac, so please advise accordingly.</p>
",4883925,413,12-01-2018 16:54,04-03-2018 19:23,51,413,11,1,3,,"{'badge_counts': {'bronze': 11, 'silver': 3, 'gold': 1}, 'account_id': 6283959, 'is_employee': False, 'last_modified_date': 1610018833, 'last_access_date': 1710356938, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 413, 'creation_date': 1431248170, 'user_type': 'registered', 'user_id': 4883925, 'location': 'Bangalore, Karnataka, India', 'website_url': 'https://getbreakout.com', 'link': 'https://stackoverflow.com/users/4883925/jugal-anchalia', 'profile_image': 'https://graph.facebook.com/990861517598542/picture?type=large', 'display_name': 'Jugal Anchalia'}","I need to do this for Pycharm. Here are the steps that I did which I'm not able to undo. I added a password for authentication using: $ jupyter notebook password I then used the below command to comment all the code in jupyter_notebook_config.py $ jupyter notebook --generate-config Then I removed the hashed password generated in the jupyter_notebook_config.json which now looks like this { ""NotebookApp"": { ""password"": """" } } I then did the following changes in jupyter_notebook_config.py file c.NotebookApp.password = '' c.NotebookApp.token = '&lt; generated>' Now, There is no token getting generated and there is no password as well when I start the Jupyter notebook. Pycharm git:(master) ✗ jupyter notebook [I 21:53:35.158 NotebookApp] Serving notebooks from local directory: /Users/... [I 21:53:35.158 NotebookApp] 0 active kernels [I 21:53:35.158 NotebookApp] The Jupyter Notebook is running at: [I 21:53:35.158 NotebookApp] http://localhost:8888/?token=%3Cgenerated%3E Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=%3Cgenerated%3E Now, how do I make it like the way it was or how do I get the token back?? PS - I even tried jupyter notebook list, but still the same URL is coming. Also, I'm doing this on a mac, so please advise accordingly.",,0,47,0,2,
493,48349048,48349180,39753,How to convert date to the first day of month in a PySpark Dataframe column?,3,<python><apache-spark><pyspark><apache-spark-sql>,15,"<p>I have the following DataFrame: </p>

<pre><code>+----------+
|      date|
+----------+
|2017-01-25|
|2017-01-21|
|2017-01-12|
+----------+
</code></pre>

<p>Here is the code the create above DataFrame: </p>

<pre><code>import pyspark.sql.functions as f
rdd = sc.parallelize([(""2017/11/25"",), (""2017/12/21"",), (""2017/09/12"",)])
df = sqlContext.createDataFrame(rdd, [""date""]).withColumn(""date"", f.to_date(f.col(""date""), ""yyyy/MM/dd""))
df.show()
</code></pre>

<p>I want a new column with the first date of month for each row, just replace the day to ""01"" in all the dates</p>

<pre><code>+----------++----------+
|      date| first_date|
+----------++----------+
|2017-11-25| 2017-11-01|
|2017-12-21| 2017-12-01|
|2017-09-12| 2017-09-01|
+----------+-----------+
</code></pre>

<p>There is a last_day function in PySpark.sql.function, however, there is no first_day function. </p>

<p>I tried using date_sub to do this but did not work: I get a column not Iterable error because the second argument to date_sub cannot be a column and has to be an integer. </p>

<pre><code>f.date_sub(f.col('date'), f.dayofmonth(f.col('date')) - 1 )
</code></pre>
",4168397,12296,19-01-2018 20:28,19-01-2018 20:38,0,12316,77,18,53,92,"{'badge_counts': {'bronze': 77, 'silver': 53, 'gold': 18}, 'account_id': 2076775, 'is_employee': False, 'last_modified_date': 1682732700, 'last_access_date': 1680809286, 'reputation_change_year': 190, 'reputation_change_quarter': 190, 'reputation_change_month': 40, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 12316, 'creation_date': 1370599635, 'user_type': 'registered', 'user_id': 4168397, 'accept_rate': 92, 'location': 'Chicago, IL, United States', 'website_url': 'http://www.adhikesavan.com/', 'link': 'https://stackoverflow.com/users/4168397/rakesh-adhikesavan', 'profile_image': 'https://www.gravatar.com/avatar/8e31d5748edf285187f67c329542ef96?s=256&d=identicon&r=PG', 'display_name': 'Rakesh Adhikesavan'}","I have the following DataFrame: Here is the code the create above DataFrame: I want a new column with the first date of month for each row, just replace the day to ""01"" in all the dates There is a last_day function in PySpark.sql.function, however, there is no first_day function. I tried using date_sub to do this but did not work: I get a column not Iterable error because the second argument to date_sub cannot be a column and has to be an integer.","+----------+
|      date|
+----------+
|2017-01-25|
|2017-01-21|
|2017-01-12|
+----------+
 import pyspark.sql.functions as f
rdd = sc.parallelize([(""2017/11/25"",), (""2017/12/21"",), (""2017/09/12"",)])
df = sqlContext.createDataFrame(rdd, [""date""]).withColumn(""date"", f.to_date(f.col(""date""), ""yyyy/MM/dd""))
df.show()
 +----------++----------+
|      date| first_date|
+----------++----------+
|2017-11-25| 2017-11-01|
|2017-12-21| 2017-12-01|
|2017-09-12| 2017-09-01|
+----------+-----------+
 f.date_sub(f.col('date'), f.dayofmonth(f.col('date')) - 1 )
",15,36,0,0,
494,49161174,49202297,99519,Tensorflow : logits and labels must have the same first dimension,7,<python><tensorflow><keras><tensorflow-datasets><tensorflow-estimator>,42,"<p>I am new in tensoflow and I want to adapt the MNIST tutorial <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/layers</a> with my own data (images of 40x40).
This is my model function : </p>

<pre><code>def cnn_model_fn(features, labels, mode):
        # Input Layer
        input_layer = tf.reshape(features, [-1, 40, 40, 1])

        # Convolutional Layer #1
        conv1 = tf.layers.conv2d(
                inputs=input_layer,
                filters=32,
                kernel_size=[5, 5],
                #  To specify that the output tensor should have the same width and height values as the input tensor
                # value can be ""same"" ou ""valid""
                padding=""same"",
                activation=tf.nn.relu)

        # Pooling Layer #1
        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

        # Convolutional Layer #2 and Pooling Layer #2
        conv2 = tf.layers.conv2d(
                inputs=pool1,
                filters=64,
                kernel_size=[5, 5],
                padding=""same"",
                activation=tf.nn.relu)
        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

        # Dense Layer
        pool2_flat = tf.reshape(pool2, [-1, 10 * 10 * 64])
        dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
        dropout = tf.layers.dropout(
                inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

        # Logits Layer
        logits = tf.layers.dense(inputs=dropout, units=2)

        predictions = {
            # Generate predictions (for PREDICT and EVAL mode)
            ""classes"":       tf.argmax(input=logits, axis=1),
            # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
            # `logging_hook`.
            ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
        }

        if mode == tf.estimator.ModeKeys.PREDICT:
            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

        # Calculate Loss (for both TRAIN and EVAL modes)
        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

        # Configure the Training Op (for TRAIN mode)
        if mode == tf.estimator.ModeKeys.TRAIN:
            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
            train_op = optimizer.minimize(
                    loss=loss,
                    global_step=tf.train.get_global_step())
            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

        # Add evaluation metrics (for EVAL mode)
        eval_metric_ops = {
            ""accuracy"": tf.metrics.accuracy(
                    labels=labels, predictions=predictions[""classes""])}
        return tf.estimator.EstimatorSpec(
                mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)
</code></pre>

<p>I have a shape size error between labels and logits : </p>

<p><strong>InvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [3,2] and labels shape [1]</strong> </p>

<p>filenames_array is an array of 16 string </p>

<pre><code>[""file1.png"", ""file2.png"", ""file3.png"", ...]
</code></pre>

<p>and labels_array is an array of 16 integer </p>

<pre><code>[0,0,1,1,0,1,0,0,0,...]
</code></pre>

<p>The main function is :</p>

<pre><code># Create the Estimator
mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=""/tmp/test_convnet_model"")

# Train the model
cust_train_input_fn = lambda: train_input_fn_custom(
        filenames_array=filenames, labels_array=labels, batch_size=1)

mnist_classifier.train(
        input_fn=cust_train_input_fn,
        steps=20000,
        hooks=[logging_hook])
</code></pre>

<p>I tried to reshape logits without success :</p>

<p>logits = tf.reshape(logits, [1, 2])</p>

<p>I need your help, thanks</p>

<hr>

<p><strong>EDIT</strong></p>

<p>After more time to search, in the first line of my model function</p>

<pre><code>input_layer = tf.reshape(features, [-1, 40, 40, 1])
</code></pre>

<p>the ""-1"" that signifies that the batch_size dimension will be dynamically calculated have here the value ""3"". The same ""3"" as in my error : <strong>logits and labels must have the same first dimension, got logits shape [3,2] and labels shape [1]</strong></p>

<p>If I force the value to ""1"" I have this new error :</p>

<p><strong>Input to reshape is a tensor with 4800 values, but the requested shape has 1600</strong></p>

<p>Maybe a problem with my features ?</p>

<hr>

<p><strong>EDIT2 :</strong> </p>

<p>the complete code is here : <a href=""https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e"" rel=""noreferrer"">https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e</a></p>

<hr>

<p><strong>EDIT3</strong></p>

<p>I updated the gist with </p>

<pre><code>logits = tf.layers.dense(inputs=dropout, units=1)
</code></pre>

<p><a href=""https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e"" rel=""noreferrer"">https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e</a></p>

<p>But I don't completely understand your answer about the batch size, how the batch size can be 3 here whereas I choose a batch size of 1 ? </p>

<p>If I choose a batch_size = 3 I have this error :
<strong>logits and labels must have the same first dimension, got logits shape [9,1] and labels shape [3]</strong></p>

<p>I tried to reshape labels : </p>

<pre><code>labels = tf.reshape(labels, [3, 1])
</code></pre>

<p>and I updated features and labels structure : </p>

<pre><code>    filenames_train = [['blackcorner-data/1.png', 'blackcorner-data/2.png', 'blackcorner-data/3.png',
                   'blackcorner-data/4.png', 'blackcorner-data/n1.png'],
                   ['blackcorner-data/n2.png',
                   'blackcorner-data/n3.png', 'blackcorner-data/n4.png',
                   'blackcorner-data/11.png', 'blackcorner-data/21.png'],
                   ['blackcorner-data/31.png',
                   'blackcorner-data/41.png', 'blackcorner-data/n11.png', 'blackcorner-data/n21.png',
                   'blackcorner-data/n31.png']
                   ]

labels = [[0, 0, 0, 0, 1], [1, 1, 1, 0, 0], [0, 0, 1, 1, 1]]
</code></pre>

<p>but without success...</p>
",4227291,633,07-03-2018 21:05,09-03-2018 21:18,2,643,14,2,7,,"{'badge_counts': {'bronze': 14, 'silver': 7, 'gold': 2}, 'account_id': 5296176, 'is_employee': False, 'last_modified_date': 1696578214, 'last_access_date': 1710335772, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 643, 'creation_date': 1415369571, 'user_type': 'registered', 'user_id': 4227291, 'location': 'Lille, France', 'website_url': 'https://geoffreypruvost.fr/', 'link': 'https://stackoverflow.com/users/4227291/geoffrey-pruvost', 'profile_image': 'https://i.stack.imgur.com/cKQ9l.png?s=256&g=1', 'display_name': 'Geoffrey Pruvost'}","I am new in tensoflow and I want to adapt the MNIST tutorial https://www.tensorflow.org/tutorials/layers with my own data (images of 40x40). This is my model function : I have a shape size error between labels and logits : InvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [3,2] and labels shape [1] filenames_array is an array of 16 string and labels_array is an array of 16 integer The main function is : I tried to reshape logits without success : logits = tf.reshape(logits, [1, 2]) I need your help, thanks EDIT After more time to search, in the first line of my model function the ""-1"" that signifies that the batch_size dimension will be dynamically calculated have here the value ""3"". The same ""3"" as in my error : logits and labels must have the same first dimension, got logits shape [3,2] and labels shape [1] If I force the value to ""1"" I have this new error : Input to reshape is a tensor with 4800 values, but the requested shape has 1600 Maybe a problem with my features ? EDIT2 : the complete code is here : https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e EDIT3 I updated the gist with https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e But I don't completely understand your answer about the batch size, how the batch size can be 3 here whereas I choose a batch size of 1 ? If I choose a batch_size = 3 I have this error : logits and labels must have the same first dimension, got logits shape [9,1] and labels shape [3] I tried to reshape labels : and I updated features and labels structure : but without success...","def cnn_model_fn(features, labels, mode):
        # Input Layer
        input_layer = tf.reshape(features, [-1, 40, 40, 1])

        # Convolutional Layer #1
        conv1 = tf.layers.conv2d(
                inputs=input_layer,
                filters=32,
                kernel_size=[5, 5],
                #  To specify that the output tensor should have the same width and height values as the input tensor
                # value can be ""same"" ou ""valid""
                padding=""same"",
                activation=tf.nn.relu)

        # Pooling Layer #1
        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

        # Convolutional Layer #2 and Pooling Layer #2
        conv2 = tf.layers.conv2d(
                inputs=pool1,
                filters=64,
                kernel_size=[5, 5],
                padding=""same"",
                activation=tf.nn.relu)
        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

        # Dense Layer
        pool2_flat = tf.reshape(pool2, [-1, 10 * 10 * 64])
        dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
        dropout = tf.layers.dropout(
                inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

        # Logits Layer
        logits = tf.layers.dense(inputs=dropout, units=2)

        predictions = {
            # Generate predictions (for PREDICT and EVAL mode)
            ""classes"":       tf.argmax(input=logits, axis=1),
            # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
            # `logging_hook`.
            ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
        }

        if mode == tf.estimator.ModeKeys.PREDICT:
            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

        # Calculate Loss (for both TRAIN and EVAL modes)
        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

        # Configure the Training Op (for TRAIN mode)
        if mode == tf.estimator.ModeKeys.TRAIN:
            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
            train_op = optimizer.minimize(
                    loss=loss,
                    global_step=tf.train.get_global_step())
            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

        # Add evaluation metrics (for EVAL mode)
        eval_metric_ops = {
            ""accuracy"": tf.metrics.accuracy(
                    labels=labels, predictions=predictions[""classes""])}
        return tf.estimator.EstimatorSpec(
                mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)
 [""file1.png"", ""file2.png"", ""file3.png"", ...]
 [0,0,1,1,0,1,0,0,0,...]
 # Create the Estimator
mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=""/tmp/test_convnet_model"")

# Train the model
cust_train_input_fn = lambda: train_input_fn_custom(
        filenames_array=filenames, labels_array=labels, batch_size=1)

mnist_classifier.train(
        input_fn=cust_train_input_fn,
        steps=20000,
        hooks=[logging_hook])
 input_layer = tf.reshape(features, [-1, 40, 40, 1])
 logits = tf.layers.dense(inputs=dropout, units=1)
 labels = tf.reshape(labels, [3, 1])
     filenames_train = [['blackcorner-data/1.png', 'blackcorner-data/2.png', 'blackcorner-data/3.png',
                   'blackcorner-data/4.png', 'blackcorner-data/n1.png'],
                   ['blackcorner-data/n2.png',
                   'blackcorner-data/n3.png', 'blackcorner-data/n4.png',
                   'blackcorner-data/11.png', 'blackcorner-data/21.png'],
                   ['blackcorner-data/31.png',
                   'blackcorner-data/41.png', 'blackcorner-data/n11.png', 'blackcorner-data/n21.png',
                   'blackcorner-data/n31.png']
                   ]

labels = [[0, 0, 0, 0, 1], [1, 1, 1, 0, 0], [0, 0, 1, 1, 1]]
",82,163,0,3,
495,49832981,49833102,5582,What's the right way to insert a CalibratedClassifierCV in a scikit-learn pipeline?,1,<python><pandas><scikit-learn>,13,"<p>I am trying to add a calibration step in a sklearn pipeline to obtain a calibrated classifier and thus <a href=""http://scikit-learn.org/stable/modules/calibration.html"" rel=""noreferrer"">have more trustworthy probabilities</a> in output.</p>

<p>So far I clumsily tried to insert a 'calibration' step using <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV"" rel=""noreferrer"">CalibratedClassifierCV</a> along the lines of (silly example for reproducibility):</p>

<pre><code>import sklearn.datasets
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.feature_extraction.text import TfidfVectorizer

data = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])
df = pd.DataFrame(data = np.c_[data['data'], data['target']])\
       .rename({0:'text', 1:'class'}, axis = 'columns')

my_pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer()),
    ('classifier', SGDClassifier(loss='modified_huber')),
    ('calibrator', CalibratedClassifierCV(cv=5, method='isotonic'))
])

my_pipeline.fit(df['text'].values, df['class'].values)
</code></pre>

<p>but that doesn't work (at least not in this way). Does anyone have tips about how to properly do this?</p>
",4240413,5565,14-04-2018 15:17,14-04-2018 15:31,0,5575,75,5,36,,"{'badge_counts': {'bronze': 75, 'silver': 36, 'gold': 5}, 'collectives': [{'collective': {'tags': ['twilio-php', 'sendgrid', 'sendgrid-rails', 'twilio-conference', 'twilio-javascript', 'twilio-programmable-voice', 'twilio-twiml', 'twilioflexwebchat', 'twilio-functions', 'twilio-api', 'twilio-cli', 'twilio-flex', 'twilio-python', 'sendgrid-ruby', 'segment-io', 'twilio-click-to-call', 'twilio-video', 'sendgrid-api-v2', 'twilio', 'twilio-programmable-chat', 'twilio-node', 'sendgrid-templates', 'twilio-taskrouter', 'twilio-amd', 'twimlet', 'sendgrid-api-v3', 'twilio-studio', 'twilio-conversations', 'authy'], 'external_links': [{'type': 'website', 'link': 'https://www.twilio.com/'}, {'type': 'support', 'link': 'mailto:stackoverflow@twilio.com'}, {'type': 'twitter', 'link': 'https://twitter.com/TwilioDevs'}, {'type': 'github', 'link': 'https://github.com/twilio'}, {'type': 'facebook', 'link': 'https://facebook.com/TeamTwilio'}, {'type': 'instagram', 'link': 'https://instagram.com/twilio'}], 'description': 'Twilio has democratized channels like voice, text, chat, video, and email by virtualizing the world’s communications infrastructure through APIs that are simple enough for any developer, yet robust enough to power the world’s most demanding applications.', 'link': '/collectives/twilio', 'name': 'Twilio', 'slug': 'twilio'}, 'role': 'member'}, {'collective': {'tags': ['azure-synapse-link', 'azure-management-groups', 'azure-deployment-slots', 'azure-data-lake-gen2', 'azure-ad-v2', 'azure-databoxfamily', 'azure-purview', 'azure-spring-boot', 'azure-table-storage', 'azure-blob-trigger', 'azure-application-registration', 'azure-calculator', 'azure-hub', 'azure-ad-b2c-custom-policy', 'azure-batch', 'azure-form-recognizer', 'azure-app-service-plans', 'azureshell', 'azure-notificationhub', 'azure-sql-database', 'azure-workflow-automation', 'azure-storage', 'azure-static-web-app', 'azure-nsg', 'azure-databricks', 'azure-app-api', 'azure-alerts', 'azure-virtual-machine', 'azure-cli', 'azure-git-deployment', 'azure-waf', 'azure-connect', 'azure-cosmosdb-cassandra-api', 'azure-database-mysql', 'azure-iot-suite', 'azure-ad-role', 'azure-dns', 'azure-emulator', 'azure-sas', 'azure-search-.net-sdk', 'azure-availability-zones', 'azure-bot-service', 'kql', 'azure-authentication', 'sql-azure-alerts', 'azure-service-fabric', 'azure-communication-services', 'azure-metrics-advisor', 'azure-iot-central', 'azure-cloud-services', 'azure-blob-storage', 'azure-regions', 'azure-store', 'azure-feature-manager', 'azure-cdn', 'azure-ad-graph-api', 'azure-industrial-iot', 'azure-ad-b2c', 'azure-synapse', 'azure-app-registration', 'azure-remote-rendering', 'azure-rtos', 'azure-storage-account', 'azure-iot-sdk', 'azure-ai-translator', 'azure-container-service', 'azure-text-translation', 'azure-fluent-api', 'azure-rest-api', 'azure-sphere', 'azure-gov', 'azure-hybrid-connections', 'azure-quantum', 'azure-digital-twins', 'azurekinect', 'azure-postgresql', 'azure-integration-account', 'microsoft-entra-private-access', 'azure-sql-server', 'azure-servicebus-topics', 'sql-server-azure', 'azure-log-analytics-workspace', 'azure-rbac', 'azure-policy', 'azure-mcd', 'azure-sdk-go', 'spark-bash-azure-databricks', 'azure-app-configuration', 'azure-iot-hub', 'azure-role-environment', 'azure-powershell', 'azure-identity', 'microsoft-entra-external-id', 'azure-deployment', 'azure-sdk-ruby', 'azure-free-services', 'azure-webjobs-triggered', 'azureadgraph-deprecation', 'azure-data-lake', 'azure-data-factory', 'azure-service-hooks', 'azure-image-builder', 'azure-backup-vault', 'azure-data-share', 'azure-data-catalog', 'azure-storage-files', 'azure-appservice', 'azure-files', 'azure-ad-domain-services', 'adal.js', 'azure-configuration', 'azure-qna-maker', 'spring-cloud-azure', 'azurerm-app-service', 'azure-sdk', 'azure-secrets', 'azure-affinity-group', 'kitchen-azurerm', 'azure-web-roles', 'azure-cli2', 'azure-log-analytics', 'azure-hdinsight', 'azure-application-gateway', 'azure-python-sdk', 'azure-sql-reporting', 'azure-disk', 'azure-devtest-labs', 'passport-azure-ad', 'azure-elasticpool', 'azure-function-app', 'azure-signalr', 'azure-webjobs-continuous', 'azure-compute-emulator', 'azure-speech', 'microsoft-entra-internet-access', 'azure-functions-runtime', 'azure-web-app-service', 'azure-front-door', 'azure-function-queue', 'azure-resource-group', 'azure-pack', 'azure-caching', 'azure-cosmosdb-mongoapi', 'azure-webjobs', 'azure-billing-api', 'azure-appfabric', 'azure-cloud-shell', 'azure-sdk-for-ruby', 'azure-virtual-network', 'azure-servicebus-subscriptions', 'azure-arc', 'azure-private-dns', 'azure-static-web-app-routing', 'azureservicebus', 'azure-defender', 'azure-data-studio', 'azure-iot-hub-device-management', 'azure-site-recovery', 'azure-application-settings', 'azure-cosmosdb-mongovcore', 'azure-security-center', 'azure-web-app-firewall', 'azure-queues', 'azure-functions', 'azure-management', 'azure-sdk-for-go', 'azure-blockchain-service', 'defaultazurecredential', 'azure-acs', 'azure-functions-core-tools', 'azure-servicebusrelay', 'pulumi-azure', 'azure-sql-edge', 'azure-subscription', 'azure-managed-database', 'azure-management-api', 'azure-bastion', 'azure-iot-dps', 'azure-vm-templates', 'azure-static-website-routing', 'azure-stack', 'azure-managed-app', 'azure-private-link', 'azure-ml-component', 'azure-function-async', 'azure-cosmosdb-tables', 'azure-timeseries-insights', 'azure-video-indexer', 'azure-ai', 'azure-elastic-scale', 'azure-clouddrive', 'azure-api-apps', 'azure-service-fabric-mesh', 'azure-functions-docker', 'microsoft-custom-vision', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-app-service-envrmnt', 'azure-sdk-js', 'azure-sdk-for-java', 'azure-advisor', 'azure-function-app-proxy', 'azure-functions-proxies', 'azure-sentinel', 'azure-anomaly-detector', 'azure-container-instances', 'azure-managed-disk', 'azure-active-directory', 'azureclicredential', 'azure-webapps', 'azure-ml-pipelines', 'azure-redis-cache', 'azure-http-trigger', 'azure-dsvm', 'azureportal', 'azure-servicebus-queues', 'azure-media-services', 'azure-ase', 'azure-node-sdk', 'azure-sql-managed-instance', 'sql-azure-federations', 'azure-debugger', 'azure-service-principal', 'azure-monitor-workbooks', 'azure-web-app-for-containers', 'azure-vm', 'azure-application-insights-profiler', 'azure-cost-calculation', 'azure-mobile-engagement', 'azure-file-copy', 'azure-diagnostics', 'azure-security', 'azure-analytics', 'azure-logic-app-standard', 'azure-vm-scale-set', 'azure-java-tools', 'azure-cognitive-services', 'django-pyodbc-azure', 'azure-application-proxy', 'azure-resource-graph', 'azure-ad-b2b', 'azure-compliance-policy', 'azure-durable-functions', 'azure-database-postgresql', 'azure-promptflow', 'azure-eventhub', 'azure-tablequery', 'azure-sdk-php', 'azure-storage-queues', 'azure-service-plan', 'azure-cosmosdb-emulator', 'azure-performancecounters', 'azure-scheduler', 'azure-availability-set', 'azure-dashboard', 'azure-mysql-database', 'azure-managed-grafana', 'azure-monitoring', 'azure-worker-roles', 'azure-service-runtime', 'azure-ddos', 'azure-data-sync', 'azure-machine-learning-service', 'azure-billing', 'azure-packaging', 'azure-container-apps', 'microsoft-entra-id', 'azure-sql', 'azure-bicep', 'azure-cosmosdb', 'azure-update-management-center', 'azure-mapping-data-flow', 'azure-lab-services', 'azure-custom-providers', 'azure-sdk-.net', 'azure-autoscaling-block', 'azure-data-explorer', 'azureml-python-sdk', 'azure-pipelines-release-pipeline', 'azure-load-balancer', 'azure-managed-identity', 'azure-ad-verifiable-credentials', 'azure-webjobssdk', 'azure-agent', 'azuremlsdk', 'azure-blueprints', 'azure-vpn', 'azure-automation', 'azure-blockchain-workbench', 'azure-api-management', 'azure-rm', 'azure-application-roles', 'azure-public-ip', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-sdk-python', 'terraform-provider-azure', 'azure-marketplace', 'azure-information-protection', 'azure-analysis-services', 'azure-zulu', 'azure-batch-account', 'azure-china', 'azure-android-sdk', 'azure-rm-template', 'azure-spring-cloud', 'azure-stream-analytics', 'azure-keyvault', 'azure-oms', 'azure-ad-msal', 'azure-webhooks', 'azure-adal-deprecation', 'azure-language-understanding', 'azure-container-registry', 'azure-web-pubsub', 'azure-maps', 'azure-migrate', 'azure-dev-spaces', 'fhir-server-for-azure', 'sitecore-azure', 'azure-cosmosdb-gremlinapi', 'azure-aks', 'azure.data.tables', 'azure-java-sdk', 'azure-static-website-hosting', 'azure-mobile-services', 'azure-triggers', 'azure-ad-powershell-v2', 'azure-adf', 'azure-in-role-cache', 'azure-iot-edge', 'azure-linux', 'azure-media-player', 'azure-storage-emulator', 'azure-eventgrid', 'azure-object-anchors', 'azure-management-portal', 'azure-notebooks', 'azure-custom-domain', 'azure-xplat-cli', 'azure-runbook', 'rebus-azureservicebus', 'azure-cognitive-search', 'azure-oauth', 'azure-application-insights', 'azure-traffic-manager', 'azure-anomaly-detection', 'azure-acr', 'adal', 'azure-storage-explorer', 'azure-private-dns-zone', 'azure', 'azure-auto-ml', 'azure-iot-hub-device-update', 'azure-logic-apps', 'azure-relay', 'azure-spatial-anchors', 'azure-monitor', 'azure-load-testing', 'azure-cosmosdb-changefeed', 'azure-function-http'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}], 'account_id': 5315302, 'is_employee': False, 'last_modified_date': 1707332700, 'last_access_date': 1711093245, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5575, 'creation_date': 1415722650, 'user_type': 'registered', 'user_id': 4240413, 'location': 'Verona, Italy', 'website_url': 'https://davidefiocco.github.io/', 'link': 'https://stackoverflow.com/users/4240413/davide-fiocco', 'profile_image': 'https://www.gravatar.com/avatar/c62c340b5358bd280ac34022453604d4?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Davide Fiocco'}",I am trying to add a calibration step in a sklearn pipeline to obtain a calibrated classifier and thus have more trustworthy probabilities in output. So far I clumsily tried to insert a 'calibration' step using CalibratedClassifierCV along the lines of (silly example for reproducibility): but that doesn't work (at least not in this way). Does anyone have tips about how to properly do this?,"import sklearn.datasets
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.feature_extraction.text import TfidfVectorizer

data = sklearn.datasets.fetch_20newsgroups(categories=['alt.atheism', 'sci.space'])
df = pd.DataFrame(data = np.c_[data['data'], data['target']])\
       .rename({0:'text', 1:'class'}, axis = 'columns')

my_pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer()),
    ('classifier', SGDClassifier(loss='modified_huber')),
    ('calibrator', CalibratedClassifierCV(cv=5, method='isotonic'))
])

my_pipeline.fit(df['text'].values, df['class'].values)
",17,25,0,2,
496,50240029,50240724,19171,nltk wordpunct_tokenize vs word_tokenize,1,<python><nltk>,24,"<p>Does anyone know the difference between <code>nltk</code>'s <code>wordpunct_tokenize</code> and <code>word_tokenize</code>? I'm using <code>nltk=3.2.4</code> and there's nothing on the doc string of <code>wordpunct_tokenize</code> that explains the difference. I couldn't find this info either in the documentation of <code>nltk</code> (perhaps I didn't search in the right place!). I would have expected that first one would get rid of punctuation tokens or the like, but it doesn't. </p>

<p><a href=""https://i.stack.imgur.com/yCbSq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yCbSq.png"" alt=""enter image description here""></a></p>
",4303016,4607,08-05-2018 18:25,08-05-2018 19:07,0,4627,35,2,36,0,"{'badge_counts': {'bronze': 35, 'silver': 36, 'gold': 2}, 'account_id': 5403977, 'is_employee': False, 'last_modified_date': 1700745301, 'last_access_date': 1683622042, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 4627, 'creation_date': 1417165142, 'user_type': 'registered', 'user_id': 4303016, 'accept_rate': 0, 'website_url': '', 'link': 'https://stackoverflow.com/users/4303016/tsando', 'profile_image': 'https://i.stack.imgur.com/JWaYc.jpg?s=256&g=1', 'display_name': 'tsando'}","Does anyone know the difference between 's and ? I'm using and there's nothing on the doc string of that explains the difference. I couldn't find this info either in the documentation of (perhaps I didn't search in the right place!). I would have expected that first one would get rid of punctuation tokens or the like, but it doesn't.",nltk wordpunct_tokenize word_tokenize nltk=3.2.4 wordpunct_tokenize nltk,-6,3,1,1,
497,49503195,49547891,11282,Reticulate - Running python chunks in Rmarkdown,3,<python><r><r-markdown><knitr><reticulate>,17,"<p>Maybe I'm missing something, but if the following code is the content of my Rmd file</p>

<pre><code>```{r}
library(reticulate)
use_virtualenv(""r-reticulate"")
py_available(TRUE)
```
```{python}
a = 7
print(a)
```
```{r}
py$a
```
</code></pre>

<p>when I Knit the file, the output for the last chunk is 7 (as expected). On the other hand, clicking the run all button in Rstudio (or running chunks one by one), results on <code>NULL</code> for the last chunk. </p>

<p>Comparing with the <a href=""https://rstudio.github.io/reticulate/articles/rstudio_ide.html"" rel=""noreferrer"">R notebook example</a> it seems like assigning something to <code>flights</code> in the python chunk should make <code>py$flights</code> available for R, but that doesn't seem the case.</p>

<p>Questions: </p>

<ol>
<li>Is there a way to access from R a variable created in a Python chunk previously ran (not knit)? How to ""export"" to R a variable created within a python chunk? </li>
<li>What is a good reference to understand what happens when I click the run button in a python chunk of a Rmarkdown file?</li>
</ol>

<p><strong>EDIT</strong>: Ok so after seeing the first answers here, I did update both knitr and rmarkdown to the latest version, but still had the same problem.
I added <code>py_available(TRUE)</code> to my file to make sure it was initialized, still, last chunk results in <code>7</code> when knitted, but running chunks one-by-one results in </p>

<pre><code>&gt; py$a
Error in py_get_attr_impl(x, name, silent) : 
  AttributeError: 'module' object has no attribute 'a'
</code></pre>

<p>The problem is: Assigning a value to <code>a</code> in the python chunk isn't doing anything to <code>py$a</code> in the R environment. Maybe this ""shared"" environment between R and python isn't how the package is supposed to work? Also, for some extra information</p>

<pre><code>&gt; py_config()
python:         /usr/bin/python
libpython:      /usr/lib/python2.7/config-x86_64-linux-gnu/libpython2.7.so
pythonhome:     /usr:/usr
version:        2.7.14 (default, Sep 23 2017, 22:06:14)  [GCC 7.2.0]
numpy:          /usr/lib/python2.7/dist-packages/numpy
numpy_version:  1.12.1

python versions found: 
 /usr/bin/python
 /usr/bin/python3
</code></pre>
",4309038,2239,27-03-2018 01:48,29-03-2018 04:09,2,2239,37,4,15,62,"{'badge_counts': {'bronze': 37, 'silver': 15, 'gold': 4}, 'account_id': 5412900, 'is_employee': False, 'last_modified_date': 1670499179, 'last_access_date': 1711106105, 'reputation_change_year': 75, 'reputation_change_quarter': 75, 'reputation_change_month': 45, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2239, 'creation_date': 1417363892, 'user_type': 'registered', 'user_id': 4309038, 'accept_rate': 62, 'location': 'Brazil', 'link': 'https://stackoverflow.com/users/4309038/vfreguglia', 'profile_image': 'https://i.stack.imgur.com/eb8G6.jpg?s=256&g=1', 'display_name': 'VFreguglia'}","Maybe I'm missing something, but if the following code is the content of my Rmd file when I Knit the file, the output for the last chunk is 7 (as expected). On the other hand, clicking the run all button in Rstudio (or running chunks one by one), results on for the last chunk. Comparing with the R notebook example it seems like assigning something to in the python chunk should make available for R, but that doesn't seem the case. Questions: Is there a way to access from R a variable created in a Python chunk previously ran (not knit)? How to ""export"" to R a variable created within a python chunk? What is a good reference to understand what happens when I click the run button in a python chunk of a Rmarkdown file? EDIT: Ok so after seeing the first answers here, I did update both knitr and rmarkdown to the latest version, but still had the same problem. I added to my file to make sure it was initialized, still, last chunk results in when knitted, but running chunks one-by-one results in The problem is: Assigning a value to in the python chunk isn't doing anything to in the R environment. Maybe this ""shared"" environment between R and python isn't how the package is supposed to work? Also, for some extra information","```{r}
library(reticulate)
use_virtualenv(""r-reticulate"")
py_available(TRUE)
```
```{python}
a = 7
print(a)
```
```{r}
py$a
```
 NULL flights py$flights py_available(TRUE) 7 &gt; py$a
Error in py_get_attr_impl(x, name, silent) : 
  AttributeError: 'module' object has no attribute 'a'
 a py$a &gt; py_config()
python:         /usr/bin/python
libpython:      /usr/lib/python2.7/config-x86_64-linux-gnu/libpython2.7.so
pythonhome:     /usr:/usr
version:        2.7.14 (default, Sep 23 2017, 22:06:14)  [GCC 7.2.0]
numpy:          /usr/lib/python2.7/dist-packages/numpy
numpy_version:  1.12.1

python versions found: 
 /usr/bin/python
 /usr/bin/python3
",16,49,0,1,
498,48282599,48282760,13157,Getting an Assertion error in my django application(1.8.4),3,<python><django-models><django-rest-framework>,13,"<pre><code>from rest_framework import serializers
from .models import Stock


class StockSerializer(serializers.ModelSerializer):

    class Meta:

        model = Stock

        #field = ('ticker','volume')
        field = '__all__'
</code></pre>
<p>I am getting an exception value on running application : <code>(&quot;Creating a ModelSerializer without either the 'fields' attribute or the 'exclude' attribute has been deprecated since 3.3.0, and is now disallowed. Add an explicit fields = '__all__' to the StockSerializer serializer.&quot;,) </code></p>
",5974810,641,16-01-2018 13:39,16-01-2018 13:48,0,641,16,2,6,,"{'badge_counts': {'bronze': 16, 'silver': 6, 'gold': 2}, 'account_id': 7910656, 'is_employee': False, 'last_modified_date': 1607614502, 'last_access_date': 1668425111, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 641, 'creation_date': 1456320993, 'user_type': 'registered', 'user_id': 5974810, 'location': 'Pune, Maharashtra, India', 'website_url': 'https://www.linkedin.com/in/nirdesh-kumar-choudhary-68115761/', 'link': 'https://stackoverflow.com/users/5974810/nirdesh-kumar-choudhary', 'profile_image': 'https://www.gravatar.com/avatar/bccd837d860c911c6494fe1e8401fa27?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nirdesh Kumar Choudhary'}",I am getting an exception value on running application :,"from rest_framework import serializers
from .models import Stock


class StockSerializer(serializers.ModelSerializer):

    class Meta:

        model = Stock

        #field = ('ticker','volume')
        field = '__all__'
 (&quot;Creating a ModelSerializer without either the 'fields' attribute or the 'exclude' attribute has been deprecated since 3.3.0, and is now disallowed. Add an explicit fields = '__all__' to the StockSerializer serializer.&quot;,) ",10,14,0,0,
499,49427151,49427442,1647,How does Python determine if two strings are identical,2,<python><string><memory>,15,"<p>I've tried to understand when Python strings are identical (aka sharing the same memory location). However during my tests, there seems to be no obvious explanation when two string variables that are equal share the same memory:</p>

<pre><code>import sys
print(sys.version) # 3.4.3

# Example 1
s1 = ""Hello""
s2 = ""Hello""
print(id(s1) == id(s2)) # True

# Example 2
s1 = ""Hello"" * 3
s2 = ""Hello"" * 3
print(id(s1) == id(s2)) # True

# Example 3
i = 3
s1 = ""Hello"" * i
s2 = ""Hello"" * i
print(id(s1) == id(s2)) # False

# Example 4
s1 = ""HelloHelloHelloHelloHello""
s2 = ""HelloHelloHelloHelloHello""
print(id(s1) == id(s2)) # True

# Example 5
s1 = ""Hello"" * 5
s2 = ""Hello"" * 5
print(id(s1) == id(s2)) # False
</code></pre>

<p>Strings are immutable, and as far as I know Python tries to re-use existing immutable objects, by having other variables point to them instead of creating new objects in memory with the same value.</p>

<p>With this in mind, it seems obvious that <code>Example 1</code> returns <code>True</code>.<br>
It's still obvious (to me) that <code>Example 2</code> returns <code>True</code>.  </p>

<p>It's not obvious to me, that <code>Example 3</code> returns <code>False</code> - am I not doing the same thing as in <code>Example 2</code>?!?</p>

<p>I stumbled upon this SO question:<br>
<a href=""https://stackoverflow.com/questions/1504717/why-does-comparing-strings-in-python-using-either-or-is-sometimes-produce/1504742#1504742"">Why does comparing strings in Python using either &#39;==&#39; or &#39;is&#39; sometimes produce a different result?</a>  </p>

<p>and read through <a href=""http://guilload.com/python-string-interning/"" rel=""noreferrer"">http://guilload.com/python-string-interning/</a> (though I probably didn't understand it all) and thougt - okay, maybe ""interned"" strings depend on the length, so I used <code>HelloHelloHelloHelloHello</code> in <code>Example 4</code>. The result was <code>True</code>. </p>

<p>And what the puzzled me, was doing the same as in <code>Example 2</code> just with a bigger number (but it would effectively return the same string as <code>Example 4</code>) - however this time the result was <code>False</code>?!?</p>

<p>I have really no idea how Python decides whether or not to use the same memory object, or when to create a new one.</p>

<p>Are the any official sources that can explain this behavior?</p>
",4349415,10622,22-03-2018 11:10,22-03-2018 11:25,0,10622,51,5,39,100,"{'badge_counts': {'bronze': 51, 'silver': 39, 'gold': 5}, 'account_id': 5469714, 'is_employee': False, 'last_modified_date': 1707346502, 'last_access_date': 1711144305, 'reputation_change_year': 23, 'reputation_change_quarter': 23, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 10622, 'creation_date': 1418291513, 'user_type': 'registered', 'user_id': 4349415, 'accept_rate': 100, 'location': 'Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/4349415/mike-scotty', 'profile_image': 'https://i.stack.imgur.com/ta4zl.png?s=256&g=1', 'display_name': 'Mike Scotty'}","I've tried to understand when Python strings are identical (aka sharing the same memory location). However during my tests, there seems to be no obvious explanation when two string variables that are equal share the same memory: Strings are immutable, and as far as I know Python tries to re-use existing immutable objects, by having other variables point to them instead of creating new objects in memory with the same value. With this in mind, it seems obvious that returns . It's still obvious (to me) that returns . It's not obvious to me, that returns - am I not doing the same thing as in ?!? I stumbled upon this SO question: Why does comparing strings in Python using either &#39;==&#39; or &#39;is&#39; sometimes produce a different result? and read through http://guilload.com/python-string-interning/ (though I probably didn't understand it all) and thougt - okay, maybe ""interned"" strings depend on the length, so I used in . The result was . And what the puzzled me, was doing the same as in just with a bigger number (but it would effectively return the same string as ) - however this time the result was ?!? I have really no idea how Python decides whether or not to use the same memory object, or when to create a new one. Are the any official sources that can explain this behavior?","import sys
print(sys.version) # 3.4.3

# Example 1
s1 = ""Hello""
s2 = ""Hello""
print(id(s1) == id(s2)) # True

# Example 2
s1 = ""Hello"" * 3
s2 = ""Hello"" * 3
print(id(s1) == id(s2)) # True

# Example 3
i = 3
s1 = ""Hello"" * i
s2 = ""Hello"" * i
print(id(s1) == id(s2)) # False

# Example 4
s1 = ""HelloHelloHelloHelloHello""
s2 = ""HelloHelloHelloHelloHello""
print(id(s1) == id(s2)) # True

# Example 5
s1 = ""Hello"" * 5
s2 = ""Hello"" * 5
print(id(s1) == id(s2)) # False
 Example 1 True Example 2 True Example 3 False Example 2 HelloHelloHelloHelloHello Example 4 True Example 2 Example 4 False",14,49,0,2,
500,48437189,49281428,159132,Python enumerate() tqdm progress-bar when reading a file?,5,<python><progress-bar><enumerate><tqdm>,91,"<p>I can't see the tqdm progress bar when I use this code to iterate my opened file:</p>
<pre><code>with open(file_path, 'r') as f:
    for i, line in enumerate(tqdm(f)):
        print(&quot;line #: %s&quot; % i)
        for j in tqdm(range(line_size)):
            ...
</code></pre>
<p>What's the right way to use tqdm here?</p>
",5301692,1123,25-01-2018 06:54,14-03-2018 15:15,48,1143,14,1,9,100,"{'badge_counts': {'bronze': 14, 'silver': 9, 'gold': 1}, 'account_id': 6902543, 'is_employee': False, 'last_modified_date': 1666347001, 'last_access_date': 1689773847, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1143, 'creation_date': 1441387635, 'user_type': 'registered', 'user_id': 5301692, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/5301692/wei-wu', 'profile_image': 'https://www.gravatar.com/avatar/e3eae77594ded1405b64ce7f36d3749a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Wei Wu'}",I can't see the tqdm progress bar when I use this code to iterate my opened file: What's the right way to use tqdm here?,"with open(file_path, 'r') as f:
    for i, line in enumerate(tqdm(f)):
        print(&quot;line #: %s&quot; % i)
        for j in tqdm(range(line_size)):
            ...
",4,8,0,0,
501,48739752,48740115,48598,How to pass schema to create a new Dataframe from existing Dataframe?,2,<python><python-3.x><apache-spark><pyspark>,15,"<p>To pass schema to a json file we do this:                  </p>

<pre><code>from pyspark.sql.types import (StructField, StringType, StructType, IntegerType)
data_schema = [StructField('age', IntegerType(), True), StructField('name', StringType(), True)]
final_struc = StructType(fields = data_schema)
df =spark.read.json('people.json', schema=final_struc)
</code></pre>

<p>The above code works as expected. However now, I have data in table which I display by:                 </p>

<pre><code>df = sqlContext.sql(""SELECT * FROM people_json"")               
</code></pre>

<p>But if I try to pass a new schema to it by using following command it does not work.                           </p>

<pre><code>df2 = spark.sql(""SELECT * FROM people_json"", schema=final_struc)
</code></pre>

<p>It gives the following error:                  </p>

<blockquote>
  <p>sql() got an unexpected keyword argument 'schema'           </p>
</blockquote>

<p>NOTE: I am using <a href=""https://community.cloud.databricks.com/"" rel=""noreferrer"">Databrics Community Edition</a></p>

<ul>
<li>What am I missing?</li>
<li>How do I pass the new schema if I have data in the table instead of some JSON file? </li>
</ul>
",5349542,10386,12-02-2018 04:53,12-02-2018 05:36,0,10384,63,7,54,83,"{'badge_counts': {'bronze': 63, 'silver': 54, 'gold': 7}, 'account_id': 6974321, 'is_employee': False, 'last_modified_date': 1703338200, 'last_access_date': 1710969049, 'reputation_change_year': 78, 'reputation_change_quarter': 78, 'reputation_change_month': 8, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 10384, 'creation_date': 1442562561, 'user_type': 'registered', 'user_id': 5349542, 'accept_rate': 83, 'location': '3rd planet from Sun', 'website_url': '', 'link': 'https://stackoverflow.com/users/5349542/blackbeard', 'profile_image': 'https://i.stack.imgur.com/B4LnC.jpg?s=256&g=1', 'display_name': 'BlackBeard'}","To pass schema to a json file we do this: The above code works as expected. However now, I have data in table which I display by: But if I try to pass a new schema to it by using following command it does not work. It gives the following error: sql() got an unexpected keyword argument 'schema' NOTE: I am using Databrics Community Edition What am I missing? How do I pass the new schema if I have data in the table instead of some JSON file?","from pyspark.sql.types import (StructField, StringType, StructType, IntegerType)
data_schema = [StructField('age', IntegerType(), True), StructField('name', StringType(), True)]
final_struc = StructType(fields = data_schema)
df =spark.read.json('people.json', schema=final_struc)
 df = sqlContext.sql(""SELECT * FROM people_json"")               
 df2 = spark.sql(""SELECT * FROM people_json"", schema=final_struc)
",3,30,0,1,
502,49157077,49157418,16390,Pandas. How to read Excel file from ZIP archive,3,<python><pandas><zip>,12,"<p>I have .zip archive with filename.xlsx inside it and I want to parse Excel sheet line by line.</p>

<p>How to proper pass filename into pandas.read_excel in this case?</p>

<p>I tried:</p>

<pre><code>import zipfile
import pandas
myzip=zipfile.ZipFile(filename.zip)
for fname in myzip.namelist():
    with myzip.open(fname) as from_archive:
        with pandas.read_excel(from_archive) as fin:
            for line in fin:
            ....
</code></pre>

<p>but it doesn't seem to work, and the result was:</p>

<pre><code>AttributeError: __exit__
</code></pre>
",5353240,129,07-03-2018 16:42,07-03-2018 16:59,0,129,7,1,1,,"{'badge_counts': {'bronze': 7, 'silver': 1, 'gold': 1}, 'account_id': 6979906, 'is_employee': False, 'last_modified_date': 1594870500, 'last_access_date': 1692244815, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 129, 'creation_date': 1442654649, 'user_type': 'registered', 'user_id': 5353240, 'location': 'Moscow, Russia', 'website_url': '', 'link': 'https://stackoverflow.com/users/5353240/ivan-vodopyanov', 'profile_image': 'https://www.gravatar.com/avatar/e79c56f62011754a3dc6710d93436d8c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ivan Vodopyanov'}","I have .zip archive with filename.xlsx inside it and I want to parse Excel sheet line by line. How to proper pass filename into pandas.read_excel in this case? I tried: but it doesn't seem to work, and the result was:","import zipfile
import pandas
myzip=zipfile.ZipFile(filename.zip)
for fname in myzip.namelist():
    with myzip.open(fname) as from_archive:
        with pandas.read_excel(from_archive) as fin:
            for line in fin:
            ....
 AttributeError: __exit__
",7,20,0,0,
503,48948308,48948461,13395,Can't use /= on numpy array,2,<python><arrays><numpy><array-broadcasting>,32,"<p>On a <code>numpy</code> array, why is it I can successfully use <code>/ 2</code>:</p>

<pre><code>&gt;&gt;&gt; a=np.array([2, 4, 6])
&gt;&gt;&gt; a = a / 2
&gt;&gt;&gt; a
array([ 1.,  2.,  3.])
</code></pre>

<p>But I cannot use <code>a /= 2</code>?</p>

<pre><code>&gt;&gt;&gt; a=np.array([2, 4, 6])
&gt;&gt;&gt; a /= 2
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: No loop matching the specified signature and casting
was found for ufunc true_divide
</code></pre>

<p>I've seen <a href=""https://github.com/numpy/numpy/issues/6464"" rel=""noreferrer"">numpy Issue 6464</a>, but don't understand from reading it and the linked release notes the reason this doesn't work.</p>

<p>Is there any way to get <code>/=</code> to work as expected?</p>
",5353461,43435,23-02-2018 12:48,23-02-2018 12:58,0,43675,252,37,194,51,"{'badge_counts': {'bronze': 252, 'silver': 194, 'gold': 37}, 'collectives': [{'collective': {'tags': ['continuous-integration', 'gitlab-ci-runner', 'github-actions', 'jenkins-groovy', 'continuous-delivery', 'jenkins', 'google-cloud-build', 'octopus-deploy', 'jenkins-plugins', 'argocd', 'teamcity', 'circleci', 'continuous-deployment', 'bitbucket-pipelines', 'tfsbuild', 'codemagic', 'hudson', 'cicd', 'azure-pipelines', 'continuous-testing', 'jenkins-pipeline', 'gitlab-ci'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where developers focused on continuous integration, delivery, and deployment can find, share, and learn about simultaneous development.', 'link': '/collectives/ci-cd', 'name': 'CI/CD', 'slug': 'ci-cd'}, 'role': 'member'}], 'account_id': 4996579, 'is_employee': False, 'last_modified_date': 1706459400, 'last_access_date': 1710991690, 'reputation_change_year': 1136, 'reputation_change_quarter': 1136, 'reputation_change_month': 400, 'reputation_change_week': 100, 'reputation_change_day': 10, 'reputation': 43675, 'creation_date': 1442662310, 'user_type': 'registered', 'user_id': 5353461, 'accept_rate': 51, 'location': 'Travelling the world, last seen in Thailand.', 'link': 'https://stackoverflow.com/users/5353461/tom-hale', 'profile_image': 'https://lh4.googleusercontent.com/-kpRTzH5zyzs/AAAAAAAAAAI/AAAAAAAAAAA/ICc26Nac6mY/photo.jpg?sz=256', 'display_name': 'Tom Hale'}","On a array, why is it I can successfully use : But I cannot use ? I've seen numpy Issue 6464, but don't understand from reading it and the linked release notes the reason this doesn't work. Is there any way to get to work as expected?","numpy / 2 &gt;&gt;&gt; a=np.array([2, 4, 6])
&gt;&gt;&gt; a = a / 2
&gt;&gt;&gt; a
array([ 1.,  2.,  3.])
 a /= 2 &gt;&gt;&gt; a=np.array([2, 4, 6])
&gt;&gt;&gt; a /= 2
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: No loop matching the specified signature and casting
was found for ufunc true_divide
 /=",4,21,0,1,
504,49164935,49215868,32543,How to use .loc with groupby and two conditions in pandas,4,<python><pandas>,11,"<p>I asked a similar question <a href=""https://stackoverflow.com/questions/49144154/how-to-use-loc-with-multiple-conditions-and-groupby/49144209#49144209"">here</a>, but I want to expand on this question because I'm asked to do something a little different where I cannot use .duplicates()</p>

<p>I have a df that's grouped by 'Key'. I want to flag any row within a group where the discharge date matches the admit date AND between those rows, the row with the discharge date has a num1 value in the range of 5-12. </p>

<pre><code>df =  pd.DataFrame({'Key': ['10003', '10003', '10003', '10003', '10003','10003','10034', '10034'], 
   'Num1': [12,13,13,13,12,13,15,12],
   'Num2': [121,122,122,124,125,126,127,128],
  'admit': [20120506, 20120508, 20121010,20121010,20121010,20121110,20120520,20120520],  'discharge': [20120508, 20120510, 20121012,20121016,20121023,20121111,20120520,20120520]})
df['admit'] = pd.to_datetime(df['admit'], format='%Y%m%d')
df['discharge'] = pd.to_datetime(df['discharge'], format='%Y%m%d')
</code></pre>

<p>initial df</p>

<pre><code>    Key     Num1    Num2    admit       discharge
0   10003   12      121     2012-05-06  2012-05-08
1   10003   13      122     2012-05-08  2012-05-10
2   10003   13      122     2012-10-10  2012-10-12
3   10003   13      124     2012-10-10  2012-10-16
4   10003   12      125     2012-10-10  2012-10-23
5   10003   13      126     2012-11-10  2012-11-11
6   10034   15      127     2012-05-20  2012-05-20
7   10034   12      128     2012-05-20  2012-05-20
</code></pre>

<p>final df</p>

<pre><code>    Key     Num1    Num2    admit       discharge   flag
0   10003   12      121     2012-05-06  2012-05-08  1
1   10003   13      122     2012-05-08  2012-05-10  1
2   10003   13      122     2012-10-10  2012-10-12  0
3   10003   13      124     2012-10-10  2012-10-16  0
4   10003   12      125     2012-10-10  2012-10-23  0
5   10003   13      126     2012-11-10  2012-11-11  0
6   10034   15      127     2012-05-20  2012-05-20  1
7   10034   12      128     2012-05-20  2012-05-20  1
</code></pre>

<p>I was trying to use filter() but I can't quite figure out how to apply any() to the discharge date. My logic was to pick the first admit date in a group and then check that date among each discharge date and once there is a match then check if the row that has the same discharge date has a value in Num1 in the range of 5-12. </p>

<pre><code>num1_range = [5,6,7,8,9,10,11,12]
df.loc[df.groupby(['Key']).filter(lambda x : (x['admit'] == x['discharge'].any())&amp;(x['Num1'].isin(num1_range).any())),'flag']=1
</code></pre>

<p>I'm getting an error</p>

<pre><code>ValueError: cannot set a Timestamp with a non-timestamp
</code></pre>
",6205382,2199,08-03-2018 03:32,11-03-2018 01:53,3,2199,52,2,30,,"{'badge_counts': {'bronze': 52, 'silver': 30, 'gold': 2}, 'account_id': 8250583, 'is_employee': False, 'last_modified_date': 1671422617, 'last_access_date': 1711128692, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2199, 'creation_date': 1460653795, 'user_type': 'registered', 'user_id': 6205382, 'website_url': '', 'link': 'https://stackoverflow.com/users/6205382/candlewax', 'profile_image': 'https://www.gravatar.com/avatar/a919cb162a5ef02154de995db426dabf?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'CandleWax'}","I asked a similar question here, but I want to expand on this question because I'm asked to do something a little different where I cannot use .duplicates() I have a df that's grouped by 'Key'. I want to flag any row within a group where the discharge date matches the admit date AND between those rows, the row with the discharge date has a num1 value in the range of 5-12. initial df final df I was trying to use filter() but I can't quite figure out how to apply any() to the discharge date. My logic was to pick the first admit date in a group and then check that date among each discharge date and once there is a match then check if the row that has the same discharge date has a value in Num1 in the range of 5-12. I'm getting an error","df =  pd.DataFrame({'Key': ['10003', '10003', '10003', '10003', '10003','10003','10034', '10034'], 
   'Num1': [12,13,13,13,12,13,15,12],
   'Num2': [121,122,122,124,125,126,127,128],
  'admit': [20120506, 20120508, 20121010,20121010,20121010,20121110,20120520,20120520],  'discharge': [20120508, 20120510, 20121012,20121016,20121023,20121111,20120520,20120520]})
df['admit'] = pd.to_datetime(df['admit'], format='%Y%m%d')
df['discharge'] = pd.to_datetime(df['discharge'], format='%Y%m%d')
     Key     Num1    Num2    admit       discharge
0   10003   12      121     2012-05-06  2012-05-08
1   10003   13      122     2012-05-08  2012-05-10
2   10003   13      122     2012-10-10  2012-10-12
3   10003   13      124     2012-10-10  2012-10-16
4   10003   12      125     2012-10-10  2012-10-23
5   10003   13      126     2012-11-10  2012-11-11
6   10034   15      127     2012-05-20  2012-05-20
7   10034   12      128     2012-05-20  2012-05-20
     Key     Num1    Num2    admit       discharge   flag
0   10003   12      121     2012-05-06  2012-05-08  1
1   10003   13      122     2012-05-08  2012-05-10  1
2   10003   13      122     2012-10-10  2012-10-12  0
3   10003   13      124     2012-10-10  2012-10-16  0
4   10003   12      125     2012-10-10  2012-10-23  0
5   10003   13      126     2012-11-10  2012-11-11  0
6   10034   15      127     2012-05-20  2012-05-20  1
7   10034   12      128     2012-05-20  2012-05-20  1
 num1_range = [5,6,7,8,9,10,11,12]
df.loc[df.groupby(['Key']).filter(lambda x : (x['admit'] == x['discharge'].any())&amp;(x['Num1'].isin(num1_range).any())),'flag']=1
 ValueError: cannot set a Timestamp with a non-timestamp
",22,48,0,1,
505,49840100,49848103,8650,tf.data.Dataset.padded_batch pad differently each feature,1,<python><tensorflow><tensorflow-datasets>,13,"<p>I have a <code>tf.data.Dataset</code> instance which holds 3 different features</p>

<ul>
<li><code>label</code> which is a scalar</li>
<li><code>sequence_feature</code> which is a sequence of scalars</li>
<li><code>seq_of_seqs_feature</code> which is a sequence of sequences feature</li>
</ul>

<p>I am trying to use <code>tf.data.Dataset.padded_batch()</code> to genereate padded data as input to my model - and I want to pad every feature differently.</p>

<p>Example batch:</p>

<pre><code>[{'label': 24,
  'sequence_feature': [1, 2],
  'seq_of_seqs_feature': [[11.1, 22.2],
                          [33.3, 44.4]]},
 {'label': 32,
  'sequence_feature': [3, 4, 5],
  'seq_of_seqs_feature': [[55.55, 66.66]]}]
</code></pre>

<p>Expected output:</p>

<pre><code>[{'label': 24,
  'sequence_feature': [1, 2, 0],
  'seq_of_seqs_feature': [[11.1, 22.2],
                          [33.3, 44.4]]},
 {'label': 32,
  'sequence_feature': [3, 4, 5],
  'seq_of_seqs_feature': [[55.55, 66.66],
                           0.0, 0.0    ]}]
</code></pre>

<p>As you can see the <code>label</code> feature should not be padded, and the <code>sequence_feature</code> and <code>seq_of_seqs_feature</code> should be padded by the corresponding longest entry in the given batch.</p>
",5368083,12145,15-04-2018 08:44,16-04-2018 00:25,1,12155,117,9,76,71,"{'badge_counts': {'bronze': 117, 'silver': 76, 'gold': 9}, 'collectives': [{'collective': {'tags': ['sentiment-analysis', 'topic-modeling', 'opennlp', 'named-entity-recognition', 'word-embedding', 'nlp', 'tf-idf', 'gensim', 'spacy', 'word2vec', 'bert-language-model', 'nlp-question-answering', 'nltk', 'huggingface-transformers', 'stanford-nlp', 'spacy-3'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective focused on NLP (natural language processing), the transformation or extraction of useful information from natural language data.', 'link': '/collectives/nlp', 'name': 'NLP', 'slug': 'nlp'}, 'role': 'member'}, {'collective': {'tags': ['azure-synapse-link', 'azure-management-groups', 'azure-deployment-slots', 'azure-data-lake-gen2', 'azure-ad-v2', 'azure-databoxfamily', 'azure-purview', 'azure-spring-boot', 'azure-table-storage', 'azure-blob-trigger', 'azure-application-registration', 'azure-calculator', 'azure-hub', 'azure-ad-b2c-custom-policy', 'azure-batch', 'azure-form-recognizer', 'azure-app-service-plans', 'azureshell', 'azure-notificationhub', 'azure-sql-database', 'azure-workflow-automation', 'azure-storage', 'azure-static-web-app', 'azure-nsg', 'azure-databricks', 'azure-app-api', 'azure-alerts', 'azure-virtual-machine', 'azure-cli', 'azure-git-deployment', 'azure-waf', 'azure-connect', 'azure-cosmosdb-cassandra-api', 'azure-database-mysql', 'azure-iot-suite', 'azure-ad-role', 'azure-dns', 'azure-emulator', 'azure-sas', 'azure-search-.net-sdk', 'azure-availability-zones', 'azure-bot-service', 'kql', 'azure-authentication', 'sql-azure-alerts', 'azure-service-fabric', 'azure-communication-services', 'azure-metrics-advisor', 'azure-iot-central', 'azure-cloud-services', 'azure-blob-storage', 'azure-regions', 'azure-store', 'azure-feature-manager', 'azure-cdn', 'azure-ad-graph-api', 'azure-industrial-iot', 'azure-ad-b2c', 'azure-synapse', 'azure-app-registration', 'azure-remote-rendering', 'azure-rtos', 'azure-storage-account', 'azure-iot-sdk', 'azure-ai-translator', 'azure-container-service', 'azure-text-translation', 'azure-fluent-api', 'azure-rest-api', 'azure-sphere', 'azure-gov', 'azure-hybrid-connections', 'azure-quantum', 'azure-digital-twins', 'azurekinect', 'azure-postgresql', 'azure-integration-account', 'microsoft-entra-private-access', 'azure-sql-server', 'azure-servicebus-topics', 'sql-server-azure', 'azure-log-analytics-workspace', 'azure-rbac', 'azure-policy', 'azure-mcd', 'azure-sdk-go', 'spark-bash-azure-databricks', 'azure-app-configuration', 'azure-iot-hub', 'azure-role-environment', 'azure-powershell', 'azure-identity', 'microsoft-entra-external-id', 'azure-deployment', 'azure-sdk-ruby', 'azure-free-services', 'azure-webjobs-triggered', 'azureadgraph-deprecation', 'azure-data-lake', 'azure-data-factory', 'azure-service-hooks', 'azure-image-builder', 'azure-backup-vault', 'azure-data-share', 'azure-data-catalog', 'azure-storage-files', 'azure-appservice', 'azure-files', 'azure-ad-domain-services', 'adal.js', 'azure-configuration', 'azure-qna-maker', 'spring-cloud-azure', 'azurerm-app-service', 'azure-sdk', 'azure-secrets', 'azure-affinity-group', 'kitchen-azurerm', 'azure-web-roles', 'azure-cli2', 'azure-log-analytics', 'azure-hdinsight', 'azure-application-gateway', 'azure-python-sdk', 'azure-sql-reporting', 'azure-disk', 'azure-devtest-labs', 'passport-azure-ad', 'azure-elasticpool', 'azure-function-app', 'azure-signalr', 'azure-webjobs-continuous', 'azure-compute-emulator', 'azure-speech', 'microsoft-entra-internet-access', 'azure-functions-runtime', 'azure-web-app-service', 'azure-front-door', 'azure-function-queue', 'azure-resource-group', 'azure-pack', 'azure-caching', 'azure-cosmosdb-mongoapi', 'azure-webjobs', 'azure-billing-api', 'azure-appfabric', 'azure-cloud-shell', 'azure-sdk-for-ruby', 'azure-virtual-network', 'azure-servicebus-subscriptions', 'azure-arc', 'azure-private-dns', 'azure-static-web-app-routing', 'azureservicebus', 'azure-defender', 'azure-data-studio', 'azure-iot-hub-device-management', 'azure-site-recovery', 'azure-application-settings', 'azure-cosmosdb-mongovcore', 'azure-security-center', 'azure-web-app-firewall', 'azure-queues', 'azure-functions', 'azure-management', 'azure-sdk-for-go', 'azure-blockchain-service', 'defaultazurecredential', 'azure-acs', 'azure-functions-core-tools', 'azure-servicebusrelay', 'pulumi-azure', 'azure-sql-edge', 'azure-subscription', 'azure-managed-database', 'azure-management-api', 'azure-bastion', 'azure-iot-dps', 'azure-vm-templates', 'azure-static-website-routing', 'azure-stack', 'azure-managed-app', 'azure-private-link', 'azure-ml-component', 'azure-function-async', 'azure-cosmosdb-tables', 'azure-timeseries-insights', 'azure-video-indexer', 'azure-ai', 'azure-elastic-scale', 'azure-clouddrive', 'azure-api-apps', 'azure-service-fabric-mesh', 'azure-functions-docker', 'microsoft-custom-vision', 'azure-resource-manager', 'azure-elastic-sharding', 'azure-app-service-envrmnt', 'azure-sdk-js', 'azure-sdk-for-java', 'azure-advisor', 'azure-function-app-proxy', 'azure-functions-proxies', 'azure-sentinel', 'azure-anomaly-detector', 'azure-container-instances', 'azure-managed-disk', 'azure-active-directory', 'azureclicredential', 'azure-webapps', 'azure-ml-pipelines', 'azure-redis-cache', 'azure-http-trigger', 'azure-dsvm', 'azureportal', 'azure-servicebus-queues', 'azure-media-services', 'azure-ase', 'azure-node-sdk', 'azure-sql-managed-instance', 'sql-azure-federations', 'azure-debugger', 'azure-service-principal', 'azure-monitor-workbooks', 'azure-web-app-for-containers', 'azure-vm', 'azure-application-insights-profiler', 'azure-cost-calculation', 'azure-mobile-engagement', 'azure-file-copy', 'azure-diagnostics', 'azure-security', 'azure-analytics', 'azure-logic-app-standard', 'azure-vm-scale-set', 'azure-java-tools', 'azure-cognitive-services', 'django-pyodbc-azure', 'azure-application-proxy', 'azure-resource-graph', 'azure-ad-b2b', 'azure-compliance-policy', 'azure-durable-functions', 'azure-database-postgresql', 'azure-promptflow', 'azure-eventhub', 'azure-tablequery', 'azure-sdk-php', 'azure-storage-queues', 'azure-service-plan', 'azure-cosmosdb-emulator', 'azure-performancecounters', 'azure-scheduler', 'azure-availability-set', 'azure-dashboard', 'azure-mysql-database', 'azure-managed-grafana', 'azure-monitoring', 'azure-worker-roles', 'azure-service-runtime', 'azure-ddos', 'azure-data-sync', 'azure-machine-learning-service', 'azure-billing', 'azure-packaging', 'azure-container-apps', 'microsoft-entra-id', 'azure-sql', 'azure-bicep', 'azure-cosmosdb', 'azure-update-management-center', 'azure-mapping-data-flow', 'azure-lab-services', 'azure-custom-providers', 'azure-sdk-.net', 'azure-autoscaling-block', 'azure-data-explorer', 'azureml-python-sdk', 'azure-pipelines-release-pipeline', 'azure-load-balancer', 'azure-managed-identity', 'azure-ad-verifiable-credentials', 'azure-webjobssdk', 'azure-agent', 'azuremlsdk', 'azure-blueprints', 'azure-vpn', 'azure-automation', 'azure-blockchain-workbench', 'azure-api-management', 'azure-rm', 'azure-application-roles', 'azure-public-ip', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-sdk-python', 'terraform-provider-azure', 'azure-marketplace', 'azure-information-protection', 'azure-analysis-services', 'azure-zulu', 'azure-batch-account', 'azure-china', 'azure-android-sdk', 'azure-rm-template', 'azure-spring-cloud', 'azure-stream-analytics', 'azure-keyvault', 'azure-oms', 'azure-ad-msal', 'azure-webhooks', 'azure-adal-deprecation', 'azure-language-understanding', 'azure-container-registry', 'azure-web-pubsub', 'azure-maps', 'azure-migrate', 'azure-dev-spaces', 'fhir-server-for-azure', 'sitecore-azure', 'azure-cosmosdb-gremlinapi', 'azure-aks', 'azure.data.tables', 'azure-java-sdk', 'azure-static-website-hosting', 'azure-mobile-services', 'azure-triggers', 'azure-ad-powershell-v2', 'azure-adf', 'azure-in-role-cache', 'azure-iot-edge', 'azure-linux', 'azure-media-player', 'azure-storage-emulator', 'azure-eventgrid', 'azure-object-anchors', 'azure-management-portal', 'azure-notebooks', 'azure-custom-domain', 'azure-xplat-cli', 'azure-runbook', 'rebus-azureservicebus', 'azure-cognitive-search', 'azure-oauth', 'azure-application-insights', 'azure-traffic-manager', 'azure-anomaly-detection', 'azure-acr', 'adal', 'azure-storage-explorer', 'azure-private-dns-zone', 'azure', 'azure-auto-ml', 'azure-iot-hub-device-update', 'azure-logic-apps', 'azure-relay', 'azure-spatial-anchors', 'azure-monitor', 'azure-load-testing', 'azure-cosmosdb-changefeed', 'azure-function-http'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}], 'account_id': 7002499, 'is_employee': False, 'last_modified_date': 1699873800, 'last_access_date': 1711030123, 'reputation_change_year': 188, 'reputation_change_quarter': 188, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 12155, 'creation_date': 1443016881, 'user_type': 'registered', 'user_id': 5368083, 'accept_rate': 71, 'location': 'Israel', 'website_url': '', 'link': 'https://stackoverflow.com/users/5368083/bluesummers', 'profile_image': 'https://lh4.googleusercontent.com/-EoolWeDVisI/AAAAAAAAAAI/AAAAAAAAAOE/BNaSCPaIrFU/photo.jpg?sz=256', 'display_name': 'bluesummers'}","I have a instance which holds 3 different features which is a scalar which is a sequence of scalars which is a sequence of sequences feature I am trying to use to genereate padded data as input to my model - and I want to pad every feature differently. Example batch: Expected output: As you can see the feature should not be padded, and the and should be padded by the corresponding longest entry in the given batch.","tf.data.Dataset label sequence_feature seq_of_seqs_feature tf.data.Dataset.padded_batch() [{'label': 24,
  'sequence_feature': [1, 2],
  'seq_of_seqs_feature': [[11.1, 22.2],
                          [33.3, 44.4]]},
 {'label': 32,
  'sequence_feature': [3, 4, 5],
  'seq_of_seqs_feature': [[55.55, 66.66]]}]
 [{'label': 24,
  'sequence_feature': [1, 2, 0],
  'seq_of_seqs_feature': [[11.1, 22.2],
                          [33.3, 44.4]]},
 {'label': 32,
  'sequence_feature': [3, 4, 5],
  'seq_of_seqs_feature': [[55.55, 66.66],
                           0.0, 0.0    ]}]
 label sequence_feature seq_of_seqs_feature",5,34,0,0,
506,48787250,55687210,156715,Set up virtualenv using a requirements.txt generated by conda,2,<python><pip><anaconda><virtualenv><conda>,72,"<p>I'm setting up a python project, using an Anaconda virtual environment. I'm generating a requirements.txt so other people can easily set up their own virtual environment for the project.</p>

<p>I was wondering though, when other developers want to contribute to the project, but want to use virtualenv instead of Anaconda, can they do that?</p>

<p>I tried the following: </p>

<ul>
<li><p>I set up an empty project in an Anaconda environment and installed the aiohttp module. Then <code>conda list --export &gt; requirements.txt</code> generates the following:</p>

<pre><code># This file may be used to create an environment using:
# $ conda create --name &lt;env&gt; --file &lt;this file&gt;
# platform: win-64
aiohttp=2.3.9=py36_0
async-timeout=2.0.0=py36hc3e01a3_0
certifi=2018.1.18=py36_0
chardet=3.0.4=py36h420ce6e_1
multidict=3.3.2=py36h72bac45_0
pip=9.0.1=py36h226ae91_4
python=3.6.4=h6538335_1
setuptools=38.4.0=py36_0
vc=14=h0510ff6_3
vs2015_runtime=14.0.25123=3
wheel=0.30.0=py36h6c3ec14_1
wincertstore=0.2=py36h7fe50ca_0
yarl=0.14.2=py36h27d1bf2_0
</code></pre></li>
<li><p>I set up an empty project in a virtualenv environment and installed the aiohttp module there too. <code>pip freeze &gt; requirements.txt</code> then generates:</p>

<pre><code>aiohttp==3.0.1
async-timeout==2.0.0
attrs==17.4.0
chardet==3.0.4
idna==2.6
idna-ssl==1.0.0
multidict==4.1.0
yarl==1.1.0
</code></pre></li>
</ul>

<p>So apparently both outputs are different, and my theory is: once I generate my requirements.txt with conda on my project, other developers can't choose virtualenv instead - at least not if they're not prepared to install a long list requirements by hand (it will be more than just the aiohttp module of course).</p>

<p>A first sight, importing the conda-generated requirements.txt in a project on virtualenv (<code>pip install -r requirements-conda.txt</code>) throws this error:</p>

<pre><code>Invalid requirement: 'aiohttp=2.3.9=py36_0'
= is not a valid operator. Did you mean == ?
</code></pre>

<p>Am I right when I think that if developers would like to do this, they would need to programmatically change the package list to the format that virtualenv understands, or they would have to import all packages manually? Meaning that I impose them to choose conda as virtual environment as well if they want to save themselves some extra work?</p>
",5433896,2283,14-02-2018 12:28,15-04-2019 10:30,425,2313,39,4,25,,"{'badge_counts': {'bronze': 39, 'silver': 25, 'gold': 4}, 'account_id': 7103240, 'is_employee': False, 'last_modified_date': 1709114942, 'last_access_date': 1711025706, 'reputation_change_year': 99, 'reputation_change_quarter': 99, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2313, 'creation_date': 1444582450, 'user_type': 'registered', 'user_id': 5433896, 'location': 'Belgium', 'website_url': '', 'link': 'https://stackoverflow.com/users/5433896/sander-vanden-hautte', 'profile_image': 'https://i.stack.imgur.com/Njkhb.jpg?s=256&g=1', 'display_name': 'Sander Vanden Hautte'}","I'm setting up a python project, using an Anaconda virtual environment. I'm generating a requirements.txt so other people can easily set up their own virtual environment for the project. I was wondering though, when other developers want to contribute to the project, but want to use virtualenv instead of Anaconda, can they do that? I tried the following: I set up an empty project in an Anaconda environment and installed the aiohttp module. Then generates the following: I set up an empty project in a virtualenv environment and installed the aiohttp module there too. then generates: So apparently both outputs are different, and my theory is: once I generate my requirements.txt with conda on my project, other developers can't choose virtualenv instead - at least not if they're not prepared to install a long list requirements by hand (it will be more than just the aiohttp module of course). A first sight, importing the conda-generated requirements.txt in a project on virtualenv () throws this error: Am I right when I think that if developers would like to do this, they would need to programmatically change the package list to the format that virtualenv understands, or they would have to import all packages manually? Meaning that I impose them to choose conda as virtual environment as well if they want to save themselves some extra work?","conda list --export &gt; requirements.txt # This file may be used to create an environment using:
# $ conda create --name &lt;env&gt; --file &lt;this file&gt;
# platform: win-64
aiohttp=2.3.9=py36_0
async-timeout=2.0.0=py36hc3e01a3_0
certifi=2018.1.18=py36_0
chardet=3.0.4=py36h420ce6e_1
multidict=3.3.2=py36h72bac45_0
pip=9.0.1=py36h226ae91_4
python=3.6.4=h6538335_1
setuptools=38.4.0=py36_0
vc=14=h0510ff6_3
vs2015_runtime=14.0.25123=3
wheel=0.30.0=py36h6c3ec14_1
wincertstore=0.2=py36h7fe50ca_0
yarl=0.14.2=py36h27d1bf2_0
 pip freeze &gt; requirements.txt aiohttp==3.0.1
async-timeout==2.0.0
attrs==17.4.0
chardet==3.0.4
idna==2.6
idna-ssl==1.0.0
multidict==4.1.0
yarl==1.1.0
 pip install -r requirements-conda.txt Invalid requirement: 'aiohttp=2.3.9=py36_0'
= is not a valid operator. Did you mean == ?
",20,48,0,0,
507,48734388,48734474,84249,ValueError: Cannot index with multidimensional key,2,<python><pandas>,18,"<p>I am trying to code a simple recommender system using only pandas and I am having trouble with the filtering part.I want to select all the rows where the RatingCounts column is greater than a value I choose.This returns me a dataframe with one column filled with the correct booleans but i cannot index my data with this selection it gives me a value error as mentioned in the title.Here is the screenshot</p>

<p><a href=""https://i.stack.imgur.com/DuzhC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DuzhC.png"" alt=""enter image description here""></a></p>
",5441624,301,11-02-2018 17:36,11-02-2018 17:43,0,301,6,1,2,,"{'badge_counts': {'bronze': 6, 'silver': 2, 'gold': 1}, 'account_id': 7114995, 'is_employee': False, 'last_modified_date': 1701723857, 'last_access_date': 1622403021, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 301, 'creation_date': 1444751629, 'user_type': 'registered', 'user_id': 5441624, 'link': 'https://stackoverflow.com/users/5441624/furkan-k%c4%b1l%c4%b1%c3%a7aslan', 'profile_image': 'https://www.gravatar.com/avatar/a991382ed1ddbb89fabfc2a207f77c4c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Furkan Kılı&#231;aslan'}",I am trying to code a simple recommender system using only pandas and I am having trouble with the filtering part.I want to select all the rows where the RatingCounts column is greater than a value I choose.This returns me a dataframe with one column filled with the correct booleans but i cannot index my data with this selection it gives me a value error as mentioned in the title.Here is the screenshot,,0,3,1,1,
508,48201729,48201957,62259,Difference between np.dot and np.multiply with np.sum in binary cross-entropy loss calculation,4,<python><numpy><neural-network><sum><difference>,34,"<p>I have tried the following code but didn't find the difference between <strong>np.dot</strong> and <strong>np.multiply with np.sum</strong> </p>

<p>Here is <strong>np.dot</strong> code</p>

<pre><code>logprobs = np.dot(Y, (np.log(A2)).T) + np.dot((1.0-Y),(np.log(1 - A2)).T)
print(logprobs.shape)
print(logprobs)
cost = (-1/m) * logprobs
print(cost.shape)
print(type(cost))
print(cost)
</code></pre>

<p>Its output is </p>

<pre><code>(1, 1)
[[-2.07917628]]
(1, 1)
&lt;class 'numpy.ndarray'&gt;
[[ 0.693058761039 ]]
</code></pre>

<p>Here is the code for <strong>np.multiply with np.sum</strong></p>

<pre><code>logprobs = np.sum(np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2)))
print(logprobs.shape)         
print(logprobs)
cost = - logprobs / m
print(cost.shape)
print(type(cost))
print(cost)
</code></pre>

<p>Its output is </p>

<pre><code>()
-2.07917628312
()
&lt;class 'numpy.float64'&gt;
0.693058761039
</code></pre>

<p>I'm unable to understand the type and shape difference whereas the result value is same in both cases </p>

<p>Even in the case of squeezing former code  <strong>cost value become same as later but type remains same</strong></p>

<pre><code>cost = np.squeeze(cost)
print(type(cost))
print(cost)
</code></pre>

<p>output is </p>

<pre><code>&lt;class 'numpy.ndarray'&gt;
0.6930587610394646
</code></pre>
",4539906,2067,11-01-2018 07:21,11-01-2018 07:36,0,2097,30,1,25,,"{'badge_counts': {'bronze': 30, 'silver': 25, 'gold': 1}, 'account_id': 5749872, 'is_employee': False, 'last_modified_date': 1661777174, 'last_access_date': 1707544167, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2097, 'creation_date': 1423292078, 'user_type': 'registered', 'user_id': 4539906, 'location': 'Lahore, Punjab, Pakistan', 'website_url': '', 'link': 'https://stackoverflow.com/users/4539906/asad-shakeel', 'profile_image': 'https://i.stack.imgur.com/0je96.jpg?s=256&g=1', 'display_name': 'Asad Shakeel'}",I have tried the following code but didn't find the difference between np.dot and np.multiply with np.sum Here is np.dot code Its output is Here is the code for np.multiply with np.sum Its output is I'm unable to understand the type and shape difference whereas the result value is same in both cases Even in the case of squeezing former code cost value become same as later but type remains same output is,"logprobs = np.dot(Y, (np.log(A2)).T) + np.dot((1.0-Y),(np.log(1 - A2)).T)
print(logprobs.shape)
print(logprobs)
cost = (-1/m) * logprobs
print(cost.shape)
print(type(cost))
print(cost)
 (1, 1)
[[-2.07917628]]
(1, 1)
&lt;class 'numpy.ndarray'&gt;
[[ 0.693058761039 ]]
 logprobs = np.sum(np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2)))
print(logprobs.shape)         
print(logprobs)
cost = - logprobs / m
print(cost.shape)
print(type(cost))
print(cost)
 ()
-2.07917628312
()
&lt;class 'numpy.float64'&gt;
0.693058761039
 cost = np.squeeze(cost)
print(type(cost))
print(cost)
 &lt;class 'numpy.ndarray'&gt;
0.6930587610394646
",23,56,0,0,
509,48622281,48622434,10266,How to save numpy ndarray as .csv file?,1,<python><arrays><python-2.7><numpy>,14,"<p>I created a <code>numpy</code> array as follows:</p>

<pre><code>import numpy as np

names  = np.array(['NAME_1', 'NAME_2', 'NAME_3'])
floats = np.array([ 0.1234 ,  0.5678 ,  0.9123 ])

ab = np.zeros(names.size, dtype=[('var1', 'U6'), ('var2', float)])
ab['var1'] = names
ab['var2'] = floats
</code></pre>

<p>The values in <code>ab</code> are shown below:</p>

<pre><code>array([(u'NAME_1',  0.1234), (u'NAME_2',  0.5678), (u'NAME_3',  0.9123)],
      dtype=[('var1', '&lt;U6'), ('var2', '&lt;f8')])
</code></pre>

<p>When I try to save <code>ab</code> as a .csv file using <code>savetxt()</code> command, </p>

<pre><code>np.savetxt('D:\test.csv',ab,delimiter=',')
</code></pre>

<p>I get below error</p>

<blockquote>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-66-a71fd201aefe&gt; in &lt;module&gt;()
----&gt; 1 np.savetxt('D:\Azim\JF-Mapping-workflow-CRM\Backup\delete.csv',ab,delimiter=',')

c:\python27\lib\site-packages\numpy\lib\npyio.pyc in savetxt(fname, X, fmt, delimiter, newline, header, footer, comments)
   1256                     raise TypeError(""Mismatch between array dtype ('%s') and ""
   1257                                     ""format specifier ('%s')""
-&gt; 1258                                     % (str(X.dtype), format))
   1259         if len(footer) &gt; 0:
   1260             footer = footer.replace('\n', '\n' + comments)

TypeError: Mismatch between array dtype ('[('var1', '&lt;U6'), ('var2', '&lt;f8')]') and format specifier ('%.18e,%.18e')
</code></pre>
</blockquote>
",4566277,2605,05-02-2018 12:09,05-02-2018 12:18,0,2615,45,4,24,58,"{'badge_counts': {'bronze': 45, 'silver': 24, 'gold': 4}, 'account_id': 3846039, 'is_employee': False, 'last_modified_date': 1672902967, 'last_access_date': 1711010107, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2615, 'creation_date': 1423909768, 'user_type': 'registered', 'user_id': 4566277, 'accept_rate': 58, 'website_url': 'https://statguyuser.github.io/feature-engg-selection-for-explainable-models.github.io/index.html', 'link': 'https://stackoverflow.com/users/4566277/statguyuser', 'profile_image': 'https://i.stack.imgur.com/FrYLK.jpg?s=256&g=1', 'display_name': 'StatguyUser'}","I created a array as follows: The values in are shown below: When I try to save as a .csv file using command, I get below error","numpy import numpy as np

names  = np.array(['NAME_1', 'NAME_2', 'NAME_3'])
floats = np.array([ 0.1234 ,  0.5678 ,  0.9123 ])

ab = np.zeros(names.size, dtype=[('var1', 'U6'), ('var2', float)])
ab['var1'] = names
ab['var2'] = floats
 ab array([(u'NAME_1',  0.1234), (u'NAME_2',  0.5678), (u'NAME_3',  0.9123)],
      dtype=[('var1', '&lt;U6'), ('var2', '&lt;f8')])
 ab savetxt() np.savetxt('D:\test.csv',ab,delimiter=',')
 ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-66-a71fd201aefe&gt; in &lt;module&gt;()
----&gt; 1 np.savetxt('D:\Azim\JF-Mapping-workflow-CRM\Backup\delete.csv',ab,delimiter=',')

c:\python27\lib\site-packages\numpy\lib\npyio.pyc in savetxt(fname, X, fmt, delimiter, newline, header, footer, comments)
   1256                     raise TypeError(""Mismatch between array dtype ('%s') and ""
   1257                                     ""format specifier ('%s')""
-&gt; 1258                                     % (str(X.dtype), format))
   1259         if len(footer) &gt; 0:
   1260             footer = footer.replace('\n', '\n' + comments)

TypeError: Mismatch between array dtype ('[('var1', '&lt;U6'), ('var2', '&lt;f8')]') and format specifier ('%.18e,%.18e')
",16,41,0,0,
510,48944296,58491770,2200,Editable plots in PowerPoint from python: equivalent of officer and rvg,1,<python><r><plot><graphics><powerpoint>,14,"<p>I am using the <code>officer</code> and <code>rvg</code> packages to get plots from R into MS PowerPoint as editable vector graphics. Reproducible example below.</p>

<p>I am looking for a way to implement an equivalent solution with python, preferably using <code>matplotlib</code>. The critical part is not the creation of slides from the IDE but rather the editable vector graphics part, i.e. plots should end up in PowerPoint as grouped objects comprised of a range of simple powerpoint geometries such as lines, squares, and text fields.</p>

<p>R example:</p>

<pre><code>library(tidyverse)
library(officer)
library(rvg)

# Get some data and make a plot
ggp &lt;- diamonds %&gt;% 
  group_by(clarity) %&gt;%
  summarise(price = mean(price)) %&gt;%
  ggplot(aes(x = clarity, y = price, fill = clarity)) +
  geom_bar(stat = 'identity', colour = 'black')

# Create a new powerpoint document
doc &lt;- read_pptx()
doc &lt;- add_slide(doc, 'Title and Content', 'Office Theme')
# Add the plot 
doc &lt;- ph_with_vg(doc, ggobj = ggp, type = 'body')  

# Write the document to a file
print(doc, target = 'plots.pptx')
</code></pre>

<p>The resulting chart is completely editable:</p>

<p><a href=""https://i.stack.imgur.com/iGtG6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/iGtG6.png"" alt=""enter image description here""></a></p>
",5457259,2325,23-02-2018 08:56,21-10-2019 17:56,605,2325,40,2,18,80,"{'badge_counts': {'bronze': 40, 'silver': 18, 'gold': 2}, 'account_id': 2638307, 'is_employee': False, 'last_modified_date': 1573679382, 'last_access_date': 1696597348, 'reputation_change_year': -2, 'reputation_change_quarter': -2, 'reputation_change_month': -2, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2325, 'creation_date': 1445092409, 'user_type': 'registered', 'user_id': 5457259, 'accept_rate': 80, 'location': 'London, United Kingdom', 'website_url': 'https://janlauge.github.io', 'link': 'https://stackoverflow.com/users/5457259/janlauge', 'profile_image': 'https://lh5.googleusercontent.com/-GGC2rHf1xX4/AAAAAAAAAAI/AAAAAAAABXw/j8iDGaYhFOo/photo.jpg?sz=256', 'display_name': 'JanLauGe'}","I am using the and packages to get plots from R into MS PowerPoint as editable vector graphics. Reproducible example below. I am looking for a way to implement an equivalent solution with python, preferably using . The critical part is not the creation of slides from the IDE but rather the editable vector graphics part, i.e. plots should end up in PowerPoint as grouped objects comprised of a range of simple powerpoint geometries such as lines, squares, and text fields. R example: The resulting chart is completely editable:","officer rvg matplotlib library(tidyverse)
library(officer)
library(rvg)

# Get some data and make a plot
ggp &lt;- diamonds %&gt;% 
  group_by(clarity) %&gt;%
  summarise(price = mean(price)) %&gt;%
  ggplot(aes(x = clarity, y = price, fill = clarity)) +
  geom_bar(stat = 'identity', colour = 'black')

# Create a new powerpoint document
doc &lt;- read_pptx()
doc &lt;- add_slide(doc, 'Title and Content', 'Office Theme')
# Add the plot 
doc &lt;- ph_with_vg(doc, ggobj = ggp, type = 'body')  

# Write the document to a file
print(doc, target = 'plots.pptx')
",15,30,1,1,
511,48450527,48450600,10783,"Conda cannot remove environment called ""tensorflow""",1,<python><python-3.x><cmd><anaconda><conda>,11,"<p>I just run the following command to create a Conda environment:</p>

<pre><code>conda create -n tensorflow python=3.5
</code></pre>

<p>However, I want to delete it now. When I try doing:</p>

<pre><code>conda remove -n tensorflow
</code></pre>

<p>or </p>

<pre><code>conda remove --name tensorflow
</code></pre>

<p>I get the following error:</p>

<pre><code>CondaValueError: no package names supplied,
       try ""conda remove -h"" for more details
</code></pre>

<p>However, if I try and see which environments I have, I can see:</p>

<pre><code>base                  *  C:\Users\Me\Anaconda3
flask_env                C:\Users\Me\Anaconda3\envs\flask_env
tensorflow               C:\Users\Me\Anaconda3\envs\tensorflow
</code></pre>

<p>My idea is that I have called the environment with the same name of the package tensorflow.. even though I don't have tensorflow installed in the ""base"" environment</p>
",6435921,3461,25-01-2018 19:30,25-01-2018 19:35,0,3469,77,9,34,81,"{'badge_counts': {'bronze': 77, 'silver': 34, 'gold': 9}, 'account_id': 7888593, 'is_employee': False, 'last_modified_date': 1711023600, 'last_access_date': 1711054318, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 50, 'reputation_change_week': -2, 'reputation_change_day': 0, 'reputation': 3469, 'creation_date': 1465311013, 'user_type': 'registered', 'user_id': 6435921, 'accept_rate': 81, 'location': 'Bristol, UK', 'link': 'https://stackoverflow.com/users/6435921/euler-salter', 'profile_image': 'https://www.gravatar.com/avatar/743c389e4c05d0573d91f201d9e1f9d6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Euler_Salter'}","I just run the following command to create a Conda environment: However, I want to delete it now. When I try doing: or I get the following error: However, if I try and see which environments I have, I can see: My idea is that I have called the environment with the same name of the package tensorflow.. even though I don't have tensorflow installed in the ""base"" environment","conda create -n tensorflow python=3.5
 conda remove -n tensorflow
 conda remove --name tensorflow
 CondaValueError: no package names supplied,
       try ""conda remove -h"" for more details
 base                  *  C:\Users\Me\Anaconda3
flask_env                C:\Users\Me\Anaconda3\envs\flask_env
tensorflow               C:\Users\Me\Anaconda3\envs\tensorflow
",3,29,0,0,
512,48720833,48735246,965829,Could not find a version that satisfies the requirement tensorflow,26,<python><python-3.x><python-2.7><tensorflow><pip>,390,"<p>I installed the latest version of Python <code>(3.6.4 64-bit)</code> and the latest version of <code>PyCharm (2017.3.3 64-bit)</code>. Then I installed some modules in PyCharm (Numpy, Pandas, etc), but when I tried installing Tensorflow it didn't install, and I got the error message: </p>

<blockquote>
  <p>Could not find a version that satisfies the requirement TensorFlow (from versions: )
      No matching distribution found for TensorFlow.</p>
</blockquote>

<p>Then I tried installing TensorFlow from the command prompt and I got the same error message.
I did however successfully install tflearn. </p>

<p>I also installed Python 2.7, but I got the same error message again. I googled the error and tried some of the things which were suggested to other people, but nothing worked (this included installing Flask). </p>

<p>How can I install Tensorflow? Thanks.</p>
",5538535,4748,10-02-2018 12:35,11-02-2018 19:01,1,4748,30,3,17,75,"{'badge_counts': {'bronze': 30, 'silver': 17, 'gold': 3}, 'account_id': 7262369, 'is_employee': False, 'last_modified_date': 1701341873, 'last_access_date': 1710589737, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4748, 'creation_date': 1446960931, 'user_type': 'registered', 'user_id': 5538535, 'accept_rate': 75, 'location': 'Belgium', 'website_url': '', 'link': 'https://stackoverflow.com/users/5538535/martin-w', 'profile_image': 'https://i.stack.imgur.com/h57XX.jpg?s=256&g=1', 'display_name': 'Martin W'}","I installed the latest version of Python and the latest version of . Then I installed some modules in PyCharm (Numpy, Pandas, etc), but when I tried installing Tensorflow it didn't install, and I got the error message: Could not find a version that satisfies the requirement TensorFlow (from versions: ) No matching distribution found for TensorFlow. Then I tried installing TensorFlow from the command prompt and I got the same error message. I did however successfully install tflearn. I also installed Python 2.7, but I got the same error message again. I googled the error and tried some of the things which were suggested to other people, but nothing worked (this included installing Flask). How can I install Tensorflow? Thanks.",(3.6.4 64-bit) PyCharm (2017.3.3 64-bit),-2,13,0,0,
513,49216357,49216382,46813,How to keep original index of a DataFrame after groupby 2 columns?,4,<python><pandas><dataframe><indexing><pandas-groupby>,22,"<p>Is there any way I can retain the original index of my large dataframe after I perform a groupby? The reason I need to this is because I need to do an inner merge back to my original df (after my groupby) to regain those lost columns. And the index value is the only 'unique' column to perform the merge back into. Does anyone know how I can achieve this?</p>

<p>My DataFrame is quite large. 
My groupby looks like this: </p>

<pre><code>df.groupby(['col1', 'col2']).agg({'col3': 'count'}).reset_index()
</code></pre>

<p>This drops my original indexes from my original dataframe, which I want to keep. </p>
",5578650,1400,11-03-2018 03:31,11-03-2018 03:34,0,1400,38,5,25,60,"{'badge_counts': {'bronze': 38, 'silver': 25, 'gold': 5}, 'account_id': 7323446, 'is_employee': False, 'last_modified_date': 1683942000, 'last_access_date': 1690829412, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1400, 'creation_date': 1447883273, 'user_type': 'registered', 'user_id': 5578650, 'accept_rate': 60, 'link': 'https://stackoverflow.com/users/5578650/hana', 'profile_image': 'https://www.gravatar.com/avatar/5f11bf1733e70600439351a970f878cc?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Hana'}","Is there any way I can retain the original index of my large dataframe after I perform a groupby? The reason I need to this is because I need to do an inner merge back to my original df (after my groupby) to regain those lost columns. And the index value is the only 'unique' column to perform the merge back into. Does anyone know how I can achieve this? My DataFrame is quite large. My groupby looks like this: This drops my original indexes from my original dataframe, which I want to keep.","df.groupby(['col1', 'col2']).agg({'col3': 'count'}).reset_index()
",0,9,0,0,
514,48082951,48213442,2729,Customize flask form as table,1,<python><flask><wtforms>,13,"<p>I've been biting my nails on this one for quite some time. In my Flask-app I currently have a product-database, for which in the app I have a page that queries each product column into a table. </p>

<p>For example I'd have product <code>1234</code> for which I could view the details (i.e. database columns) in <code>example.com/items/1234</code> which would give me the following: </p>

<pre><code>&lt;div class=""container""&gt;
  &lt;div class=""content""&gt;
  &lt;a href=""/item""&gt;Back&lt;/a&gt;
  &lt;table class=""table""&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;#&lt;/th&gt;
      &lt;th&gt;Value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th scope=""row""&gt;Detail-1&lt;th&gt;
      &lt;td&gt;Example&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
       &lt;th scope=""row""&gt;Detail-2&lt;/th&gt;
       &lt;td&gt;Example&lt;/td&gt;
    &lt;/tr&gt;
</code></pre>

<p>What I'm struggling with is the following: </p>

<p>I'd like to be able to add new products with same table-style. For this I have created a form like the following:</p>

<pre><code>class NewArticleForm(FlaskForm):
    detail_1 = StringField(""Detail-1"")
    detail_2 = IntegerField(""Detail-2"")
    ...
    submit = SubmitField(""Submit"")
</code></pre>

<p>I am now at a total loss on how to customize the form appearance in the template beyond just <code>{{ wtf.quick_form(form) }}</code>. What I tried is the following: </p>

<pre><code>&lt;form method=""POST""&gt;
&lt;table class=""table""&gt;
  {% for name in form %}
  &lt;tr&gt;
    &lt;th scope=""row""&gt;{{ name.label }}&lt;/th&gt;
    &lt;td&gt;{{ name }}&lt;/td&gt;
  &lt;/tr&gt;
  {% endfor %}
 &lt;/table&gt;
 &lt;/form&gt;
</code></pre>

<p>The table looks good (at least that) but the request is not being sent correctly I presume. The page loads correctly with <code>""POST /url HTTP/1.1""</code> but it doesn't seem to come through correctly. </p>

<p>What I mean by that is that though the request is being sent correctly, as I can see if I run the app via the Flask server. However it seems that nothing is transmitted to the database. The page simply reloads with the entered data still in the fields and nothing transmitted to the database. If I simply use the <code>wtf.quick_form</code> then the data is correctly sent to the database. </p>

<p>So how can I correctly customize the form appearance and/or crucial step did I miss?</p>
",6503432,725,03-01-2018 18:15,11-01-2018 18:12,8,725,20,0,7,83,"{'badge_counts': {'bronze': 20, 'silver': 7, 'gold': 0}, 'account_id': 8690382, 'is_employee': False, 'last_modified_date': 1623665073, 'last_access_date': 1711104229, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 725, 'creation_date': 1466674507, 'user_type': 'registered', 'user_id': 6503432, 'accept_rate': 83, 'website_url': '', 'link': 'https://stackoverflow.com/users/6503432/rongon', 'profile_image': 'https://www.gravatar.com/avatar/df63d816e5a5e1309b2752172bc95b0f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'rongon'}","I've been biting my nails on this one for quite some time. In my Flask-app I currently have a product-database, for which in the app I have a page that queries each product column into a table. For example I'd have product for which I could view the details (i.e. database columns) in which would give me the following: What I'm struggling with is the following: I'd like to be able to add new products with same table-style. For this I have created a form like the following: I am now at a total loss on how to customize the form appearance in the template beyond just . What I tried is the following: The table looks good (at least that) but the request is not being sent correctly I presume. The page loads correctly with but it doesn't seem to come through correctly. What I mean by that is that though the request is being sent correctly, as I can see if I run the app via the Flask server. However it seems that nothing is transmitted to the database. The page simply reloads with the entered data still in the fields and nothing transmitted to the database. If I simply use the then the data is correctly sent to the database. So how can I correctly customize the form appearance and/or crucial step did I miss?","1234 example.com/items/1234 &lt;div class=""container""&gt;
  &lt;div class=""content""&gt;
  &lt;a href=""/item""&gt;Back&lt;/a&gt;
  &lt;table class=""table""&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;#&lt;/th&gt;
      &lt;th&gt;Value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th scope=""row""&gt;Detail-1&lt;th&gt;
      &lt;td&gt;Example&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
       &lt;th scope=""row""&gt;Detail-2&lt;/th&gt;
       &lt;td&gt;Example&lt;/td&gt;
    &lt;/tr&gt;
 class NewArticleForm(FlaskForm):
    detail_1 = StringField(""Detail-1"")
    detail_2 = IntegerField(""Detail-2"")
    ...
    submit = SubmitField(""Submit"")
 {{ wtf.quick_form(form) }} &lt;form method=""POST""&gt;
&lt;table class=""table""&gt;
  {% for name in form %}
  &lt;tr&gt;
    &lt;th scope=""row""&gt;{{ name.label }}&lt;/th&gt;
    &lt;td&gt;{{ name }}&lt;/td&gt;
  &lt;/tr&gt;
  {% endfor %}
 &lt;/table&gt;
 &lt;/form&gt;
 ""POST /url HTTP/1.1"" wtf.quick_form",26,55,0,0,
515,49822552,49823060,185539,python-asyncio TypeError: object dict can't be used in 'await' expression,4,<python><python-asyncio>,79,"<p>I am using a third party module to retrieve data from an API. I simply would like to asynchronously await the module to return the data which occasionally takes several seconds and freezes up my app. However, when I try to await a call to that module I receive the TypeError:</p>

<p><code>TypeError: object dict can't be used in 'await' expression</code></p>

<pre><code>import thirdPartyAPIwrapper

async def getData():
    retrienveData = await thirdPartyAPIWrapper.data()
    return await retrieveData

def main():
    loop = asncio.get_event_loop()
    data = loop.run_until_complete(getData())
    loop.close
    return data
</code></pre>

<p>Why can I not await a type('dict')? Is there a way around this?
If async/await with asyncio will not work with a third party module that doesn't return a coroutine then what are my other options?</p>
",5736700,1344,13-04-2018 17:45,13-04-2018 18:21,0,1354,25,2,12,33,"{'badge_counts': {'bronze': 25, 'silver': 12, 'gold': 2}, 'account_id': 7557803, 'is_employee': False, 'last_modified_date': 1701483900, 'last_access_date': 1711139779, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1354, 'creation_date': 1451679827, 'user_type': 'registered', 'user_id': 5736700, 'accept_rate': 33, 'location': 'Springfield, Missouri, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/5736700/riley-hughes', 'profile_image': 'https://lh5.googleusercontent.com/-YvcYJKRXxmg/AAAAAAAAAAI/AAAAAAAAARs/zGOz0_twPdI/photo.jpg?sz=256', 'display_name': 'Riley Hughes'}","I am using a third party module to retrieve data from an API. I simply would like to asynchronously await the module to return the data which occasionally takes several seconds and freezes up my app. However, when I try to await a call to that module I receive the TypeError: Why can I not await a type('dict')? Is there a way around this? If async/await with asyncio will not work with a third party module that doesn't return a coroutine then what are my other options?","TypeError: object dict can't be used in 'await' expression import thirdPartyAPIwrapper

async def getData():
    retrienveData = await thirdPartyAPIWrapper.data()
    return await retrieveData

def main():
    loop = asncio.get_event_loop()
    data = loop.run_until_complete(getData())
    loop.close
    return data
",9,19,0,0,
516,48693825,50514619,18837,"Making a graphQL mutation from my python code, getting error",2,<python><python-requests><graphql><parse-error><express-graphql>,11,"<p>I am trying to make a mutation to my Shopify store from python.
I am new to graphQL, I have been able to make the mutation using graphiQL but I am not certain how to do it directly from my code.</p>

<p>This is my make query file, it has worked successfully for a simple query</p>

<pre><code>`import requests 
 def make_query(self, query, url, headers):
    """"""
    Return query response
    """"""
    request = requests.post(url, json={'query': query}, headers=headers)
    if request.status_code == 200:
        return request.json()
    else:
        raise Exception(""Query failed to run by returning code of {}. {}"".format(request.status_code, query))`
</code></pre>

<p>Now an example of the mutation that worked in graphiQL is this:</p>

<p><code>""mutation {customerCreate(input: {email: 'wamblamkazam@send22u.info', password: 'password'}) {userErrors { field message}customer{id}}}""</code></p>

<p>But when I pass it into my make_query function it gives this error</p>

<pre><code>{'errors': [{'message': 'Parse error on ""\'"" (error) at [1, 41]', 'locations': [{'line': 1, 'column': 41}]}]}
</code></pre>

<p>How do I fix this?
Also one of the mutations I am making uses variables, and I haven't been able to find an example of how to do this directly from my code</p>
",5806856,157,08-02-2018 20:00,24-05-2018 17:01,105,157,6,1,1,,"{'badge_counts': {'bronze': 6, 'silver': 1, 'gold': 1}, 'account_id': 7661781, 'is_employee': False, 'last_modified_date': 1646445300, 'last_access_date': 1699754641, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 157, 'creation_date': 1453142463, 'user_type': 'registered', 'user_id': 5806856, 'location': 'Toronto, ON, Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/5806856/daniel-visca', 'profile_image': 'https://i.stack.imgur.com/nols0.jpg?s=256&g=1', 'display_name': 'Daniel Visca'}","I am trying to make a mutation to my Shopify store from python. I am new to graphQL, I have been able to make the mutation using graphiQL but I am not certain how to do it directly from my code. This is my make query file, it has worked successfully for a simple query Now an example of the mutation that worked in graphiQL is this: But when I pass it into my make_query function it gives this error How do I fix this? Also one of the mutations I am making uses variables, and I haven't been able to find an example of how to do this directly from my code","`import requests 
 def make_query(self, query, url, headers):
    """"""
    Return query response
    """"""
    request = requests.post(url, json={'query': query}, headers=headers)
    if request.status_code == 200:
        return request.json()
    else:
        raise Exception(""Query failed to run by returning code of {}. {}"".format(request.status_code, query))`
 ""mutation {customerCreate(input: {email: 'wamblamkazam@send22u.info', password: 'password'}) {userErrors { field message}customer{id}}}"" {'errors': [{'message': 'Parse error on ""\'"" (error) at [1, 41]', 'locations': [{'line': 1, 'column': 41}]}]}
",8,28,0,0,
517,49939274,49990241,24956,"How to access the outlook folders other than default ones (like Inbox, Sent) using python win32com?",1,<python><outlook><win32com>,11,"<p>This is how I am able to access the inbox:</p>

<pre><code>   outlook = Dispatch(""Outlook.Application"").GetNamespace(""MAPI"")
   inbox = outlook.GetDefaultFolder(""6"")
</code></pre>

<p>When I tried to access the user created folders in Outlook using the below code:</p>

<pre><code>   outlook = Dispatch(""Outlook.Application"").GetNamespace(""MAPI"")
   Folder = outlook.Folders[1]
   print (Folder)
</code></pre>

<p>I got this error:</p>

<pre><code>  raise IndexError(""list index out of range"")

IndexError: list index out of range
</code></pre>

<p>Any help would be appreciated.</p>
",4863767,113,20-04-2018 10:13,23-04-2018 21:26,3,113,5,1,1,,"{'badge_counts': {'bronze': 5, 'silver': 1, 'gold': 1}, 'account_id': 6254820, 'is_employee': False, 'last_modified_date': 1573679646, 'last_access_date': 1530558440, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 113, 'creation_date': 1430768966, 'user_type': 'registered', 'user_id': 4863767, 'link': 'https://stackoverflow.com/users/4863767/soldy', 'profile_image': 'https://lh5.googleusercontent.com/-DvN7jNrFs7k/AAAAAAAAAAI/AAAAAAAAAI4/Ek5XohIJMuY/photo.jpg?sz=256', 'display_name': 'soldy'}",This is how I am able to access the inbox: When I tried to access the user created folders in Outlook using the below code: I got this error: Any help would be appreciated.,"   outlook = Dispatch(""Outlook.Application"").GetNamespace(""MAPI"")
   inbox = outlook.GetDefaultFolder(""6"")
    outlook = Dispatch(""Outlook.Application"").GetNamespace(""MAPI"")
   Folder = outlook.Folders[1]
   print (Folder)
   raise IndexError(""list index out of range"")

IndexError: list index out of range
",5,21,0,0,
518,48375753,48375935,899,Why are chained operator expressions slower than their expanded equivalent?,2,<python><comparison><python-internals>,17,"<p>In python, it is possible to <a href=""https://docs.python.org/3/reference/expressions.html#comparisons"" rel=""noreferrer"">chain operators</a> in this manner:</p>

<pre><code>a op b op c
</code></pre>

<p>Which is evaluated to </p>

<pre><code>a op b and b op c 
</code></pre>

<p>With the only difference being that <code>b</code> is evaluated only once (so, something more like <code>t = eval(b); a op t and t op c</code>).</p>

<p>This is advantageous from the view point that it is <em>very readable</em> and more concise than the equivalent version with explicit conjunction (using <code>and</code>). </p>

<p>However... I've noticed that there is a minor performance difference between chained expressions and the equivalent, be it for 3 operands or 20. This becomes apparent when you time these operations.</p>

<pre><code>import timeit 

timeit.timeit(""a &lt;= b &lt;= c"", setup=""a,b,c=1,2,3"")
0.1086414959972899

timeit.timeit(""a &lt;= b and b &lt;= c"", setup=""a,b,c=1,2,3"")
0.09434155100097996
</code></pre>

<p>And,</p>

<pre><code>timeit.timeit(""a &lt;= b &lt;= c &lt;= d &lt;= e &lt;= f"", setup=""a,b,c,d,e,f=1,2,3,4,5,6"")
0.2151330839988077

timeit.timeit(""a &lt;= b and b &lt;= c and c &lt;= d and d &lt;= e and e &lt;= f"", setup=""a,b,c,d,e,f=1,2,3,4,5,6"")
0.19196406500122976
</code></pre>

<p>Note: <em>All tests were done with Python-3.4</em>.</p>

<p>Examining the byte code for both expressions, I noticed that one performs significantly more (actually, 4 more) operations than the other.</p>

<pre><code>import dis

dis.dis(""a &lt;= b &lt;= c"")
  1           0 LOAD_NAME                0 (a)
              3 LOAD_NAME                1 (b)
              6 DUP_TOP
              7 ROT_THREE
              8 COMPARE_OP               1 (&lt;=)
             11 JUMP_IF_FALSE_OR_POP    21
             14 LOAD_NAME                2 (c)
             17 COMPARE_OP               1 (&lt;=)
             20 RETURN_VALUE
        &gt;&gt;   21 ROT_TWO
             22 POP_TOP
             23 RETURN_VALUE 
</code></pre>

<p>Contrast this with,</p>

<pre><code>dis.dis(""a &lt;= b and b &lt;= c"")
  1           0 LOAD_NAME                0 (a)
              3 LOAD_NAME                1 (b)
              6 COMPARE_OP               1 (&lt;=)
              9 JUMP_IF_FALSE_OR_POP    21
             12 LOAD_NAME                1 (b)
             15 LOAD_NAME                2 (c)
             18 COMPARE_OP               1 (&lt;=)
        &gt;&gt;   21 RETURN_VALUE
</code></pre>

<p>I am not experienced with reading byte code, but the first code snippet <em>definitely</em> performs more operations at the byte code level than the second.</p>

<p>Here's how I've interpreted this. In the first case, variables are pushed onto some sort of stack, and popped successively for comparison. All variables are popped only once. In the second case, there is no stack, but at least (N - 2) of the operands have to be loaded into memory twice for comparison. It appears the stack popping operation is more expensive than loading (N - 2) variables twice for comparison, accounting for the speed difference.</p>

<p>In a nutshell, I'm trying to understand why one operation is always slower than the other by a constant factor. Is my hypothesis correct? Or is there something more to the python internals I'm missing? </p>

<hr>

<p>More benchmarks:</p>

<pre><code>| System | a &lt;= b &lt;= c         | a &lt;= b and b &lt;= c   | a &lt;= b &lt;= ... &lt;= e &lt;= f | a &lt;= b and ... and e &lt;= f | Credit         |
|--------|---------------------|---------------------|-------------------------|---------------------------|----------------|
| 3.4    | 0.1086414959972899  | 0.09434155100097996 | 0.2151330839988077      | 0.19196406500122976       | @cᴏʟᴅsᴘᴇᴇᴅ     |
| 3.6.2  | 0.06788300536572933 | 0.059271858073771   | 0.1505890181288123      | 0.12044331897050142       | @Bailey Parker |
| 2.7.10 | 0.05009198188781738 | 0.04472208023071289 | 0.11113405227661133     | 0.09062719345092773       | @Bailey Parker |
</code></pre>
",4909087,390235,22-01-2018 06:29,22-01-2018 06:43,0,391021,775,102,720,97,"{'badge_counts': {'bronze': 775, 'silver': 720, 'gold': 102}, 'collectives': [{'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 6321039, 'is_employee': False, 'last_modified_date': 1709572500, 'last_access_date': 1710735667, 'reputation_change_year': 4488, 'reputation_change_quarter': 4488, 'reputation_change_month': 1116, 'reputation_change_week': 416, 'reputation_change_day': 30, 'reputation': 391021, 'creation_date': 1431873091, 'user_type': 'registered', 'user_id': 4909087, 'accept_rate': 97, 'location': 'At an ipython shell near you', 'website_url': '', 'link': 'https://stackoverflow.com/users/4909087/this-be-shiva', 'profile_image': 'https://i.stack.imgur.com/Zcszo.png?s=256&g=1', 'display_name': 'this be Shiva'}","In python, it is possible to chain operators in this manner: Which is evaluated to With the only difference being that is evaluated only once (so, something more like ). This is advantageous from the view point that it is very readable and more concise than the equivalent version with explicit conjunction (using ). However... I've noticed that there is a minor performance difference between chained expressions and the equivalent, be it for 3 operands or 20. This becomes apparent when you time these operations. And, Note: All tests were done with Python-3.4. Examining the byte code for both expressions, I noticed that one performs significantly more (actually, 4 more) operations than the other. Contrast this with, I am not experienced with reading byte code, but the first code snippet definitely performs more operations at the byte code level than the second. Here's how I've interpreted this. In the first case, variables are pushed onto some sort of stack, and popped successively for comparison. All variables are popped only once. In the second case, there is no stack, but at least (N - 2) of the operands have to be loaded into memory twice for comparison. It appears the stack popping operation is more expensive than loading (N - 2) variables twice for comparison, accounting for the speed difference. In a nutshell, I'm trying to understand why one operation is always slower than the other by a constant factor. Is my hypothesis correct? Or is there something more to the python internals I'm missing? More benchmarks:","a op b op c
 a op b and b op c 
 b t = eval(b); a op t and t op c and import timeit 

timeit.timeit(""a &lt;= b &lt;= c"", setup=""a,b,c=1,2,3"")
0.1086414959972899

timeit.timeit(""a &lt;= b and b &lt;= c"", setup=""a,b,c=1,2,3"")
0.09434155100097996
 timeit.timeit(""a &lt;= b &lt;= c &lt;= d &lt;= e &lt;= f"", setup=""a,b,c,d,e,f=1,2,3,4,5,6"")
0.2151330839988077

timeit.timeit(""a &lt;= b and b &lt;= c and c &lt;= d and d &lt;= e and e &lt;= f"", setup=""a,b,c,d,e,f=1,2,3,4,5,6"")
0.19196406500122976
 import dis

dis.dis(""a &lt;= b &lt;= c"")
  1           0 LOAD_NAME                0 (a)
              3 LOAD_NAME                1 (b)
              6 DUP_TOP
              7 ROT_THREE
              8 COMPARE_OP               1 (&lt;=)
             11 JUMP_IF_FALSE_OR_POP    21
             14 LOAD_NAME                2 (c)
             17 COMPARE_OP               1 (&lt;=)
             20 RETURN_VALUE
        &gt;&gt;   21 ROT_TWO
             22 POP_TOP
             23 RETURN_VALUE 
 dis.dis(""a &lt;= b and b &lt;= c"")
  1           0 LOAD_NAME                0 (a)
              3 LOAD_NAME                1 (b)
              6 COMPARE_OP               1 (&lt;=)
              9 JUMP_IF_FALSE_OR_POP    21
             12 LOAD_NAME                1 (b)
             15 LOAD_NAME                2 (c)
             18 COMPARE_OP               1 (&lt;=)
        &gt;&gt;   21 RETURN_VALUE
 | System | a &lt;= b &lt;= c         | a &lt;= b and b &lt;= c   | a &lt;= b &lt;= ... &lt;= e &lt;= f | a &lt;= b and ... and e &lt;= f | Credit         |
|--------|---------------------|---------------------|-------------------------|---------------------------|----------------|
| 3.4    | 0.1086414959972899  | 0.09434155100097996 | 0.2151330839988077      | 0.19196406500122976       | @cᴏʟᴅsᴘᴇᴇᴅ     |
| 3.6.2  | 0.06788300536572933 | 0.059271858073771   | 0.1505890181288123      | 0.12044331897050142       | @Bailey Parker |
| 2.7.10 | 0.05009198188781738 | 0.04472208023071289 | 0.11113405227661133     | 0.09062719345092773       | @Bailey Parker |
",33,84,0,1,
519,49721089,49721133,82687,Django viewset has not attribute 'get_extra_actions',8,<python><django><python-3.x><django-rest-framework>,47,"<p>I am working with Django for a first time and I'm trying to build an API and I am following some tutorials and examples and it works right, but I am running the project now in a Raspberry Pi after install all the requirements and the project is failing with the following error:</p>

<pre><code>    Performing system checks...

Unhandled exception in thread started by &lt;function check_errors.&lt;locals&gt;.wrapper at 0xb547adb0&gt;
Traceback (most recent call last):
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/autoreload.py"", line 225, in wrapper
    fn(*args, **kwargs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/commands/runserver.py"", line 120, in inner_run
    self.check(display_num_errors=True)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/base.py"", line 364, in check
    include_deployment_checks=include_deployment_checks,
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/base.py"", line 351, in _run_checks
    return checks.run_checks(**kwargs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/registry.py"", line 73, in run_checks
    new_errors = check(app_configs=app_configs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/urls.py"", line 13, in check_url_config
    return check_resolver(resolver)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/urls.py"", line 23, in check_resolver
    return check_method()
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 397, in check
    for pattern in self.url_patterns:
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/functional.py"", line 36, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 536, in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/functional.py"", line 36, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 529, in urlconf_module
    return import_module(self.urlconf_name)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 673, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""/home/pi/Projects/openvpn-monitor/openvpnmonitor/urls.py"", line 24, in &lt;module&gt;
    url(r'^api/', include('openvpnmonitor.api.urls')),
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/conf.py"", line 34, in include
    urlconf_module = import_module(urlconf_module)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 673, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""/home/pi/Projects/openvpn-monitor/openvpnmonitor/api/urls.py"", line 16, in &lt;module&gt;
    urlpatterns += router.urls
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 101, in urls
    self._urls = self.get_urls()
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 363, in get_urls
    urls = super(DefaultRouter, self).get_urls()
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 261, in get_urls
    routes = self.get_routes(viewset)
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 176, in get_routes
    extra_actions = viewset.get_extra_actions()
AttributeError: type object 'SessionViewSet' has no attribute 'get_extra_actions'
</code></pre>

<p>My views.py has the following code:</p>

<pre><code>from django.shortcuts import render

from rest_framework import viewsets
from .models import Session
from .serializers import SessionSerializer

from rest_framework.views import APIView, Response


class SessionViewSet(APIView):
    queryset = Session.objects.all()
    serializer_class = SessionSerializer

    def get(self, request, format=None):
        return Response(""test"")
</code></pre>

<p>I really don't know why is working on my laptop but it is not working on my Raspberry Pi.</p>

<p>Has this happened to someone or anyone knows why is happening this?</p>

<p>Thank you so much!</p>

<p>Edit:</p>

<p>Here is my urls.py</p>

<pre><code>from django.conf.urls import url
from rest_framework import routers
from openvpnmonitor.api.views import SessionViewSet

router = routers.DefaultRouter()
router.register(r'sessions', SessionViewSet)

urlpatterns = [
    url(r'sessions', SessionViewSet.as_view()),
    url(r'^docs/', schema_view),
]

urlpatterns += router.urls
</code></pre>
",4990714,529,08-04-2018 18:09,08-04-2018 18:12,0,529,11,1,5,,"{'badge_counts': {'bronze': 11, 'silver': 5, 'gold': 1}, 'account_id': 6440586, 'is_employee': False, 'last_modified_date': 1573679586, 'last_access_date': 1523219282, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 529, 'creation_date': 1433856996, 'user_type': 'registered', 'user_id': 4990714, 'link': 'https://stackoverflow.com/users/4990714/alexca', 'profile_image': 'https://www.gravatar.com/avatar/0e4a10937179c2bc1b22a0073f18445d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'alexca'}","I am working with Django for a first time and I'm trying to build an API and I am following some tutorials and examples and it works right, but I am running the project now in a Raspberry Pi after install all the requirements and the project is failing with the following error: My views.py has the following code: I really don't know why is working on my laptop but it is not working on my Raspberry Pi. Has this happened to someone or anyone knows why is happening this? Thank you so much! Edit: Here is my urls.py","    Performing system checks...

Unhandled exception in thread started by &lt;function check_errors.&lt;locals&gt;.wrapper at 0xb547adb0&gt;
Traceback (most recent call last):
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/autoreload.py"", line 225, in wrapper
    fn(*args, **kwargs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/commands/runserver.py"", line 120, in inner_run
    self.check(display_num_errors=True)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/base.py"", line 364, in check
    include_deployment_checks=include_deployment_checks,
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/base.py"", line 351, in _run_checks
    return checks.run_checks(**kwargs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/registry.py"", line 73, in run_checks
    new_errors = check(app_configs=app_configs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/urls.py"", line 13, in check_url_config
    return check_resolver(resolver)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/urls.py"", line 23, in check_resolver
    return check_method()
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 397, in check
    for pattern in self.url_patterns:
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/functional.py"", line 36, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 536, in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/functional.py"", line 36, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 529, in urlconf_module
    return import_module(self.urlconf_name)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 673, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""/home/pi/Projects/openvpn-monitor/openvpnmonitor/urls.py"", line 24, in &lt;module&gt;
    url(r'^api/', include('openvpnmonitor.api.urls')),
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/conf.py"", line 34, in include
    urlconf_module = import_module(urlconf_module)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 673, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""/home/pi/Projects/openvpn-monitor/openvpnmonitor/api/urls.py"", line 16, in &lt;module&gt;
    urlpatterns += router.urls
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 101, in urls
    self._urls = self.get_urls()
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 363, in get_urls
    urls = super(DefaultRouter, self).get_urls()
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 261, in get_urls
    routes = self.get_routes(viewset)
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 176, in get_routes
    extra_actions = viewset.get_extra_actions()
AttributeError: type object 'SessionViewSet' has no attribute 'get_extra_actions'
 from django.shortcuts import render

from rest_framework import viewsets
from .models import Session
from .serializers import SessionSerializer

from rest_framework.views import APIView, Response


class SessionViewSet(APIView):
    queryset = Session.objects.all()
    serializer_class = SessionSerializer

    def get(self, request, format=None):
        return Response(""test"")
 from django.conf.urls import url
from rest_framework import routers
from openvpnmonitor.api.views import SessionViewSet

router = routers.DefaultRouter()
router.register(r'sessions', SessionViewSet)

urlpatterns = [
    url(r'sessions', SessionViewSet.as_view()),
    url(r'^docs/', schema_view),
]

urlpatterns += router.urls
",84,106,0,0,
520,49997934,49998338,16450,Change color of specific ticks at plot with matplotlib,1,<python><matplotlib>,22,"<p>Using matplotlib, is there an option to change the color of specific <strong>tick labels</strong> on the axis?</p>

<p>I have a simple plot that show some values by days, and I need to mark some days as 'special' day so I want to mark these with a different color but not all ticks just some specific.</p>
",5040234,513,24-04-2018 09:22,24-04-2018 09:41,0,513,14,1,5,43,"{'badge_counts': {'bronze': 14, 'silver': 5, 'gold': 1}, 'account_id': 5460652, 'is_employee': False, 'last_modified_date': 1663248900, 'last_access_date': 1710765931, 'reputation_change_year': 2, 'reputation_change_quarter': 2, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 513, 'creation_date': 1435062440, 'user_type': 'registered', 'user_id': 5040234, 'accept_rate': 43, 'link': 'https://stackoverflow.com/users/5040234/ron', 'profile_image': 'https://www.gravatar.com/avatar/0f382036c3a8ad3ed3afd1913159f4be?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ron'}","Using matplotlib, is there an option to change the color of specific tick labels on the axis? I have a simple plot that show some values by days, and I need to mark some days as 'special' day so I want to mark these with a different color but not all ticks just some specific.",,0,3,0,0,
521,49621169,49638608,9435,joblib.load __main__ AttributeError,2,<python><machine-learning><flask><pickle>,15,"<p>I'm starting to dive into deploying a predictive model to a web app using Flask, and unfortunately getting stuck at the starting gate. </p>

<p><strong>What I did:</strong></p>

<p>I pickled my model in my <strong>model.py</strong> program:</p>

<pre><code>import numpy as np
from sklearn.externals import joblib

class NeuralNetwork():
    """"""
    Two (hidden) layer neural network model. 
    First and second layer contain the same number of hidden units
    """"""
    def __init__(self, input_dim, units, std=0.0001):
        self.params = {}
        self.input_dim = input_dim

        self.params['W1'] = np.random.rand(self.input_dim, units)
        self.params['W1'] *= std
        self.params['b1'] = np.zeros((units))

        self.params['W2'] = np.random.rand(units, units)
        self.params['W2'] *= std * 10  # Compensate for vanishing gradients
        self.params['b2'] = np.zeros((units))

        self.params['W3'] = np.random.rand(units, 1)
        self.params['b3'] = np.zeros((1,))

model = NeuralNetwork(input_dim=12, units=64)

#####THIS RIGHT HERE ##############
joblib.dump(model, 'demo_model.pkl')
</code></pre>

<p>then I created an <strong>api.py</strong> file in the same directory as my <em>demo_model.pkl</em>, per this tutorial (<a href=""https://blog.hyperiondev.com/index.php/2018/02/01/deploy-machine-learning-models-flask-api/"" rel=""noreferrer"">https://blog.hyperiondev.com/index.php/2018/02/01/deploy-machine-learning-models-flask-api/</a>):</p>

<pre><code>import flask
from flask import Flask, render_template, request
from sklearn.externals import joblib

app = Flask(__name__)


@app.route(""/"")
@app.route(""/index"")
def index():
    return flask.render_template('index.html')


# create endpoint for the predictions (HTTP POST requests)
@app.route('/predict', methods=['POST'])
def make_prediction():
    if request.method == 'POST':
        return render_template('index.html', label='3')


if __name__ == '__main__':
    # LOAD MODEL WHEN APP RUNS ####
    model = joblib.load('demo_model.pkl')
    app.run(host='0.0.0.0', port=8000, debug=True)
</code></pre>

<p>I also made a templates/index.html file in the same directory with this info:</p>

<pre><code>&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;NN Model as Flask API&lt;/title&gt;
        &lt;meta charset=""utf-8""&gt;
        &lt;meta name=""viewport"" content=""width=device-width, initial-scale=1""&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;h1&gt;Boston Housing Price Predictor&lt;/h1&gt;
        &lt;form action=""/predict"" method=""post"" enctype=""multipart/form-data""&gt;
            &lt;input type=""file"" name=""image"" value=""Upload""&gt;
            &lt;input type=""submit"" value=""Predict""&gt; {% if label %} {{ label }} {% endif %}
        &lt;/form&gt;
    &lt;/body&gt;

&lt;/html&gt;
</code></pre>

<p>running:</p>

<pre><code>&gt;&gt; python api.py
</code></pre>

<p>gives me an error with the pickler:</p>

<pre><code>Traceback (most recent call last):
  File ""api.py"", line 22, in &lt;module&gt;
    model = joblib.load('model.pkl')
  File ""C:\Users\joshu\Anaconda3\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py"", line 578, in load
    obj = _unpickle(fobj, filename, mmap_mode)
  File ""C:\Users\joshu\Anaconda3\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py"", line 508, in _unpickle
    obj = unpickler.load()
  File ""C:\Users\joshu\Anaconda3\lib\pickle.py"", line 1043, in load
    dispatch[key[0]](self)
  File ""C:\Users\joshu\Anaconda3\lib\pickle.py"", line 1342, in load_global
    klass = self.find_class(module, name)
  File ""C:\Users\joshu\Anaconda3\lib\pickle.py"", line 1396, in find_class
    return getattr(sys.modules[module], name)
AttributeError: module '__main__' has no attribute 'NeuralNetwork'
</code></pre>

<p>Why is the main module of the program getting involved with my NeuralNetwork model? I'm very confused at the moment... any advice would be appreciated.</p>

<p><strong>UPDATE:</strong></p>

<p>Adding a class definition <code>class NeuralNetwork(object): pass</code> to my <strong>api.py</strong> program fixed the bug. </p>

<pre><code>import flask
from flask import Flask, render_template, request
from sklearn.externals import joblib


class NeuralNetwork(object):
    pass


app = Flask(__name__)
</code></pre>

<p><strong><em>If anyone would be willing to offer me an explanation of what was going on that would be hugely appreciated!</em></strong> </p>
",5040920,1365,03-04-2018 02:02,03-04-2018 20:27,0,1365,32,4,17,45,"{'badge_counts': {'bronze': 32, 'silver': 17, 'gold': 4}, 'account_id': 6514713, 'is_employee': False, 'last_modified_date': 1607614549, 'last_access_date': 1710977353, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1365, 'creation_date': 1435071754, 'user_type': 'registered', 'user_id': 5040920, 'accept_rate': 45, 'location': 'Boston, MA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/5040920/joshua-zastrow', 'profile_image': 'https://www.gravatar.com/avatar/d9af8091899dd8fb5e3d30234cebcf7c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Joshua Zastrow'}","I'm starting to dive into deploying a predictive model to a web app using Flask, and unfortunately getting stuck at the starting gate. What I did: I pickled my model in my model.py program: then I created an api.py file in the same directory as my demo_model.pkl, per this tutorial (https://blog.hyperiondev.com/index.php/2018/02/01/deploy-machine-learning-models-flask-api/): I also made a templates/index.html file in the same directory with this info: running: gives me an error with the pickler: Why is the main module of the program getting involved with my NeuralNetwork model? I'm very confused at the moment... any advice would be appreciated. UPDATE: Adding a class definition to my api.py program fixed the bug. If anyone would be willing to offer me an explanation of what was going on that would be hugely appreciated!","import numpy as np
from sklearn.externals import joblib

class NeuralNetwork():
    """"""
    Two (hidden) layer neural network model. 
    First and second layer contain the same number of hidden units
    """"""
    def __init__(self, input_dim, units, std=0.0001):
        self.params = {}
        self.input_dim = input_dim

        self.params['W1'] = np.random.rand(self.input_dim, units)
        self.params['W1'] *= std
        self.params['b1'] = np.zeros((units))

        self.params['W2'] = np.random.rand(units, units)
        self.params['W2'] *= std * 10  # Compensate for vanishing gradients
        self.params['b2'] = np.zeros((units))

        self.params['W3'] = np.random.rand(units, 1)
        self.params['b3'] = np.zeros((1,))

model = NeuralNetwork(input_dim=12, units=64)

#####THIS RIGHT HERE ##############
joblib.dump(model, 'demo_model.pkl')
 import flask
from flask import Flask, render_template, request
from sklearn.externals import joblib

app = Flask(__name__)


@app.route(""/"")
@app.route(""/index"")
def index():
    return flask.render_template('index.html')


# create endpoint for the predictions (HTTP POST requests)
@app.route('/predict', methods=['POST'])
def make_prediction():
    if request.method == 'POST':
        return render_template('index.html', label='3')


if __name__ == '__main__':
    # LOAD MODEL WHEN APP RUNS ####
    model = joblib.load('demo_model.pkl')
    app.run(host='0.0.0.0', port=8000, debug=True)
 &lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;NN Model as Flask API&lt;/title&gt;
        &lt;meta charset=""utf-8""&gt;
        &lt;meta name=""viewport"" content=""width=device-width, initial-scale=1""&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;h1&gt;Boston Housing Price Predictor&lt;/h1&gt;
        &lt;form action=""/predict"" method=""post"" enctype=""multipart/form-data""&gt;
            &lt;input type=""file"" name=""image"" value=""Upload""&gt;
            &lt;input type=""submit"" value=""Predict""&gt; {% if label %} {{ label }} {% endif %}
        &lt;/form&gt;
    &lt;/body&gt;

&lt;/html&gt;
 &gt;&gt; python api.py
 Traceback (most recent call last):
  File ""api.py"", line 22, in &lt;module&gt;
    model = joblib.load('model.pkl')
  File ""C:\Users\joshu\Anaconda3\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py"", line 578, in load
    obj = _unpickle(fobj, filename, mmap_mode)
  File ""C:\Users\joshu\Anaconda3\lib\site-packages\sklearn\externals\joblib\numpy_pickle.py"", line 508, in _unpickle
    obj = unpickler.load()
  File ""C:\Users\joshu\Anaconda3\lib\pickle.py"", line 1043, in load
    dispatch[key[0]](self)
  File ""C:\Users\joshu\Anaconda3\lib\pickle.py"", line 1342, in load_global
    klass = self.find_class(module, name)
  File ""C:\Users\joshu\Anaconda3\lib\pickle.py"", line 1396, in find_class
    return getattr(sys.modules[module], name)
AttributeError: module '__main__' has no attribute 'NeuralNetwork'
 class NeuralNetwork(object): pass import flask
from flask import Flask, render_template, request
from sklearn.externals import joblib


class NeuralNetwork(object):
    pass


app = Flask(__name__)
",84,124,0,1,
522,49070242,49070833,79534,Converting images to csv file in python,5,<python><image><csv>,11,"<p>I have converted my image into a csv file and it's like a matrix but I want  it to be a single row.
How can I convert all of the images in dataset into a csv file (each image into one line).</p>

<p>Here's the code I've used:</p>

<pre><code>from PIL import Image
import numpy as np
import os, os.path, time

format='.jpg'
myDir = ""Lotus1""
def createFileList(myDir, format='.jpg'):
    fileList = []
    print(myDir)
    for root, dirs, files in os.walk(myDir, topdown=False):
            for name in files:
               if name.endswith(format):
                  fullName = os.path.join(root, name)
                  fileList.append(fullName)
                  return fileList

fileList = createFileList(myDir)
fileFormat='.jpg'
for fileFormat in fileList:
 format = '.jpg'
 # get original image parameters...
 width, height = fileList.size
 format = fileList.format
 mode = fileList.mode
 # Make image Greyscale
 img_grey = fileList.convert('L')
 # Save Greyscale values
 value = np.asarray(fileList.getdata(),dtype=np.float64).reshape((fileList.size[1],fileList.size[0]))
 np.savetxt(""img_pixels.csv"", value, delimiter=',')
</code></pre>

<p>input :
<a href=""http://uupload.ir/files/pto0_lotus1_1.jpg"" rel=""noreferrer"">http://uupload.ir/files/pto0_lotus1_1.jpg</a></p>

<p>output:<a href=""http://uupload.ir/files/huwh_output.png"" rel=""noreferrer"">http://uupload.ir/files/huwh_output.png</a></p>
",5113137,159,02-03-2018 13:38,02-03-2018 14:09,0,159,16,2,2,60,"{'badge_counts': {'bronze': 16, 'silver': 2, 'gold': 2}, 'account_id': 6621675, 'is_employee': False, 'last_modified_date': 1619230501, 'last_access_date': 1690363973, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 159, 'creation_date': 1436832563, 'user_type': 'registered', 'user_id': 5113137, 'accept_rate': 60, 'website_url': '', 'link': 'https://stackoverflow.com/users/5113137/nebula', 'profile_image': 'https://www.gravatar.com/avatar/cb7923de24b4571c0bf7effac7d6729c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nebula'}",I have converted my image into a csv file and it's like a matrix but I want it to be a single row. How can I convert all of the images in dataset into a csv file (each image into one line). Here's the code I've used: input : http://uupload.ir/files/pto0_lotus1_1.jpg output:http://uupload.ir/files/huwh_output.png,"from PIL import Image
import numpy as np
import os, os.path, time

format='.jpg'
myDir = ""Lotus1""
def createFileList(myDir, format='.jpg'):
    fileList = []
    print(myDir)
    for root, dirs, files in os.walk(myDir, topdown=False):
            for name in files:
               if name.endswith(format):
                  fullName = os.path.join(root, name)
                  fileList.append(fullName)
                  return fileList

fileList = createFileList(myDir)
fileFormat='.jpg'
for fileFormat in fileList:
 format = '.jpg'
 # get original image parameters...
 width, height = fileList.size
 format = fileList.format
 mode = fileList.mode
 # Make image Greyscale
 img_grey = fileList.convert('L')
 # Save Greyscale values
 value = np.asarray(fileList.getdata(),dtype=np.float64).reshape((fileList.size[1],fileList.size[0]))
 np.savetxt(""img_pixels.csv"", value, delimiter=',')
",28,40,0,2,
523,49073799,49087269,56351,PyTorch: Testing with torchvision.datasets.ImageFolder and DataLoader,2,<python><pytorch>,19,"<p>I'm a newbie trying to make this PyTorch CNN work with the <a href=""https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/"" rel=""noreferrer"">Cats&amp;Dogs dataset from kaggle</a>. As there are no targets for the test images, I manually classified some of the test images and put the class in the filename, to be able to test (maybe should have just used some of the train images).</p>

<p>I used the torchvision.datasets.ImageFolder class to load the train and test images. The training seems to work.</p>

<p>But what do I need to do to make the test-routine work? I don't know, how to connect my test_data_loader with the test loop at the bottom, via test_x and test_y. </p>

<p>The Code is based on <a href=""https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py"" rel=""noreferrer"">this MNIST example CNN.</a> There, something like this is used right after the loaders are created. But I failed to rewrite it for my dataset:</p>

<pre><code>test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)
test_y = test_data.test_labels[:2000]
</code></pre>

<p>The Code:</p>

<pre><code>import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torch.utils.data as data
import torchvision
from torchvision import transforms

EPOCHS = 2
BATCH_SIZE = 10
LEARNING_RATE = 0.003
TRAIN_DATA_PATH = ""./train_cl/""
TEST_DATA_PATH = ""./test_named_cl/""
TRANSFORM_IMG = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225] )
    ])

train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM_IMG)
train_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)
test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM_IMG)
test_data_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) 

class CNN(nn.Module):
    # omitted...

if __name__ == '__main__':

    print(""Number of train samples: "", len(train_data))
    print(""Number of test samples: "", len(test_data))
    print(""Detected Classes are: "", train_data.class_to_idx) # classes are detected by folder structure

    model = CNN()    
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_func = nn.CrossEntropyLoss()    

    # Training and Testing
    for epoch in range(EPOCHS):        
        for step, (x, y) in enumerate(train_data_loader):
            b_x = Variable(x)   # batch x (image)
            b_y = Variable(y)   # batch y (target)
            output = model(b_x)[0]          
            loss = loss_func(output, b_y)   
            optimizer.zero_grad()           
            loss.backward()                 
            optimizer.step()

            # Test -&gt; this is where I have no clue
            if step % 50 == 0:
                test_x = Variable(test_data_loader)
                test_output, last_layer = model(test_x)
                pred_y = torch.max(test_output, 1)[1].data.squeeze()
                accuracy = sum(pred_y == test_y) / float(test_y.size(0))
                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)
</code></pre>
",6799476,917,02-03-2018 16:56,03-03-2018 17:47,1,917,22,3,9,,"{'badge_counts': {'bronze': 22, 'silver': 9, 'gold': 3}, 'account_id': 9056890, 'is_employee': False, 'last_modified_date': 1626873600, 'last_access_date': 1650896251, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 917, 'creation_date': 1473152292, 'user_type': 'registered', 'user_id': 6799476, 'location': 'Rostock, Germany', 'link': 'https://stackoverflow.com/users/6799476/kett', 'profile_image': 'https://www.gravatar.com/avatar/c2df7a44ab86f372bbf7c9fb7c124a56?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'kett'}","I'm a newbie trying to make this PyTorch CNN work with the Cats&amp;Dogs dataset from kaggle. As there are no targets for the test images, I manually classified some of the test images and put the class in the filename, to be able to test (maybe should have just used some of the train images). I used the torchvision.datasets.ImageFolder class to load the train and test images. The training seems to work. But what do I need to do to make the test-routine work? I don't know, how to connect my test_data_loader with the test loop at the bottom, via test_x and test_y. The Code is based on this MNIST example CNN. There, something like this is used right after the loaders are created. But I failed to rewrite it for my dataset: The Code:","test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)
test_y = test_data.test_labels[:2000]
 import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torch.utils.data as data
import torchvision
from torchvision import transforms

EPOCHS = 2
BATCH_SIZE = 10
LEARNING_RATE = 0.003
TRAIN_DATA_PATH = ""./train_cl/""
TEST_DATA_PATH = ""./test_named_cl/""
TRANSFORM_IMG = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(256),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225] )
    ])

train_data = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=TRANSFORM_IMG)
train_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)
test_data = torchvision.datasets.ImageFolder(root=TEST_DATA_PATH, transform=TRANSFORM_IMG)
test_data_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) 

class CNN(nn.Module):
    # omitted...

if __name__ == '__main__':

    print(""Number of train samples: "", len(train_data))
    print(""Number of test samples: "", len(test_data))
    print(""Detected Classes are: "", train_data.class_to_idx) # classes are detected by folder structure

    model = CNN()    
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    loss_func = nn.CrossEntropyLoss()    

    # Training and Testing
    for epoch in range(EPOCHS):        
        for step, (x, y) in enumerate(train_data_loader):
            b_x = Variable(x)   # batch x (image)
            b_y = Variable(y)   # batch y (target)
            output = model(b_x)[0]          
            loss = loss_func(output, b_y)   
            optimizer.zero_grad()           
            loss.backward()                 
            optimizer.step()

            # Test -&gt; this is where I have no clue
            if step % 50 == 0:
                test_x = Variable(test_data_loader)
                test_output, last_layer = model(test_x)
                pred_y = torch.max(test_output, 1)[1].data.squeeze()
                accuracy = sum(pred_y == test_y) / float(test_y.size(0))
                print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)
",59,74,0,2,
524,48562383,48897431,23388,sasl/saslwrapper.h:22:23: fatal error: sasl/sasl.h: No such file or directory,2,<python><python-2.7><sasl>,24,"<p>I have OS </p>

<pre><code>Red Hat Enterprise Linux Server release 7.4 (Maipo)
</code></pre>

<p>and python </p>

<pre><code>Python 2.7.13 :: Anaconda 4.4.0 (64-bit)
</code></pre>

<p>Tried to install lib sasl</p>

<p>sudo pip install sasl</p>

<pre><code>Collecting sasl
  Downloading http://repo.com/api/pypi/pypi/packages/8e/2c/45dae93d666aea8492678499e0999269b4e55f1829b1e4de5b8204706ad9/sasl-0.2.1.tar.gz
Collecting six (from sasl)
  Downloading http://repo.com/api/pypi/pypi/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Installing collected packages: six, sasl

    building 'sasl.saslwrapper' extension
    creating build/temp.linux-x86_64-2.7
    creating build/temp.linux-x86_64-2.7/sasl
    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -Isasl -I/usr/include/python2.7 -c sasl/saslwrapper.cpp -o build/temp.linux-x86_64-2.7/sasl/saslwrapper.o
    In file included from sasl/saslwrapper.cpp:254:0:
    sasl/saslwrapper.h:22:23: fatal error: sasl/sasl.h: No such file or directory
     #include &lt;sasl/sasl.h&gt;
                           ^
    compilation terminated.
    error: command 'gcc' failed with exit status 1

    ----------------------------------------
Command ""/usr/bin/python2 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-Ym7ZOA/sasl/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-_8ahws-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-Ym7ZOA/sasl/
</code></pre>

<p>How solve this problem?</p>
",5163984,1584,01-02-2018 12:05,21-02-2018 02:46,20,1584,62,6,35,93,"{'badge_counts': {'bronze': 62, 'silver': 35, 'gold': 6}, 'collectives': [{'collective': {'tags': ['continuous-integration', 'gitlab-ci-runner', 'github-actions', 'jenkins-groovy', 'continuous-delivery', 'jenkins', 'google-cloud-build', 'octopus-deploy', 'jenkins-plugins', 'argocd', 'teamcity', 'circleci', 'continuous-deployment', 'bitbucket-pipelines', 'tfsbuild', 'codemagic', 'hudson', 'cicd', 'azure-pipelines', 'continuous-testing', 'jenkins-pipeline', 'gitlab-ci'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where developers focused on continuous integration, delivery, and deployment can find, share, and learn about simultaneous development.', 'link': '/collectives/ci-cd', 'name': 'CI/CD', 'slug': 'ci-cd'}, 'role': 'member'}], 'account_id': 6695972, 'is_employee': False, 'last_modified_date': 1706925000, 'last_access_date': 1710840246, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1584, 'creation_date': 1438072228, 'user_type': 'registered', 'user_id': 5163984, 'accept_rate': 93, 'location': 'Krasnodar, Krasnodar Krai, Russia', 'website_url': '', 'link': 'https://stackoverflow.com/users/5163984/nikolay-baranenko', 'profile_image': 'https://i.stack.imgur.com/9Yev9.png?s=256&g=1', 'display_name': 'Nikolay Baranenko'}",I have OS and python Tried to install lib sasl sudo pip install sasl How solve this problem?,"Red Hat Enterprise Linux Server release 7.4 (Maipo)
 Python 2.7.13 :: Anaconda 4.4.0 (64-bit)
 Collecting sasl
  Downloading http://repo.com/api/pypi/pypi/packages/8e/2c/45dae93d666aea8492678499e0999269b4e55f1829b1e4de5b8204706ad9/sasl-0.2.1.tar.gz
Collecting six (from sasl)
  Downloading http://repo.com/api/pypi/pypi/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Installing collected packages: six, sasl

    building 'sasl.saslwrapper' extension
    creating build/temp.linux-x86_64-2.7
    creating build/temp.linux-x86_64-2.7/sasl
    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -Isasl -I/usr/include/python2.7 -c sasl/saslwrapper.cpp -o build/temp.linux-x86_64-2.7/sasl/saslwrapper.o
    In file included from sasl/saslwrapper.cpp:254:0:
    sasl/saslwrapper.h:22:23: fatal error: sasl/sasl.h: No such file or directory
     #include &lt;sasl/sasl.h&gt;
                           ^
    compilation terminated.
    error: command 'gcc' failed with exit status 1

    ----------------------------------------
Command ""/usr/bin/python2 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-Ym7ZOA/sasl/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-_8ahws-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-Ym7ZOA/sasl/
",18,36,0,0,
525,50165477,50166667,11703,pytest-mock assert_called_with failed for class function,1,<python><unit-testing><mocking><pytest>,12,"<p>I am planning to use pytest and pytest-mock for validating the Python code. Being a newbie, wrote a sample code to validate the mock on class and seeing failure. I am wondering what went wrong.</p>

<p>src/main.py  </p>

<pre><code>class Main(object):
    def __init__(self, my_var=None):
        self.var = my_var

    def internal_func(self, var=10):
        my_var = var + 20
        return my_var

    def test_func(self):
        val = self.internal_func(20)
        return val + 40
</code></pre>

<p>tests/test_main.py</p>

<pre><code>    import pytest
    from pytest_mock import mocker
    from src.main import Main

    def new_func(cls, *args, **kwargs):
        return 2

    def test_main_mock(mocker):
        mocker.patch.object(Main, 'internal_func')
        val = Main().test_func()
        assert Main.internal_func.assert_called_with(20)
</code></pre>

<p>It fails with the following error</p>

<pre><code>    ======================================================================================== FAILURES ========================================================================================
    _____________________________________________________________________________________ test_main_mock _____________________________________________________________________________________

    mocker = &lt;pytest_mock.MockFixture object at 0x7f34f490d8d0&gt;

        def test_main_mock(mocker):
            mocker.patch.object(Main, 'internal_func')
            main = Main()
            val = main.test_func()
        #    assert val == 80
    &gt;       assert Main.internal_func.assert_called_with(20)
    E       AssertionError: assert None
    E        +  where None = &lt;bound method MagicMock.wrap_assert_called_with of &lt;MagicMock name='internal_func' id='139865418160784'&gt;&gt;(20)
    E        +    where &lt;bound method MagicMock.wrap_assert_called_with of &lt;MagicMock name='internal_func' id='139865418160784'&gt;&gt; = &lt;MagicMock name='internal_func' id='139865418160784'&gt;.assert_called_with
    E        +      where &lt;MagicMock name='internal_func' id='139865418160784'&gt; = Main.internal_func

    tests/test_main.py:13: AssertionError
</code></pre>
",5170563,171,04-05-2018 00:24,04-05-2018 03:25,0,171,7,1,1,,"{'badge_counts': {'bronze': 7, 'silver': 1, 'gold': 1}, 'account_id': 6705398, 'is_employee': False, 'last_modified_date': 1674870600, 'last_access_date': 1707533364, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 171, 'creation_date': 1438192218, 'user_type': 'registered', 'user_id': 5170563, 'website_url': '', 'link': 'https://stackoverflow.com/users/5170563/suri', 'profile_image': 'https://www.gravatar.com/avatar/f32889e1635e1c476f8a31120e842906?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'suri'}","I am planning to use pytest and pytest-mock for validating the Python code. Being a newbie, wrote a sample code to validate the mock on class and seeing failure. I am wondering what went wrong. src/main.py tests/test_main.py It fails with the following error","class Main(object):
    def __init__(self, my_var=None):
        self.var = my_var

    def internal_func(self, var=10):
        my_var = var + 20
        return my_var

    def test_func(self):
        val = self.internal_func(20)
        return val + 40
     import pytest
    from pytest_mock import mocker
    from src.main import Main

    def new_func(cls, *args, **kwargs):
        return 2

    def test_main_mock(mocker):
        mocker.patch.object(Main, 'internal_func')
        val = Main().test_func()
        assert Main.internal_func.assert_called_with(20)
     ======================================================================================== FAILURES ========================================================================================
    _____________________________________________________________________________________ test_main_mock _____________________________________________________________________________________

    mocker = &lt;pytest_mock.MockFixture object at 0x7f34f490d8d0&gt;

        def test_main_mock(mocker):
            mocker.patch.object(Main, 'internal_func')
            main = Main()
            val = main.test_func()
        #    assert val == 80
    &gt;       assert Main.internal_func.assert_called_with(20)
    E       AssertionError: assert None
    E        +  where None = &lt;bound method MagicMock.wrap_assert_called_with of &lt;MagicMock name='internal_func' id='139865418160784'&gt;&gt;(20)
    E        +    where &lt;bound method MagicMock.wrap_assert_called_with of &lt;MagicMock name='internal_func' id='139865418160784'&gt;&gt; = &lt;MagicMock name='internal_func' id='139865418160784'&gt;.assert_called_with
    E        +      where &lt;MagicMock name='internal_func' id='139865418160784'&gt; = Main.internal_func

    tests/test_main.py:13: AssertionError
",36,52,0,0,
526,48148131,48148513,46707,How can we call one route from another route with parameters in Flask?,1,<python><flask>,21,"<p>I am sending a POST request from one form with two inputs to a Flask route.</p>
<pre><code>  &lt;form action = &quot;http://localhost:5000/xyz&quot; method = &quot;POST&quot;&gt;
     &lt;p&gt;x &lt;input type = &quot;text&quot; name = &quot;x&quot; /&gt;&lt;/p&gt;
     &lt;p&gt;y &lt;input type = &quot;text&quot; name = &quot;y&quot; /&gt;&lt;/p&gt;
     &lt;p&gt;&lt;input type = &quot;submit&quot; value = &quot;submit&quot; /&gt;&lt;/p&gt;
  &lt;/form&gt;
</code></pre>
<p>The Flask code is like this.</p>
<pre><code>@app.route('/xyz', methods = ['POST', 'GET'])
def xyz():
    if request.method == 'POST':
       x = request.form[&quot;x&quot;]
       y = request.form[&quot;y&quot;]
       callonemethod(x,y)
    return render_template('index.html', var1=var1, var2=var2)
       #abc(x,y) #can i call abc() like this .i want to call abc() immediately, as it is streaming log of callonemethod(x,y) in console.

@app.route('/abc', methods = ['POST', 'GET'])       
def abc():
    callanothermethod(x,y)
    return render_template('index.html', var1=var3, var2=var4)
    #I want to use that x, y here. also want to call abc() whenever i call xyz()
</code></pre>
<p>How can I call one route from another route with parameters in Flask?</p>
",5188778,422,08-01-2018 10:11,08-01-2018 10:36,0,422,14,1,4,,"{'badge_counts': {'bronze': 14, 'silver': 4, 'gold': 1}, 'account_id': 6733152, 'is_employee': False, 'last_modified_date': 1594477800, 'last_access_date': 1563946444, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 422, 'creation_date': 1438678399, 'user_type': 'registered', 'user_id': 5188778, 'website_url': '', 'link': 'https://stackoverflow.com/users/5188778/purna-ram', 'profile_image': 'https://www.gravatar.com/avatar/33d71060aefe8715b3e4b9e1ea8246cb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'purna ram'}",I am sending a POST request from one form with two inputs to a Flask route. The Flask code is like this. How can I call one route from another route with parameters in Flask?,"  &lt;form action = &quot;http://localhost:5000/xyz&quot; method = &quot;POST&quot;&gt;
     &lt;p&gt;x &lt;input type = &quot;text&quot; name = &quot;x&quot; /&gt;&lt;/p&gt;
     &lt;p&gt;y &lt;input type = &quot;text&quot; name = &quot;y&quot; /&gt;&lt;/p&gt;
     &lt;p&gt;&lt;input type = &quot;submit&quot; value = &quot;submit&quot; /&gt;&lt;/p&gt;
  &lt;/form&gt;
 @app.route('/xyz', methods = ['POST', 'GET'])
def xyz():
    if request.method == 'POST':
       x = request.form[&quot;x&quot;]
       y = request.form[&quot;y&quot;]
       callonemethod(x,y)
    return render_template('index.html', var1=var1, var2=var2)
       #abc(x,y) #can i call abc() like this .i want to call abc() immediately, as it is streaming log of callonemethod(x,y) in console.

@app.route('/abc', methods = ['POST', 'GET'])       
def abc():
    callanothermethod(x,y)
    return render_template('index.html', var1=var3, var2=var4)
    #I want to use that x, y here. also want to call abc() whenever i call xyz()
",17,24,0,0,
527,48345857,48950072,7803,BatchNorm momentum convention PyTorch,1,<python><neural-network><deep-learning><pytorch><batch-normalization>,13,"<p>Is the <a href=""http://pytorch.org/docs/master/_modules/torch/nn/modules/batchnorm.html"" rel=""noreferrer"">batchnorm momentum convention</a> (default=0.1) correct as in other libraries e.g. Tensorflow it seems to usually be 0.9 or 0.99 by default? Or maybe we are just using a different convention?</p>
",6097525,1328,19-01-2018 16:49,23-02-2018 14:28,35,1328,25,1,12,,"{'badge_counts': {'bronze': 25, 'silver': 12, 'gold': 1}, 'account_id': 8091839, 'is_employee': False, 'last_modified_date': 1633755600, 'last_access_date': 1710574603, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1328, 'creation_date': 1458634793, 'user_type': 'registered', 'user_id': 6097525, 'location': 'Berlin, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/6097525/peter554', 'profile_image': 'https://lh4.googleusercontent.com/-VmfJsgdKZts/AAAAAAAAAAI/AAAAAAAAFjs/wxMX_QIPjdk/photo.jpg?sz=256', 'display_name': 'peter554'}",Is the batchnorm momentum convention (default=0.1) correct as in other libraries e.g. Tensorflow it seems to usually be 0.9 or 0.99 by default? Or maybe we are just using a different convention?,,0,1,0,1,
528,48164843,48195425,18651,How to run python in Visual Studio Code as a main module,3,<python><visual-studio-code>,22,"<p>How to run python in Visual Studio Code as a <strong>main module</strong>?</p>

<p>From the command line I would use the <code>-m</code> switch, like</p>

<pre><code>python -m program.py
</code></pre>

<p>I need this to make relative imports work.</p>

<p>Is there something I could add to the <code>launch.json</code> file?</p>

<p>If this isn't possible, I maybe need to do something with <code>runpy</code> <a href=""https://docs.python.org/3/library/runpy.html"" rel=""noreferrer"">see python docs</a>, but it would be nice if vscode can do this.</p>

<p><strong>Edit:</strong></p>

<p>For the moment I use, as a workaround, an extra <code>run.py</code> file which I place outside the package I want to run. Then configure vscode to run that file:</p>

<pre><code>""program"": ""${workspaceRoot}/../run.py""
</code></pre>

<p>From <code>run.py</code> I import the package and call its entry-point function.</p>
",6109688,623,09-01-2018 09:18,10-01-2018 20:19,1,623,10,1,6,75,"{'badge_counts': {'bronze': 10, 'silver': 6, 'gold': 1}, 'account_id': 8108959, 'is_employee': False, 'last_modified_date': 1573679152, 'last_access_date': 1705072040, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 623, 'creation_date': 1458826772, 'user_type': 'registered', 'user_id': 6109688, 'accept_rate': 75, 'link': 'https://stackoverflow.com/users/6109688/pieter-jan-bonestroo', 'profile_image': 'https://www.gravatar.com/avatar/dac712cb171f357a8092b746653e8ba2?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Pieter Jan Bonestroo'}","How to run python in Visual Studio Code as a main module? From the command line I would use the switch, like I need this to make relative imports work. Is there something I could add to the file? If this isn't possible, I maybe need to do something with see python docs, but it would be nice if vscode can do this. Edit: For the moment I use, as a workaround, an extra file which I place outside the package I want to run. Then configure vscode to run that file: From I import the package and call its entry-point function.","-m python -m program.py
 launch.json runpy run.py ""program"": ""${workspaceRoot}/../run.py""
 run.py",-5,21,0,1,
529,49403536,49404185,110365,What does '%% time' mean in python-3?,1,<python><python-3.x>,57,"<p>What does <code>%%time</code> mean in python? I am using python 3 and have some source code containing </p>

<pre><code>%%time
</code></pre>

<p>Does this call the time module? or does it have another function? </p>
",6133730,683,21-03-2018 10:13,21-03-2018 10:43,0,683,8,1,5,,"{'badge_counts': {'bronze': 8, 'silver': 5, 'gold': 1}, 'account_id': 7712308, 'is_employee': False, 'last_modified_date': 1607965588, 'last_access_date': 1600271785, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 683, 'creation_date': 1459328575, 'user_type': 'registered', 'user_id': 6133730, 'location': 'Stockholm, Sweden', 'website_url': 'https://www.kth.se/en/itm/inst/energiteknik/forskning/desa/personnel/clews-personnel/youssef-almulla-1.632506', 'link': 'https://stackoverflow.com/users/6133730/jzf', 'profile_image': 'https://www.gravatar.com/avatar/de352fffe7d87ad1d8e70bc3b7def929?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'JZF'}",What does mean in python? I am using python 3 and have some source code containing Does this call the time module? or does it have another function?,"%%time %%time
",-1,6,0,0,
530,48897265,48897285,6050,How to count longest uninterrupted sequence in pandas,6,<python><pandas>,16,"<p>Let's say I have <code>pd.Series</code> like below</p>

<pre><code>s = pd.Series([False, True, False,True,True,True,False, False])    

0    False
1     True
2    False
3     True
4     True
5     True
6    False
7    False
dtype: bool
</code></pre>

<p>I want to know how long is the longest <code>True</code> sequence, in this example, it is 3.</p>

<p>I tried it in a stupid way.</p>

<pre><code>s_list = s.tolist()
count = 0
max_count = 0
for item in s_list:
    if item:
        count +=1
    else:
        if count&gt;max_count:
            max_count = count
        count = 0
print(max_count)
</code></pre>

<p>It will print <code>3</code>, but in a <code>Series</code> of all <code>True</code>, it will print <code>0</code></p>
",6278334,1066,21-02-2018 02:26,21-02-2018 02:28,0,1066,23,0,12,90,"{'badge_counts': {'bronze': 23, 'silver': 12, 'gold': 0}, 'account_id': 8359332, 'is_employee': False, 'last_modified_date': 1650073500, 'last_access_date': 1710731124, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1066, 'creation_date': 1462120129, 'user_type': 'registered', 'user_id': 6278334, 'accept_rate': 90, 'location': 'Tokyo, 日本', 'website_url': '', 'link': 'https://stackoverflow.com/users/6278334/dawei', 'profile_image': 'https://www.gravatar.com/avatar/1c01ed7ca3702f738a879c2f25eefcbd?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Dawei'}","Let's say I have like below I want to know how long is the longest sequence, in this example, it is 3. I tried it in a stupid way. It will print , but in a of all , it will print","pd.Series s = pd.Series([False, True, False,True,True,True,False, False])    

0    False
1     True
2    False
3     True
4     True
5     True
6    False
7    False
dtype: bool
 True s_list = s.tolist()
count = 0
max_count = 0
for item in s_list:
    if item:
        count +=1
    else:
        if count&gt;max_count:
            max_count = count
        count = 0
print(max_count)
 3 Series True 0",14,33,0,0,
531,48784908,48785220,17634,numpy.unique sort based on counts,1,<python><python-3.x><sorting><numpy><unique>,16,"<p>The <code>numpy.unique</code> function allows to return the counts of unique elements if <code>return_counts</code> is <code>True</code>. Now the returned tuple consists of two arrays one containing the unique elements and the 2nd one containing a count array, both are sorted by the unique elements. Now is there a way to have both sorted according to the counts array instead of the unique elements? I mean I know how to do it the hard way but is there some concise one-liner or lambda functionality for such cases?</p>

<p>Current result:</p>

<pre><code>my_chr_list = [""a"",""a"",""a"", ""b"", ""c"", ""b"",""d"", ""d""]
unique_els, counts = np.unique(my_chr_list, return_counts=True)
print(unique_els, counts)
</code></pre>

<p>Which returns something along the lines of this:</p>

<pre><code>&gt;&gt;&gt; (array(['a', 'b', 'c', 'd'], 
     dtype='&lt;U1'), array([3, 2, 1, 2], dtype=int64))
</code></pre>

<p>However, what I would want to have:</p>

<pre><code>&gt;&gt;&gt; (array(['a', 'b', 'd', 'c'], 
     dtype='&lt;U1'), array([3, 2, 2, 1], dtype=int64))
</code></pre>
",6313007,2152,14-02-2018 10:29,14-02-2018 10:45,0,2152,30,2,17,92,"{'badge_counts': {'bronze': 30, 'silver': 17, 'gold': 2}, 'account_id': 6772956, 'is_employee': False, 'last_modified_date': 1698649004, 'last_access_date': 1711127323, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2152, 'creation_date': 1462846454, 'user_type': 'registered', 'user_id': 6313007, 'accept_rate': 92, 'location': 'Palo Alto, CA, USA', 'website_url': 'https://stackoverflow.com/users/edit/6313007', 'link': 'https://stackoverflow.com/users/6313007/meow', 'profile_image': 'https://i.stack.imgur.com/XsKEK.jpg?s=256&g=1', 'display_name': 'meow'}","The function allows to return the counts of unique elements if is . Now the returned tuple consists of two arrays one containing the unique elements and the 2nd one containing a count array, both are sorted by the unique elements. Now is there a way to have both sorted according to the counts array instead of the unique elements? I mean I know how to do it the hard way but is there some concise one-liner or lambda functionality for such cases? Current result: Which returns something along the lines of this: However, what I would want to have:","numpy.unique return_counts True my_chr_list = [""a"",""a"",""a"", ""b"", ""c"", ""b"",""d"", ""d""]
unique_els, counts = np.unique(my_chr_list, return_counts=True)
print(unique_els, counts)
 &gt;&gt;&gt; (array(['a', 'b', 'c', 'd'], 
     dtype='&lt;U1'), array([3, 2, 1, 2], dtype=int64))
 &gt;&gt;&gt; (array(['a', 'b', 'd', 'c'], 
     dtype='&lt;U1'), array([3, 2, 2, 1], dtype=int64))
",1,20,0,0,
532,48052217,48052347,40261,How to use an async for loop to iterate over a list?,2,<python><python-3.x><asynchronous><iterator><python-asyncio>,30,"<p>So I need to call an <code>async</code> function for all items in a list. This could be a list of URLs and an async function using <code>aiohttp</code> that gets a response back from every URL. Now obviously I cannot do the following:</p>

<pre><code>async for url in ['www.google.com', 'www.youtube.com', 'www.aol.com']:
</code></pre>

<p>I can use a normal for loop but then my code will act synchronously and I lose the benefits and speed of having an <code>async</code> response fetching function.</p>

<p>Is there any way I can convert a list such that the above works? I just need to change the list's <code>__iter__()</code> to a <code>__aiter__()</code> method right? Can this be achieved by subclassing a list? Maybe encapsulating it in a class?</p>
",5390619,945,01-01-2018 18:32,01-01-2018 18:50,0,945,25,1,15,77,"{'badge_counts': {'bronze': 25, 'silver': 15, 'gold': 1}, 'account_id': 7036821, 'is_employee': False, 'last_modified_date': 1686585908, 'last_access_date': 1645745180, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 945, 'creation_date': 1443562121, 'user_type': 'registered', 'user_id': 5390619, 'accept_rate': 77, 'link': 'https://stackoverflow.com/users/5390619/max-smith', 'profile_image': 'https://www.gravatar.com/avatar/15e1c1a26765b00dfbf83825b681acbd?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Max Smith'}",So I need to call an function for all items in a list. This could be a list of URLs and an async function using that gets a response back from every URL. Now obviously I cannot do the following: I can use a normal for loop but then my code will act synchronously and I lose the benefits and speed of having an response fetching function. Is there any way I can convert a list such that the above works? I just need to change the list's to a method right? Can this be achieved by subclassing a list? Maybe encapsulating it in a class?,"async aiohttp async for url in ['www.google.com', 'www.youtube.com', 'www.aol.com']:
 async __iter__() __aiter__()",-5,8,0,0,
533,48088686,48088723,5795,"Remove duplicates from rows and columns (cell) in a dataframe, python",4,<python><pandas><dataframe>,12,"<p>I have two columns with a lot of duplicated items per cell in a dataframe. Something similar to this:</p>

<pre><code>Index   x    y  
  1     1    ec, us, us, gbr, lst
  2     5    ec, us, us, us, us, ec, ec, ec, ec
  3     8    ec, us, us, gbr, lst, lst, lst, lst, gbr
  4     5    ec, ec, ec, us, us, ir, us, ec, ir, ec, ec
  5     7    chn, chn, chn, ec, ec, us, us, gbr, lst
</code></pre>

<p>I need to eliminate all the duplicate items an get a resulting dataframe like this:</p>

<pre><code>Index   x    y  
  1     1    ec, us, gbr, lst
  2     5    ec, us
  3     8    ec, us, gbr,lst
  4     5    ec, us, ir
  5     7    chn, ec, us, gbr, lst
</code></pre>

<p>Thanks!! </p>
",6331008,669,04-01-2018 04:29,04-01-2018 04:34,0,669,24,0,13,41,"{'badge_counts': {'bronze': 24, 'silver': 13, 'gold': 0}, 'account_id': 8437219, 'is_employee': False, 'last_modified_date': 1626484200, 'last_access_date': 1695140190, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 669, 'creation_date': 1463149893, 'user_type': 'registered', 'user_id': 6331008, 'accept_rate': 41, 'website_url': '', 'link': 'https://stackoverflow.com/users/6331008/pastudilloe', 'profile_image': 'https://i.stack.imgur.com/T39oH.gif?s=256&g=1', 'display_name': 'PAstudilloE'}",I have two columns with a lot of duplicated items per cell in a dataframe. Something similar to this: I need to eliminate all the duplicate items an get a resulting dataframe like this: Thanks!!,"Index   x    y  
  1     1    ec, us, us, gbr, lst
  2     5    ec, us, us, us, us, ec, ec, ec, ec
  3     8    ec, us, us, gbr, lst, lst, lst, lst, gbr
  4     5    ec, ec, ec, us, us, ir, us, ec, ir, ec, ec
  5     7    chn, chn, chn, ec, ec, us, us, gbr, lst
 Index   x    y  
  1     1    ec, us, gbr, lst
  2     5    ec, us
  3     8    ec, us, gbr,lst
  4     5    ec, us, ir
  5     7    chn, ec, us, gbr, lst
",10,21,0,0,
534,50327906,50328215,35696,"ImportError: No module named '_tkinter', please install the python3-tk package",2,<python><matplotlib><tkinter>,15,"<p>I've already gone through all the similar questions in this regard and tried the solutions proposed there. But I'm unable to get this error sorted out though my <code>python3-tk</code> package is installed in the proper virtualenv that I'm using for my project.</p>

<p>Though in my project, I don't use tkinter, when i try to run the file, I'm getting the following error related to the <code>_tkinter</code> module.</p>

<blockquote>
  <p>Traceback (most recent call last):<br>
    File ""/usr/lib/python3.5/tkinter/<strong>init</strong>.py"", line 36, in 
      import _tkinter<br>
  ImportError: No module named '_tkinter'</p>
  
  <p>During handling of the above exception, another exception occurred:</p>
  
  <p>Traceback (most recent call last):<br>
    File ""/home/manuelanayantarajeyaraj/PycharmProjects/ChatbotWord2Vec/main.py"", line 2, in 
      from matplotlib import pyplot as plt<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/pyplot.py"", line 115, in 
      _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/<strong>init</strong>.py"", line 62, in pylab_setup
      [backend_name], 0)<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/backend_tkagg.py"", line 4, in 
      from . import tkagg  # Paint image to Tk photo blitter extension.<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/tkagg.py"", line 5, in 
      from six.moves import tkinter as Tk<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 92, in <strong>get</strong>
      result = self._resolve()<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 115, in _resolve
      return _import_module(self.mod)<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 82, in _import_module
      <strong>import</strong>(name)<br>
    File ""/usr/lib/python3.5/tkinter/<strong>init</strong>.py"", line 38, in 
      raise ImportError(str(msg) + ', please install the python3-tk package')<br>
  ImportError: No module named '_tkinter', please install the python3-tk package</p>
</blockquote>

<p>Hence, I navigated to the location of my interpreter and created a virtualenv and installed the <code>python3-tk</code> package using the following</p>

<pre><code>sudo apt-get install python3-tk
</code></pre>

<p>When I checked, all the packages seem to be up to date</p>

<pre><code>Reading package lists... Done
Building dependency tree       
Reading state information... Done
python3-tk is already the newest version (3.6.5-3~16.04.york0.2).
The following packages were automatically installed and are no longer required:
  libappindicator1 libindicator7 libllvm4.0 linux-headers-4.10.0-28
  linux-headers-4.10.0-28-generic linux-headers-4.13.0-36
  linux-headers-4.13.0-36-generic linux-headers-4.13.0-37
  linux-headers-4.13.0-37-generic linux-image-4.10.0-28-generic
  linux-image-4.13.0-36-generic linux-image-4.13.0-37-generic
  linux-image-extra-4.10.0-28-generic linux-image-extra-4.13.0-36-generic
  linux-image-extra-4.13.0-37-generic linux-signed-image-4.10.0-28-generic
  linux-signed-image-4.13.0-36-generic linux-signed-image-4.13.0-37-generic
Use 'sudo apt autoremove' to remove them.
0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.
</code></pre>

<p>But I'm still getting the same import error <code>ImportError: No module named '_tkinter', please install the python3-tk package</code>.</p>

<p>Any suggestions in this regard will be highly appreciated.</p>
",6339494,2664,14-05-2018 10:14,14-05-2018 10:32,0,2664,64,7,35,57,"{'badge_counts': {'bronze': 64, 'silver': 35, 'gold': 7}, 'account_id': 8450525, 'is_employee': False, 'last_modified_date': 1683303000, 'last_access_date': 1709716490, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2664, 'creation_date': 1463381444, 'user_type': 'registered', 'user_id': 6339494, 'accept_rate': 57, 'location': 'Ireland', 'website_url': 'https://scholar.google.com/citations?user=shCQxcUAAAAJ&hl=en', 'link': 'https://stackoverflow.com/users/6339494/nayantara-jeyaraj', 'profile_image': 'https://i.stack.imgur.com/y07QW.jpg?s=256&g=1', 'display_name': 'Nayantara Jeyaraj'}","I've already gone through all the similar questions in this regard and tried the solutions proposed there. But I'm unable to get this error sorted out though my package is installed in the proper virtualenv that I'm using for my project. Though in my project, I don't use tkinter, when i try to run the file, I'm getting the following error related to the module. Traceback (most recent call last): File ""/usr/lib/python3.5/tkinter/init.py"", line 36, in import _tkinter ImportError: No module named '_tkinter' During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/home/manuelanayantarajeyaraj/PycharmProjects/ChatbotWord2Vec/main.py"", line 2, in from matplotlib import pyplot as plt File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/pyplot.py"", line 115, in _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup() File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/init.py"", line 62, in pylab_setup [backend_name], 0) File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/backend_tkagg.py"", line 4, in from . import tkagg # Paint image to Tk photo blitter extension. File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/tkagg.py"", line 5, in from six.moves import tkinter as Tk File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 92, in get result = self._resolve() File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 115, in _resolve return _import_module(self.mod) File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 82, in _import_module import(name) File ""/usr/lib/python3.5/tkinter/init.py"", line 38, in raise ImportError(str(msg) + ', please install the python3-tk package') ImportError: No module named '_tkinter', please install the python3-tk package Hence, I navigated to the location of my interpreter and created a virtualenv and installed the package using the following When I checked, all the packages seem to be up to date But I'm still getting the same import error . Any suggestions in this regard will be highly appreciated.","python3-tk _tkinter python3-tk sudo apt-get install python3-tk
 Reading package lists... Done
Building dependency tree       
Reading state information... Done
python3-tk is already the newest version (3.6.5-3~16.04.york0.2).
The following packages were automatically installed and are no longer required:
  libappindicator1 libindicator7 libllvm4.0 linux-headers-4.10.0-28
  linux-headers-4.10.0-28-generic linux-headers-4.13.0-36
  linux-headers-4.13.0-36-generic linux-headers-4.13.0-37
  linux-headers-4.13.0-37-generic linux-image-4.10.0-28-generic
  linux-image-4.13.0-36-generic linux-image-4.13.0-37-generic
  linux-image-extra-4.10.0-28-generic linux-image-extra-4.13.0-36-generic
  linux-image-extra-4.13.0-37-generic linux-signed-image-4.10.0-28-generic
  linux-signed-image-4.13.0-36-generic linux-signed-image-4.13.0-37-generic
Use 'sudo apt autoremove' to remove them.
0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.
 ImportError: No module named '_tkinter', please install the python3-tk package",10,61,0,0,
535,50278300,50278361,60896,Convert a columns of string to list in pandas,5,<python><string><list><pandas><tuples>,27,"<p>I have a problem with the type of one of my column in a pandas dataframe. Basically the column is saved in a csv file as a string, and I wanna use it as a tuple to be able to convert it in a list of numbers. Following there is a very simple csv:</p>

<pre><code>ID,LABELS
1,""(1.0,2.0,2.0,3.0,3.0,1.0,4.0)""
2,""(1.0,2.0,2.0,3.0,3.0,1.0,4.0)""
</code></pre>

<p>If a load it with the function ""read_csv"" I get a list of strings. I have tried to convert to a list, but I get the list version of a string:</p>

<pre><code>df.LABELS.apply(lambda x: list(x))
</code></pre>

<p>returns:</p>

<pre><code>['(','1','.','0',.,.,.,.,.,'4','.','0',')']
</code></pre>

<p>Any idea on how to be able to do it?</p>

<p>Thank you.</p>
",6394941,1253,10-05-2018 17:23,10-05-2018 17:27,0,1263,40,3,16,100,"{'badge_counts': {'bronze': 40, 'silver': 16, 'gold': 3}, 'account_id': 8531407, 'is_employee': False, 'last_modified_date': 1649868901, 'last_access_date': 1699486523, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1263, 'creation_date': 1464452424, 'user_type': 'registered', 'user_id': 6394941, 'accept_rate': 100, 'location': 'Chicago, IL, USA', 'website_url': 'https://www.linkedin.com/in/guidomuscioni/', 'link': 'https://stackoverflow.com/users/6394941/guido-muscioni', 'profile_image': 'https://lh6.googleusercontent.com/-c87IOIWr0QU/AAAAAAAAAAI/AAAAAAAAB-g/i2VM2saO040/photo.jpg?sz=256', 'display_name': 'Guido Muscioni'}","I have a problem with the type of one of my column in a pandas dataframe. Basically the column is saved in a csv file as a string, and I wanna use it as a tuple to be able to convert it in a list of numbers. Following there is a very simple csv: If a load it with the function ""read_csv"" I get a list of strings. I have tried to convert to a list, but I get the list version of a string: returns: Any idea on how to be able to do it? Thank you.","ID,LABELS
1,""(1.0,2.0,2.0,3.0,3.0,1.0,4.0)""
2,""(1.0,2.0,2.0,3.0,3.0,1.0,4.0)""
 df.LABELS.apply(lambda x: list(x))
 ['(','1','.','0',.,.,.,.,.,'4','.','0',')']
",2,20,0,0,
536,49493482,49493530,22729,Numpy np.multiply vs *-Operator,2,<python><numpy>,30,"<p>Is there any difference in using</p>

<pre><code>import numpy as np

a, b = np.random([1024, 1024]), np.random([1024, 1024])
c = np.multiply(a, b)
</code></pre>

<p>over</p>

<pre><code>c = a * b
</code></pre>

<p>or is the <code>*</code>-Operator on numpy-arrays simply overridden with <code>np.multiply</code>?</p>

<hr>

<p><strong><em>Edit</em></strong>: This question is marked as duplicate because <a href=""https://stackoverflow.com/questions/49459661/differences-between-numpy-divide-and-python-divide"">a question</a> asks the same thing about the division operator (<code>np.divide()</code> vs <code>/</code>) and similar answers followed, but unless it is changed to ""numpy arithmetic vs. python arithmetic"" or something of the kind, it won't help people wondering the same thing as I did (about multiplication) and not being ""clever"" enough to assume a question about a related arithmetic operation (division) generalizes to all the basic arithmetic operations. To <a href=""https://stackoverflow.blog/2010/11/16/dr-strangedupe-or-how-i-learned-to-stop-worrying-and-love-duplication/"">make it easier finding answers</a>, I'd advocate for keeping this question as is.</p>
",6409572,3028,26-03-2018 14:08,26-03-2018 14:11,0,3038,52,2,30,83,"{'badge_counts': {'bronze': 52, 'silver': 30, 'gold': 2}, 'account_id': 4746500, 'is_employee': False, 'last_modified_date': 1701298905, 'last_access_date': 1710248751, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3038, 'creation_date': 1464783798, 'user_type': 'registered', 'user_id': 6409572, 'accept_rate': 83, 'location': 'Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/6409572/honeybear', 'profile_image': 'https://i.stack.imgur.com/i2paq.jpg?s=256&g=1', 'display_name': 'Honeybear'}","Is there any difference in using over or is the -Operator on numpy-arrays simply overridden with ? Edit: This question is marked as duplicate because a question asks the same thing about the division operator ( vs ) and similar answers followed, but unless it is changed to ""numpy arithmetic vs. python arithmetic"" or something of the kind, it won't help people wondering the same thing as I did (about multiplication) and not being ""clever"" enough to assume a question about a related arithmetic operation (division) generalizes to all the basic arithmetic operations. To make it easier finding answers, I'd advocate for keeping this question as is.","import numpy as np

a, b = np.random([1024, 1024]), np.random([1024, 1024])
c = np.multiply(a, b)
 c = a * b
 * np.multiply np.divide() /",-1,18,0,2,
537,49802412,49802489,7155,How to implement deprecation in python with argument alias,2,<python><python-3.x><alias><deprecation-warning>,18,"<p>We are developing a python library and would like to change the way some function arguments are named in some functions. </p>

<p>We would like to keep backward compatibility and thus we would like to find a way to create alias for function arguments.</p>

<p>Here is an example:</p>

<p><strong>Old Version:</strong></p>

<pre><code>class MyClass(object):
  def __init__(self, object_id):
    self.id = object_id
</code></pre>

<p><strong>New Version:</strong></p>

<pre><code>class MyClass(object):
  def __init__(self, id_object):
    self.id = id_object
</code></pre>

<p>How can we make the class to be compatible with both calling ways:</p>

<pre><code>object1 = MyClass(object_id=1234)
object2 = MyClass(id_object=1234)
</code></pre>

<p>I could of course create something like this:</p>

<pre><code>class MyClass(object):
  def __init__(self, object_id=None, id_object=None):
    if id_object is not None:
      self.id = id_object
    else:
      self.id = object_id
</code></pre>

<p>However, it would change the number of arguments and we strictly want to avoid this.</p>

<p>Is there any way to declare a method alias or an argument alias ?</p>
",6451244,3486,12-04-2018 17:11,12-04-2018 17:17,0,3486,44,1,21,79,"{'badge_counts': {'bronze': 44, 'silver': 21, 'gold': 1}, 'account_id': 8613515, 'is_employee': False, 'last_modified_date': 1667315700, 'last_access_date': 1697522057, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3486, 'creation_date': 1465580064, 'user_type': 'registered', 'user_id': 6451244, 'accept_rate': 79, 'location': 'Santa Clara, California', 'website_url': 'http://www.jonathandekhtiar.eu', 'link': 'https://stackoverflow.com/users/6451244/jonathan-dekhtiar', 'profile_image': 'https://lh5.googleusercontent.com/-YX-fKMoU-ps/AAAAAAAAAAI/AAAAAAAAACE/qDO4Xj_LSFk/photo.jpg?sz=256', 'display_name': 'Jonathan DEKHTIAR'}","We are developing a python library and would like to change the way some function arguments are named in some functions. We would like to keep backward compatibility and thus we would like to find a way to create alias for function arguments. Here is an example: Old Version: New Version: How can we make the class to be compatible with both calling ways: I could of course create something like this: However, it would change the number of arguments and we strictly want to avoid this. Is there any way to declare a method alias or an argument alias ?","class MyClass(object):
  def __init__(self, object_id):
    self.id = object_id
 class MyClass(object):
  def __init__(self, id_object):
    self.id = id_object
 object1 = MyClass(object_id=1234)
object2 = MyClass(id_object=1234)
 class MyClass(object):
  def __init__(self, object_id=None, id_object=None):
    if id_object is not None:
      self.id = id_object
    else:
      self.id = object_id
",10,39,0,0,
538,50083553,50120983,8155,Why do I need to include sub-packages in setup.py,1,<python><setuptools><setup.py>,15,"<p>I have a python package called <code>mltester</code> which contains two sub-packages (<code>actions</code>, <code>dialogs</code>) and a main script <code>ml_tester.py</code>, structured as follows:</p>

<pre><code>+ &lt;ProjectFolder&gt;
+---+ &lt;mltester&gt;
|   +---- &lt;actions&gt;
|   +---- &lt;dialogs&gt;
|   +---- ml_tester.py
|   +---- __init__.py
+---- setup.py
</code></pre>

<p>My <code>__init__.py</code> looks as follows:</p>

<pre><code>import actions
import dialogs
import ml_tester
</code></pre>

<p>In <code>ml_tester.py</code> I do something like:</p>

<pre><code>from actions import *
from dialogs import *
</code></pre>

<p>All works fine when running from eclipse. When doing <code>pip install</code>, the following <code>setup.py</code> works fine:</p>

<pre><code>from setuptools import setup
setup(
    name=""MLTester"",
    version=""1.0"",
    packages=[""mltester"",
              ""mltester.actions"",
              ""mltester.dialogs""],
    install_requires=[
        ""matplotlib"",
    ],
    entry_points='''
        [console_scripts]
        ml_tester_gui=mltester.ml_tester:main
    '''
)
</code></pre>

<p>But when I remove <code>""mltester.actions""</code>, <code>""mltester.dialogs""</code> from the list of packages, I now get an error like:</p>

<pre><code>File ""/usr/local/lib/python2.7/dist-packages/mltester/__init__.py"", line 1, in &lt;module&gt;
    import actions
ImportError: No module named actions
</code></pre>

<p>And I don't understand why listing just the containing <code>mltester</code> package is not enough. Of Course I can simply add the packages back, but now I think that I'm missing something more conceptual here.</p>
",5470144,3752,29-04-2018 05:02,01-05-2018 17:26,2,3752,51,3,24,83,"{'badge_counts': {'bronze': 51, 'silver': 24, 'gold': 3}, 'account_id': 7158752, 'is_employee': False, 'last_modified_date': 1706742302, 'last_access_date': 1711042154, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3752, 'creation_date': 1445408468, 'user_type': 'registered', 'user_id': 5470144, 'accept_rate': 83, 'website_url': '', 'link': 'https://stackoverflow.com/users/5470144/elad-weiss', 'profile_image': 'https://i.stack.imgur.com/VVhju.jpg?s=256&g=1', 'display_name': 'Elad Weiss'}","I have a python package called which contains two sub-packages (, ) and a main script , structured as follows: My looks as follows: In I do something like: All works fine when running from eclipse. When doing , the following works fine: But when I remove , from the list of packages, I now get an error like: And I don't understand why listing just the containing package is not enough. Of Course I can simply add the packages back, but now I think that I'm missing something more conceptual here.","mltester actions dialogs ml_tester.py + &lt;ProjectFolder&gt;
+---+ &lt;mltester&gt;
|   +---- &lt;actions&gt;
|   +---- &lt;dialogs&gt;
|   +---- ml_tester.py
|   +---- __init__.py
+---- setup.py
 __init__.py import actions
import dialogs
import ml_tester
 ml_tester.py from actions import *
from dialogs import *
 pip install setup.py from setuptools import setup
setup(
    name=""MLTester"",
    version=""1.0"",
    packages=[""mltester"",
              ""mltester.actions"",
              ""mltester.dialogs""],
    install_requires=[
        ""matplotlib"",
    ],
    entry_points='''
        [console_scripts]
        ml_tester_gui=mltester.ml_tester:main
    '''
)
 ""mltester.actions"" ""mltester.dialogs"" File ""/usr/local/lib/python2.7/dist-packages/mltester/__init__.py"", line 1, in &lt;module&gt;
    import actions
ImportError: No module named actions
 mltester",14,51,0,0,
539,49015957,49016419,105678,How to get python graph output into html webpage directly,4,<python><html><pandas><graph>,15,"<p>I am using different libraries like pandas and numpy for generating a dataframe, which eventually generate a graph. </p>

<p>Now, I need to show this graph into a simple webpage which is in HTML.</p>

<p><strong>Note: I am also willing to take 2-3 input from user in HTML page then pass that data to my python file. Afterwards, python file generates a graph based on given data(from HTML page) and I need to pass this graph to an HTML page.</strong></p>

<pre><code>df[[main_data]].plot()
</code></pre>

<p>Here, main_data is variable whose value is coming from HTML page. And I am doing python code in SPYDER. <strong>And I am not using any Framework.</strong></p>
",6455987,231,27-02-2018 18:51,27-02-2018 19:21,0,231,12,1,2,44,"{'badge_counts': {'bronze': 12, 'silver': 2, 'gold': 1}, 'account_id': 8621064, 'is_employee': False, 'last_modified_date': 1711115700, 'last_access_date': 1660063659, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 231, 'creation_date': 1465730293, 'user_type': 'registered', 'user_id': 6455987, 'accept_rate': 44, 'link': 'https://stackoverflow.com/users/6455987/jay-desai', 'profile_image': 'https://www.gravatar.com/avatar/354bb075cebdd93c702fc6c6f1f033d8?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jay Desai'}","I am using different libraries like pandas and numpy for generating a dataframe, which eventually generate a graph. Now, I need to show this graph into a simple webpage which is in HTML. Note: I am also willing to take 2-3 input from user in HTML page then pass that data to my python file. Afterwards, python file generates a graph based on given data(from HTML page) and I need to pass this graph to an HTML page. Here, main_data is variable whose value is coming from HTML page. And I am doing python code in SPYDER. And I am not using any Framework.","df[[main_data]].plot()
",0,10,0,0,
540,48809458,48809523,4125,`del` on a package has some kind of memory,3,<python><python-3.x><ipython><package><del>,43,"<p><code>del</code> seems to have some memory which puzzles me. See the following:</p>

<pre><code>In [1]: import math

In [2]: math.cos(0)
Out[2]: 1.0

In [3]: del math.cos

In [4]: math.cos(0)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-4-9cdcc157d079&gt; in &lt;module&gt;()
----&gt; 1 math.cos(0)

AttributeError: module 'math' has no attribute 'cos'
</code></pre>

<p>Fine. Let's see what happens if we delete the whole math package:</p>

<pre><code>In [5]: del math

In [6]: math.cos(0)
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-6-9cdcc157d079&gt; in &lt;module&gt;()
----&gt; 1 math.cos(0)

NameError: name 'math' is not defined
</code></pre>

<p>So now math itself is gone, as expected. </p>

<p>Now let's import math again:</p>

<pre><code>In [7]: import math

In [8]: math.cos(0)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-8-9cdcc157d079&gt; in &lt;module&gt;()
----&gt; 1 math.cos(0)

AttributeError: module 'math' has no attribute 'cos'
</code></pre>

<p>So somehow interactive python remembers that math.cos was deleted specifically even after we deleted the whole math package and imported it again.</p>

<p>Where does python keeps this knowledge? Can we access it? Can we change it?</p>
",6510497,7981,15-02-2018 14:20,15-02-2018 14:24,0,7981,59,5,31,85,"{'badge_counts': {'bronze': 59, 'silver': 31, 'gold': 5}, 'account_id': 8700392, 'is_employee': False, 'last_modified_date': 1648035000, 'last_access_date': 1697284627, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7981, 'creation_date': 1466796580, 'user_type': 'registered', 'user_id': 6510497, 'accept_rate': 85, 'website_url': '', 'link': 'https://stackoverflow.com/users/6510497/aguy', 'profile_image': 'https://i.stack.imgur.com/3go0E.png?s=256&g=1', 'display_name': 'Aguy'}","seems to have some memory which puzzles me. See the following: Fine. Let's see what happens if we delete the whole math package: So now math itself is gone, as expected. Now let's import math again: So somehow interactive python remembers that math.cos was deleted specifically even after we deleted the whole math package and imported it again. Where does python keeps this knowledge? Can we access it? Can we change it?","del In [1]: import math

In [2]: math.cos(0)
Out[2]: 1.0

In [3]: del math.cos

In [4]: math.cos(0)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-4-9cdcc157d079&gt; in &lt;module&gt;()
----&gt; 1 math.cos(0)

AttributeError: module 'math' has no attribute 'cos'
 In [5]: del math

In [6]: math.cos(0)
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-6-9cdcc157d079&gt; in &lt;module&gt;()
----&gt; 1 math.cos(0)

NameError: name 'math' is not defined
 In [7]: import math

In [8]: math.cos(0)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-8-9cdcc157d079&gt; in &lt;module&gt;()
----&gt; 1 math.cos(0)

AttributeError: module 'math' has no attribute 'cos'
",28,49,0,0,
541,49770344,49770815,8029,The DECIMAL type field fetch data become string,3,<python><django><django-rest-framework>,16,"<p>In my table:</p>
<p>My discount type is DECIMAL:</p>
<p><a href=""https://i.stack.imgur.com/x2X8R.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/x2X8R.jpg"" alt=""Enter image description here"" /></a></p>
<p>My data in table:</p>
<p><a href=""https://i.stack.imgur.com/C8j5S.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/C8j5S.jpg"" alt=""Enter image description here"" /></a></p>
<p>But why when I fetch data in API, there gets string?</p>
<p><a href=""https://i.stack.imgur.com/gYang.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gYang.jpg"" alt=""enter image description here"" /></a></p>
<p>I use Django and Django REST framework as the backend.</p>
<hr />
<pre class=""lang-none prettyprint-override""><code>belong_product: &quot;server&quot;
ctime: &quot;2018-04-11T15:41:15.744959+08:00&quot;
desc: &quot;&quot;
discount: &quot;0.005&quot;
id: 1
is_enable: false
max_count: 5
min_count: 0
name: &quot;one&quot;
uptime: &quot;2018-04-11T15:41:15.745226+08:00&quot;
</code></pre>
<p>My ListAPI view:</p>
<pre><code>class DiscountItemByUserOwnCountListAPIView(ListAPIView):
    serializer_class = DiscountItemByUserOwnCountSerializer
    permission_classes = [IsSuperAdmin]
    pagination_class = CommonPagination
    def get_queryset(self):
        return DiscountItemByUserOwnCount.objects.all()
</code></pre>
<p>My model:</p>
<pre><code>class DiscountItemByUserOwnCount(models.Model):
    name = models.CharField(max_length=16, help_text=&quot;name&quot;)
    desc = models.CharField(max_length=512, null=True, blank=True, help_text=&quot;desc&quot;)
    min_count = models.IntegerField(help_text=&quot;min&quot;)
    max_count = models.IntegerField(help_text=&quot;max&quot;)   
    discount = models.DecimalField(max_digits=4, decimal_places=3, default=0.000, unique=True,
                                   help_text=&quot;point&quot;)  
    belong_product = models.CharField(max_length=16, help_text=&quot;belongDISCOUNT_PRODUCT_TYPE&quot;)

    is_enable = models.BooleanField(default=False)

    ctime = models.DateTimeField(auto_now_add=True)
    uptime = models.DateTimeField(auto_now=True)

    def __str__(self):
        return self.name
    def __unicode__(self):
        return self.name

    class Meta:
        ordering = ['min_count', '-id']
</code></pre>
",7693832,6409,11-04-2018 08:36,11-04-2018 09:00,0,6461,124,20,71,66,"{'badge_counts': {'bronze': 124, 'silver': 71, 'gold': 20}, 'account_id': 10435346, 'is_employee': False, 'last_modified_date': 1711011615, 'last_access_date': 1711013034, 'reputation_change_year': 152, 'reputation_change_quarter': 152, 'reputation_change_month': 82, 'reputation_change_week': 22, 'reputation_change_day': 0, 'reputation': 6461, 'creation_date': 1489216876, 'user_type': 'registered', 'user_id': 7693832, 'accept_rate': 66, 'website_url': '', 'link': 'https://stackoverflow.com/users/7693832/user7693832', 'profile_image': 'https://i.stack.imgur.com/i0kVl.png?s=256&g=1', 'display_name': 'user7693832'}","In my table: My discount type is DECIMAL: My data in table: But why when I fetch data in API, there gets string? I use Django and Django REST framework as the backend. My ListAPI view: My model:","belong_product: &quot;server&quot;
ctime: &quot;2018-04-11T15:41:15.744959+08:00&quot;
desc: &quot;&quot;
discount: &quot;0.005&quot;
id: 1
is_enable: false
max_count: 5
min_count: 0
name: &quot;one&quot;
uptime: &quot;2018-04-11T15:41:15.745226+08:00&quot;
 class DiscountItemByUserOwnCountListAPIView(ListAPIView):
    serializer_class = DiscountItemByUserOwnCountSerializer
    permission_classes = [IsSuperAdmin]
    pagination_class = CommonPagination
    def get_queryset(self):
        return DiscountItemByUserOwnCount.objects.all()
 class DiscountItemByUserOwnCount(models.Model):
    name = models.CharField(max_length=16, help_text=&quot;name&quot;)
    desc = models.CharField(max_length=512, null=True, blank=True, help_text=&quot;desc&quot;)
    min_count = models.IntegerField(help_text=&quot;min&quot;)
    max_count = models.IntegerField(help_text=&quot;max&quot;)   
    discount = models.DecimalField(max_digits=4, decimal_places=3, default=0.000, unique=True,
                                   help_text=&quot;point&quot;)  
    belong_product = models.CharField(max_length=16, help_text=&quot;belongDISCOUNT_PRODUCT_TYPE&quot;)

    is_enable = models.BooleanField(default=False)

    ctime = models.DateTimeField(auto_now_add=True)
    uptime = models.DateTimeField(auto_now=True)

    def __str__(self):
        return self.name
    def __unicode__(self):
        return self.name

    class Meta:
        ordering = ['min_count', '-id']
",34,51,3,3,
542,48831838,49215234,3667,Django: how to fully decouple apps when it seems they are coupled?,3,<python><django>,23,"<p><strong>Note</strong>: I am not a <em>proper</em> python programmer... but I use python extensively. I do things like write classes with inheritance, use iterators and comprehension, etc. My point is that I do not have a <em>full</em> grasp of the language, e.g. what <em>exactly</em> constitutes an python object, why <code>__init__.py</code> is needed other than to specify a module, etc. In relation to Django, I have written multi-app sites (with the help of S.O.) and have really enjoyed Django's templating system, blocks, and how they can be nested. Now are my apps fully decoupled and reusable? That this is subject of this post.</p>

<p>I state this disclaimer because a lot of the Django resources seem to assume that one knows these things. This makes understanding some of the documentation and S.O. questions difficult for a person who is just an (subpower)-user.  So please answer this question with that in mind. </p>

<h1>Question</h1>

<p>These questions are inspired by both the question <a href=""https://stackoverflow.com/questions/64237/when-to-create-a-new-app-with-startapp-in-django"">When to create a new app with startapp in django?</a> by <a href=""https://stackoverflow.com/users/8420/h%C3%A5kan"">@håkan</a> and the <a href=""https://stackoverflow.com/a/64492/5623899"">answer</a> given by <a href=""https://stackoverflow.com/users/8570/antti-rasinen"">@antti rasinen</a> which links to James Bennett's 2008 <a href=""http://media.b-list.org/presentations/2008/pycon/reusable_apps.pdf"" rel=""noreferrer"">PyCon presentation</a></p>

<p>A few key points from Bennett's presentation are:</p>

<ol>
<li>sites are a collection of apps</li>
<li>an app does one thing and one thing well</li>
</ol>

<p>Which directs me to his section ""Project coupling kills re-use""  that mentions:</p>

<ul>
<li>Single module directly on Python path (registration, tagging, etc.)</li>
<li>Related modules under a package (ellington.events, ellington.podcasts, etc.)</li>
</ul>

<h2>Question 0</h2>

<p>A ""module"" in this case is just an app made of other apps?</p>

<h2>Question 1</h2>

<p>(Apps with related functionality and shared models )</p>

<p>What should I do when apps share models?</p>

<p>In Barrett's slides he implies that user registration and user profiles are distinct and should be distinct apps. (He certainly states that profiles have nothing to do with user registration). </p>

<p>So if I wanted both, would my project have two apps like:</p>

<ul>
<li>user-registration</li>
<li>user-profile</li>
</ul>

<p>even though the app <code>user-profile</code> will need the user model from <code>user-registration</code>? Or do I make a single app (<code>module</code>):</p>

<ul>
<li>user-app

<ul>
<li>registration</li>
<li>profile</li>
</ul></li>
</ul>

<p>which contains both?</p>

<h1>Question 2</h1>

<p>(Apps with distinct functions but shared models)</p>

<p>Extending the example from question 1, lets say that my main app (or some other app that is used by the main app) utilizes some aspect of the user model (e.g. recently active members if it was a chat site).</p>

<p>Clearly my main app gets this information from the user model.  Does my main app now get bundled under the <code>user-app</code> module?</p>

<p>This may not be the best example, but the point is as follows:</p>

<p>I have two apps <code>app-dependency</code> and <code>app-needs-dependency</code>, where each app does its one thing and one thing well... It is just that <code>app-needs-dependency</code> needs information from <code>app-dependency</code>. What do I do in this case, if <em>everything</em> else about <code>app-needs-dependency</code> is completely decoupled from <code>app-dependency</code> (so it could be used in other projects)?</p>

<h1>Question 3</h1>

<p>(writing apps for flexibility)</p>

<p>Now I have my site with its couple of apps. Each app does its one thing and does it well. The main app serves as the landing page/ overview in this case. </p>

<p>I want all my other apps to use / inherit the static and template files of the main app.</p>

<p>Where do I store all the static files and templates? In the main app and set that as the default for the other apps? Or where should these static files / templates (e.g. <code>base.css</code>, <code>base.html</code>) go?
Do I make a copy of these files for each other app, so they can be run even though this is redundant?</p>

<p>Which makes my app more flexible?</p>
",5623899,4998,16-02-2018 17:17,10-03-2018 23:54,22,4998,113,7,43,77,"{'badge_counts': {'bronze': 113, 'silver': 43, 'gold': 7}, 'account_id': 3651943, 'is_employee': False, 'last_modified_date': 1709344800, 'last_access_date': 1711047565, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4998, 'creation_date': 1448941265, 'user_type': 'registered', 'user_id': 5623899, 'accept_rate': 77, 'location': 'G&#246;ttingen', 'website_url': 'http://dsmagruder.com', 'link': 'https://stackoverflow.com/users/5623899/sumneuron', 'profile_image': 'https://i.stack.imgur.com/Y5lUH.jpg?s=256&g=1', 'display_name': 'SumNeuron'}","Note: I am not a proper python programmer... but I use python extensively. I do things like write classes with inheritance, use iterators and comprehension, etc. My point is that I do not have a full grasp of the language, e.g. what exactly constitutes an python object, why is needed other than to specify a module, etc. In relation to Django, I have written multi-app sites (with the help of S.O.) and have really enjoyed Django's templating system, blocks, and how they can be nested. Now are my apps fully decoupled and reusable? That this is subject of this post. I state this disclaimer because a lot of the Django resources seem to assume that one knows these things. This makes understanding some of the documentation and S.O. questions difficult for a person who is just an (subpower)-user. So please answer this question with that in mind. Question These questions are inspired by both the question When to create a new app with startapp in django? by @håkan and the answer given by @antti rasinen which links to James Bennett's 2008 PyCon presentation A few key points from Bennett's presentation are: sites are a collection of apps an app does one thing and one thing well Which directs me to his section ""Project coupling kills re-use"" that mentions: Single module directly on Python path (registration, tagging, etc.) Related modules under a package (ellington.events, ellington.podcasts, etc.) Question 0 A ""module"" in this case is just an app made of other apps? Question 1 (Apps with related functionality and shared models ) What should I do when apps share models? In Barrett's slides he implies that user registration and user profiles are distinct and should be distinct apps. (He certainly states that profiles have nothing to do with user registration). So if I wanted both, would my project have two apps like: user-registration user-profile even though the app will need the user model from ? Or do I make a single app (): user-app registration profile which contains both? Question 2 (Apps with distinct functions but shared models) Extending the example from question 1, lets say that my main app (or some other app that is used by the main app) utilizes some aspect of the user model (e.g. recently active members if it was a chat site). Clearly my main app gets this information from the user model. Does my main app now get bundled under the module? This may not be the best example, but the point is as follows: I have two apps and , where each app does its one thing and one thing well... It is just that needs information from . What do I do in this case, if everything else about is completely decoupled from (so it could be used in other projects)? Question 3 (writing apps for flexibility) Now I have my site with its couple of apps. Each app does its one thing and does it well. The main app serves as the landing page/ overview in this case. I want all my other apps to use / inherit the static and template files of the main app. Where do I store all the static files and templates? In the main app and set that as the default for the other apps? Or where should these static files / templates (e.g. , ) go? Do I make a copy of these files for each other app, so they can be run even though this is redundant? Which makes my app more flexible?",__init__.py user-profile user-registration module user-app app-dependency app-needs-dependency app-needs-dependency app-dependency app-needs-dependency app-dependency base.css base.html,-13,78,0,5,
543,49189402,49189522,133735,auth.User.groups: (fields.E304) Reverse accessor for 'User.groups' clashes with reverse accessor for 'UserManage.groups',7,<python><django>,159,"<p>In my Django project I have a <code>user_manage</code> app.</p>

<p>I create a model named <code>UserManage</code> in my <code>user_manage</code> app's model.py:</p>

<pre><code>from django.db import models
from django.contrib.auth.models import AbstractUser

class UserManage(AbstractUser):
    username = models.CharField(max_length=12)
</code></pre>

<p>Then I run:</p>

<pre><code>$ python3 manage.py makemigrations
</code></pre>

<p>There comes the error:</p>

<pre><code>ERRORS:
auth.User.groups: (fields.E304) Reverse accessor for 'User.groups' clashes with reverse accessor for 'UserManage.groups'.
        HINT: Add or change a related_name argument to the definition for 'User.groups' or 'UserManage.groups'.
auth.User.user_permissions: (fields.E304) Reverse accessor for 'User.user_permissions' clashes with reverse accessor for 'UserManage.user_permissions'.
        HINT: Add or change a related_name argument to the definition for 'User.user_permissions' or 'UserManage.user_permissions'.
users_management.UserManage.groups: (fields.E304) Reverse accessor for 'UserManage.groups' clashes with reverse accessor for 'User.groups'.
        HINT: Add or change a related_name argument to the definition for 'UserManage.groups' or 'User.groups'.
users_management.UserManage.user_permissions: (fields.E304) Reverse accessor for 'UserManage.user_permissions' clashes with reverse accessor for 'User.user_permissions'.
        HINT: Add or change a related_name argument to the definition for 'UserManage.user_permissions' or 'User.user_permissions'.
</code></pre>
",6523163,26004,09-03-2018 08:21,09-03-2018 08:30,0,26084,170,28,97,72,"{'badge_counts': {'bronze': 170, 'silver': 97, 'gold': 28}, 'collectives': [{'collective': {'tags': ['wso2-identity-server', 'ballerina-composer', 'wso2-wsas', 'ballerina-kafka', 'ballerina-vscode-plugin', 'ballerina', 'wso2-es', 'wso2-choreo', 'wso2-das', 'wso2-business-process', 'wso2-ml', 'wso2-governance-registry', 'wso2-as', 'wso2-streaming-integrator', 'wso2-cep', 'wso2', 'wso2-iot', 'wso2-msf4j', 'wso2-asgardeo', 'ballerina-swan-lake', 'wso2-stratos', 'wso2-bi', 'ballerina-java-interop', 'wso2-appm', 'ballerina-http', 'wso2-bam', 'wso2-data-services-server', 'wso2-enterprise-integrator', 'wso2-cloud', 'wso2-esb', 'wso2-micro-gateway', 'wso2-api-manager', 'wso2-integration-studio', 'wso2-micro-integrator', 'wso2-message-broker'], 'external_links': [{'type': 'website', 'link': 'https://wso2.com'}, {'type': 'support', 'link': 'https://discord.gg/wso2'}, {'type': 'twitter', 'link': 'https://twitter.com/wso2'}, {'type': 'github', 'link': 'https://github.com/wso2'}, {'type': 'facebook', 'link': 'https://facebook.com/OfficialWSO2'}, {'type': 'instagram', 'link': 'https://instagram.com/officialwso2'}, {'type': 'linkedin', 'link': 'https://www.linkedin.com/company/wso2/'}], 'description': 'WSO2 solutions give enterprises the flexibility to deploy applications and services on-premises, on private or public clouds, or in hybrid environments. Our collective aims to enable developers to build value-added services and get to market faster.', 'link': '/collectives/wso2', 'name': 'WSO2', 'slug': 'wso2'}, 'role': 'member'}], 'account_id': 8719608, 'is_employee': False, 'last_modified_date': 1708628700, 'last_access_date': 1711012996, 'reputation_change_year': 340, 'reputation_change_quarter': 340, 'reputation_change_month': 100, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 26084, 'creation_date': 1467114673, 'user_type': 'registered', 'user_id': 6523163, 'accept_rate': 72, 'location': 'Chengdu', 'website_url': '', 'link': 'https://stackoverflow.com/users/6523163/aircraft', 'profile_image': 'https://i.stack.imgur.com/5GEyC.jpg?s=256&g=1', 'display_name': 'aircraft'}",In my Django project I have a app. I create a model named in my app's model.py: Then I run: There comes the error:,"user_manage UserManage user_manage from django.db import models
from django.contrib.auth.models import AbstractUser

class UserManage(AbstractUser):
    username = models.CharField(max_length=12)
 $ python3 manage.py makemigrations
 ERRORS:
auth.User.groups: (fields.E304) Reverse accessor for 'User.groups' clashes with reverse accessor for 'UserManage.groups'.
        HINT: Add or change a related_name argument to the definition for 'User.groups' or 'UserManage.groups'.
auth.User.user_permissions: (fields.E304) Reverse accessor for 'User.user_permissions' clashes with reverse accessor for 'UserManage.user_permissions'.
        HINT: Add or change a related_name argument to the definition for 'User.user_permissions' or 'UserManage.user_permissions'.
users_management.UserManage.groups: (fields.E304) Reverse accessor for 'UserManage.groups' clashes with reverse accessor for 'User.groups'.
        HINT: Add or change a related_name argument to the definition for 'UserManage.groups' or 'User.groups'.
users_management.UserManage.user_permissions: (fields.E304) Reverse accessor for 'UserManage.user_permissions' clashes with reverse accessor for 'User.user_permissions'.
        HINT: Add or change a related_name argument to the definition for 'UserManage.user_permissions' or 'User.user_permissions'.
",9,28,0,0,
544,50190676,50191446,30891,Install pandas in a Dockerfile,7,<python><pandas><docker><dockerfile><python-3.6>,16,"<p>I am trying to create a Docker image.
The Dockerfile is the following:</p>

<pre><code># Use the official Python 3.6.5 image
FROM python:3.6.5-alpine3.7

# Set the working directory to /app
WORKDIR /app

# Get the 
COPY requirements.txt /app
RUN pip3 install --no-cache-dir -r requirements.txt

# Configuring access to Jupyter
RUN mkdir /notebooks
RUN jupyter notebook --no-browser --ip 0.0.0.0 --port 8888 /notebooks
</code></pre>

<p>The requirements.txt file is:</p>

<pre><code>jupyter
numpy==1.14.3
pandas==0.23.0rc2
scipy==1.0.1
scikit-learn==0.19.1
pillow==5.1.1
matplotlib==2.2.2
seaborn==0.8.1
</code></pre>

<p>Running the command <code>docker build -t standard .</code> gives me an error when docker it trying to install pandas.
The error is the following:</p>

<pre><code>Collecting pandas==0.23.0rc2 (from -r requirements.txt (line 3))
  Downloading https://files.pythonhosted.org/packages/46/5c/a883712dad8484ef907a2f42992b122acf2bcecbb5c2aa751d1033908502/pandas-0.23.0rc2.tar.gz (12.5MB)
    Complete output from command python setup.py egg_info:
    /bin/sh: svnversion: not found
    /bin/sh: svnversion: not found
    non-existing path in 'numpy/distutils': 'site.cfg'
    Could not locate executable gfortran
    ... (loads of other stuff)
    Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-xb6f6a5o/pandas/
The command '/bin/sh -c pip3 install --no-cache-dir -r requirements.txt' returned a non-zero code: 1
</code></pre>

<p>When I try to install a lower version of pandas==0.22.0, I get this error:</p>

<pre><code>Step 5/7 : RUN pip3 install --no-cache-dir -r requirements.txt
 ---&gt; Running in 5810ea896689
Collecting jupyter (from -r requirements.txt (line 1))
  Downloading https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl
Collecting numpy==1.14.3 (from -r requirements.txt (line 2))
  Downloading https://files.pythonhosted.org/packages/b0/2b/497c2bb7c660b2606d4a96e2035e92554429e139c6c71cdff67af66b58d2/numpy-1.14.3.zip (4.9MB)
Collecting pandas==0.22.0 (from -r requirements.txt (line 3))
  Downloading https://files.pythonhosted.org/packages/08/01/803834bc8a4e708aedebb133095a88a4dad9f45bbaf5ad777d2bea543c7e/pandas-0.22.0.tar.gz (11.3MB)
  Could not find a version that satisfies the requirement Cython (from versions: )
No matching distribution found for Cython
The command '/bin/sh -c pip3 install --no-cache-dir -r requirements.txt' returned a non-zero code: 1
</code></pre>

<p>I also tried to install Cyphon and setuptools before pandas, but it gave the same <code>No matching distribution found for Cython</code> error at the pip3 install pandas line.</p>

<p>How could I get pandas installed.</p>
",6583557,425,05-05-2018 14:46,05-05-2018 16:07,0,425,23,1,7,89,"{'badge_counts': {'bronze': 23, 'silver': 7, 'gold': 1}, 'account_id': 8810306, 'is_employee': False, 'last_modified_date': 1602411900, 'last_access_date': 1680288175, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 425, 'creation_date': 1468400901, 'user_type': 'registered', 'user_id': 6583557, 'accept_rate': 89, 'location': 'Bangkok Thailand', 'website_url': '', 'link': 'https://stackoverflow.com/users/6583557/ccasimiro9444', 'profile_image': 'https://www.gravatar.com/avatar/f13e301fb8d98fd657b84768c0d6cfa0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ccasimiro9444'}","I am trying to create a Docker image. The Dockerfile is the following: The requirements.txt file is: Running the command gives me an error when docker it trying to install pandas. The error is the following: When I try to install a lower version of pandas==0.22.0, I get this error: I also tried to install Cyphon and setuptools before pandas, but it gave the same error at the pip3 install pandas line. How could I get pandas installed.","# Use the official Python 3.6.5 image
FROM python:3.6.5-alpine3.7

# Set the working directory to /app
WORKDIR /app

# Get the 
COPY requirements.txt /app
RUN pip3 install --no-cache-dir -r requirements.txt

# Configuring access to Jupyter
RUN mkdir /notebooks
RUN jupyter notebook --no-browser --ip 0.0.0.0 --port 8888 /notebooks
 jupyter
numpy==1.14.3
pandas==0.23.0rc2
scipy==1.0.1
scikit-learn==0.19.1
pillow==5.1.1
matplotlib==2.2.2
seaborn==0.8.1
 docker build -t standard . Collecting pandas==0.23.0rc2 (from -r requirements.txt (line 3))
  Downloading https://files.pythonhosted.org/packages/46/5c/a883712dad8484ef907a2f42992b122acf2bcecbb5c2aa751d1033908502/pandas-0.23.0rc2.tar.gz (12.5MB)
    Complete output from command python setup.py egg_info:
    /bin/sh: svnversion: not found
    /bin/sh: svnversion: not found
    non-existing path in 'numpy/distutils': 'site.cfg'
    Could not locate executable gfortran
    ... (loads of other stuff)
    Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-xb6f6a5o/pandas/
The command '/bin/sh -c pip3 install --no-cache-dir -r requirements.txt' returned a non-zero code: 1
 Step 5/7 : RUN pip3 install --no-cache-dir -r requirements.txt
 ---&gt; Running in 5810ea896689
Collecting jupyter (from -r requirements.txt (line 1))
  Downloading https://files.pythonhosted.org/packages/83/df/0f5dd132200728a86190397e1ea87cd76244e42d39ec5e88efd25b2abd7e/jupyter-1.0.0-py2.py3-none-any.whl
Collecting numpy==1.14.3 (from -r requirements.txt (line 2))
  Downloading https://files.pythonhosted.org/packages/b0/2b/497c2bb7c660b2606d4a96e2035e92554429e139c6c71cdff67af66b58d2/numpy-1.14.3.zip (4.9MB)
Collecting pandas==0.22.0 (from -r requirements.txt (line 3))
  Downloading https://files.pythonhosted.org/packages/08/01/803834bc8a4e708aedebb133095a88a4dad9f45bbaf5ad777d2bea543c7e/pandas-0.22.0.tar.gz (11.3MB)
  Could not find a version that satisfies the requirement Cython (from versions: )
No matching distribution found for Cython
The command '/bin/sh -c pip3 install --no-cache-dir -r requirements.txt' returned a non-zero code: 1
 No matching distribution found for Cython",36,63,0,0,
545,50033312,50048179,9070,How to monitor gradient vanish and explosion in keras with tensorboard?,1,<python><tensorflow><keras><tensorboard><tensorflow-gradient>,18,"<p>I would like to monitor the gradient changes in tensorboard with keras to decide whether gradient vanish or explosion. What should I do?</p>
",6585271,195,26-04-2018 00:59,26-04-2018 16:43,0,195,9,0,1,,"{'badge_counts': {'bronze': 9, 'silver': 1, 'gold': 0}, 'account_id': 8812759, 'is_employee': False, 'last_modified_date': 1584776708, 'last_access_date': 1702871499, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 195, 'creation_date': 1468423553, 'user_type': 'registered', 'user_id': 6585271, 'link': 'https://stackoverflow.com/users/6585271/joey-chia', 'profile_image': 'https://lh6.googleusercontent.com/-GAxsZpk6a7U/AAAAAAAAAAI/AAAAAAAAFa4/q4uolbHb3HA/photo.jpg?sz=256', 'display_name': 'Joey Chia'}",I would like to monitor the gradient changes in tensorboard with keras to decide whether gradient vanish or explosion. What should I do?,,0,1,0,0,
546,48340341,48345619,199390,How to read csv to dataframe in Google Colab,7,<python><csv><dataframe><google-colaboratory>,56,"<p>I am trying to read a csv file which I stored locally on my machine. (Just for additional reference it is titanic data from Kaggle which is <a href=""https://www.kaggle.com/c/titanic/data"" rel=""noreferrer"">here</a>.)</p>

<p>From <a href=""https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory"">this</a> question and answers I learnt that you can import data using this code which works well from me. </p>

<pre><code>from google.colab import files
uploaded = files.upload()
</code></pre>

<p>Where I am lost is how to convert it to dataframe from here. The <a href=""https://colab.research.google.com/notebook#fileId=/v2/external/notebooks/io.ipynb&amp;scrollTo=JiJVCmu3dhFa"" rel=""noreferrer"">sample google notebook page</a> listed in the answer above does not talk about it. </p>

<p>I am trying to convert the dictionary <code>uploaded</code> to dataframe using <code>from_dict</code> command but not able to make it work. There is some discussion on converting dict to dataframe <a href=""https://stackoverflow.com/questions/18837262/convert-python-dict-into-a-dataframe"">here</a> but the solutions are not applicable to me (I think). </p>

<p>So summarizing, my question is: </p>

<blockquote>
  <p>How do I convert a csv file stored locally on my files to pandas 
  dataframe on Google Colaboratory?</p>
</blockquote>
",5826590,8268,19-01-2018 11:43,19-01-2018 16:34,0,8288,41,8,26,87,"{'badge_counts': {'bronze': 41, 'silver': 26, 'gold': 8}, 'account_id': 7689995, 'is_employee': False, 'last_modified_date': 1607614509, 'last_access_date': 1709199086, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 40, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 8288, 'creation_date': 1453470126, 'user_type': 'registered', 'user_id': 5826590, 'accept_rate': 87, 'location': 'India', 'link': 'https://stackoverflow.com/users/5826590/pagmax', 'profile_image': 'https://www.gravatar.com/avatar/2dd2f99897706c4568b9f20c00d96d00?s=256&d=identicon&r=PG', 'display_name': 'PagMax'}","I am trying to read a csv file which I stored locally on my machine. (Just for additional reference it is titanic data from Kaggle which is here.) From this question and answers I learnt that you can import data using this code which works well from me. Where I am lost is how to convert it to dataframe from here. The sample google notebook page listed in the answer above does not talk about it. I am trying to convert the dictionary to dataframe using command but not able to make it work. There is some discussion on converting dict to dataframe here but the solutions are not applicable to me (I think). So summarizing, my question is: How do I convert a csv file stored locally on my files to pandas dataframe on Google Colaboratory?","from google.colab import files
uploaded = files.upload()
 uploaded from_dict",-1,18,0,4,
547,48209667,48209785,128642,Using monotonically_increasing_id() for assigning row number to pyspark dataframe,6,<python><indexing><merge><pyspark>,47,"<p>I am using monotonically_increasing_id() to assign row number to pyspark dataframe using syntax below:</p>

<pre><code>df1 = df1.withColumn(""idx"", monotonically_increasing_id())
</code></pre>

<p>Now df1 has 26,572,528 records. So I was expecting idx value from 0-26,572,527. </p>

<p>But when I select max(idx), its value is strangely huge: 335,008,054,165.</p>

<p>What's going on with this function?
is it reliable to use this function for merging with another dataset having similar number of records?</p>

<p>I have some 300 dataframes which I want to combine into a single dataframe. So one dataframe contains IDs and others contain different records corresponding to them row-wise</p>
",6608664,1303,11-01-2018 14:41,11-01-2018 14:48,0,1313,33,4,22,61,"{'badge_counts': {'bronze': 33, 'silver': 22, 'gold': 4}, 'account_id': 8848623, 'is_employee': False, 'last_modified_date': 1615594500, 'last_access_date': 1580139675, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1313, 'creation_date': 1468932902, 'user_type': 'registered', 'user_id': 6608664, 'accept_rate': 61, 'link': 'https://stackoverflow.com/users/6608664/muni', 'profile_image': 'https://www.gravatar.com/avatar/4df6d759549487170e6403be0591ff3c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'muni'}","I am using monotonically_increasing_id() to assign row number to pyspark dataframe using syntax below: Now df1 has 26,572,528 records. So I was expecting idx value from 0-26,572,527. But when I select max(idx), its value is strangely huge: 335,008,054,165. What's going on with this function? is it reliable to use this function for merging with another dataset having similar number of records? I have some 300 dataframes which I want to combine into a single dataframe. So one dataframe contains IDs and others contain different records corresponding to them row-wise","df1 = df1.withColumn(""idx"", monotonically_increasing_id())
",0,13,0,0,
548,50280640,50280844,19125,Open / load image as numpy ndarray directly,2,<python><image><numpy><scipy><numpy-ndarray>,15,"<p>I used to use scipy which would load an image from file straight into an ndarray.</p>

<pre><code>from scipy import misc
img = misc.imread('./myimage.jpg')
type(img)
&gt;&gt;&gt; numpy.ndarray
</code></pre>

<p>But now it gives me a <code>DeprecationWarning</code> and the <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html"" rel=""noreferrer"">docs say it will be removed in 1.2.0.</a> and I should use imageio.imread instead. But:</p>

<pre><code>import imageio
img = imageio.imread('./myimage.jpg')
type(img)
&gt;&gt;&gt; imageio.core.util.Image
</code></pre>

<p>I could convert it by doing</p>

<pre><code>img = numpy.array(img)
</code></pre>

<p>But this seems hacky. Is there any way to load an image straight into a numpy array as I was doing before with scipy's <code>misc.imread</code> (other than using OpenCV)?</p>
",6641872,647,10-05-2018 19:58,10-05-2018 20:12,0,657,19,2,6,,"{'badge_counts': {'bronze': 19, 'silver': 6, 'gold': 2}, 'account_id': 6199834, 'is_employee': False, 'last_modified_date': 1607614477, 'last_access_date': 1707024628, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 657, 'creation_date': 1469568577, 'user_type': 'registered', 'user_id': 6641872, 'location': 'United States', 'link': 'https://stackoverflow.com/users/6641872/nic', 'profile_image': 'https://www.gravatar.com/avatar/038dcbe29172970f394334ba1f81f862?s=256&d=identicon&r=PG', 'display_name': 'Nic'}",I used to use scipy which would load an image from file straight into an ndarray. But now it gives me a and the docs say it will be removed in 1.2.0. and I should use imageio.imread instead. But: I could convert it by doing But this seems hacky. Is there any way to load an image straight into a numpy array as I was doing before with scipy's (other than using OpenCV)?,"from scipy import misc
img = misc.imread('./myimage.jpg')
type(img)
&gt;&gt;&gt; numpy.ndarray
 DeprecationWarning import imageio
img = imageio.imread('./myimage.jpg')
type(img)
&gt;&gt;&gt; imageio.core.util.Image
 img = numpy.array(img)
 misc.imread",4,22,0,1,
549,48177914,48178021,28722,Why use pandas.assign rather than simply initialize new column?,2,<python><pandas>,67,"<p>I just discovered the <code>assign</code> method for pandas dataframes, and it looks nice and very similar to dplyr's <code>mutate</code> in R. However, I've always gotten by by just initializing a new column 'on the fly'. Is there a reason why <code>assign</code> is better?</p>
<p>For instance (based on the example in the pandas documentation), to create a new column in a dataframe, I could just do this:</p>
<pre><code>df = DataFrame({'A': range(1, 11), 'B': np.random.randn(10)})
df['ln_A'] = np.log(df['A'])
</code></pre>
<p>but the <code>pandas.DataFrame.assign</code> documentation recommends doing this:</p>
<pre><code>df.assign(ln_A = lambda x: np.log(x.A))
# or 
newcol = np.log(df['A'])
df.assign(ln_A=newcol)
</code></pre>
<p>Both methods return the same dataframe. In fact, the first method (my 'on the fly' assignment) is significantly faster (0.202 seconds for 1000 iterations) than the <code>.assign</code> method (0.353 seconds for 1000 iterations).</p>
<p>So is there a reason I should stop using my old method in favour of <code>df.assign</code>?</p>
",6671176,50490,09-01-2018 23:01,09-01-2018 23:13,0,50560,110,8,84,100,"{'badge_counts': {'bronze': 110, 'silver': 84, 'gold': 8}, 'account_id': 8942675, 'is_employee': False, 'last_modified_date': 1685999101, 'last_access_date': 1711127858, 'reputation_change_year': 330, 'reputation_change_quarter': 330, 'reputation_change_month': 120, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 50560, 'creation_date': 1470210221, 'user_type': 'registered', 'user_id': 6671176, 'accept_rate': 100, 'location': 'Montreal, QC, Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/6671176/sacul', 'profile_image': 'https://i.stack.imgur.com/4DvZ4.png?s=256&g=1', 'display_name': 'sacuL'}","I just discovered the method for pandas dataframes, and it looks nice and very similar to dplyr's in R. However, I've always gotten by by just initializing a new column 'on the fly'. Is there a reason why is better? For instance (based on the example in the pandas documentation), to create a new column in a dataframe, I could just do this: but the documentation recommends doing this: Both methods return the same dataframe. In fact, the first method (my 'on the fly' assignment) is significantly faster (0.202 seconds for 1000 iterations) than the method (0.353 seconds for 1000 iterations). So is there a reason I should stop using my old method in favour of ?","assign mutate assign df = DataFrame({'A': range(1, 11), 'B': np.random.randn(10)})
df['ln_A'] = np.log(df['A'])
 pandas.DataFrame.assign df.assign(ln_A = lambda x: np.log(x.A))
# or 
newcol = np.log(df['A'])
df.assign(ln_A=newcol)
 .assign df.assign",-2,13,0,0,
550,50203732,50203789,22236,Compare column names of Pandas Dataframe,2,<python><pandas><numpy><machine-learning><data-science>,15,"<p>How to compare column names of 2 different Pandas data frame. I want to compare train and test data frames where there are some columns missing in test Data frames??</p>
",6743360,978,06-05-2018 19:31,06-05-2018 19:37,0,988,20,2,10,,"{'badge_counts': {'bronze': 20, 'silver': 10, 'gold': 2}, 'account_id': 9054553, 'is_employee': False, 'last_modified_date': 1648512600, 'last_access_date': 1634726701, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 988, 'creation_date': 1471865390, 'user_type': 'registered', 'user_id': 6743360, 'location': 'Bangalore, Karnataka, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/6743360/aptha-gowda', 'profile_image': 'https://i.stack.imgur.com/sFaVv.png?s=256&g=1', 'display_name': 'Aptha Gowda'}",How to compare column names of 2 different Pandas data frame. I want to compare train and test data frames where there are some columns missing in test Data frames??,,0,1,0,0,
551,48966606,48967117,56643,jhipster - gyp verb `which` failed Error: not found: python2,5,<python><jhipster>,16,"<p>Below is the error that I get, when I try to generate a new project with JHipster:</p>

<pre><code>gyp verb check python checking for Python executable ""python2"" in the PATH
gyp verb `which` failed Error: not found: python2
gyp verb check python checking for Python executable ""python"" in the PATH
gyp verb `which` succeeded python C:\Program Files\Python36\python.EXE
gyp verb check python version `C:\Program Files\Python36\python.EXE -c ""import platform; print(platform.python_version());""` returned: ""3.6.4\r\n""
gyp verb could not find ""C:\Program Files\Python36\python.EXE"". checking python launcher
gyp verb could not find ""C:\Program Files\Python36\python.EXE"". guessing location
gyp verb ensuring that file exists: C:\Python27\python.exe
gyp ERR! configure error
gyp ERR! stack Error: Can't find Python executable ""C:\Program Files\Python36\python.EXE"", you can set the PYTHON env variable.
</code></pre>

<p>The rest is the stacktrace. The weird thing is I do have python installed and the environment variable is also set. My python version is ""Python 3.6.4"". Should I run it in Python 2? Is that the case?</p>
",6999882,3359,24-02-2018 19:08,24-02-2018 20:10,0,3359,40,7,24,86,"{'badge_counts': {'bronze': 40, 'silver': 24, 'gold': 7}, 'account_id': 135268, 'is_employee': False, 'last_modified_date': 1621186500, 'last_access_date': 1711100411, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3359, 'creation_date': 1409658744, 'user_type': 'registered', 'user_id': 6999882, 'accept_rate': 86, 'website_url': '', 'link': 'https://stackoverflow.com/users/6999882/leventunver', 'profile_image': 'https://www.gravatar.com/avatar/492f438a43acd806ee81ccb74caffb73?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'leventunver'}","Below is the error that I get, when I try to generate a new project with JHipster: The rest is the stacktrace. The weird thing is I do have python installed and the environment variable is also set. My python version is ""Python 3.6.4"". Should I run it in Python 2? Is that the case?","gyp verb check python checking for Python executable ""python2"" in the PATH
gyp verb `which` failed Error: not found: python2
gyp verb check python checking for Python executable ""python"" in the PATH
gyp verb `which` succeeded python C:\Program Files\Python36\python.EXE
gyp verb check python version `C:\Program Files\Python36\python.EXE -c ""import platform; print(platform.python_version());""` returned: ""3.6.4\r\n""
gyp verb could not find ""C:\Program Files\Python36\python.EXE"". checking python launcher
gyp verb could not find ""C:\Program Files\Python36\python.EXE"". guessing location
gyp verb ensuring that file exists: C:\Python27\python.exe
gyp ERR! configure error
gyp ERR! stack Error: Can't find Python executable ""C:\Program Files\Python36\python.EXE"", you can set the PYTHON env variable.
",9,15,0,0,
552,48958282,48958305,18974,How do I split a string into several columns in a dataframe with pandas Python?,2,<python><pandas>,15,"<p>I am aware of the following questions:</p>

<p>1.) <a href=""https://stackoverflow.com/questions/25252200/how-to-split-a-column-based-on-several-string-indices-using-pandas"">How to split a column based on several string indices using pandas?</a>
2.) <a href=""https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-columns"">How do I split text in a column into multiple rows?</a></p>

<p>I want to split these into several new columns though. Suppose I have a dataframe that looks like this:</p>

<pre><code>id    | string
-----------------------------
1     | astring, isa, string
2     | another, string, la
3     | 123, 232, another
</code></pre>

<p>I know that using:</p>

<pre><code>df['string'].str.split(',')
</code></pre>

<p>I can split a string. But as a next step, I want to efficiently put the split string into new columns like so:</p>

<pre><code>id    | string_1 | string_2 | string_3
-----------------|---------------------
1     | astring  | isa      | string
2     | another  | string   | la
3     | 123      | 232      | another
---------------------------------------
</code></pre>

<p>I could for example do this:</p>

<pre><code>for index, row in df.iterrows():
    i = 0
    for item in row['string'].split():
        df.set_values(index, 'string_{0}'.format(i), item)
        i = i + 1
</code></pre>

<p>But how could one achieve the same result more elegantly?a</p>
",5848524,2206,24-02-2018 01:00,24-02-2018 01:04,0,2206,41,1,25,90,"{'badge_counts': {'bronze': 41, 'silver': 25, 'gold': 1}, 'account_id': 7722855, 'is_employee': False, 'last_modified_date': 1707527700, 'last_access_date': 1690089234, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2206, 'creation_date': 1453921204, 'user_type': 'registered', 'user_id': 5848524, 'accept_rate': 90, 'location': 'San Diego, CA, United States', 'website_url': 'http://www.biobasedideas.com', 'link': 'https://stackoverflow.com/users/5848524/thornhale', 'profile_image': 'https://i.stack.imgur.com/hEc4a.png?s=256&g=1', 'display_name': 'Thornhale'}","I am aware of the following questions: 1.) How to split a column based on several string indices using pandas? 2.) How do I split text in a column into multiple rows? I want to split these into several new columns though. Suppose I have a dataframe that looks like this: I know that using: I can split a string. But as a next step, I want to efficiently put the split string into new columns like so: I could for example do this: But how could one achieve the same result more elegantly?a","id    | string
-----------------------------
1     | astring, isa, string
2     | another, string, la
3     | 123, 232, another
 df['string'].str.split(',')
 id    | string_1 | string_2 | string_3
-----------------|---------------------
1     | astring  | isa      | string
2     | another  | string   | la
3     | 123      | 232      | another
---------------------------------------
 for index, row in df.iterrows():
    i = 0
    for item in row['string'].split():
        df.set_values(index, 'string_{0}'.format(i), item)
        i = i + 1
",13,39,0,2,
553,48988182,49045202,2907,How to achieve stratified K fold splitting for arbitrary number of categorical variables?,1,<python><pandas><numpy><machine-learning><scikit-learn>,12,"<p>I have a dataframe of the form, <strong>df</strong>:</p>

<pre><code>    cat_var_1    cat_var_2     num_var_1
0    Orange       Monkey         34
1    Banana        Cat           56
2    Orange        Dog           22
3    Banana       Monkey          6
..
</code></pre>

<p>Suppose the possible values of cat_var_1 in the dataset have the ratios- ['Orange': 0.6, 'Banana': 0.4] and the possible values of cat_var_2 have the ratios ['Monkey': 0.2, 'Cat': 0.7, 'Dog': 0.1].</p>

<p>How to I split the data into train, test and validation sets (60:20:20 split) such that the ratios of the categorical variables remain preserved? In practice, these variables can be of any number, not just two. Also, clearly, the exact ratios may never be achieved in practice, but we would like it to be as near as possible.</p>

<p>I have looked into the StratifiedKFold method from sklearn described here: <a href=""https://stackoverflow.com/questions/29082001/how-to-split-a-dataset-into-training-and-validation-set-keeping-ratio-between-cl"">how to split a dataset into training and validation set keeping ratio between classes?</a> but this is restricted to evaluating on the basis of one categorical variable only.</p>

<p>Additionally, I would be grateful if you could provide the complexity of the solution you achieve.</p>
",5858873,2585,26-02-2018 12:07,01-03-2018 08:08,3,2595,39,2,21,88,"{'badge_counts': {'bronze': 39, 'silver': 21, 'gold': 2}, 'account_id': 7737921, 'is_employee': False, 'last_modified_date': 1607614507, 'last_access_date': 1691688254, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2595, 'creation_date': 1454096195, 'user_type': 'registered', 'user_id': 5858873, 'accept_rate': 88, 'website_url': '', 'link': 'https://stackoverflow.com/users/5858873/melsauce', 'profile_image': 'https://www.gravatar.com/avatar/dfcba3a901f91690f84c9839bcf72cf6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Melsauce'}","I have a dataframe of the form, df: Suppose the possible values of cat_var_1 in the dataset have the ratios- ['Orange': 0.6, 'Banana': 0.4] and the possible values of cat_var_2 have the ratios ['Monkey': 0.2, 'Cat': 0.7, 'Dog': 0.1]. How to I split the data into train, test and validation sets (60:20:20 split) such that the ratios of the categorical variables remain preserved? In practice, these variables can be of any number, not just two. Also, clearly, the exact ratios may never be achieved in practice, but we would like it to be as near as possible. I have looked into the StratifiedKFold method from sklearn described here: how to split a dataset into training and validation set keeping ratio between classes? but this is restricted to evaluating on the basis of one categorical variable only. Additionally, I would be grateful if you could provide the complexity of the solution you achieve.","    cat_var_1    cat_var_2     num_var_1
0    Orange       Monkey         34
1    Banana        Cat           56
2    Orange        Dog           22
3    Banana       Monkey          6
..
",5,17,0,1,
554,49141969,49143979,11935,Vectorized groupby with NumPy,5,<python><numpy>,17,"<p>Pandas has a widely-used <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""noreferrer"">groupby</a> facility to split up a DataFrame based on a corresponding mapping, from which you can apply a calculation on each subgroup and recombine the results.</p>

<p>Can this be done flexibly in NumPy without a native Python for-loop?  With a Python loop, this would look like:</p>

<pre><code>&gt;&gt;&gt; import numpy as np

&gt;&gt;&gt; X = np.arange(10).reshape(5, 2)
&gt;&gt;&gt; groups = np.array([0, 0, 0, 1, 1])

# Split up elements (rows) of `X` based on their element wise group
&gt;&gt;&gt; np.array([X[groups==i].sum() for i in np.unique(groups)])
array([15, 30])
</code></pre>

<p>Above 15 is the sum of the first three rows of <code>X</code>, and 30 is the sum of the remaining two.</p>

<p>By ""flexibly,” I just mean that we aren't focusing on one particular computation such as sum, count, maximum, etc, but rather passing any computation to the grouped arrays.</p>

<p>If not, is there a faster approach than the above?</p>
",7954504,39578,06-03-2018 23:55,07-03-2018 04:12,1,39618,244,34,154,99,"{'badge_counts': {'bronze': 244, 'silver': 154, 'gold': 34}, 'account_id': 10815338, 'is_employee': False, 'last_modified_date': 1709973300, 'last_access_date': 1705676794, 'reputation_change_year': 301, 'reputation_change_quarter': 301, 'reputation_change_month': 81, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 39618, 'creation_date': 1493765210, 'user_type': 'registered', 'user_id': 7954504, 'accept_rate': 99, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/7954504/brad-solomon', 'profile_image': 'https://i.stack.imgur.com/6POkt.jpg?s=256&g=1', 'display_name': 'Brad Solomon'}","Pandas has a widely-used groupby facility to split up a DataFrame based on a corresponding mapping, from which you can apply a calculation on each subgroup and recombine the results. Can this be done flexibly in NumPy without a native Python for-loop? With a Python loop, this would look like: Above 15 is the sum of the first three rows of , and 30 is the sum of the remaining two. By ""flexibly,” I just mean that we aren't focusing on one particular computation such as sum, count, maximum, etc, but rather passing any computation to the grouped arrays. If not, is there a faster approach than the above?","&gt;&gt;&gt; import numpy as np

&gt;&gt;&gt; X = np.arange(10).reshape(5, 2)
&gt;&gt;&gt; groups = np.array([0, 0, 0, 1, 1])

# Split up elements (rows) of `X` based on their element wise group
&gt;&gt;&gt; np.array([X[groups==i].sum() for i in np.unique(groups)])
array([15, 30])
 X",6,19,0,1,
555,48072655,48072961,9477,Django settings.cpython-36.pyc not ignored by gitignore,3,<python><django><git><pycharm><gitkraken>,11,"<p>The file in my application <code>settings.cpython-36.pyc</code> is not being ignored even though I have added it to the <code>.gitignore</code> file.</p>

<p>My gitignore code:</p>

<pre><code>*.log
*.pot
*.pyc
.idea
LearnDjango/__pycache__/
venv/
/LearnDjango/settings.py
LearnDjango/__pycache__/settings.cpython-36.pyc
</code></pre>

<p><a href=""https://i.stack.imgur.com/ibtp1.png"" rel=""noreferrer"">Gitkraken view, you can see it still picks it up</a></p>

<p>This line <code>LearnDjango/__pycache__/</code> in the gitignore file ignores the other <code>.cpython-36.pyc</code> files but not <code>settings.cpython-36.pyc</code></p>

<p><a href=""https://i.stack.imgur.com/nBLD7.png"" rel=""noreferrer"">View of <code>__pycache__</code> folder, the 1st and 3rd files are ignored but not the 2nd</a></p>

<p>P.S 
I am new to Django and git.</p>
",7975480,113,03-01-2018 07:00,03-01-2018 07:26,0,113,8,0,1,,"{'badge_counts': {'bronze': 8, 'silver': 1, 'gold': 0}, 'account_id': 10846038, 'is_employee': False, 'last_modified_date': 1573678655, 'last_access_date': 1700662837, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 113, 'creation_date': 1494142132, 'user_type': 'registered', 'user_id': 7975480, 'website_url': '', 'link': 'https://stackoverflow.com/users/7975480/akhilesh-shiuram', 'profile_image': 'https://lh4.googleusercontent.com/-Qz8AOH4XLyY/AAAAAAAAAAI/AAAAAAAAACM/OnhG8ZXrKdo/photo.jpg?sz=256', 'display_name': 'Akhilesh Shiuram'}","The file in my application is not being ignored even though I have added it to the file. My gitignore code: Gitkraken view, you can see it still picks it up This line in the gitignore file ignores the other files but not View of folder, the 1st and 3rd files are ignored but not the 2nd P.S I am new to Django and git.","settings.cpython-36.pyc .gitignore *.log
*.pot
*.pyc
.idea
LearnDjango/__pycache__/
venv/
/LearnDjango/settings.py
LearnDjango/__pycache__/settings.cpython-36.pyc
 LearnDjango/__pycache__/ .cpython-36.pyc settings.cpython-36.pyc __pycache__",1,22,0,2,
556,48218455,48218483,8279,How to create an edge list dataframe from a adjacency matrix in Python?,4,<python><pandas><dataframe>,16,"<p>I have a pandas dataframe (think of if as a weighted adjacency matrix of nodes in a network) of the form, <strong>df</strong>,</p>

<pre><code>    A    B    C    D
A   0   0.5   0.5  0 
B   1    0    0    0
C   0.8  0    0   0.2
D   0    0    1    0
</code></pre>

<p>I want to get a dataframe that instead represents an edge list. for the above example, I would need something of the form, <strong>edge_list_df</strong>,</p>

<pre><code>    Source    Target    Weight    
0   A           B        0.5 
1   A           C        0.5
2   A           D        0
3   B           A        1
4   B           C        0
5   B           D        0
6   C           A        0.8
7   C           B        0
8   C           D        0.2
9   D           A        0
10  D           B        0
11  D           C        1
</code></pre>

<p>What is the most efficient way to create this?</p>
",5858873,2585,12-01-2018 01:50,12-01-2018 01:55,0,2595,39,2,21,88,"{'badge_counts': {'bronze': 39, 'silver': 21, 'gold': 2}, 'account_id': 7737921, 'is_employee': False, 'last_modified_date': 1607614507, 'last_access_date': 1691688254, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2595, 'creation_date': 1454096195, 'user_type': 'registered', 'user_id': 5858873, 'accept_rate': 88, 'website_url': '', 'link': 'https://stackoverflow.com/users/5858873/melsauce', 'profile_image': 'https://www.gravatar.com/avatar/dfcba3a901f91690f84c9839bcf72cf6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Melsauce'}","I have a pandas dataframe (think of if as a weighted adjacency matrix of nodes in a network) of the form, df, I want to get a dataframe that instead represents an edge list. for the above example, I would need something of the form, edge_list_df, What is the most efficient way to create this?","    A    B    C    D
A   0   0.5   0.5  0 
B   1    0    0    0
C   0.8  0    0   0.2
D   0    0    1    0
     Source    Target    Weight    
0   A           B        0.5 
1   A           C        0.5
2   A           D        0
3   B           A        1
4   B           C        0
5   B           D        0
6   C           A        0.8
7   C           B        0
8   C           D        0.2
9   D           A        0
10  D           B        0
11  D           C        1
",16,27,0,0,
557,48898104,48963537,4820,Q-values exploding when training DQN,2,<python><tensorflow><machine-learning><neural-network><keras>,11,"<p>I'm training a DQN to play OpenAI's Atari environment, but the Q-values of my network quickly explode far above what is realistic.</p>

<p>Here's the relevant portion of the code:</p>

<pre><code>for state, action, reward, next_state, done in minibatch:
        if not done:
            # To save on memory, next_state is just one frame
            # So we have to add it to the current state to get the actual input for the network
            next_4_states = np.array(state)
            next_4_states = np.roll(next_4_states, 1, axis=3)
            next_4_states[:, :, :, 0] = next_state
            target = reward + self.gamma * \
                np.amax(self.target_model.predict(next_4_states))
        else:
            target = reward
        target_f = self.target_model.predict(state)
        target_f[0][action] = target

        self.target_model.fit(state, target_f, epochs=1, verbose=0)
</code></pre>

<p>The discount factor is 0.99 (it doesn't happen with discount factor 0.9, but also doesn't converge because it can't think far enough ahead).</p>

<p>Stepping through the code, the reason it's happening is all the Q values that aren't meant to be updated (the ones for actions we didn't take) increase slightly. It's my understanding that passing the networks own output to the network during training should keep the output the same, not increase or decrease it. Is there something wrong with my model? Is there some way I can mask the update so it only updates the relevant Q value?</p>

<p>EDIT: My model creation code is here:</p>

<pre><code>def create_model(self, input_shape, num_actions, learning_rate):
        model = Sequential()
        model.add(Convolution2D(32, 8, strides=(4, 4),
                                activation='relu', input_shape=(input_shape)))
        model.add(Convolution2D(64, 4, strides=(2, 2), activation='relu'))
        model.add(Convolution2D(64, 3, strides=(1, 1), activation='relu'))
        model.add(Flatten())
        model.add(Dense(512, activation='relu'))
        model.add(Dense(num_actions))

        model.compile(loss='mse', optimizer=Adam(lr=learning_rate))

        return model
</code></pre>

<p>I create two of these. One for the online network and one for the target.</p>
",5905951,1833,21-02-2018 04:18,24-02-2018 13:46,3,1833,37,1,22,80,"{'badge_counts': {'bronze': 37, 'silver': 22, 'gold': 1}, 'account_id': 7809129, 'is_employee': False, 'last_modified_date': 1689989700, 'last_access_date': 1711035811, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1833, 'creation_date': 1455065741, 'user_type': 'registered', 'user_id': 5905951, 'accept_rate': 80, 'location': 'Bucharest, Romania', 'website_url': '', 'link': 'https://stackoverflow.com/users/5905951/omegastick', 'profile_image': 'https://www.gravatar.com/avatar/f4022daa1c2066b0a2ac23658f573a60?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Omegastick'}","I'm training a DQN to play OpenAI's Atari environment, but the Q-values of my network quickly explode far above what is realistic. Here's the relevant portion of the code: The discount factor is 0.99 (it doesn't happen with discount factor 0.9, but also doesn't converge because it can't think far enough ahead). Stepping through the code, the reason it's happening is all the Q values that aren't meant to be updated (the ones for actions we didn't take) increase slightly. It's my understanding that passing the networks own output to the network during training should keep the output the same, not increase or decrease it. Is there something wrong with my model? Is there some way I can mask the update so it only updates the relevant Q value? EDIT: My model creation code is here: I create two of these. One for the online network and one for the target.","for state, action, reward, next_state, done in minibatch:
        if not done:
            # To save on memory, next_state is just one frame
            # So we have to add it to the current state to get the actual input for the network
            next_4_states = np.array(state)
            next_4_states = np.roll(next_4_states, 1, axis=3)
            next_4_states[:, :, :, 0] = next_state
            target = reward + self.gamma * \
                np.amax(self.target_model.predict(next_4_states))
        else:
            target = reward
        target_f = self.target_model.predict(state)
        target_f[0][action] = target

        self.target_model.fit(state, target_f, epochs=1, verbose=0)
 def create_model(self, input_shape, num_actions, learning_rate):
        model = Sequential()
        model.add(Convolution2D(32, 8, strides=(4, 4),
                                activation='relu', input_shape=(input_shape)))
        model.add(Convolution2D(64, 4, strides=(2, 2), activation='relu'))
        model.add(Convolution2D(64, 3, strides=(1, 1), activation='relu'))
        model.add(Flatten())
        model.add(Dense(512, activation='relu'))
        model.add(Dense(num_actions))

        model.compile(loss='mse', optimizer=Adam(lr=learning_rate))

        return model
",26,43,0,0,
558,49330905,49333864,15460,How to run a coroutine and wait it result from a sync func when the loop is running?,3,<python><python-3.x><python-asyncio>,18,"<p>I have a code like the foolowing:</p>

<pre><code>def render():
    loop = asyncio.get_event_loop()

    async def test():
        await asyncio.sleep(2)
        print(""hi"")
        return 200

    if loop.is_running():
        result = asyncio.ensure_future(test())
    else:
        result = loop.run_until_complete(test())
</code></pre>

<p>When the <code>loop</code> is not running is quite easy, just use <code>loop.run_until_complete</code> and it return the coro result but if the loop is already running (my blocking code running in app which is already running the loop) I cannot use <code>loop.run_until_complete</code> since it will raise an exception; when I call <code>asyncio.ensure_future</code> the task gets scheduled and run, but I want to wait there for the result, does anybody knows how to do this? Docs are not very clear how to do this.</p>

<p>I tried passing a <code>concurrent.futures.Future</code> calling <code>set_result</code> inside the coro and then calling <code>Future.result()</code> on my blocking code, but it doesn't work, it blocks there and do not let anything else to run. ANy help would be appreciated.</p>
",6045468,371,16-03-2018 23:38,17-03-2018 08:07,1,371,12,2,4,44,"{'badge_counts': {'bronze': 12, 'silver': 4, 'gold': 2}, 'account_id': 8014809, 'is_employee': False, 'last_modified_date': 1573679172, 'last_access_date': 1567268179, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 371, 'creation_date': 1457621611, 'user_type': 'registered', 'user_id': 6045468, 'accept_rate': 44, 'link': 'https://stackoverflow.com/users/6045468/ordani-sanchez', 'profile_image': 'https://lh6.googleusercontent.com/-H7LV0SJMYCE/AAAAAAAAAAI/AAAAAAAABso/5TNLInaxe6U/photo.jpg?sz=256', 'display_name': 'Ordani Sanchez'}","I have a code like the foolowing: When the is not running is quite easy, just use and it return the coro result but if the loop is already running (my blocking code running in app which is already running the loop) I cannot use since it will raise an exception; when I call the task gets scheduled and run, but I want to wait there for the result, does anybody knows how to do this? Docs are not very clear how to do this. I tried passing a calling inside the coro and then calling on my blocking code, but it doesn't work, it blocks there and do not let anything else to run. ANy help would be appreciated.","def render():
    loop = asyncio.get_event_loop()

    async def test():
        await asyncio.sleep(2)
        print(""hi"")
        return 200

    if loop.is_running():
        result = asyncio.ensure_future(test())
    else:
        result = loop.run_until_complete(test())
 loop loop.run_until_complete loop.run_until_complete asyncio.ensure_future concurrent.futures.Future set_result Future.result()",4,19,0,0,
559,49194147,49194433,7211,How to set only one word in a specific color using Click.secho,3,<python><click>,11,"<p>I'm using the click module.</p>

<pre><code>pip install click
</code></pre>

<p>This gives me red text</p>

<pre><code>import click
click.secho('Error: This error is ...xx', fg='red')
</code></pre>

<p>Now I want that only 'Error:' is shown in red. How can I do this using <code>click.secho</code>?</p>
",6077803,14428,09-03-2018 12:47,09-03-2018 13:02,0,14460,221,42,122,62,"{'badge_counts': {'bronze': 221, 'silver': 122, 'gold': 42}, 'collectives': [{'collective': {'tags': ['amazon-elastic-beanstalk', 'aws-fargate', 'aws-sam', 'aws-codecommit', 'amazon-glacier', 'amazon-ami', 'aws-security-hub', 'alexa-sdk-nodejs', 'amazon-iam', 'amazon-guardduty', 'aws-glue', 'aws-sdk-ruby', 'amazon-cloudfront', 'aws-batch', 'aws-mediatailor', 'aws-global-accelerator', 'amazon-neptune', 'aws-sdk-go-v2', 'amazon-dynamodb-dax', 'aws-vpn', 'amazon-sumerian', 'aws-ssm', 'amazon-route53', 'aws-app-config', 'aws-sdk-ios', 'aws-cli', 'aws-app-mesh', 'aws-event-bridge', 'aws-directory-services', 'amazon-web-services', 'aws-copilot-cli', 'aws-transfer-family', 'aws-parameter-store', 'amazon-imagebuilder', 'amazon-sagemaker', 'amazon-workdocs', 'amazon-keyspaces', 'amazon-ecr', 'amazon-elb', 'aws-cloudformation', 'aws-config', 'aws-snowball', 'aws-sdk-mock', 'amazon-app-runner', 'aws-iot-analytics', 'amazon-sns', 'amazon-memory-db', 'aws-pinpoint', 'aws-deeplens', 'amazon-elasticache', 'aws-mediastore', 'amazon-bedrock', 'amazon-athena', 'amazon-gamelift', 'aws-codecatalyst', 'aws-graviton', 'aws-codeartifact', 'aws-elb', 'aws-cloudmap', 'aws-codeguru', 'amazon-translate', 'aws-sdk-java', 'aws-resource-group', 'amazon-data-pipeline', 'aws-sdk-js', 'amazon-ivs', 'aws-sdk-cpp', 'amazon-kinesis-analytics', 'amazon-cloudwatch', 'amazon-cloudhsm', 'aws-iot-sitewise', 'amazon-vpc', 'alexa-smart-home-skill', 'amazon-kendra', 'amazon-inspector', 'aws-datasync', 'aws-cloud9', 'amazon-ecs', 'amazon-rekognition', 'amazon-swf', 'aws-media-live', 'aws-sdk-js-v3', 'amazon-fsx', 'amazon-s3-select', 'aws-sdk-nodejs', 'aws-iam-identity-center', 'aws-chatbot', 'amazon-opensearch', 'aws-lambda', 'aws-lake-formation', 'aws-cdk', 'amazon-ses', 'aws-security-group', 'aws-mediapackage', 'amazon-connect', 'amazon-qldb', 'aws-iot-core', 'aws-sdk-rust', 'amazon-elastic-transcoder', 'aws-code-deploy', 'aws-serverless', 'amazon-honeycode', 'amazon-ec2', 'alexa-interaction-model', 'aws-xray', 'amazon-waf', 'aws-elemental', 'amazon-sqs', 'amazon-kms', 'aws-certificate-manager', 'aws-sam-cli', 'amazon-kinesis-video-streams', 'aws-lambda-powertools', 'amazon-ec2-spot-market', 'aws-documentdb', 'aws-control-tower', 'aws-service-catalog', 'aws-direct-connect', 'aws-billing', 'aws-iot', 'amazon-cloudwatchlogs', 'amazon-textract', 'alexa-sdk-python', 'aws-lambda-edge', 'amazon-dynamodb', 'amazon-rds', 'aws-iot-events', 'alexa-smapi', 'alexa-flash-briefing-skill', 'aws-dms', 'aws-mediaconnect', 'aws-organizations', 'amazon-macie', 'aws-sdk-comprehend', 'aws-device-farm', 'amazon-redshift-spectrum', 'aws-appsync', 'alexa-account-linking', 'amazon-transcribe', 'aws-acm', 'aws-opsworks', 'aws-step-functions', 'amazon-simpledb', 'amazon-lightsail', 'alexa-presentation-language', 'aws-amplify', 'amazon-workspaces', 'amazon-aurora', 'elastic-ip', 'aws-codepipeline', 'amazon-managed-blockchain', 'aws-application-load-balancer', 'amazon-forecast', 'aws-cloudshell', 'aws-mobilehub', 'aws-reserved-instances', 'amazon-efs', 'aws-sdk', 'aws-backup', 'amazon-timestream', 'amazon-cloudtrail', 'aws-sdk-go', 'amazon-appflow', 'amazon-emr', 'amazon-elasticsearch', 'aws-iot-greengrass', 'aws-sct', 'aws-private-link', 'amazon-quicksight', 'aws-fis', 'aws-sdk-net', 'alexa-skills-kit', 'amazon-kinesis-firehose', 'aws-sdk-java-2.0', 'amazon-ebs', 'aws-codestar', 'aws-sdk-android', 'aws-appstream', 'amazon-s3', 'amazon-lex', 'amazon-cloudsearch', 'aws-databrew', 'amazon-cognito', 'aws-elastictranscoder', 'amazon-workmail', 'amazon-comprehend', 'aws-auto-scaling', 'aws-codebuild', 'aws-api-gateway', 'aws-sso', 'amazon-eks', 'aws-storage-gateway', 'amazon-mq', 'aws-data-exchange', 'amazon-location-service', 'amazon-kinesis', 'amazon-sagemaker-compilers', 'aws-secrets-manager', 'aws-msk', 'amazon-personalize', 'aws-nlb', 'amazon-redshift', 'aws-copilot', 'aws-media-convert', 'amazon-polly'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 8062719, 'is_employee': False, 'last_modified_date': 1711157400, 'last_access_date': 1700647641, 'reputation_change_year': 252, 'reputation_change_quarter': 252, 'reputation_change_month': 42, 'reputation_change_week': 12, 'reputation_change_day': -8, 'reputation': 14460, 'creation_date': 1458226900, 'user_type': 'registered', 'user_id': 6077803, 'accept_rate': 62, 'link': 'https://stackoverflow.com/users/6077803/dencowboy', 'profile_image': 'https://www.gravatar.com/avatar/8d254992655f04fcecb67c23ebae432a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'DenCowboy'}",I'm using the click module. This gives me red text Now I want that only 'Error:' is shown in red. How can I do this using ?,"pip install click
 import click
click.secho('Error: This error is ...xx', fg='red')
 click.secho",0,12,0,0,
560,48407790,48407834,9075,Restricting all the views to authenticated users in Django,2,<python><django>,15,"<p>I'm new to Django and I'm working on a project which has a login page as its index and a signup page. The rest of the pages all must be restricted to logged in users and if an unauthenticated user attempts to reach them, he/she must be redirected to the login page.</p>

<p>I see that <code>@login_required</code> decorator would make a single view restricted to logged in users but is there a better way to make all the views restricted and only a few available to unauthenticated users?</p>
",7145310,496,23-01-2018 17:41,23-01-2018 17:44,0,496,18,0,5,100,"{'badge_counts': {'bronze': 18, 'silver': 5, 'gold': 0}, 'account_id': 9625581, 'is_employee': False, 'last_modified_date': 1619837700, 'last_access_date': 1692388468, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 496, 'creation_date': 1478855463, 'user_type': 'registered', 'user_id': 7145310, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/7145310/mohammed-farahmand', 'profile_image': 'https://lh3.googleusercontent.com/-S9lf9TRQa18/AAAAAAAAAAI/AAAAAAAAACU/CEVO_20SnkI/photo.jpg?sz=256', 'display_name': 'Mohammed Farahmand'}","I'm new to Django and I'm working on a project which has a login page as its index and a signup page. The rest of the pages all must be restricted to logged in users and if an unauthenticated user attempts to reach them, he/she must be redirected to the login page. I see that decorator would make a single view restricted to logged in users but is there a better way to make all the views restricted and only a few available to unauthenticated users?",@login_required,-1,3,0,0,
561,48402009,48559107,38244,Given input size: (128x1x1). Calculated output size: (128x0x0). Output size is too small,2,<python><neural-network><deep-learning><conv-neural-network><pytorch>,14,"<p>I am trying to train a U-Net which looks like this </p>

<pre><code>`class UNet(nn.Module):
def __init__(self, imsize):
    super(UNet, self).__init__()
    self.imsize = imsize

    self.activation = F.relu
    self.pool1 = nn.MaxPool2d(2)
    self.pool2 = nn.MaxPool2d(2)
    self.pool3 = nn.MaxPool2d(2)
    self.pool4 = nn.MaxPool2d(2)
    self.conv_block1_64 = UNetConvBlock(4, 64)
    self.conv_block64_128 = UNetConvBlock(64, 128)
    self.conv_block128_256 = UNetConvBlock(128, 256)
    self.conv_block256_512 = UNetConvBlock(256, 512)
    self.conv_block512_1024 = UNetConvBlock(512, 1024)

    self.up_block1024_512 = UNetUpBlock(1024, 512)
    self.up_block512_256 = UNetUpBlock(512, 256)
    self.up_block256_128 = UNetUpBlock(256, 128)
    self.up_block128_64 = UNetUpBlock(128, 64)

    self.last = nn.Conv2d(64, 1, 1)`
</code></pre>

<p>The loss function i am using is </p>

<pre><code>`class BCELoss2d(nn.Module):

def __init__(self, weight=None, size_average=True):
    super(BCELoss2d, self).__init__()
    self.bce_loss = nn.BCELoss(weight, size_average)

def forward(self, logits, targets):
    probs = F.sigmoid(logits)
    probs_flat = probs.view(-1)
    targets_flat = targets.view(-1)
    return self.bce_loss(probs_flat, targets_flat)`
</code></pre>

<p>The input image tensor is [1,1,68,68] and labels are also of the same shape </p>

<p>I get this error:</p>

<pre><code>&lt;ipython-input-72-270210759010&gt; in forward(self, x)
 75 
 76         block4 = self.conv_block256_512(pool3)
---&gt; 77         pool4 = self.pool4(block4)
     78 
  79         block5 = self.conv_block512_1024(pool4)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in _    _call__(self, *input, **kwargs)
    323         for hook in self._forward_pre_hooks.values():
    324             hook(self, input)
 325         result = self.forward(*input, **kwargs)
    326         for hook in self._forward_hooks.values():
    327             hook_result = hook(self, input, result)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/pooling.py in forward(self, input)
    141         return F.max_pool2d(input, self.kernel_size, self.stride,
    142                             self.padding, self.dilation, self.ceil_mode,
--&gt; 143                             self.return_indices)
    144 
    145     def __repr__(self):

/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)
    332     See :class:`~torch.nn.MaxPool2d` for details.
    333     """"""
--&gt; 334     ret = torch._C._nn.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
    335     return ret if return_indices else ret[0]
    336 

RuntimeError: Given input size: (128x1x1). Calculated output size: (128x0x0). Output size is too small at /pytorch/torch/lib/THCUNN/generic/SpatialDilatedMaxPooling.cu:69
</code></pre>

<p>I'm guessing I'm making a mistake in my channel size or pooling size but i'm not sure where exactly is the mistake.</p>
",8176285,9249,23-01-2018 12:40,01-02-2018 09:17,9,9339,68,14,40,73,"{'badge_counts': {'bronze': 68, 'silver': 40, 'gold': 14}, 'account_id': 11138885, 'is_employee': False, 'last_modified_date': 1671307500, 'last_access_date': 1591387367, 'reputation_change_year': 310, 'reputation_change_quarter': 310, 'reputation_change_month': 100, 'reputation_change_week': 50, 'reputation_change_day': 0, 'reputation': 9339, 'creation_date': 1497711892, 'user_type': 'registered', 'user_id': 8176285, 'accept_rate': 73, 'link': 'https://stackoverflow.com/users/8176285/ryan', 'profile_image': 'https://www.gravatar.com/avatar/51535938bd9cf87d44432562af9eb914?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ryan '}","I am trying to train a U-Net which looks like this The loss function i am using is The input image tensor is [1,1,68,68] and labels are also of the same shape I get this error: I'm guessing I'm making a mistake in my channel size or pooling size but i'm not sure where exactly is the mistake.","`class UNet(nn.Module):
def __init__(self, imsize):
    super(UNet, self).__init__()
    self.imsize = imsize

    self.activation = F.relu
    self.pool1 = nn.MaxPool2d(2)
    self.pool2 = nn.MaxPool2d(2)
    self.pool3 = nn.MaxPool2d(2)
    self.pool4 = nn.MaxPool2d(2)
    self.conv_block1_64 = UNetConvBlock(4, 64)
    self.conv_block64_128 = UNetConvBlock(64, 128)
    self.conv_block128_256 = UNetConvBlock(128, 256)
    self.conv_block256_512 = UNetConvBlock(256, 512)
    self.conv_block512_1024 = UNetConvBlock(512, 1024)

    self.up_block1024_512 = UNetUpBlock(1024, 512)
    self.up_block512_256 = UNetUpBlock(512, 256)
    self.up_block256_128 = UNetUpBlock(256, 128)
    self.up_block128_64 = UNetUpBlock(128, 64)

    self.last = nn.Conv2d(64, 1, 1)`
 `class BCELoss2d(nn.Module):

def __init__(self, weight=None, size_average=True):
    super(BCELoss2d, self).__init__()
    self.bce_loss = nn.BCELoss(weight, size_average)

def forward(self, logits, targets):
    probs = F.sigmoid(logits)
    probs_flat = probs.view(-1)
    targets_flat = targets.view(-1)
    return self.bce_loss(probs_flat, targets_flat)`
 &lt;ipython-input-72-270210759010&gt; in forward(self, x)
 75 
 76         block4 = self.conv_block256_512(pool3)
---&gt; 77         pool4 = self.pool4(block4)
     78 
  79         block5 = self.conv_block512_1024(pool4)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in _    _call__(self, *input, **kwargs)
    323         for hook in self._forward_pre_hooks.values():
    324             hook(self, input)
 325         result = self.forward(*input, **kwargs)
    326         for hook in self._forward_hooks.values():
    327             hook_result = hook(self, input, result)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/pooling.py in forward(self, input)
    141         return F.max_pool2d(input, self.kernel_size, self.stride,
    142                             self.padding, self.dilation, self.ceil_mode,
--&gt; 143                             self.return_indices)
    144 
    145     def __repr__(self):

/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)
    332     See :class:`~torch.nn.MaxPool2d` for details.
    333     """"""
--&gt; 334     ret = torch._C._nn.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
    335     return ret if return_indices else ret[0]
    336 

RuntimeError: Given input size: (128x1x1). Calculated output size: (128x0x0). Output size is too small at /pytorch/torch/lib/THCUNN/generic/SpatialDilatedMaxPooling.cu:69
",59,77,0,0,
562,49745105,50265809,187982,Could not find a version that satisfies the requirement in python,9,<python><pip><virtualenv>,19,"<p>I am trying to create virtual env with python2 in mac os from <a href=""http://sourabhbajaj.com/mac-setup/Python/virtualenv.html"" rel=""noreferrer"">here</a>.
While running <code>pip install virtualenv</code> command in terminal I am getting following error.</p>

<pre><code>Could not find a version that satisfies the requirement virtualenv (from versions: )
No matching distribution found for virtualenv
</code></pre>
",7274617,303,10-04-2018 04:01,10-05-2018 05:02,30,303,10,2,4,,"{'badge_counts': {'bronze': 10, 'silver': 4, 'gold': 2}, 'account_id': 9819148, 'is_employee': False, 'last_modified_date': 1573678822, 'last_access_date': 1711139811, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 303, 'creation_date': 1481318100, 'user_type': 'registered', 'user_id': 7274617, 'location': 'New York, NY, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/7274617/sshah', 'profile_image': 'https://www.gravatar.com/avatar/a0b09e785cad1aefbb7b8f05f5518ab5?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'sshah'}",I am trying to create virtual env with python2 in mac os from here. While running command in terminal I am getting following error.,"pip install virtualenv Could not find a version that satisfies the requirement virtualenv (from versions: )
No matching distribution found for virtualenv
",0,6,0,1,
563,49427638,49428098,22821,Removing self-loops from undirected networkx graph,4,<python><networkx>,20,"<p>I have created a graph from list of nodes using <code>networkx</code>. It has self loops. How to remove them? Following is sample:</p>
<pre class=""lang-py prettyprint-override""><code>import networkx as NX
G = NX.Graph()
G.add_edge(1,2)
G.add_edge(1,1)
print(G.edges())
</code></pre>
<p>Prints: <code>[(1, 2), (1, 1)]</code></p>
<p>I don't want <code>(1, 1)</code> edge.</p>
",7280300,2553,22-03-2018 11:34,22-03-2018 11:56,0,2553,39,6,22,100,"{'badge_counts': {'bronze': 39, 'silver': 22, 'gold': 6}, 'account_id': 9492333, 'is_employee': False, 'last_modified_date': 1710407400, 'last_access_date': 1697182530, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2553, 'creation_date': 1481460097, 'user_type': 'registered', 'user_id': 7280300, 'accept_rate': 100, 'location': 'Lahore, Pakistan', 'website_url': 'http://haroonshakeel.githubio.com', 'link': 'https://stackoverflow.com/users/7280300/haroon-s', 'profile_image': 'https://www.gravatar.com/avatar/11863cde089ba6a2d8fd91dc0f2df7a6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Haroon S.'}",I have created a graph from list of nodes using . It has self loops. How to remove them? Following is sample: Prints: I don't want edge.,"networkx import networkx as NX
G = NX.Graph()
G.add_edge(1,2)
G.add_edge(1,1)
print(G.edges())
 [(1, 2), (1, 1)] (1, 1)",1,9,0,0,
564,49153253,49153755,80048,pandas rounding when converting float to integer,4,<python><pandas><floating-point><int><rounding>,40,"<p>I've got a pandas DataFrame with a float (on decimal) index which I use to look up values (similar to a dictionary). As floats are not exactly the value they are supposed to be multiplied everything by 10 and converted it to integers <code>.astype(int)</code> before setting it as index. However this seems to do a <code>floor</code> instead of rounding. Thus 1.999999999999999992 is converted to 1 instead of 2. Rounding with the <code>pandas.DataFrame.round()</code> method before does not avoid this problem as the values are still stored as floats.</p>

<p>The original idea (which obviously rises a key error) was this:</p>

<pre><code>idx = np.arange(1,3,0.001)
s = pd.Series(range(2000))
s.index=idx
print(s[2.022])
</code></pre>

<p>trying with converting to integers:</p>

<pre><code>idx_int = idx*1000
idx_int = idx_int.astype(int)
s.index = idx_int
for i in range(1000,3000):
    print(s[i])
</code></pre>

<p>the output is always a bit random as the 'real' value of an integer can be slightly above or below the wanted value. In this case the index contains two times the value 1000 and does not contain the value 2999.</p>
",7346706,1292,07-03-2018 13:34,07-03-2018 14:00,0,1300,25,3,12,100,"{'badge_counts': {'bronze': 25, 'silver': 12, 'gold': 3}, 'account_id': 9924667, 'is_employee': False, 'last_modified_date': 1711157700, 'last_access_date': 1710763936, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 18, 'reputation_change_week': 8, 'reputation_change_day': 8, 'reputation': 1300, 'creation_date': 1482857671, 'user_type': 'registered', 'user_id': 7346706, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/7346706/nicoh', 'profile_image': 'https://www.gravatar.com/avatar/ec8c2f03d0f7ac280e23a30620c3d024?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'NicoH'}",I've got a pandas DataFrame with a float (on decimal) index which I use to look up values (similar to a dictionary). As floats are not exactly the value they are supposed to be multiplied everything by 10 and converted it to integers before setting it as index. However this seems to do a instead of rounding. Thus 1.999999999999999992 is converted to 1 instead of 2. Rounding with the method before does not avoid this problem as the values are still stored as floats. The original idea (which obviously rises a key error) was this: trying with converting to integers: the output is always a bit random as the 'real' value of an integer can be slightly above or below the wanted value. In this case the index contains two times the value 1000 and does not contain the value 2999.,".astype(int) floor pandas.DataFrame.round() idx = np.arange(1,3,0.001)
s = pd.Series(range(2000))
s.index=idx
print(s[2.022])
 idx_int = idx*1000
idx_int = idx_int.astype(int)
s.index = idx_int
for i in range(1000,3000):
    print(s[i])
",4,20,0,0,
565,49247310,49247556,44797,No module named 'PIL',6,<python><python-3.x><ubuntu><python-imaging-library>,17,"<p>I'm running into an error where when I try </p>

<p><code>from PIL import Image, ImageFilter</code></p>

<p>in a Python file I get an error stating <code>ModuleNotFoundError: No module named 'PIL'</code>.</p>

<p>So far I've tried uninstalling/reinstalling both PIL and Pillow, along with just doing <code>import Image</code>, but the error keeps on occurring and I have no idea why. All the solutions I've found so far have had no effect on my issue. </p>

<p>I'm running Python 3.5 on Ubuntu 16.04</p>
",7356374,405,13-03-2018 02:19,13-03-2018 02:51,0,415,14,1,3,,"{'badge_counts': {'bronze': 14, 'silver': 3, 'gold': 1}, 'account_id': 9939148, 'is_employee': False, 'last_modified_date': 1648687208, 'last_access_date': 1681777970, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 415, 'creation_date': 1483066164, 'user_type': 'registered', 'user_id': 7356374, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/7356374/hf1', 'profile_image': 'https://i.stack.imgur.com/cUNHI.jpg?s=256&g=1', 'display_name': 'HF1'}","I'm running into an error where when I try in a Python file I get an error stating . So far I've tried uninstalling/reinstalling both PIL and Pillow, along with just doing , but the error keeps on occurring and I have no idea why. All the solutions I've found so far have had no effect on my issue. I'm running Python 3.5 on Ubuntu 16.04","from PIL import Image, ImageFilter ModuleNotFoundError: No module named 'PIL' import Image",-3,9,0,0,
566,50268092,50268790,7707,Save 1 bit deep binary image in Python,3,<python><image><binary><computer-vision><png>,11,"<p>I have a binary image in Python and I want to save it in my pc.
I need it to be a 1 bit deep png image once stored in my computer.
How can I do that? I tried with both PIL and cv2 but I'm not able to save it with 1 bit depth.</p>
",7373736,454,10-05-2018 07:50,10-05-2018 08:38,0,464,22,2,7,,"{'badge_counts': {'bronze': 22, 'silver': 7, 'gold': 2}, 'account_id': 9966069, 'is_employee': False, 'last_modified_date': 1600202803, 'last_access_date': 1648893765, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 464, 'creation_date': 1483526956, 'user_type': 'registered', 'user_id': 7373736, 'website_url': '', 'link': 'https://stackoverflow.com/users/7373736/jimbo', 'profile_image': 'https://www.gravatar.com/avatar/9605937b67e7022ab12002899b2f374e?s=256&d=identicon&r=PG', 'display_name': 'Jimbo'}",I have a binary image in Python and I want to save it in my pc. I need it to be a 1 bit deep png image once stored in my computer. How can I do that? I tried with both PIL and cv2 but I'm not able to save it with 1 bit depth.,,0,3,0,0,
567,49567804,49567851,18388,How to reduce the size of packaged python zip files for AWS Lambda,5,<python><amazon-web-services><aws-lambda>,15,"<p>Afternoon,</p>

<p>I recently came across AWS Lambda and Azure Functions. AWS imposes a limit on the size of zipped as well as unzipped files, which for python scripts need to include all of the dependent modules. I have been using lambda-uploader to package my script and it's module dependencies, but the pandas package is too big.</p>

<p>I have seen examples of people completing machine learning and using pandas on AWS Lambda (a little outdated though) but I can't see how they're doing it. Any suggestions?</p>
",7378150,153,30-03-2018 02:57,30-03-2018 03:05,0,153,5,1,1,,"{'badge_counts': {'bronze': 5, 'silver': 1, 'gold': 1}, 'account_id': 9940519, 'is_employee': False, 'last_modified_date': 1573678792, 'last_access_date': 1570818830, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 153, 'creation_date': 1483604742, 'user_type': 'registered', 'user_id': 7378150, 'location': 'Panama City, Panama', 'link': 'https://stackoverflow.com/users/7378150/hendrix', 'profile_image': 'https://www.gravatar.com/avatar/f10fa2711812127179154e9fdeef4411?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Hendrix'}","Afternoon, I recently came across AWS Lambda and Azure Functions. AWS imposes a limit on the size of zipped as well as unzipped files, which for python scripts need to include all of the dependent modules. I have been using lambda-uploader to package my script and it's module dependencies, but the pandas package is too big. I have seen examples of people completing machine learning and using pandas on AWS Lambda (a little outdated though) but I can't see how they're doing it. Any suggestions?",,0,5,0,0,
568,50390674,51890271,4233,What's the difference between pip3 and pip?,1,<python><python-3.x><pip>,13,"<p>I know that <code>pip3</code> refers to <code>python3</code> and <code>pip</code> refers to <code>python2</code>. 
When I use <code>anaconda</code> environment and set the <code>python</code> version as <code>3.5</code>, I install a package names <code>itchat</code> as following.</p>

<pre><code>pip3 install itchat
</code></pre>

<p>The installation goes on successfully without any errors.</p>

<p><strong>But</strong> when I type the following commands, strange things happen.
<a href=""https://i.stack.imgur.com/U0Rmy.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/U0Rmy.jpg"" alt=""enter image description here""></a></p>

<p><strong>But</strong> if I just use <code>pip install itchat</code> and type <code>python</code> instead of <code>python3</code>, things go on as I think.
<a href=""https://i.stack.imgur.com/jG7NN.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jG7NN.jpg"" alt=""enter image description here""></a></p>

<p>I am wondering what leads to this result. 
Why there is the difference between <code>python</code> and <code>python3</code>?</p>

<p>Thanks for providing your answers!</p>
",7424478,306,17-05-2018 11:47,17-08-2018 07:21,92,306,21,0,4,,"{'badge_counts': {'bronze': 21, 'silver': 4, 'gold': 0}, 'account_id': 10041924, 'is_employee': False, 'last_modified_date': 1613067600, 'last_access_date': 1704699440, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 306, 'creation_date': 1484556663, 'user_type': 'registered', 'user_id': 7424478, 'location': 'Beijing, 北京市中国', 'link': 'https://stackoverflow.com/users/7424478/boooooooooms', 'profile_image': 'https://www.gravatar.com/avatar/aebb005cd07017e74194cddf9eb588f3?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Boooooooooms'}","I know that refers to and refers to . When I use environment and set the version as , I install a package names as following. The installation goes on successfully without any errors. But when I type the following commands, strange things happen. But if I just use and type instead of , things go on as I think. I am wondering what leads to this result. Why there is the difference between and ? Thanks for providing your answers!","pip3 python3 pip python2 anaconda python 3.5 itchat pip3 install itchat
 pip install itchat python python3 python python3",-13,18,2,2,
569,50279905,50280108,12854,How to install python packages in a Google Dataproc cluster,1,<python><google-cloud-platform><google-compute-engine><google-cloud-dataproc>,12,"<p>Is it possible to install python packages in a Google Dataproc cluster after the cluster is created and running?</p>

<p>I tried to use ""<code>pip install xxxxxxx</code>"" in the master command line but it does not seem to work.</p>

<p>Google's Dataproc documentation does not mention this situation.</p>
",7535911,193,10-05-2018 19:07,10-05-2018 19:22,0,193,10,1,1,,"{'badge_counts': {'bronze': 10, 'silver': 1, 'gold': 1}, 'account_id': 5557310, 'is_employee': False, 'last_modified_date': 1646082614, 'last_access_date': 1706122436, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 193, 'creation_date': 1486573970, 'user_type': 'registered', 'user_id': 7535911, 'location': 'S&#227;o Paulo, Brazil', 'website_url': 'http://pablobrenner.com.br', 'link': 'https://stackoverflow.com/users/7535911/pablo-brenner', 'profile_image': 'https://i.stack.imgur.com/IIpDe.jpg?s=256&g=1', 'display_name': 'Pablo Brenner'}","Is it possible to install python packages in a Google Dataproc cluster after the cluster is created and running? I tried to use """" in the master command line but it does not seem to work. Google's Dataproc documentation does not mention this situation.",pip install xxxxxxx,-1,5,0,0,
570,48235169,48364708,129935,How to fix AttributeError: module 'numpy' has no attribute 'square',1,<python><python-3.x><numpy><keras><attributeerror>,24,"<p>I have updated numpy to 1.14.0. I use Windows 10. I tried to run my code and I got this error:</p>

<blockquote>
  <p>AttributeError: module 'numpy' has no attribute 'square'</p>
</blockquote>

<p>Here are my imports:</p>

<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from sklearn.metrics import confusion_matrix
import math
</code></pre>
",7561641,727,12-01-2018 22:58,21-01-2018 07:46,9,727,18,3,7,100,"{'badge_counts': {'bronze': 18, 'silver': 7, 'gold': 3}, 'account_id': 10245828, 'is_employee': False, 'last_modified_date': 1686845400, 'last_access_date': 1673222811, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 727, 'creation_date': 1487057962, 'user_type': 'registered', 'user_id': 7561641, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/7561641/hoda-fakharzadeh', 'profile_image': 'https://www.gravatar.com/avatar/3ef4a40600342e9611a8c0d577e6da85?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Hoda Fakharzadeh'}",I have updated numpy to 1.14.0. I use Windows 10. I tried to run my code and I got this error: AttributeError: module 'numpy' has no attribute 'square' Here are my imports:,"%matplotlib inline
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from sklearn.metrics import confusion_matrix
import math
",5,15,0,0,
571,48157575,48168337,3257,What to do when pip & conda overlap?,1,<python><numpy><pip><conda>,14,"<p>I have a reasonable understanding of the difference between <code>conda install</code> &amp; <code>pip install</code>;  How <code>pip</code> installs python only packages &amp; <code>conda</code> can install non-python binaries.  However, there is some overlap between these two.  Which leads me to ask:</p>

<p><strong>What's the rule of thumb for whether to use <code>conda</code> or <code>pip</code> when both offer a package?</strong></p>

<p>For example, <code>TensorFlow</code> is available on both repositories but from the <a href=""https://www.tensorflow.org/install/install_linux"" rel=""noreferrer"">tensorflow docs</a>:</p>

<blockquote>
  <p>within Anaconda, we recommend installing TensorFlow with the
  <code>pip install</code> command, not with the <code>conda install</code> command.</p>
</blockquote>

<p>But, there are many other packages that overlap, like <code>numpy</code>, <code>scipy</code> etc.</p>

<p><br>
However, <a href=""https://stackoverflow.com/a/33626087/7607701"">this Stackoverflow answer</a> suggests that <code>conda install</code> should be the default &amp; <code>pip</code> should only be used if a package is unavailable from <code>conda</code>.  Is this true even for <code>TensorFlow</code> or other python-only packages?</p>
",7607701,4396,08-01-2018 20:26,09-01-2018 12:34,1,4396,43,2,27,100,"{'badge_counts': {'bronze': 43, 'silver': 27, 'gold': 2}, 'collectives': [{'collective': {'tags': ['google-cloud-storage-r', 'google-cloud-composer', 'firebase-cloud-messaging', 'google-cloud-sql', 'google-cloud-dataprep', 'google-cloud-registry', 'google-translate', 'google-cloud-tools', 'google-compute-engine', 'google-prediction', 'google-cloud-resource-manager', 'google-container-builder', 'google-cloud-shell-editor', 'google-cloud-instance-template', 'google-cloud-instances', 'firebase-performance', 'google-cloud-robotics', 'google-cloud-marketplace', 'firebase-predictions', 'vertex-ai-search', 'google-dataflow', 'google-cloud-data-fusion', 'google-cloud-networking', 'google-cloud-language', 'firebase-analytics', 'google-cloud-proxy', 'google-cloud-pubsublite', 'google-cloud-cdn', 'google-cloud-automl-nl', 'google-cloud-router', 'google-app-engine-launch', 'google-cloud-dns', 'google-cloud-spanner', 'google-cloud-python', 'google-cloud-functions', 'google-container-registry', 'google-app-engine-patch', 'firebase-admob', 'dialogflow-es-fulfillment', 'google-cloud-translate', 'firebase-app-distribution', 'google-cloud-tasks', 'google-cloud-cpp', 'cordova-plugin-firebasex', 'google-cloud-pubsub', 'google-cloud-monitoring', 'google-cloud-ops-agent', 'google-cloud-healthcare', 'react-redux-firebase', 'google-cloud-launcher', 'google-container-os', 'google-app-engine-python', 'google-cloud-ml-engine', 'firebase-mlkit', 'google-cloud-spanner-emulator', 'dialogflow-cx', 'google-cloud-http-load-balancer', 'google-cloud-vpn', 'google-cloud-dlp', 'firebase-app-indexing', 'google-cloud-api-gateway', 'google-cloud-iot', 'google-cloud-talent-solution', 'firebase-database', 'google-cloud-scheduler', 'google-cloud-build', 'google-cloud-print-privet', 'firebase-security', 'google-cloud-profiler', 'firebase', 'firebase-console', 'google-cloud-firestore', 'google-cloud-webrisk', 'firebase-machine-learning', 'google-cloud-data-transfer', 'google-cloud-repository', 'google-cloud-dataproc-metastore', 'firebase-storage', 'firebase-hosting', 'google-cloud-internal-load-balancer', 'google-app-engine', 'apigee-baas', 'google-anthos', 'firebase-polymer', 'google-cloud-storage', 'google-cloud-url-maps', 'firebase-dynamic-links', 'google-cloud-load-balancer', 'google-cloud-code', 'google-cloud-asset-inventory', 'google-cloud-iam', 'google-cloud-vertex-ai', 'google-migrate-for-compute-engine', 'firebase-admin', 'google-cloud-shell', 'google-cloud-billing', 'google-cloud-interconnect', 'google-cloud-powershell', 'google-cloud-endpoints-v2', 'google-cloud-stackdriver', 'google-cloud-sdk', 'looker', 'google-cloud-datalab', 'google-cloud-logging', 'google-cloud-ai-platform-pipelines', 'firebase-test-lab', 'rest-firebase', 'firebaseui', 'google-cloud-dataflow', 'google-cloud-deploy', 'gcloud', 'google-cloud-tpu', 'nativescript-firebase', 'google-cloud-identity-aware-proxy', 'google-cloud-network-load-balancer', 'firebase-util', 'google-cloud-armor', 'firebase-invites', 'firebase-in-app-messaging', 'firebase-assistant', 'google-cloud-nl', 'google-app-engine-deploy', 'recaptcha-enterprise', 'google-bigquery', 'firebase-extensions', 'firebase-crash-reporting', 'google-app-engine-go', 'google-cloud-node', 'google-cloud-kms', 'cloud-document-ai', 'firebase-queue', 'google-cloud-search', 'google-cloud-ml', 'dialogflow-es', 'google-cloud-ai', 'bigtable', 'firebase-realtime-database', 'google-cloud-bigtable', 'google-cloud-automl', 'google-cloud-messaging', 'firebasesimplelogin', 'google-cloud-datastore', 'jib', 'firebase-ab-testing', 'apigee', 'google-cloud-endpoints', 'google-cloud-intellij', 'google-cloud-platform', 'google-cloud-run', 'google-cloud-source-repos', 'google-cloud-visualstudio', 'firebase-authentication', 'google-container-optimized-os', 'google-cloud-memorystore', 'google-app-engine-php', 'google-cloud-test-lab', 'google-cloud-filestore', 'firebase-tools', 'react-native-firebase', 'google-app-engine-golang', 'firebase-app-check', 'google-cloud-save', 'google-cloud-identity', 'google-cloud-vision', 'looker-studio', 'firebase-remote-config', 'google-cloud-dataproc', 'google-cloud-metrics', 'stackdriver', 'firebase-cli', 'google-cloud-speech', 'google-cloud-debugger', 'firebase-notifications', 'google-cloud-php-client', 'google-cloud-transcoder', 'maven-jib', 'google-cloud-trace', 'google-cloud-workstations', 'google-fusion-tables', 'google-kubernetes-engine', 'google-cloud-print', 'firebase-job-dispatcher', 'redux-saga-firebase', 'google-cloud-recommendation', 'google-cloud-console', 'google-analytics-firebase', 'google-cloud-error-reporting'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 10312298, 'is_employee': False, 'last_modified_date': 1652902201, 'last_access_date': 1704232898, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4396, 'creation_date': 1487802950, 'user_type': 'registered', 'user_id': 7607701, 'accept_rate': 100, 'location': 'Princeton, NJ, United States', 'website_url': 'http://www.AaronNBrock.com', 'link': 'https://stackoverflow.com/users/7607701/aaron-n-brock', 'profile_image': 'https://lh3.googleusercontent.com/-0eLxeTB_SdQ/AAAAAAAAAAI/AAAAAAAAABg/XZT1oaa0wqA/photo.jpg?sz=256', 'display_name': 'Aaron N. Brock'}","I have a reasonable understanding of the difference between &amp; ; How installs python only packages &amp; can install non-python binaries. However, there is some overlap between these two. Which leads me to ask: What's the rule of thumb for whether to use or when both offer a package? For example, is available on both repositories but from the tensorflow docs: within Anaconda, we recommend installing TensorFlow with the command, not with the command. But, there are many other packages that overlap, like , etc. However, this Stackoverflow answer suggests that should be the default &amp; should only be used if a package is unavailable from . Is this true even for or other python-only packages?",conda install pip install pip conda conda pip TensorFlow pip install conda install numpy scipy conda install pip conda TensorFlow,-15,15,0,2,
572,49871020,49871203,35607,Extract file from file storage object in flask,2,<python><python-2.7><file-upload><flask><object-storage>,26,"<p>I am learning how to work with file uploads in flask. Earlier from <a href=""https://stackoverflow.com/questions/49773141/typeerror-expected-str-bytes-or-os-pathlike-object-not-filestorage-while-read"">here</a>, i worked with pdf file uploads and read the contents of it. This happens inside client.py file.</p>

<p>Now i would like to pass my file from client to server that is running locally. When i use request.file, it will get it as FileStorage Object. So, without saving or providing file path, i want to upload file from client and pass it to the server to do further process. </p>

<pre><code>class mainSessRunning():
     def proces(input):
     ...
     ...
     return result

run = mainSessRunning()

@app.route('/input', methods=['POST'])
def input():
    input_file = request.files['file']
   ...(extract file from filestorage object ""input_file"")...
    result = run.process(file) ## process is user defined function 
    return (result)
</code></pre>

<p>here i want to send the incoming file through <code>process()</code> function to server running locally. How do i do this? I came across <a href=""https://stackoverflow.com/questions/39437909/flask-filestorage-object-to-file-object?rq=1"">same question</a> but couldn't able to find anything</p>
",7719284,831,17-04-2018 06:10,17-04-2018 06:23,0,831,40,5,21,42,"{'badge_counts': {'bronze': 40, 'silver': 21, 'gold': 5}, 'account_id': 10471570, 'is_employee': False, 'last_modified_date': 1629873664, 'last_access_date': 1646025882, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 831, 'creation_date': 1489644560, 'user_type': 'registered', 'user_id': 7719284, 'accept_rate': 42, 'location': 'Planet Earth', 'website_url': '', 'link': 'https://stackoverflow.com/users/7719284/dhinar1991', 'profile_image': 'https://www.gravatar.com/avatar/d4e3432832a44d548b485cc80a68f1de?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'dhinar1991'}","I am learning how to work with file uploads in flask. Earlier from here, i worked with pdf file uploads and read the contents of it. This happens inside client.py file. Now i would like to pass my file from client to server that is running locally. When i use request.file, it will get it as FileStorage Object. So, without saving or providing file path, i want to upload file from client and pass it to the server to do further process. here i want to send the incoming file through function to server running locally. How do i do this? I came across same question but couldn't able to find anything","class mainSessRunning():
     def proces(input):
     ...
     ...
     return result

run = mainSessRunning()

@app.route('/input', methods=['POST'])
def input():
    input_file = request.files['file']
   ...(extract file from filestorage object ""input_file"")...
    result = run.process(file) ## process is user defined function 
    return (result)
 process()",12,21,0,2,
573,48647534,48647720,442224,Find difference between two data frames,19,<python><pandas><dataframe>,270,"<p>I have two data frames df1 and df2, where df2 is a subset of df1. How do I get a new data frame (df3) which is the difference between the two data frames?</p>

<p>In other word, a data frame that has all the rows/columns in df1 that are not in df2?</p>

<p><a href=""https://i.stack.imgur.com/aOCGb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/aOCGb.png"" alt=""enter image description here""></a></p>
",7719336,3791,06-02-2018 16:25,06-02-2018 16:33,0,3791,24,4,15,100,"{'badge_counts': {'bronze': 24, 'silver': 15, 'gold': 4}, 'account_id': 10471646, 'is_employee': False, 'last_modified_date': 1607614445, 'last_access_date': 1695704998, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3791, 'creation_date': 1489645256, 'user_type': 'registered', 'user_id': 7719336, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/7719336/userpygeo', 'profile_image': 'https://lh4.googleusercontent.com/-02qwV9PzbOo/AAAAAAAAAAI/AAAAAAAAABI/zubAT3aKQ2g/photo.jpg?sz=256', 'display_name': 'userPyGeo'}","I have two data frames df1 and df2, where df2 is a subset of df1. How do I get a new data frame (df3) which is the difference between the two data frames? In other word, a data frame that has all the rows/columns in df1 that are not in df2?",,0,5,1,1,
574,48690984,54396790,42227,portaudio.h: No such file or directory,5,<python><speech-recognition><google-speech-api>,44,"<p>I got the following error while trying to install pyaudio using pip3 in ubuntu 16.04:</p>

<pre><code>Collecting pyaudio
  Downloading PyAudio-0.2.11.tar.gz
Installing collected packages: pyaudio
  Running setup.py install for pyaudio ... error
    Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-mxgvewdb/pyaudio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-v55chjee-record/install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.5
    copying src/pyaudio.py -&gt; build/lib.linux-x86_64-3.5
    running build_ext
    building '_portaudio' extension
    creating build/temp.linux-x86_64-3.5
    creating build/temp.linux-x86_64-3.5/src
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.5m -c src/_portaudiomodule.c -o build/temp.linux-x86_64-3.5/src/_portaudiomodule.o
    src/_portaudiomodule.c:29:23: fatal error: portaudio.h: No such file or directory
    compilation terminated.
    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1

    ----------------------------------------
Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-mxgvewdb/pyaudio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-v55chjee-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-mxgvewdb/pyaudio/
</code></pre>
",7735772,697,08-02-2018 17:03,28-01-2019 06:45,354,697,17,1,5,,"{'badge_counts': {'bronze': 17, 'silver': 5, 'gold': 1}, 'account_id': 10495301, 'is_employee': False, 'last_modified_date': 1676686200, 'last_access_date': 1711174675, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 697, 'creation_date': 1489938515, 'user_type': 'registered', 'user_id': 7735772, 'website_url': '', 'link': 'https://stackoverflow.com/users/7735772/monojit-sarkar', 'profile_image': 'https://www.gravatar.com/avatar/470ebb616262eb87771a759281cd3cd1?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Monojit Sarkar'}",I got the following error while trying to install pyaudio using pip3 in ubuntu 16.04:,"Collecting pyaudio
  Downloading PyAudio-0.2.11.tar.gz
Installing collected packages: pyaudio
  Running setup.py install for pyaudio ... error
    Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-mxgvewdb/pyaudio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-v55chjee-record/install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.5
    copying src/pyaudio.py -&gt; build/lib.linux-x86_64-3.5
    running build_ext
    building '_portaudio' extension
    creating build/temp.linux-x86_64-3.5
    creating build/temp.linux-x86_64-3.5/src
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.5m -c src/_portaudiomodule.c -o build/temp.linux-x86_64-3.5/src/_portaudiomodule.o
    src/_portaudiomodule.c:29:23: fatal error: portaudio.h: No such file or directory
    compilation terminated.
    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1

    ----------------------------------------
Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-mxgvewdb/pyaudio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-v55chjee-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-mxgvewdb/pyaudio/
",21,25,0,0,
575,48961892,48961940,80254,Python & Pandas - Group by day and count for each day,3,<python><python-3.x><pandas><time-series>,44,"<p>I am new on pandas and for now i don't get how to arrange my time serie, take a look at it :</p>

<pre><code>date &amp; time of connection
19/06/2017 12:39
19/06/2017 12:40
19/06/2017 13:11
20/06/2017 12:02
20/06/2017 12:04
21/06/2017 09:32
21/06/2017 18:23
21/06/2017 18:51
21/06/2017 19:08
21/06/2017 19:50
22/06/2017 13:22
22/06/2017 13:41
22/06/2017 18:01
23/06/2017 16:18
23/06/2017 17:00
23/06/2017 19:25
23/06/2017 20:58
23/06/2017 21:03
23/06/2017 21:05
</code></pre>

<p>This is a sample of a dataset of 130 k raws,I tried :
<code>df.groupby('date &amp; time of connection')['date &amp; time of connection'].apply(list)</code></p>

<p>Not enough i guess</p>

<p>I think i should :</p>

<ul>
<li>Create a dictionnary with index from dd/mm/yyyy to dd/mm/yyyy </li>
<li>Convert ""date &amp; time of connection"" type dateTime to Date</li>
<li>Group and count Date of ""date &amp; time of connection""</li>
<li>Put the numbers i count inside the dictionary ?</li>
</ul>

<p>What do you think about my logic ? Do you know some tutos ?
Thank you very much</p>
",7851217,841,24-02-2018 10:40,24-02-2018 10:45,0,841,12,2,8,25,"{'badge_counts': {'bronze': 12, 'silver': 8, 'gold': 2}, 'account_id': 10664085, 'is_employee': False, 'last_modified_date': 1652947800, 'last_access_date': 1689517204, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 841, 'creation_date': 1491919561, 'user_type': 'registered', 'user_id': 7851217, 'accept_rate': 25, 'location': 'Clermont-Ferrand, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/7851217/erwan-pesle', 'profile_image': 'https://i.stack.imgur.com/z84k9.jpg?s=256&g=1', 'display_name': 'Erwan Pesle'}","I am new on pandas and for now i don't get how to arrange my time serie, take a look at it : This is a sample of a dataset of 130 k raws,I tried : Not enough i guess I think i should : Create a dictionnary with index from dd/mm/yyyy to dd/mm/yyyy Convert ""date &amp; time of connection"" type dateTime to Date Group and count Date of ""date &amp; time of connection"" Put the numbers i count inside the dictionary ? What do you think about my logic ? Do you know some tutos ? Thank you very much","date &amp; time of connection
19/06/2017 12:39
19/06/2017 12:40
19/06/2017 13:11
20/06/2017 12:02
20/06/2017 12:04
21/06/2017 09:32
21/06/2017 18:23
21/06/2017 18:51
21/06/2017 19:08
21/06/2017 19:50
22/06/2017 13:22
22/06/2017 13:41
22/06/2017 18:01
23/06/2017 16:18
23/06/2017 17:00
23/06/2017 19:25
23/06/2017 20:58
23/06/2017 21:03
23/06/2017 21:05
 df.groupby('date &amp; time of connection')['date &amp; time of connection'].apply(list)",18,40,0,0,
576,48700275,48702409,14456,imdecode returns None Python opencv2,1,<python><image><python-2.7><opencv>,11,"<p>Look at this code,</p>

<pre><code>img = cv2.imread(""image2.jpg"",0)
img_str = cv2.imencode('.jpg', img)[1] #Encodes and stores in buffer
print(img_str)

#for i,item in enumerate(img_str):    }  THIS
 #   img_str[i] = 255-item            }  IS CONFUSING

nparr = np.frombuffer(img_str, np.uint8)
img2 = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
</code></pre>

<p>My question is without this,</p>

<pre><code>#for i,item in enumerate(img_str):    }  THIS
 #   img_str[i] = 255-item            }  IS CONFUSING
</code></pre>

<p>The code runs fine imdecode returns the same! But however when I uncomment it imdecode returns <strong>None</strong></p>

<p><a href=""https://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html#imencode"" rel=""noreferrer"">Opencv docs say</a></p>

<blockquote>
  <p>If the buffer is too short or contains invalid data, the empty matrix/image is returned.</p>
</blockquote>

<p>What exactly is the invalid data that makes imdecode return none? Or any other mistake?</p>
",7863149,2611,09-02-2018 06:55,09-02-2018 09:18,0,2611,35,2,21,56,"{'badge_counts': {'bronze': 35, 'silver': 21, 'gold': 2}, 'account_id': 10681062, 'is_employee': False, 'last_modified_date': 1683495025, 'last_access_date': 1710149168, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2611, 'creation_date': 1492098058, 'user_type': 'registered', 'user_id': 7863149, 'accept_rate': 56, 'location': 'speck in the universe', 'website_url': '', 'link': 'https://stackoverflow.com/users/7863149/void', 'profile_image': 'https://i.stack.imgur.com/iv2fh.jpg?s=256&g=1', 'display_name': 'void'}","Look at this code, My question is without this, The code runs fine imdecode returns the same! But however when I uncomment it imdecode returns None Opencv docs say If the buffer is too short or contains invalid data, the empty matrix/image is returned. What exactly is the invalid data that makes imdecode return none? Or any other mistake?","img = cv2.imread(""image2.jpg"",0)
img_str = cv2.imencode('.jpg', img)[1] #Encodes and stores in buffer
print(img_str)

#for i,item in enumerate(img_str):    }  THIS
 #   img_str[i] = 255-item            }  IS CONFUSING

nparr = np.frombuffer(img_str, np.uint8)
img2 = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
 #for i,item in enumerate(img_str):    }  THIS
 #   img_str[i] = 255-item            }  IS CONFUSING
",9,28,0,1,
577,49946157,50123288,29437,jupyter notebook bad interpreter error message,7,<python><jupyter-notebook>,23,"<p>I tried to use jupyter notebook with kernel python3, but I got this error message.</p>

<pre><code>anonymous$ jupyter notebook
/usr/local/opt/python/bin/python2.7: bad interpreter: No such file or directory
</code></pre>

<p>Information about python and pip installed on mac (i used pip3 to install jupyter ):</p>

<pre><code>anonymous$ which -a python python2 python2.7 python3 python3.6
/usr/bin/python
/usr/bin/python2.7
/usr/local/bin/python3
/usr/local/bin/python3.6
anonymous$ which -a pip pip2 pip3
/usr/local/bin/pip
/usr/local/bin/pip3
</code></pre>

<p>Tried to solve it with ""brew update &amp;&amp; brew upgrade jupyter"" as the other post suggested but did not work. Got an error message saying that ""Error: jupyter not installed"".</p>
",7872857,469,20-04-2018 16:37,01-05-2018 20:22,11,469,17,1,3,50,"{'badge_counts': {'bronze': 17, 'silver': 3, 'gold': 1}, 'account_id': 10601708, 'is_employee': False, 'last_modified_date': 1595732201, 'last_access_date': 1710998933, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 469, 'creation_date': 1492296804, 'user_type': 'registered', 'user_id': 7872857, 'accept_rate': 50, 'website_url': '', 'link': 'https://stackoverflow.com/users/7872857/anonny', 'profile_image': 'https://www.gravatar.com/avatar/982a36decb6599cd1f77ce941baeb545?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Anonny'}","I tried to use jupyter notebook with kernel python3, but I got this error message. Information about python and pip installed on mac (i used pip3 to install jupyter ): Tried to solve it with ""brew update &amp;&amp; brew upgrade jupyter"" as the other post suggested but did not work. Got an error message saying that ""Error: jupyter not installed"".","anonymous$ jupyter notebook
/usr/local/opt/python/bin/python2.7: bad interpreter: No such file or directory
 anonymous$ which -a python python2 python2.7 python3 python3.6
/usr/bin/python
/usr/bin/python2.7
/usr/local/bin/python3
/usr/local/bin/python3.6
anonymous$ which -a pip pip2 pip3
/usr/local/bin/pip
/usr/local/bin/pip3
",8,19,0,0,
578,48760475,48777857,34675,Not able to upload local files in google colab,5,<python><file-upload><google-colaboratory>,24,"<p>I am trying to upload a word2vec file in Google Colaboratory from local system and use it further in the in code.</p>

<p>This is the code that I used.</p>

<pre><code>from google.colab import files
uploaded = files.upload()
</code></pre>

<p>Everytime I execute it, it is showing the following error.</p>

<blockquote>
  <p>Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable</p>
</blockquote>

<pre><code> MessageError  Traceback (most recent call last)
&lt;ipython-input-1-292f82be1b7a&gt; in &lt;module&gt;()  

      1 from google.colab import files
      2 
----&gt; 3 uploaded = files.upload()`
      4 
      5 for fn in uploaded.keys():

/usr/local/lib/python3.6/dist-packages/google/colab/files.py in upload() 

     59   result = output.eval_js(
     60       'google.colab._files._uploadFiles(""{input_id}"", ""{output_id}"")'.format(
---&gt; 61           input_id=input_id, output_id=output_id))
     62   files = collections.defaultdict(six.binary_type)
     63 

/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py in 
eval_js(script, ignore_result)

     37   if ignore_result:
     38     return
---&gt; 39   return _message.read_reply_from_input(request_id)
     40 
     41 

/usr/local/lib/python3.6/dist-packages/google/colab/_message.py in 
read_reply_from_input(message_id, timeout_sec)

     84         reply.get('colab_msg_id') == message_id):
     85       if 'error' in reply:
---&gt; 86         raise MessageError(reply['error'])
     87       return reply.get('data', None)
     88 

MessageError: TypeError: google.colab._files is undefined
</code></pre>

<p>Why is this happening? What should I do?</p>
",7873909,265,13-02-2018 06:09,14-02-2018 00:25,1,265,8,1,3,,"{'badge_counts': {'bronze': 8, 'silver': 3, 'gold': 1}, 'account_id': 10697275, 'is_employee': False, 'last_modified_date': 1573678679, 'last_access_date': 1621547172, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 265, 'creation_date': 1492331361, 'user_type': 'registered', 'user_id': 7873909, 'link': 'https://stackoverflow.com/users/7873909/megha-mishra', 'profile_image': 'https://www.gravatar.com/avatar/2fc37b402edd86233447c21eb9ff567e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'MEGHA MISHRA'}","I am trying to upload a word2vec file in Google Colaboratory from local system and use it further in the in code. This is the code that I used. Everytime I execute it, it is showing the following error. Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable Why is this happening? What should I do?","from google.colab import files
uploaded = files.upload()
  MessageError  Traceback (most recent call last)
&lt;ipython-input-1-292f82be1b7a&gt; in &lt;module&gt;()  

      1 from google.colab import files
      2 
----&gt; 3 uploaded = files.upload()`
      4 
      5 for fn in uploaded.keys():

/usr/local/lib/python3.6/dist-packages/google/colab/files.py in upload() 

     59   result = output.eval_js(
     60       'google.colab._files._uploadFiles(""{input_id}"", ""{output_id}"")'.format(
---&gt; 61           input_id=input_id, output_id=output_id))
     62   files = collections.defaultdict(six.binary_type)
     63 

/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py in 
eval_js(script, ignore_result)

     37   if ignore_result:
     38     return
---&gt; 39   return _message.read_reply_from_input(request_id)
     40 
     41 

/usr/local/lib/python3.6/dist-packages/google/colab/_message.py in 
read_reply_from_input(message_id, timeout_sec)

     84         reply.get('colab_msg_id') == message_id):
     85       if 'error' in reply:
---&gt; 86         raise MessageError(reply['error'])
     87       return reply.get('data', None)
     88 

MessageError: TypeError: google.colab._files is undefined
",36,53,0,0,
579,48825335,48884592,5461,Dynamic task definition in Airflow,2,<python><bash><airflow><orchestration>,12,"<p>I’m currently trying to use Airflow to orchestrate a process where some operators are defined dynamically and depend on the output of another (earlier) operator. </p>

<p>In the code below t1 updates a text file with new records (these are actually read from an external queue but, for simplicity, I hard coded them as  A, B and C here). Then, I want to create separate operators for each record read from that text file. These operators will create directories A, B and C, respectively, and in Airflow UI will be seen as separate bash processes Create_directory_A, Create_directory_B and Create_directory_C.</p>

<pre><code>dag = DAG('Test_DAG',
          description=""Lorem ipsum."",
          start_date=datetime(2017, 3, 20),
          schedule_interval=None,
          catchup=False)


def create_text_file(list_of_rows):
    text_file = open('text_file.txt', ""w"")
    for row in list_of_rows:
        text_file.write(row + '\n')
    text_file.close()


def read_text():
    txt_file = open('text_file.txt', 'r')
    return [element for element in txt_file.readlines()]


t1 = PythonOperator(
    task_id='Create_text_file',
    python_callable=create_text_file,
    op_args=[['A', 'B', 'C']],
    dag=dag
)

for row in read_text():
    t2 = BashOperator(
        task_id='Create_directory_{}'.format(row),
        bash_command=""mkdir {{params.dir_name}}"",
        params={'dir_name': row},
        dag=dag
    )

    t1 &gt;&gt; t2
</code></pre>

<p>In <a href=""https://airflow.apache.org/tutorial.html#it-s-a-dag-definition-file"" rel=""noreferrer"">Airflow’s documentation</a> I can see that <em>the scheduler will execute it [DAG] periodically to reflect the changes if any</em>. Does that mean that there is a risk that, even though my t1 operator is executed before t2, the bash operators are created for the list of records before the update (as that's when the DAG was evaluated)?</p>
",7036795,662,16-02-2018 11:00,20-02-2018 11:45,4,672,25,1,11,,"{'badge_counts': {'bronze': 25, 'silver': 11, 'gold': 1}, 'account_id': 1331841, 'is_employee': False, 'last_modified_date': 1673596199, 'last_access_date': 1671097806, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 672, 'creation_date': 1476795938, 'user_type': 'registered', 'user_id': 7036795, 'link': 'https://stackoverflow.com/users/7036795/dawid', 'profile_image': 'https://www.gravatar.com/avatar/017e890ce2dc9d5bb91243e1ce2d230a?s=256&d=identicon&r=PG', 'display_name': 'Dawid'}","I’m currently trying to use Airflow to orchestrate a process where some operators are defined dynamically and depend on the output of another (earlier) operator. In the code below t1 updates a text file with new records (these are actually read from an external queue but, for simplicity, I hard coded them as A, B and C here). Then, I want to create separate operators for each record read from that text file. These operators will create directories A, B and C, respectively, and in Airflow UI will be seen as separate bash processes Create_directory_A, Create_directory_B and Create_directory_C. In Airflow’s documentation I can see that the scheduler will execute it [DAG] periodically to reflect the changes if any. Does that mean that there is a risk that, even though my t1 operator is executed before t2, the bash operators are created for the list of records before the update (as that's when the DAG was evaluated)?","dag = DAG('Test_DAG',
          description=""Lorem ipsum."",
          start_date=datetime(2017, 3, 20),
          schedule_interval=None,
          catchup=False)


def create_text_file(list_of_rows):
    text_file = open('text_file.txt', ""w"")
    for row in list_of_rows:
        text_file.write(row + '\n')
    text_file.close()


def read_text():
    txt_file = open('text_file.txt', 'r')
    return [element for element in txt_file.readlines()]


t1 = PythonOperator(
    task_id='Create_text_file',
    python_callable=create_text_file,
    op_args=[['A', 'B', 'C']],
    dag=dag
)

for row in read_text():
    t2 = BashOperator(
        task_id='Create_directory_{}'.format(row),
        bash_command=""mkdir {{params.dir_name}}"",
        params={'dir_name': row},
        dag=dag
    )

    t1 &gt;&gt; t2
",34,42,0,1,
580,50391429,50392182,75647,Logging in classes - Python,1,<python><class><logging>,16,"<p>I'm trying to use Python's <code>logging</code> module, and have questions on the best way to use it. </p>

<p>I define several classes, want to write logs and be able to set the level of all of them at the same time. I tried :</p>

<ul>
<li>to use the same logger everywhere. But my classes are used by a framework, I don't have a single entrypoint where I could define a main logger. If so, how should I create it and add handlers ?</li>
<li>to use one logger per file. Should I create it as a class attribute, and adding handlers only the first time the class is instantiated ? Or put it with the imports before the class definition ? <a href=""https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/"" rel=""noreferrer"">This tutorial</a> told me not to, but I don't really get why.</li>
</ul>

<p>Thanks for any hints. I've found lots of docs on the basic way to use a logger, but not much on how to use it in classes.</p>

<p>EDIT: I don't think it's a duplicate of the link below. The accepted answer explains how to load the config in a main program, and then use it in all the modules. But what if I don't have a main program ? Where do I define it ?</p>
",7050062,1257,17-05-2018 12:24,17-05-2018 13:02,0,1267,33,1,15,92,"{'badge_counts': {'bronze': 33, 'silver': 15, 'gold': 1}, 'account_id': 9482682, 'is_employee': False, 'last_modified_date': 1618091100, 'last_access_date': 1711123610, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1267, 'creation_date': 1476994833, 'user_type': 'registered', 'user_id': 7050062, 'accept_rate': 92, 'location': 'France', 'website_url': '', 'link': 'https://stackoverflow.com/users/7050062/meanstreet', 'profile_image': 'https://www.gravatar.com/avatar/6d96237d1638cdae39c2624d74db331d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'MeanStreet'}","I'm trying to use Python's module, and have questions on the best way to use it. I define several classes, want to write logs and be able to set the level of all of them at the same time. I tried : to use the same logger everywhere. But my classes are used by a framework, I don't have a single entrypoint where I could define a main logger. If so, how should I create it and add handlers ? to use one logger per file. Should I create it as a class attribute, and adding handlers only the first time the class is instantiated ? Or put it with the imports before the class definition ? This tutorial told me not to, but I don't really get why. Thanks for any hints. I've found lots of docs on the basic way to use a logger, but not much on how to use it in classes. EDIT: I don't think it's a duplicate of the link below. The accepted answer explains how to load the config in a main program, and then use it in all the modules. But what if I don't have a main program ? Where do I define it ?",logging,-1,12,0,1,
581,49527159,49527269,61002,How to get the output shape of a layer in Keras?,2,<python><keras><lstm><recurrent-neural-network>,31,"<p>I have the following code in Keras (Basically I am modifying this code for my use) and I get this error:</p>

<p>'ValueError: Error when checking target: expected conv3d_3 to have 5 dimensions, but got array with shape (10, 4096)'</p>

<p>Code:</p>

<pre><code>from keras.models import Sequential
from keras.layers.convolutional import Conv3D
from keras.layers.convolutional_recurrent import ConvLSTM2D
from keras.layers.normalization import BatchNormalization
import numpy as np
import pylab as plt
from keras import layers

# We create a layer which take as input movies of shape
# (n_frames, width, height, channels) and returns a movie
# of identical shape.

model = Sequential()
model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),
                   input_shape=(None, 64, 64, 1),
                   padding='same', return_sequences=True))
model.add(BatchNormalization())

model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),
                   padding='same', return_sequences=True))
model.add(BatchNormalization())

model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),
                   padding='same', return_sequences=True))
model.add(BatchNormalization())

model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),
                   padding='same', return_sequences=True))
model.add(BatchNormalization())

model.add(Conv3D(filters=1, kernel_size=(3, 3, 3),
               activation='sigmoid',
               padding='same', data_format='channels_last'))
model.compile(loss='binary_crossentropy', optimizer='adadelta')
</code></pre>

<p>the data I feed is in the following format: [1, 10, 64, 64, 1].
So I would like to know where I am wrong and also how to see the output_shape of each layer.</p>
",8234464,1139,28-03-2018 06:01,28-03-2018 06:08,0,1149,30,2,13,67,"{'badge_counts': {'bronze': 30, 'silver': 13, 'gold': 2}, 'account_id': 11224352, 'is_employee': False, 'last_modified_date': 1702692300, 'last_access_date': 1707521744, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1149, 'creation_date': 1498778445, 'user_type': 'registered', 'user_id': 8234464, 'accept_rate': 67, 'website_url': '', 'link': 'https://stackoverflow.com/users/8234464/mrm', 'profile_image': 'https://lh3.googleusercontent.com/-JslwPPAq_c4/AAAAAAAAAAI/AAAAAAAAAvU/KT_qYuPhfZI/photo.jpg?sz=256', 'display_name': 'MRM'}","I have the following code in Keras (Basically I am modifying this code for my use) and I get this error: 'ValueError: Error when checking target: expected conv3d_3 to have 5 dimensions, but got array with shape (10, 4096)' Code: the data I feed is in the following format: [1, 10, 64, 64, 1]. So I would like to know where I am wrong and also how to see the output_shape of each layer.","from keras.models import Sequential
from keras.layers.convolutional import Conv3D
from keras.layers.convolutional_recurrent import ConvLSTM2D
from keras.layers.normalization import BatchNormalization
import numpy as np
import pylab as plt
from keras import layers

# We create a layer which take as input movies of shape
# (n_frames, width, height, channels) and returns a movie
# of identical shape.

model = Sequential()
model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),
                   input_shape=(None, 64, 64, 1),
                   padding='same', return_sequences=True))
model.add(BatchNormalization())

model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),
                   padding='same', return_sequences=True))
model.add(BatchNormalization())

model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),
                   padding='same', return_sequences=True))
model.add(BatchNormalization())

model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),
                   padding='same', return_sequences=True))
model.add(BatchNormalization())

model.add(Conv3D(filters=1, kernel_size=(3, 3, 3),
               activation='sigmoid',
               padding='same', data_format='channels_last'))
model.compile(loss='binary_crossentropy', optimizer='adadelta')
",33,44,0,0,
582,49648391,49660479,34879,How to install TA-Lib in Google Colab?,4,<python><ta-lib><google-colaboratory>,24,"<p>I'm trying to install the TA-Lib package in Google Colab but without success.</p>
<p>I tried this <a href=""https://github.com/afnhsn/TA-Lib_x64"" rel=""nofollow noreferrer"">guide</a> and also
<a href=""https://stackoverflow.com/questions/48251292/installing-ta-lib-on-python-x64"">Installing TA-Lib on python x64</a>.</p>
<p>I get this error:</p>
<pre><code>import platform
print (platform.architecture())

import sys
print(sys.version)

!pip install C:/ta-lib/TA_Lib-0.4.17-cp36-cp36m-win_amd64.whl

#########
('64bit', '')
3.6.3 (default, Oct  3 2017, 21:45:48) 
[GCC 7.2.0]
 Requirement 'C:/ta-lib/TA_Lib-0.4.17-cp36-cp36m-win_amd64.whl' looks like a 
  filename, but the file does not exist
  TA_Lib-0.4.17-cp36-cp36m-win_amd64.whl is not a supported wheel on this 
  platform.
</code></pre>
",8241868,245,04-04-2018 10:11,04-04-2018 21:09,0,245,5,1,2,,"{'badge_counts': {'bronze': 5, 'silver': 2, 'gold': 1}, 'account_id': 11235505, 'is_employee': False, 'last_modified_date': 1573678600, 'last_access_date': 1529327781, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 245, 'creation_date': 1498929616, 'user_type': 'registered', 'user_id': 8241868, 'link': 'https://stackoverflow.com/users/8241868/orid', 'profile_image': 'https://www.gravatar.com/avatar/32a1d6482869568cb8b592cf802e053f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'OriD'}",I'm trying to install the TA-Lib package in Google Colab but without success. I tried this guide and also Installing TA-Lib on python x64. I get this error:,"import platform
print (platform.architecture())

import sys
print(sys.version)

!pip install C:/ta-lib/TA_Lib-0.4.17-cp36-cp36m-win_amd64.whl

#########
('64bit', '')
3.6.3 (default, Oct  3 2017, 21:45:48) 
[GCC 7.2.0]
 Requirement 'C:/ta-lib/TA_Lib-0.4.17-cp36-cp36m-win_amd64.whl' looks like a 
  filename, but the file does not exist
  TA_Lib-0.4.17-cp36-cp36m-win_amd64.whl is not a supported wheel on this 
  platform.
",15,21,0,2,
583,49488119,49488271,25072,Python requests - check if a particular header exists,1,<python><http-headers><python-requests>,13,"<p>I am working with python <code>requests</code> module where I am getting  a url.</p>

<p><code>response = requests.get(url)</code> </p>

<p>This url has a set of headers where one particular header is sometimes there and sometimes not. I want to get the value of this header if it is there. I am using</p>

<pre><code>retry_after = response.headers['Retry-After']
</code></pre>

<p>However when the header is not there an exception is thrown. what I want is something like this</p>

<pre><code>If this header exists ( if header is not None)
    retry_after = response.header['Retry_After']
</code></pre>

<p>How can this be achieved with python <code>requests</code></p>
",7331538,2112,26-03-2018 09:30,26-03-2018 09:36,0,2112,72,3,27,70,"{'badge_counts': {'bronze': 72, 'silver': 27, 'gold': 3}, 'account_id': 9901716, 'is_employee': False, 'last_modified_date': 1709947800, 'last_access_date': 1711126977, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 23, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2112, 'creation_date': 1482426281, 'user_type': 'registered', 'user_id': 7331538, 'accept_rate': 70, 'website_url': '', 'link': 'https://stackoverflow.com/users/7331538/bcsta', 'profile_image': 'https://www.gravatar.com/avatar/603a48331ac6169d86f15d0d1b3a32e6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'bcsta'}",I am working with python module where I am getting a url. This url has a set of headers where one particular header is sometimes there and sometimes not. I want to get the value of this header if it is there. I am using However when the header is not there an exception is thrown. what I want is something like this How can this be achieved with python,"requests response = requests.get(url) retry_after = response.headers['Retry-After']
 If this header exists ( if header is not None)
    retry_after = response.header['Retry_After']
 requests",-2,16,0,0,
584,50315989,52800540,39369,How to extract rar files inside google colab,7,<python><google-colaboratory><rar>,13,"<p>I have a <strong>dataset in google drive</strong> which I want to use in <strong>google colab</strong>.But I can't <strong>unrar the rar files</strong> by any means.So far I have tried installing python libraries and also ubuntu packages like ""unrar ,rar ,unrar-free ,unar ,unp"" and I just can't open the damn file.Here are the results of each command:</p>

<p>!rar x data.rar</p>

<pre><code>RAR 5.40   Copyright (c) 1993-2016 Alexander Roshal   15 Aug 2016
Trial version             Type RAR -? for help


Extracting from meta-data.rar

Cannot create meta-data/sample_submission.csv
No such file or directory
Cannot create meta-data/test.csv
No such file or directory
Cannot create meta-data/train.csv
No such file or directory
Cannot create directory meta-data
Input/output error
Total errors: 4
</code></pre>

<p>!unrar data.rar</p>

<pre><code>UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal


Extracting from meta-data.rar

Cannot create meta-data/sample_submission.csv
No such file or directory
Cannot create meta-data/test.csv
No such file or directory
Cannot create meta-data/train.csv
No such file or directory
Cannot create directory meta-data
Input/output error
Total errors: 4
</code></pre>

<p>!unp meta-data.rar</p>

<pre><code>RAR 5.40   Copyright (c) 1993-2016 Alexander Roshal   15 Aug 2016
Trial version             Type RAR -? for help


Extracting from meta-data.rar

Cannot create meta-data/sample_submission.csv
No such file or directory
Cannot create meta-data/test.csv
No such file or directory
Cannot create meta-data/train.csv
No such file or directory
Cannot create directory meta-data
Input/output error
Total errors: 4

UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal


Extracting from meta-data.rar

Cannot create meta-data/sample_submission.csv
No such file or directory
Cannot create meta-data/test.csv
No such file or directory
Cannot create meta-data/train.csv
No such file or directory
Cannot create directory meta-data
Input/output error
Total errors: 4
Can't exec ""file"": No such file or directory at /usr/bin/unp line 419.
Failed to detect file type of meta-data.rar.
WARNING: There were errors while processing files!
</code></pre>

<p>None of the others a re working either so any ideas are welcomed.</p>
",7342371,468,13-05-2018 12:12,14-10-2018 07:42,154,468,17,3,5,,"{'badge_counts': {'bronze': 17, 'silver': 5, 'gold': 3}, 'account_id': 9918269, 'is_employee': False, 'last_modified_date': 1663611600, 'last_access_date': 1708873101, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 468, 'creation_date': 1482756880, 'user_type': 'registered', 'user_id': 7342371, 'location': 'mumbai, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/7342371/dhrumil-barot', 'profile_image': 'https://i.stack.imgur.com/jYQg8.jpg?s=256&g=1', 'display_name': 'dhrumil barot'}","I have a dataset in google drive which I want to use in google colab.But I can't unrar the rar files by any means.So far I have tried installing python libraries and also ubuntu packages like ""unrar ,rar ,unrar-free ,unar ,unp"" and I just can't open the damn file.Here are the results of each command: !rar x data.rar !unrar data.rar !unp meta-data.rar None of the others a re working either so any ideas are welcomed.","RAR 5.40   Copyright (c) 1993-2016 Alexander Roshal   15 Aug 2016
Trial version             Type RAR -? for help


Extracting from meta-data.rar

Cannot create meta-data/sample_submission.csv
No such file or directory
Cannot create meta-data/test.csv
No such file or directory
Cannot create meta-data/train.csv
No such file or directory
Cannot create directory meta-data
Input/output error
Total errors: 4
 UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal


Extracting from meta-data.rar

Cannot create meta-data/sample_submission.csv
No such file or directory
Cannot create meta-data/test.csv
No such file or directory
Cannot create meta-data/train.csv
No such file or directory
Cannot create directory meta-data
Input/output error
Total errors: 4
 RAR 5.40   Copyright (c) 1993-2016 Alexander Roshal   15 Aug 2016
Trial version             Type RAR -? for help


Extracting from meta-data.rar

Cannot create meta-data/sample_submission.csv
No such file or directory
Cannot create meta-data/test.csv
No such file or directory
Cannot create meta-data/train.csv
No such file or directory
Cannot create directory meta-data
Input/output error
Total errors: 4

UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal


Extracting from meta-data.rar

Cannot create meta-data/sample_submission.csv
No such file or directory
Cannot create meta-data/test.csv
No such file or directory
Cannot create meta-data/train.csv
No such file or directory
Cannot create directory meta-data
Input/output error
Total errors: 4
Can't exec ""file"": No such file or directory at /usr/bin/unp line 419.
Failed to detect file type of meta-data.rar.
WARNING: There were errors while processing files!
",59,77,0,0,
585,48614158,48614181,106270,How to read a JSON file as a pandas DataFrame?,6,<python><json><python-3.x><pandas>,31,"<p>I am using python 3.6 and trying to download json file (350 MB) as pandas dataframe using the code below. However, I get the following error:</p>

<blockquote>
<pre><code>data_json_str = ""["" + "","".join(data) + ""]
""TypeError: sequence item 0: expected str instance, bytes found
</code></pre>
</blockquote>

<p>How can I fix the error?</p>

<pre><code>import pandas as pd

# read the entire file into a python array
with open('C:/Users/Alberto/nutrients.json', 'rb') as f:
   data = f.readlines()

# remove the trailing ""\n"" from each line
data = map(lambda x: x.rstrip(), data)

# each element of 'data' is an individual JSON object.
# i want to convert it into an *array* of JSON objects
# which, in and of itself, is one large JSON object
# basically... add square brackets to the beginning
# and end, and have all the individual business JSON objects
# separated by a comma
data_json_str = ""["" + "","".join(data) + ""]""

# now, load it into pandas
data_df = pd.read_json(data_json_str)
</code></pre>
",8260088,815,04-02-2018 23:46,04-02-2018 23:51,0,815,20,3,12,31,"{'badge_counts': {'bronze': 20, 'silver': 12, 'gold': 3}, 'account_id': 10917339, 'is_employee': False, 'last_modified_date': 1695082800, 'last_access_date': 1697551343, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 815, 'creation_date': 1499267626, 'user_type': 'registered', 'user_id': 8260088, 'accept_rate': 31, 'link': 'https://stackoverflow.com/users/8260088/alberto-alvarez', 'profile_image': 'https://www.gravatar.com/avatar/3dda0d58e14e4466cd89af8d9d48d22e?s=256&d=identicon&r=PG', 'display_name': 'Alberto Alvarez'}","I am using python 3.6 and trying to download json file (350 MB) as pandas dataframe using the code below. However, I get the following error: How can I fix the error?","data_json_str = ""["" + "","".join(data) + ""]
""TypeError: sequence item 0: expected str instance, bytes found
 import pandas as pd

# read the entire file into a python array
with open('C:/Users/Alberto/nutrients.json', 'rb') as f:
   data = f.readlines()

# remove the trailing ""\n"" from each line
data = map(lambda x: x.rstrip(), data)

# each element of 'data' is an individual JSON object.
# i want to convert it into an *array* of JSON objects
# which, in and of itself, is one large JSON object
# basically... add square brackets to the beginning
# and end, and have all the individual business JSON objects
# separated by a comma
data_json_str = ""["" + "","".join(data) + ""]""

# now, load it into pandas
data_df = pd.read_json(data_json_str)
",19,30,0,0,
586,48643256,48643464,66613,TypeError: iteration over a 0-d array Python,2,<python><arrays><numpy><iterator><typeerror>,15,"<p>I am trying to write a very basic nearest neighbor calculation. I basically want to see what t looks like but I got this type error. When I asked the funciton to return just t it said """". When I asked it to turn to list it threw ""TypeError: iteration over a 0-d array Python ""</p>

<p>How do I fix this please?</p>

<pre><code>...

t = np.array(map(lambda v:
             map(lambda w: distance(v, w, L), x_train.values),
             x_test.values)) 

...
</code></pre>

<p>Full trace:
<a href=""https://i.stack.imgur.com/2RiUA.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2RiUA.png"" alt=""enter image description here""></a></p>
",8260677,287,06-02-2018 12:37,06-02-2018 12:49,0,287,15,2,5,57,"{'badge_counts': {'bronze': 15, 'silver': 5, 'gold': 2}, 'account_id': 11262918, 'is_employee': False, 'last_modified_date': 1636494900, 'last_access_date': 1659373724, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 287, 'creation_date': 1499275049, 'user_type': 'registered', 'user_id': 8260677, 'accept_rate': 57, 'location': 'Raleigh, NC, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/8260677/anna-huang', 'profile_image': 'https://lh4.googleusercontent.com/-qiEJ0vAWkI4/AAAAAAAAAAI/AAAAAAAAABI/h3HGAdwzcTM/photo.jpg?sz=256', 'display_name': 'Anna Huang'}","I am trying to write a very basic nearest neighbor calculation. I basically want to see what t looks like but I got this type error. When I asked the funciton to return just t it said """". When I asked it to turn to list it threw ""TypeError: iteration over a 0-d array Python "" How do I fix this please? Full trace:","...

t = np.array(map(lambda v:
             map(lambda w: distance(v, w, L), x_train.values),
             x_test.values)) 

...
",6,15,1,1,
587,48200524,48201571,14169,Named entity recognition in Spacy,2,<python><named-entity-recognition><spacy>,12,"<p>I am trying to find Named entities for a sentence as below </p>

<pre><code>import spacy.lang.en
parser = spacy.lang.en.English()
ParsedSentence = parser(u""Alphabet is a new startup in China"")
for Entity in  ParsedSentence.ents:  
    print (Entity.label, Entity.label_, ' '.join(t.orth_ for t in Entity))
</code></pre>

<p>I am expecting to get the result ""Alphabet"",""China"" but I am getting an empty set as result. What am I doing wrong here</p>
",8262744,477,11-01-2018 05:48,11-01-2018 07:10,0,477,20,4,9,80,"{'badge_counts': {'bronze': 20, 'silver': 9, 'gold': 4}, 'account_id': 11265930, 'is_employee': False, 'last_modified_date': 1582363500, 'last_access_date': 1559130670, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 477, 'creation_date': 1499316711, 'user_type': 'registered', 'user_id': 8262744, 'accept_rate': 80, 'location': 'India', 'link': 'https://stackoverflow.com/users/8262744/shan', 'profile_image': 'https://www.gravatar.com/avatar/b571c14e5f98b125d8f073dbd6c40062?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'shan'}","I am trying to find Named entities for a sentence as below I am expecting to get the result ""Alphabet"",""China"" but I am getting an empty set as result. What am I doing wrong here","import spacy.lang.en
parser = spacy.lang.en.English()
ParsedSentence = parser(u""Alphabet is a new startup in China"")
for Entity in  ParsedSentence.ents:  
    print (Entity.label, Entity.label_, ' '.join(t.orth_ for t in Entity))
",4,10,0,0,
588,49325509,49325550,6201,How to find alternating repetitive digit pair?,2,<python><regex><python-3.x>,16,"<p><code>121426</code> &lt;- Here, 1 is an alternating repetitive digit.</p>

<p><code>523563</code> &lt;- Here, NO digit is an alternating repetitive digit.</p>

<p><code>552523</code> &lt;- Here, both 2 and 5 are alternating repetitive digits.</p>

<p><code>333567</code> &lt;- Here, 3 is an alternating repetitive digit.</p>

<p>I found <code>re.findall(r'(?=(\d)\d\1)',P)</code> as the solution in editorial but not able to understand it.</p>

<p>Edit - Not allowed to use <code>if</code> conditions.</p>
",8374202,743,16-03-2018 16:29,16-03-2018 16:31,0,743,29,2,9,60,"{'badge_counts': {'bronze': 29, 'silver': 9, 'gold': 2}, 'account_id': 11423432, 'is_employee': False, 'last_modified_date': 1607614428, 'last_access_date': 1691235720, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 743, 'creation_date': 1501137112, 'user_type': 'registered', 'user_id': 8374202, 'accept_rate': 60, 'location': 'Hyderabad, Telangana, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/8374202/vikash-yadav', 'profile_image': 'https://www.gravatar.com/avatar/0659cd4695631a1bc88ad41d830c36d4?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Vikash Yadav'}","&lt;- Here, 1 is an alternating repetitive digit. &lt;- Here, NO digit is an alternating repetitive digit. &lt;- Here, both 2 and 5 are alternating repetitive digits. &lt;- Here, 3 is an alternating repetitive digit. I found as the solution in editorial but not able to understand it. Edit - Not allowed to use conditions.","121426 523563 552523 333567 re.findall(r'(?=(\d)\d\1)',P) if",-6,11,0,0,
589,49391687,49400015,1214,Render current status only on template in StreamingHttpResponse in Django,2,<python><django><python-3.x><python-2.7><python-requests>,12,"<p>I was trying to display the status of processing to the user on the front end when I was using <em>StreamingHttpResponse</em>.</p>

<p>I was able to get the current status but it is being appended to the previous one.<br>
I want the response template to contain only the current yield.</p>

<blockquote>
  <p>views.py</p>
</blockquote>

<pre><code>from django.shortcuts import render
from django.http import StreamingHttpResponse,HttpResponse
import time

def f1():
    x = 0
    while x&lt;5:
        time.sleep(1)
        x = x+1
        code = """"""&lt;p&gt;{}&lt;/p&gt;"""""".format(x)
        yield code


def home(request):
    return StreamingHttpResponse(f1())
</code></pre>

<blockquote>
  <p>output in the browser</p>
</blockquote>

<pre><code> &lt;p&gt;1&lt;/p&gt;
 &lt;p&gt;2&lt;/p&gt;
 &lt;p&gt;3&lt;/p&gt;
 &lt;p&gt;4&lt;/p&gt;
</code></pre>

<blockquote>
  <p>expected output</p>
</blockquote>

<p>1st: <code>&lt;p&gt;1&lt;/p&gt;</code></p>

<p>2nd: <code>&lt;p&gt;2&lt;/p&gt;</code> instead of  <code>&lt;p&gt;1&lt;/p&gt;&lt;p&gt;2&lt;/p&gt;</code></p>

<p>3rd: <code>&lt;p&gt;3&lt;/p&gt;</code> instead of  <code>&lt;p&gt;1&lt;/p&gt;&lt;p&gt;2&lt;/p&gt;&lt;p&gt;3&lt;/p&gt;</code></p>

<p>4th: <code>&lt;p&gt;4&lt;/p&gt;</code> instead of  <code>&lt;p&gt;1&lt;/p&gt;&lt;p&gt;2&lt;/p&gt;3&lt;p&gt;&lt;/p&gt;4&lt;p&gt;&lt;/p&gt;</code></p>

<p>instead of appending the previous yield I want the template to be filled with the current yield.</p>
",8383177,766,20-03-2018 18:27,21-03-2018 06:55,1,766,32,1,11,43,"{'badge_counts': {'bronze': 32, 'silver': 11, 'gold': 1}, 'account_id': 11435705, 'is_employee': False, 'last_modified_date': 1685152500, 'last_access_date': 1686839200, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 766, 'creation_date': 1501261177, 'user_type': 'registered', 'user_id': 8383177, 'accept_rate': 43, 'location': 'Hyderabad, Telangana, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/8383177/sai-teja-pakalapati', 'profile_image': 'https://www.gravatar.com/avatar/a83c6a38beaa92a30bfba80b3fcb6498?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Sai Teja Pakalapati'}",I was trying to display the status of processing to the user on the front end when I was using StreamingHttpResponse. I was able to get the current status but it is being appended to the previous one. I want the response template to contain only the current yield. views.py output in the browser expected output 1st: 2nd: instead of 3rd: instead of 4th: instead of instead of appending the previous yield I want the template to be filled with the current yield.,"from django.shortcuts import render
from django.http import StreamingHttpResponse,HttpResponse
import time

def f1():
    x = 0
    while x&lt;5:
        time.sleep(1)
        x = x+1
        code = """"""&lt;p&gt;{}&lt;/p&gt;"""""".format(x)
        yield code


def home(request):
    return StreamingHttpResponse(f1())
  &lt;p&gt;1&lt;/p&gt;
 &lt;p&gt;2&lt;/p&gt;
 &lt;p&gt;3&lt;/p&gt;
 &lt;p&gt;4&lt;/p&gt;
 &lt;p&gt;1&lt;/p&gt; &lt;p&gt;2&lt;/p&gt; &lt;p&gt;1&lt;/p&gt;&lt;p&gt;2&lt;/p&gt; &lt;p&gt;3&lt;/p&gt; &lt;p&gt;1&lt;/p&gt;&lt;p&gt;2&lt;/p&gt;&lt;p&gt;3&lt;/p&gt; &lt;p&gt;4&lt;/p&gt; &lt;p&gt;1&lt;/p&gt;&lt;p&gt;2&lt;/p&gt;3&lt;p&gt;&lt;/p&gt;4&lt;p&gt;&lt;/p&gt;",10,49,0,0,
590,50012525,50012559,44816,How to sort a pandas dataframe by a custom order on a string index,6,<python><pandas><sorting><indexing><categorical-data>,39,"<p>I have the following dataframe:</p>
<pre><code>import pandas as pd

df = pd.DataFrame({'id': [2967, 5335, 13950, 6141, 6169],
                   'Player': ['Cedric Hunter', 'Maurice Baker',
                              'Ratko Varda', 'Ryan Bowen', 'Adrian Caldwell'],
                   'Year': [1991, 2004, 2001, 2009, 1997],
                   'Age': [27, 25, 22, 34, 31],
                   'Tm': ['CHH', 'VAN', 'TOT', 'OKC', 'DAL'],
                   'G': [6, 7, 60, 52, 81]})


df.set_index('Player', inplace=True)
</code></pre>
<p>It shows:</p>
<pre class=""lang-none prettyprint-override""><code>Out[128]:

                 Age   G   Tm  Year     id
Player
Cedric Hunter     27   6  CHH  1991   2967
Maurice Baker     25   7  VAN  2004   5335
Ratko Varda       22  60  TOT  2001  13950
Ryan Bowen        34  52  OKC  2009   6141
Adrian Caldwell   31  81  DAL  1997   6169
</code></pre>
<p>How can I sort by the index ('Player') using some arbitrary order? For example, as in the following.</p>
<pre><code>reorderlist = ['Maurice Baker',
               'Adrian Caldwell',
               'Ratko Varda',
               'Ryan Bowen',
               'Cedric Hunter']
</code></pre>
",8391698,5029,25-04-2018 00:44,25-04-2018 00:49,0,5039,86,7,49,61,"{'badge_counts': {'bronze': 86, 'silver': 49, 'gold': 7}, 'collectives': [{'collective': {'tags': ['stringr', 'tibble', 'r', 'shinydashboard', 'shiny', 'plyr', 'rlang', 'rvest', 'lubridate', 'ggplot2', 'r-caret', 'quantmod', 'dplyr', 'knitr', 'shinyapps', 'data.table', 'shiny-server', 'zoo', 'dtplyr', 'tidyr', 'purrr', 'r-raster', 'rstudio', 'readr', 'forcats', 'tidyverse', 'r-package'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where data scientists and AI researchers gather to find, share, and learn about R and other subtags like knitr and dplyr.', 'link': '/collectives/r-language', 'name': 'R Language', 'slug': 'r-language'}, 'role': 'member'}], 'account_id': 11448733, 'is_employee': False, 'last_modified_date': 1708583400, 'last_access_date': 1710033394, 'reputation_change_year': 110, 'reputation_change_quarter': 110, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5039, 'creation_date': 1501474871, 'user_type': 'registered', 'user_id': 8391698, 'accept_rate': 61, 'website_url': '', 'link': 'https://stackoverflow.com/users/8391698/littleworth', 'profile_image': 'https://www.gravatar.com/avatar/a5d5efaddc0a2339910e59436cbc2c49?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'littleworth'}","I have the following dataframe: It shows: How can I sort by the index ('Player') using some arbitrary order? For example, as in the following.","import pandas as pd

df = pd.DataFrame({'id': [2967, 5335, 13950, 6141, 6169],
                   'Player': ['Cedric Hunter', 'Maurice Baker',
                              'Ratko Varda', 'Ryan Bowen', 'Adrian Caldwell'],
                   'Year': [1991, 2004, 2001, 2009, 1997],
                   'Age': [27, 25, 22, 34, 31],
                   'Tm': ['CHH', 'VAN', 'TOT', 'OKC', 'DAL'],
                   'G': [6, 7, 60, 52, 81]})


df.set_index('Player', inplace=True)
 Out[128]:

                 Age   G   Tm  Year     id
Player
Cedric Hunter     27   6  CHH  1991   2967
Maurice Baker     25   7  VAN  2004   5335
Ratko Varda       22  60  TOT  2001  13950
Ryan Bowen        34  52  OKC  2009   6141
Adrian Caldwell   31  81  DAL  1997   6169
 reorderlist = ['Maurice Baker',
               'Adrian Caldwell',
               'Ratko Varda',
               'Ryan Bowen',
               'Cedric Hunter']
",23,32,0,0,
591,48473056,48473096,7515,Number of unique elements per row in a NumPy array,4,<python><numpy>,12,"<p>For example, for</p>

<pre><code>a = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 4]])
</code></pre>

<p>I want to get </p>

<pre><code>[2, 2, 3]
</code></pre>

<p>Is there a way to do this without for loops or using <code>np.vectorize</code>?</p>

<p>Edit: Actual data consists of 1000 rows of 100 elements each, with each element ranging from 1 to 365. The ultimate goal is to determine the percentage of rows that have duplicates. This was a homework problem which I already solved (with a for loop), but I was just wondering if there was a better way to do it with numpy.</p>
",7754305,131,27-01-2018 05:55,27-01-2018 06:03,0,131,5,0,1,,"{'badge_counts': {'bronze': 5, 'silver': 1, 'gold': 0}, 'account_id': 10522182, 'is_employee': False, 'last_modified_date': 1573678705, 'last_access_date': 1529029836, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 131, 'creation_date': 1490231885, 'user_type': 'registered', 'user_id': 7754305, 'link': 'https://stackoverflow.com/users/7754305/drape', 'profile_image': 'https://www.gravatar.com/avatar/a4af63843aa223ae2f7ecfb244d381e5?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'DrApe'}","For example, for I want to get Is there a way to do this without for loops or using ? Edit: Actual data consists of 1000 rows of 100 elements each, with each element ranging from 1 to 365. The ultimate goal is to determine the percentage of rows that have duplicates. This was a homework problem which I already solved (with a for loop), but I was just wondering if there was a better way to do it with numpy.","a = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 4]])
 [2, 2, 3]
 np.vectorize",-1,13,0,0,
592,48917121,48917137,11827,Split on more than one space?,2,<python><string><split>,11,"<p>I have a program that needs to split lines that are of the format:</p>

<pre><code>IDNumber      Firstname Lastname    GPA      Credits
</code></pre>

<p>but I want to keep <code>Firstname</code> and <code>Lastname</code> in the same string.  </p>

<p>Is there any easy way to do this (other than just splitting into five strings instead of four) and somehow have the split method only split when there is more than one space?</p>
",7794841,129,21-02-2018 23:15,21-02-2018 23:17,0,129,8,1,1,57,"{'badge_counts': {'bronze': 8, 'silver': 1, 'gold': 1}, 'account_id': 10581037, 'is_employee': False, 'last_modified_date': 1625920200, 'last_access_date': 1626559515, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 129, 'creation_date': 1490914527, 'user_type': 'registered', 'user_id': 7794841, 'accept_rate': 57, 'link': 'https://stackoverflow.com/users/7794841/pcrevolt', 'profile_image': 'https://www.gravatar.com/avatar/068312aaf4a560609e24cc7338c09cee?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'PCRevolt'}",I have a program that needs to split lines that are of the format: but I want to keep and in the same string. Is there any easy way to do this (other than just splitting into five strings instead of four) and somehow have the split method only split when there is more than one space?,"IDNumber      Firstname Lastname    GPA      Credits
 Firstname Lastname",-2,8,0,0,
593,48795387,48797263,16582,Pandas Write to Excel rearranging columns based on alphabetic order,1,<python><excel><pandas>,13,"<p>I have a python dict that has the format</p>

<pre><code>dict = {
        D:""""
        B:""""
        A:""""
        C:""""
}
</code></pre>

<p>However, when I write this dict to a csv file in excel, the columns are rearranged to </p>

<pre><code>A B C D
</code></pre>

<p>How do I keep the order of my dict in python when I write to excel?</p>

<pre><code>    writer = pd.ExcelWriter('list_of_detected_words.xlsx', engine='xlsxwriter')
    list_of_detected_words = pd.DataFrame.from_records(form_info)
    list_of_detected_words.to_excel(writer, ""Sheet1"",startrow=1)
</code></pre>

<p>Above is the code that writes to excel. </p>
",7903466,765,14-02-2018 20:08,14-02-2018 22:26,0,765,19,1,7,43,"{'badge_counts': {'bronze': 19, 'silver': 7, 'gold': 1}, 'account_id': 10740025, 'is_employee': False, 'last_modified_date': 1607614440, 'last_access_date': 1601303123, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 765, 'creation_date': 1492805697, 'user_type': 'registered', 'user_id': 7903466, 'accept_rate': 43, 'location': 'Chicago, IL, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/7903466/m-waz', 'profile_image': 'https://www.gravatar.com/avatar/ecf428d80844356c653d710c9438e4fd?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'M Waz'}","I have a python dict that has the format However, when I write this dict to a csv file in excel, the columns are rearranged to How do I keep the order of my dict in python when I write to excel? Above is the code that writes to excel.","dict = {
        D:""""
        B:""""
        A:""""
        C:""""
}
 A B C D
     writer = pd.ExcelWriter('list_of_detected_words.xlsx', engine='xlsxwriter')
    list_of_detected_words = pd.DataFrame.from_records(form_info)
    list_of_detected_words.to_excel(writer, ""Sheet1"",startrow=1)
",7,23,0,0,
594,50105094,50111108,27306,Python: upload large files S3 fast,3,<python><amazon-s3><upload><boto3>,13,"<p>I am trying to upload programmatically an very large file up to 1GB on S3. As I found that AWS S3 supports multipart upload for large files, and I found some Python code to do it. (<a href=""https://gist.github.com/teasherm/bb73f21ed2f3b46bc1c2ca48ec2c1cf5"" rel=""noreferrer"">link</a> )</p>
<p>My point: the speed of upload was too slow (almost 1 min).</p>
<p>Is there any way to increase the performance of multipart upload. Or any good library support S3 uploading</p>
",7915196,2796,30-04-2018 17:06,01-05-2018 03:48,1,2796,52,6,26,35,"{'badge_counts': {'bronze': 52, 'silver': 26, 'gold': 6}, 'account_id': 10757772, 'is_employee': False, 'last_modified_date': 1615626300, 'last_access_date': 1562748511, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2796, 'creation_date': 1493055052, 'user_type': 'registered', 'user_id': 7915196, 'accept_rate': 35, 'location': 'London, UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/7915196/phong-vu', 'profile_image': 'https://i.stack.imgur.com/UWnOd.png?s=256&g=1', 'display_name': 'Phong Vu'}","I am trying to upload programmatically an very large file up to 1GB on S3. As I found that AWS S3 supports multipart upload for large files, and I found some Python code to do it. (link ) My point: the speed of upload was too slow (almost 1 min). Is there any way to increase the performance of multipart upload. Or any good library support S3 uploading",,0,3,0,1,
595,49909710,49910044,38027,Suppress Scientific Format in a Dataframe Column,3,<python><pandas><dataframe><scientific-notation>,18,"<p>I have a column called accountnumber with values similar to 4.11889000e+11 in a pandas dataframe. I want to suppress the scientific notation and convert the values to 4118890000. I have tried the following method and did not work. </p>

<pre><code>df = pd.read_csv(data.csv)
pd.options.display.float_format = '{:,.3f}'.format
</code></pre>

<p>Please recommend. </p>
",7933418,701,18-04-2018 22:05,18-04-2018 22:38,0,701,20,2,6,,"{'badge_counts': {'bronze': 20, 'silver': 6, 'gold': 2}, 'account_id': 10689992, 'is_employee': False, 'last_modified_date': 1676980200, 'last_access_date': 1680648025, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 701, 'creation_date': 1493322411, 'user_type': 'registered', 'user_id': 7933418, 'website_url': '', 'link': 'https://stackoverflow.com/users/7933418/iprof0214', 'profile_image': 'https://www.gravatar.com/avatar/2d3a2c60d0990409ec9a4f694f2f1459?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'iprof0214'}",I have a column called accountnumber with values similar to 4.11889000e+11 in a pandas dataframe. I want to suppress the scientific notation and convert the values to 4118890000. I have tried the following method and did not work. Please recommend.,"df = pd.read_csv(data.csv)
pd.options.display.float_format = '{:,.3f}'.format
",1,7,0,0,
596,48851558,48851837,186534,"Tensorflow estimator ValueError: logits and labels must have the same shape ((?, 1) vs (?,))",7,<python><tensorflow><keras>,55,"<p>I'm classifying movie reviews as positive or negative using binary crossentropy. So, when I'm trying to wrap my keras model with tensorflow estimator, I get the error:</p>
<pre><code>Tensorflow estimator ValueError: logits and labels must have the same shape ((?, 1) vs (?,))
</code></pre>
<p>I'm using sigmoid activation as my last layer, guess I'm missing something trivial here. Any help?</p>
<pre><code>from tensorflow import keras
import tensorflow as tf
print(&quot;Tensorflow {} loaded&quot;.format(tf.__version__))
import numpy as np

keras.__version__
from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
def vectorize_sequences(sequences, dimension=10000):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.  # set specific indices of results[i] to 1s
    return results.astype('float32')

# Our vectorized training data
x_train = vectorize_sequences(train_data)

# Our vectorized test data
x_test = vectorize_sequences(test_data)

# Our vectorized labels
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

model = keras.models.Sequential()
model.add(keras.layers.Dense(16, activation='relu', input_shape=(10000,), name='reviews'))
model.add(keras.layers.Dense(16, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
estimator_model = keras.estimator.model_to_estimator(keras_model=model)

def input_function(features,labels=None,shuffle=False,epochs=None,batch_size=None):
    input_fn = tf.estimator.inputs.numpy_input_fn(
        x={&quot;reviews_input&quot;: features},
        y=labels,
        shuffle=shuffle,
        num_epochs=epochs,
        batch_size=batch_size
    )
    return input_fn

estimator_model.train(input_fn=input_function(partial_x_train, partial_y_train, True,20,512))
score = estimator_model.evaluate(input_function(x_val, labels=y_val))
print(score)
</code></pre>
",8010010,589,18-02-2018 12:15,18-02-2018 12:46,0,589,13,1,4,,"{'badge_counts': {'bronze': 13, 'silver': 4, 'gold': 1}, 'account_id': 4732574, 'is_employee': False, 'last_modified_date': 1573678649, 'last_access_date': 1657817548, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 589, 'creation_date': 1494761626, 'user_type': 'registered', 'user_id': 8010010, 'website_url': '', 'link': 'https://stackoverflow.com/users/8010010/bluecrow', 'profile_image': 'https://www.gravatar.com/avatar/cda1380fbb0d840babe7b1bed10ec287?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Bluecrow'}","I'm classifying movie reviews as positive or negative using binary crossentropy. So, when I'm trying to wrap my keras model with tensorflow estimator, I get the error: I'm using sigmoid activation as my last layer, guess I'm missing something trivial here. Any help?","Tensorflow estimator ValueError: logits and labels must have the same shape ((?, 1) vs (?,))
 from tensorflow import keras
import tensorflow as tf
print(&quot;Tensorflow {} loaded&quot;.format(tf.__version__))
import numpy as np

keras.__version__
from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
def vectorize_sequences(sequences, dimension=10000):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.  # set specific indices of results[i] to 1s
    return results.astype('float32')

# Our vectorized training data
x_train = vectorize_sequences(train_data)

# Our vectorized test data
x_test = vectorize_sequences(test_data)

# Our vectorized labels
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

model = keras.models.Sequential()
model.add(keras.layers.Dense(16, activation='relu', input_shape=(10000,), name='reviews'))
model.add(keras.layers.Dense(16, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
estimator_model = keras.estimator.model_to_estimator(keras_model=model)

def input_function(features,labels=None,shuffle=False,epochs=None,batch_size=None):
    input_fn = tf.estimator.inputs.numpy_input_fn(
        x={&quot;reviews_input&quot;: features},
        y=labels,
        shuffle=shuffle,
        num_epochs=epochs,
        batch_size=batch_size
    )
    return input_fn

estimator_model.train(input_fn=input_function(partial_x_train, partial_y_train, True,20,512))
score = estimator_model.evaluate(input_function(x_val, labels=y_val))
print(score)
",52,58,0,0,
597,48370708,48371587,12505,Best way to subset a pandas dataframe,2,<python><pandas><dataframe><data-science>,11,"<p>Hey I'm new to Pandas and I just came across <code>df.query()</code>.</p>

<p>Why people would use <code>df.query()</code> when you can directly filter your Dataframes using brackets notation ? The official pandas tutorial also seems to prefer the latter approach.</p>

<p>With brackets notation :</p>

<pre><code>df[df['age'] &lt;= 21]
</code></pre>

<p>With pandas query method :</p>

<pre><code>df.query('age &lt;= 21')
</code></pre>

<p>Besides some of the stylistic or flexibility differences that have been mentioned, is one canonically preferred - namely for performance of operations on large dataframes?</p>
",8053816,133,21-01-2018 19:16,21-01-2018 20:50,0,133,7,1,1,,"{'badge_counts': {'bronze': 7, 'silver': 1, 'gold': 1}, 'account_id': 10960284, 'is_employee': False, 'last_modified_date': 1573678639, 'last_access_date': 1655460723, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 133, 'creation_date': 1495545551, 'user_type': 'registered', 'user_id': 8053816, 'location': 'Paris, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/8053816/pierre-eric-garcia', 'profile_image': 'https://graph.facebook.com/10209909651120339/picture?type=large', 'display_name': 'Pierre-Eric Garcia'}","Hey I'm new to Pandas and I just came across . Why people would use when you can directly filter your Dataframes using brackets notation ? The official pandas tutorial also seems to prefer the latter approach. With brackets notation : With pandas query method : Besides some of the stylistic or flexibility differences that have been mentioned, is one canonically preferred - namely for performance of operations on large dataframes?","df.query() df.query() df[df['age'] &lt;= 21]
 df.query('age &lt;= 21')
",-2,15,0,0,
598,49098466,49115448,6016,Plot 3D convex closed regions in matplotlib,2,<python><numpy><matplotlib><scipy><mplot3d>,12,"<p>I am trying to plot in 3D a polytope defined by a set of inequalities. Essentially, I try to reproduce the functionality of this matlab <a href=""https://www.mathworks.com/matlabcentral/fileexchange/9261-plot-2d-3d-region?focused=5143921&amp;tab=function"" rel=""noreferrer"">plotregion</a> library in matplotlib.</p>

<p>My approach is to get the intersection vertices, construct the convex hull of them, and then get and plot the resulting faces (simplices).</p>

<p>The problem is that many simplices are coplanar, and they are making the plot very busy for no reason (see all of these diagonal edges in the plot below).</p>

<p>Is there any easy way to just print the ""outside"" edges of the polyhedron, without having to consolidate by my self, one by one, all of the coplanar simplices?</p>

<p>Thank you</p>

<pre><code>from scipy.spatial import HalfspaceIntersection
from scipy.spatial import ConvexHull
import scipy as sp
import numpy as np
import matplotlib.pyplot as plt
import mpl_toolkits.mplot3d as a3
import matplotlib.colors as colors


w = np.array([1., 1., 1.])


# ∑ᵢ hᵢ wᵢ qᵢ - ∑ᵢ gᵢ wᵢ &lt;= 0 
#  qᵢ - ubᵢ &lt;= 0
# -qᵢ + lbᵢ &lt;= 0 
halfspaces = np.array([
                    [1.*w[0], 1.*w[1], 1.*w[2], -10 ],
                    [ 1.,  0.,  0., -4],
                    [ 0.,  1.,  0., -4],
                    [ 0.,  0.,  1., -4],
                    [-1.,  0.,  0.,  0],
                    [ 0., -1.,  0.,  0],
                    [ 0.,  0., -1.,  0]
                    ])
feasible_point = np.array([0.1, 0.1, 0.1])
hs = HalfspaceIntersection(halfspaces, feasible_point)
verts = hs.intersections
hull = ConvexHull(verts)
faces = hull.simplices

ax = a3.Axes3D(plt.figure())
ax.dist=10
ax.azim=30
ax.elev=10
ax.set_xlim([0,5])
ax.set_ylim([0,5])
ax.set_zlim([0,5])

for s in faces:
    sq = [
        [verts[s[0], 0], verts[s[0], 1], verts[s[0], 2]],
        [verts[s[1], 0], verts[s[1], 1], verts[s[1], 2]],
        [verts[s[2], 0], verts[s[2], 1], verts[s[2], 2]]
    ]

    f = a3.art3d.Poly3DCollection([sq])
    f.set_color(colors.rgb2hex(sp.rand(3)))
    f.set_edgecolor('k')
    f.set_alpha(0.1)
    ax.add_collection3d(f)

plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/dacwb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dacwb.png"" alt=""Result of the above code""></a></p>
",7122932,187,04-03-2018 17:52,05-03-2018 16:47,1,187,11,0,2,60,"{'badge_counts': {'bronze': 11, 'silver': 2, 'gold': 0}, 'account_id': 9592573, 'is_employee': False, 'last_modified_date': 1573678861, 'last_access_date': 1643948192, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 187, 'creation_date': 1478449920, 'user_type': 'registered', 'user_id': 7122932, 'accept_rate': 60, 'location': 'Pittsburgh, PA, United States', 'link': 'https://stackoverflow.com/users/7122932/nikferrari', 'profile_image': 'https://www.gravatar.com/avatar/58ef13fc0d81602f2c1d1111111b5235?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'nikferrari'}","I am trying to plot in 3D a polytope defined by a set of inequalities. Essentially, I try to reproduce the functionality of this matlab plotregion library in matplotlib. My approach is to get the intersection vertices, construct the convex hull of them, and then get and plot the resulting faces (simplices). The problem is that many simplices are coplanar, and they are making the plot very busy for no reason (see all of these diagonal edges in the plot below). Is there any easy way to just print the ""outside"" edges of the polyhedron, without having to consolidate by my self, one by one, all of the coplanar simplices? Thank you","from scipy.spatial import HalfspaceIntersection
from scipy.spatial import ConvexHull
import scipy as sp
import numpy as np
import matplotlib.pyplot as plt
import mpl_toolkits.mplot3d as a3
import matplotlib.colors as colors


w = np.array([1., 1., 1.])


# ∑ᵢ hᵢ wᵢ qᵢ - ∑ᵢ gᵢ wᵢ &lt;= 0 
#  qᵢ - ubᵢ &lt;= 0
# -qᵢ + lbᵢ &lt;= 0 
halfspaces = np.array([
                    [1.*w[0], 1.*w[1], 1.*w[2], -10 ],
                    [ 1.,  0.,  0., -4],
                    [ 0.,  1.,  0., -4],
                    [ 0.,  0.,  1., -4],
                    [-1.,  0.,  0.,  0],
                    [ 0., -1.,  0.,  0],
                    [ 0.,  0., -1.,  0]
                    ])
feasible_point = np.array([0.1, 0.1, 0.1])
hs = HalfspaceIntersection(halfspaces, feasible_point)
verts = hs.intersections
hull = ConvexHull(verts)
faces = hull.simplices

ax = a3.Axes3D(plt.figure())
ax.dist=10
ax.azim=30
ax.elev=10
ax.set_xlim([0,5])
ax.set_ylim([0,5])
ax.set_zlim([0,5])

for s in faces:
    sq = [
        [verts[s[0], 0], verts[s[0], 1], verts[s[0], 2]],
        [verts[s[1], 0], verts[s[1], 1], verts[s[1], 2]],
        [verts[s[2], 0], verts[s[2], 1], verts[s[2], 2]]
    ]

    f = a3.art3d.Poly3DCollection([sq])
    f.set_color(colors.rgb2hex(sp.rand(3)))
    f.set_edgecolor('k')
    f.set_alpha(0.1)
    ax.add_collection3d(f)

plt.show()
",51,65,1,2,
599,49530738,49539075,15913,Precision Measurement with Opencv python,1,<python><opencv><computer-vision><dimension><measurement>,12,"<p>I am actually working on a Machine Vision project using OpenCV and Python.</p>

<p><strong>Objective</strong> : The objective of the project is to measure the dimensions of a component with high accuracy.</p>

<p><strong>Main Hardware</strong> :</p>

<ul>
<li><p>Basler 5MP camera (aca-2500-14gm)</p></li>
<li><p>A red backlight (100 mm x 100 mm) (Size of my component is around 60mm)</p></li>
</ul>

<p><strong>Experiment</strong> </p>

<p>Since I am Looking at very tight tolerance limits, I first did a precision study. I kept the component on the backlight source and took 100 images without moving the part (imagine like a video with 100 frames). I measured the Outer Diameter(OD) of all the 100 images. My mm/pixel ratio is <strong>0.042</strong>. I measured the standard deviation of the measurement to find out the precision, which turned out to be around <strong>0.03 mm which is bad</strong>. The component nor the setup is touched thus I was expecting a precision of 0.005 mm. But I am off by an order of magnitude. I am using OpenCV's Hough circle to calculate the OD of the component.</p>

<p>Code:</p>

<pre><code>import sys
import pickle
import cv2
import matplotlib.pyplot as plt
import glob
import os
import numpy as np
import pandas as pd

def find_circles(image,dp=1.7,minDist=100,param1=50,param2=50,minRadius=0,maxRadius=0):
    """""" finds the center of circular objects in image using hough circle transform

    Keyword arguments
    image -- uint8: numpy ndarray of a single image (no default).
    dp -- Inverse ratio of the accumulator resolution to the image resolution (default 1.7).
    minDist -- Minimum distance in pixel distance between the centers of the detected circles (default 100).
    param1 -- First method-specific parameter (default = 50).
    param2 -- Second method-specific parameter (default = 50).
    minRadius -- Minimum circle radius in pixel distance (default = 0).
    maxRadius -- Maximum circle radius in pixel distance (default = 0).

    Output
    center -- tuple: (x,y).
    radius -- int : radius.
    ERROR if circle is not detected. returns(-1) in this case    
    """"""

    circles=cv2.HoughCircles(image, 
                             cv2.HOUGH_GRADIENT, 
                             dp = dp, 
                             minDist = minDist, 
                             param1=param1, 
                             param2=param2, 
                             minRadius=minRadius, 
                             maxRadius=maxRadius)
    if circles is not None:
            circles = circles.reshape(circles.shape[1],circles.shape[2])
            return(circles)
    else:
        raise ValueError(""ERROR!!!!!! circle not detected try tweaking the parameters or the min and max radius"")

def find_od(image_path_list):
    image_path_list.sort()
    print(len(image_path_list))
    result_df = pd.DataFrame(columns=[""component_name"",""measured_dia_pixels"",""center_in_pixels""])
    for i,name in enumerate(image_path_list):
        img = cv2.imread(name,0) # read the image in grayscale
        ret,thresh_img = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY_INV)
        thresh_img = cv2.bilateralFilter(thresh_img,5,91,91) #smoothing
        edges = cv2.Canny(thresh_img,100,200)
        circles = find_circles(edges,dp=1.7,minDist=100,param1=50,param2=30,minRadius=685,maxRadius=700)
        circles = np.squeeze(circles)
        result_df.loc[i] = os.path.basename(name),circles[2]*2,(circles[0],circles[1])
    result_df.sort_values(""component_name"",inplace=True)
    result_df.reset_index(drop=True,inplace=True)
    return(result_df)

df = find_od(glob.glob(""./images/*""))
mean_d = df.measured_dia_pixels.mean()
std_deviation = np.sqrt(np.mean(np.square([abs(x-mean_d) for x in df.measured_dia_pixels])))

mm_per_pixel = 0.042
print(std_deviation * mm_per_pixel)
</code></pre>

<p><strong>OUTPUT: 0.024</strong> </p>

<p>The image of the component:</p>

<p><a href=""https://i.stack.imgur.com/N6uqw.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/N6uqw.jpg"" alt=""enter image description here""></a></p>

<p>Since the Images are taken without disturbing the setup, I expect the measurement's repeatability to be around 0.005 mm (5 microns) (For 100 images).But this is not so. Is it a problem of hough circle? or what am I missing here</p>
",8199433,1920,28-03-2018 09:24,28-03-2018 15:54,0,1920,40,4,18,67,"{'badge_counts': {'bronze': 40, 'silver': 18, 'gold': 4}, 'account_id': 11172996, 'is_employee': False, 'last_modified_date': 1664381101, 'last_access_date': 1711083155, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1920, 'creation_date': 1498129583, 'user_type': 'registered', 'user_id': 8199433, 'accept_rate': 67, 'location': 'Dallas, TX, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/8199433/abhijit-balaji', 'profile_image': 'https://graph.facebook.com/1998941780341976/picture?type=large', 'display_name': 'Abhijit Balaji'}","I am actually working on a Machine Vision project using OpenCV and Python. Objective : The objective of the project is to measure the dimensions of a component with high accuracy. Main Hardware : Basler 5MP camera (aca-2500-14gm) A red backlight (100 mm x 100 mm) (Size of my component is around 60mm) Experiment Since I am Looking at very tight tolerance limits, I first did a precision study. I kept the component on the backlight source and took 100 images without moving the part (imagine like a video with 100 frames). I measured the Outer Diameter(OD) of all the 100 images. My mm/pixel ratio is 0.042. I measured the standard deviation of the measurement to find out the precision, which turned out to be around 0.03 mm which is bad. The component nor the setup is touched thus I was expecting a precision of 0.005 mm. But I am off by an order of magnitude. I am using OpenCV's Hough circle to calculate the OD of the component. Code: OUTPUT: 0.024 The image of the component: Since the Images are taken without disturbing the setup, I expect the measurement's repeatability to be around 0.005 mm (5 microns) (For 100 images).But this is not so. Is it a problem of hough circle? or what am I missing here","import sys
import pickle
import cv2
import matplotlib.pyplot as plt
import glob
import os
import numpy as np
import pandas as pd

def find_circles(image,dp=1.7,minDist=100,param1=50,param2=50,minRadius=0,maxRadius=0):
    """""" finds the center of circular objects in image using hough circle transform

    Keyword arguments
    image -- uint8: numpy ndarray of a single image (no default).
    dp -- Inverse ratio of the accumulator resolution to the image resolution (default 1.7).
    minDist -- Minimum distance in pixel distance between the centers of the detected circles (default 100).
    param1 -- First method-specific parameter (default = 50).
    param2 -- Second method-specific parameter (default = 50).
    minRadius -- Minimum circle radius in pixel distance (default = 0).
    maxRadius -- Maximum circle radius in pixel distance (default = 0).

    Output
    center -- tuple: (x,y).
    radius -- int : radius.
    ERROR if circle is not detected. returns(-1) in this case    
    """"""

    circles=cv2.HoughCircles(image, 
                             cv2.HOUGH_GRADIENT, 
                             dp = dp, 
                             minDist = minDist, 
                             param1=param1, 
                             param2=param2, 
                             minRadius=minRadius, 
                             maxRadius=maxRadius)
    if circles is not None:
            circles = circles.reshape(circles.shape[1],circles.shape[2])
            return(circles)
    else:
        raise ValueError(""ERROR!!!!!! circle not detected try tweaking the parameters or the min and max radius"")

def find_od(image_path_list):
    image_path_list.sort()
    print(len(image_path_list))
    result_df = pd.DataFrame(columns=[""component_name"",""measured_dia_pixels"",""center_in_pixels""])
    for i,name in enumerate(image_path_list):
        img = cv2.imread(name,0) # read the image in grayscale
        ret,thresh_img = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY_INV)
        thresh_img = cv2.bilateralFilter(thresh_img,5,91,91) #smoothing
        edges = cv2.Canny(thresh_img,100,200)
        circles = find_circles(edges,dp=1.7,minDist=100,param1=50,param2=30,minRadius=685,maxRadius=700)
        circles = np.squeeze(circles)
        result_df.loc[i] = os.path.basename(name),circles[2]*2,(circles[0],circles[1])
    result_df.sort_values(""component_name"",inplace=True)
    result_df.reset_index(drop=True,inplace=True)
    return(result_df)

df = find_od(glob.glob(""./images/*""))
mean_d = df.measured_dia_pixels.mean()
std_deviation = np.sqrt(np.mean(np.square([abs(x-mean_d) for x in df.measured_dia_pixels])))

mm_per_pixel = 0.042
print(std_deviation * mm_per_pixel)
",62,89,1,1,
600,50049891,50049939,17207,What is the inverse of numpy's log1p()?,2,<python><numpy>,14,"<p>I transformed my data with:</p>
<pre><code>Y = np.log1p(Y)
</code></pre>
<p>What is the formula to transform the values back to the natural values?</p>
<pre><code>back  = np.e**Y
</code></pre>
<p>This did not work.</p>
",8736498,379,26-04-2018 18:36,26-04-2018 18:40,0,379,11,1,2,100,"{'badge_counts': {'bronze': 11, 'silver': 2, 'gold': 1}, 'account_id': 11939075, 'is_employee': False, 'last_modified_date': 1610699674, 'last_access_date': 1624912183, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 379, 'creation_date': 1507370930, 'user_type': 'registered', 'user_id': 8736498, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/8736498/meiiso', 'profile_image': 'https://www.gravatar.com/avatar/d498a2290afd554bd40485e89e41637f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Meiiso'}",I transformed my data with: What is the formula to transform the values back to the natural values? This did not work.,"Y = np.log1p(Y)
 back  = np.e**Y
",0,7,0,0,
601,48562935,48563271,10531,"What are the ""parts"" in a multipart email?",2,<python><python-3.x><email><imap><mime>,11,"<h1>A bit of context...</h1>

<p>Some time ago, I wrote Python a program that deals with email messages, one thing that always comes across is to know whether an email is ""multipart"" or not.</p>

<p>After a bit of research, I knew that it has something to do with emails containing HTML, or attachments etc... But I didn't really understand it.</p>

<h3>My usage of it was limited to 2 instances:</h3>

<p><strong>1. When I had to save the attachment from the raw email</strong> </p>

<p>I just found this on the internet (probably on here - Sorry for not crediting the person who wrote it but I can't seem to find him again :/) and pasted it in my code</p>

<pre><code>def downloadAttachments(emailMsg, pathToSaveFile):
    """"""
    Save Attachments to pathToSaveFile (Example: pathToSaveFile = ""C:\\Program Files\\"")
    """"""
    att_path_list = []
    for part in emailMsg.walk():
        # multipart are just containers, so we skip them
        if part.get_content_maintype() == 'multipart':
            continue

        # is this part an attachment ?
        if part.get('Content-Disposition') is None:
            continue

        filename = part.get_filename()

        att_path = os.path.join(pathToSaveFile, filename)

        #Check if its already there
        if not os.path.isfile(att_path) :
            # finally write the stuff
            fp = open(att_path, 'wb')
            fp.write(part.get_payload(decode=True))
            fp.close()
        att_path_list.append(att_path)
    return att_path_list
</code></pre>

<p><strong>2. When I had to get the text from the raw email</strong></p>

<p>Also pasted from someone on the internet without really understanding how it works.</p>

<pre><code>def get_text(emailMsg):
    """"""
    Output: body of the email (text content)
    """"""
    if emailMsg.is_multipart():
        return get_text(emailMsg.get_payload(0))
    else:
        return emailMsg.get_payload(None, True)
</code></pre>

<h2>What I do understand...</h2>

<p>Is that if the email message is multipart, the parts can be iterated over. </p>

<h1>My question is</h1>

<p>What <strong>exactly</strong> are these parts? How do you know which one is html for example? Or which one is an attachment? Or just the body?</p>
",8744459,389,01-02-2018 12:35,01-02-2018 12:54,0,389,18,1,6,,"{'badge_counts': {'bronze': 18, 'silver': 6, 'gold': 1}, 'account_id': 11950715, 'is_employee': False, 'last_modified_date': 1658576100, 'last_access_date': 1710942556, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 389, 'creation_date': 1507534810, 'user_type': 'registered', 'user_id': 8744459, 'location': 'Switzerland', 'website_url': '', 'link': 'https://stackoverflow.com/users/8744459/0-jump', 'profile_image': 'https://www.gravatar.com/avatar/778932b27d24038548a1e1e97b124d6d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': '0_jump'}","A bit of context... Some time ago, I wrote Python a program that deals with email messages, one thing that always comes across is to know whether an email is ""multipart"" or not. After a bit of research, I knew that it has something to do with emails containing HTML, or attachments etc... But I didn't really understand it. My usage of it was limited to 2 instances: 1. When I had to save the attachment from the raw email I just found this on the internet (probably on here - Sorry for not crediting the person who wrote it but I can't seem to find him again :/) and pasted it in my code 2. When I had to get the text from the raw email Also pasted from someone on the internet without really understanding how it works. What I do understand... Is that if the email message is multipart, the parts can be iterated over. My question is What exactly are these parts? How do you know which one is html for example? Or which one is an attachment? Or just the body?","def downloadAttachments(emailMsg, pathToSaveFile):
    """"""
    Save Attachments to pathToSaveFile (Example: pathToSaveFile = ""C:\\Program Files\\"")
    """"""
    att_path_list = []
    for part in emailMsg.walk():
        # multipart are just containers, so we skip them
        if part.get_content_maintype() == 'multipart':
            continue

        # is this part an attachment ?
        if part.get('Content-Disposition') is None:
            continue

        filename = part.get_filename()

        att_path = os.path.join(pathToSaveFile, filename)

        #Check if its already there
        if not os.path.isfile(att_path) :
            # finally write the stuff
            fp = open(att_path, 'wb')
            fp.write(part.get_payload(decode=True))
            fp.close()
        att_path_list.append(att_path)
    return att_path_list
 def get_text(emailMsg):
    """"""
    Output: body of the email (text content)
    """"""
    if emailMsg.is_multipart():
        return get_text(emailMsg.get_payload(0))
    else:
        return emailMsg.get_payload(None, True)
",32,61,0,0,
602,50372509,50373021,3640,Why are attributes lost after copying a Pandas DataFrame,4,<python>,18,"<p>Why is it not possible to pass attributes of an instance through a copy? I want to pass the <code>name</code> attribute to another dataframe.</p>

<pre><code>import copy
df = pd.DataFrame([1,2,3])
df.name = 'sheet1'
df2 = copy.deepcopy(df)

print(f'df.name: {df.name}')
&gt;&gt; df.name: sheet1

print(f'df2.name: {df2.name}')
&gt;&gt;    AttributeError    
        ...      
      'DataFrame' object has no attribute 'name'
</code></pre>

<p>Similarly, why does this also not work, when creating a class and inheriting from it?</p>

<pre><code>class DfWithName(pd.DataFrame):

    def __init__(self, *args, **kwargs):
        self.__init__ = super().__init__(*args, **kwargs)
        print('lol')

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, value):
        self._name = value
</code></pre>

<p>and using the same code:</p>

<pre><code>import copy
df = DfWithName([1,2,3])
df.name = 'sheet1'
df2 = copy.deepcopy(df) 
print(f'df.name: {df2.name}')
&gt;&gt;    AttributeError    
        ...      
      'DataFrame' object has no attribute 'name'
</code></pre>
",8751871,2414,16-05-2018 13:39,16-05-2018 14:02,0,2414,40,1,22,94,"{'badge_counts': {'bronze': 40, 'silver': 22, 'gold': 1}, 'account_id': 11960749, 'is_employee': False, 'last_modified_date': 1692056100, 'last_access_date': 1710392877, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2414, 'creation_date': 1507633257, 'user_type': 'registered', 'user_id': 8751871, 'accept_rate': 94, 'link': 'https://stackoverflow.com/users/8751871/a-h', 'profile_image': 'https://www.gravatar.com/avatar/1c1fe1feea37ff1882787c3dfcf08ad2?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'A H'}","Why is it not possible to pass attributes of an instance through a copy? I want to pass the attribute to another dataframe. Similarly, why does this also not work, when creating a class and inheriting from it? and using the same code:","name import copy
df = pd.DataFrame([1,2,3])
df.name = 'sheet1'
df2 = copy.deepcopy(df)

print(f'df.name: {df.name}')
&gt;&gt; df.name: sheet1

print(f'df2.name: {df2.name}')
&gt;&gt;    AttributeError    
        ...      
      'DataFrame' object has no attribute 'name'
 class DfWithName(pd.DataFrame):

    def __init__(self, *args, **kwargs):
        self.__init__ = super().__init__(*args, **kwargs)
        print('lol')

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, value):
        self._name = value
 import copy
df = DfWithName([1,2,3])
df.name = 'sheet1'
df2 = copy.deepcopy(df) 
print(f'df.name: {df2.name}')
&gt;&gt;    AttributeError    
        ...      
      'DataFrame' object has no attribute 'name'
",29,44,0,0,
603,49705047,49705239,56266,Downloading multiple stocks at once from Yahoo Finance,5,<python><yahoo-finance><pandas-datareader>,18,"<p>I have a question about the function of Yahoo Finance using the pandas data reader. I'm using for months now a list with stock tickers and execute it in the following lines:</p>
<pre><code>import pandas_datareader as pdr
import datetime

stocks = [&quot;stock1&quot;,&quot;stock2&quot;,....]
start = datetime.datetime(2012,5,31)
end = datetime.datetime(2018,3,1)

f = pdr.DataReader(stocks, 'yahoo',start,end)
</code></pre>
<p>Since yesterday I get the error <code>&quot;IndexError: list index out of range&quot;</code>, which appears only if I try to get multiple stocks.</p>
<p>Has anything changed in recent days, which I have to consider, or do you have a better solution for my problem?</p>
",8759280,183,07-04-2018 07:33,07-04-2018 08:00,0,183,5,1,1,,"{'badge_counts': {'bronze': 5, 'silver': 1, 'gold': 1}, 'account_id': 11970709, 'is_employee': False, 'last_modified_date': 1573678502, 'last_access_date': 1604736204, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 183, 'creation_date': 1507728280, 'user_type': 'registered', 'user_id': 8759280, 'website_url': '', 'link': 'https://stackoverflow.com/users/8759280/scharcomolten', 'profile_image': 'https://www.gravatar.com/avatar/d176a3c00e8a244e97d10610877e2b96?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ScharcoMolten'}","I have a question about the function of Yahoo Finance using the pandas data reader. I'm using for months now a list with stock tickers and execute it in the following lines: Since yesterday I get the error , which appears only if I try to get multiple stocks. Has anything changed in recent days, which I have to consider, or do you have a better solution for my problem?","import pandas_datareader as pdr
import datetime

stocks = [&quot;stock1&quot;,&quot;stock2&quot;,....]
start = datetime.datetime(2012,5,31)
end = datetime.datetime(2018,3,1)

f = pdr.DataReader(stocks, 'yahoo',start,end)
 &quot;IndexError: list index out of range&quot;",6,12,0,0,
604,48072007,48072491,5326,How to use numpy.argsort() as indices in more than 2 dimensions?,3,<python><arrays><sorting><numpy>,15,"<p>I know something similar to this question has been asked many times over already, but all answers given to similar questions only seem to work for arrays with 2 dimensions.</p>

<p>My understanding of <code>np.argsort()</code> is that <code>np.sort(array) == array[np.argsort(array)]</code> should be <code>True</code>.
I have found out that this is indeed correct if <code>np.ndim(array) == 2</code>, but it gives different results if <code>np.ndim(array) &gt; 2</code>.</p>

<p>Example:</p>

<pre><code>&gt;&gt;&gt; array = np.array([[[ 0.81774634,  0.62078744],
                       [ 0.43912609,  0.29718462]],
                      [[ 0.1266578 ,  0.82282054],
                       [ 0.98180375,  0.79134389]]])
&gt;&gt;&gt; np.sort(array)
array([[[ 0.62078744,  0.81774634],
        [ 0.29718462,  0.43912609]],

       [[ 0.1266578 ,  0.82282054],
        [ 0.79134389,  0.98180375]]])
&gt;&gt;&gt; array.argsort()
array([[[1, 0],
        [1, 0]],

       [[0, 1],
        [1, 0]]])
&gt;&gt;&gt; array[array.argsort()]
array([[[[[ 0.1266578 ,  0.82282054],
          [ 0.98180375,  0.79134389]],

         [[ 0.81774634,  0.62078744],
          [ 0.43912609,  0.29718462]]],


        [[[ 0.1266578 ,  0.82282054],
          [ 0.98180375,  0.79134389]],

         [[ 0.81774634,  0.62078744],
          [ 0.43912609,  0.29718462]]]],



       [[[[ 0.81774634,  0.62078744],
          [ 0.43912609,  0.29718462]],

         [[ 0.1266578 ,  0.82282054],
          [ 0.98180375,  0.79134389]]],


        [[[ 0.1266578 ,  0.82282054],
          [ 0.98180375,  0.79134389]],

         [[ 0.81774634,  0.62078744],
          [ 0.43912609,  0.29718462]]]]])
</code></pre>

<p>So, can anybody explain to me how exactly <code>np.argsort()</code> can be used as the indices to obtain the sorted array?
The only way I can come up with is:</p>

<pre><code>args = np.argsort(array)
array_sort = np.zeros_like(array)
for i in range(array.shape[0]):
    for j in range(array.shape[1]):
        array_sort[i, j] = array[i, j, args[i, j]]
</code></pre>

<p>which is extremely tedious and cannot be generalized for any given number of dimensions.</p>
",8787505,1162,03-01-2018 06:04,03-01-2018 06:45,0,1162,18,0,10,,"{'badge_counts': {'bronze': 18, 'silver': 10, 'gold': 0}, 'account_id': 12010736, 'is_employee': False, 'last_modified_date': 1693211400, 'last_access_date': 1647450219, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1162, 'creation_date': 1508206475, 'user_type': 'registered', 'user_id': 8787505, 'location': 'Melbourne, Victoria, Australia', 'website_url': '', 'link': 'https://stackoverflow.com/users/8787505/1313e', 'profile_image': 'https://i.stack.imgur.com/zY7CD.jpg?s=256&g=1', 'display_name': '1313e'}","I know something similar to this question has been asked many times over already, but all answers given to similar questions only seem to work for arrays with 2 dimensions. My understanding of is that should be . I have found out that this is indeed correct if , but it gives different results if . Example: So, can anybody explain to me how exactly can be used as the indices to obtain the sorted array? The only way I can come up with is: which is extremely tedious and cannot be generalized for any given number of dimensions.","np.argsort() np.sort(array) == array[np.argsort(array)] True np.ndim(array) == 2 np.ndim(array) &gt; 2 &gt;&gt;&gt; array = np.array([[[ 0.81774634,  0.62078744],
                       [ 0.43912609,  0.29718462]],
                      [[ 0.1266578 ,  0.82282054],
                       [ 0.98180375,  0.79134389]]])
&gt;&gt;&gt; np.sort(array)
array([[[ 0.62078744,  0.81774634],
        [ 0.29718462,  0.43912609]],

       [[ 0.1266578 ,  0.82282054],
        [ 0.79134389,  0.98180375]]])
&gt;&gt;&gt; array.argsort()
array([[[1, 0],
        [1, 0]],

       [[0, 1],
        [1, 0]]])
&gt;&gt;&gt; array[array.argsort()]
array([[[[[ 0.1266578 ,  0.82282054],
          [ 0.98180375,  0.79134389]],

         [[ 0.81774634,  0.62078744],
          [ 0.43912609,  0.29718462]]],


        [[[ 0.1266578 ,  0.82282054],
          [ 0.98180375,  0.79134389]],

         [[ 0.81774634,  0.62078744],
          [ 0.43912609,  0.29718462]]]],



       [[[[ 0.81774634,  0.62078744],
          [ 0.43912609,  0.29718462]],

         [[ 0.1266578 ,  0.82282054],
          [ 0.98180375,  0.79134389]]],


        [[[ 0.1266578 ,  0.82282054],
          [ 0.98180375,  0.79134389]],

         [[ 0.81774634,  0.62078744],
          [ 0.43912609,  0.29718462]]]]])
 np.argsort() args = np.argsort(array)
array_sort = np.zeros_like(array)
for i in range(array.shape[0]):
    for j in range(array.shape[1]):
        array_sort[i, j] = array[i, j, args[i, j]]
",41,64,0,0,
605,48569166,48569627,115628,multiple if else conditions in pandas dataframe and derive multiple columns,3,<python><pandas><if-statement><dataframe>,23,"<p>I have a dataframe like below.</p>
<pre><code>import pandas as pd
import numpy as np
raw_data = {'student':['A','B','C','D','E'],
        'score': [100, 96, 80, 105,156], 
    'height': [7, 4,9,5,3],
    'trigger1' : [84,95,15,78,16],
    'trigger2' : [99,110,30,93,31],
    'trigger3' : [114,125,45,108,46]}

df2 = pd.DataFrame(raw_data, columns = ['student','score', 'height','trigger1','trigger2','trigger3'])

print(df2)
</code></pre>
<p>I need to derive Flag column based on multiple conditions.</p>
<p>i need to compare score and height columns with trigger 1 -3 columns.</p>
<p>Flag Column:</p>
<ol>
<li><p>if Score greater than equal trigger 1 and height less than 8 then Red --</p>
</li>
<li><p>if Score greater than equal trigger 2 and height less than 8 then Yellow --</p>
</li>
<li><p>if Score greater than equal trigger 3 and height less than 8 then Orange --</p>
</li>
<li><p>if height greater than 8 then leave it as blank</p>
</li>
</ol>
<p>How to write if else conditions in pandas dataframe and derive columns?</p>
<p>Expected Output</p>
<pre><code>  student  score  height  trigger1  trigger2  trigger3    Flag
0       A    100       7        84        99       114  Yellow
1       B     96       4        95       110       125     Red
2       C     80       9        15        30        45     NaN
3       D    105       5        78        93       108  Yellow
4       E    156       3        16        31        46  Orange
</code></pre>
<p>For other column Text1 in my original question I have tried this one but the integer columns not converting the string when concatenation using astype(str) any other approach?</p>
<pre><code>def text_df(df):

    if (df['trigger1'] &lt;= df['score'] &lt; df['trigger2']) and (df['height'] &lt; 8):
        return df['student'] + &quot; score &quot; + df['score'].astype(str) + &quot; greater than &quot; + df['trigger1'].astype(str) + &quot; and less than height 5&quot;
    elif (df['trigger2'] &lt;= df['score'] &lt; df['trigger3']) and (df['height'] &lt; 8):
        return df['student'] + &quot; score &quot; + df['score'].astype(str) + &quot; greater than &quot; + df['trigger2'].astype(str) + &quot; and less than height 5&quot;
    elif (df['trigger3'] &lt;= df['score']) and (df['height'] &lt; 8):
        return df['student'] + &quot; score &quot; + df['score'].astype(str) + &quot; greater than &quot; + df['trigger3'].astype(str) + &quot; and less than height 5&quot;
    elif (df['height'] &gt; 8):
        return np.nan
</code></pre>
",8791503,1037,01-02-2018 18:08,01-02-2018 18:37,0,1037,25,3,12,100,"{'badge_counts': {'bronze': 25, 'silver': 12, 'gold': 3}, 'account_id': 12016560, 'is_employee': False, 'last_modified_date': 1618075200, 'last_access_date': 1654758592, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1037, 'creation_date': 1508261085, 'user_type': 'registered', 'user_id': 8791503, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/8791503/kumar-ak', 'profile_image': 'https://www.gravatar.com/avatar/1792626b1b57d53d675c3c825107109d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Kumar AK'}",I have a dataframe like below. I need to derive Flag column based on multiple conditions. i need to compare score and height columns with trigger 1 -3 columns. Flag Column: if Score greater than equal trigger 1 and height less than 8 then Red -- if Score greater than equal trigger 2 and height less than 8 then Yellow -- if Score greater than equal trigger 3 and height less than 8 then Orange -- if height greater than 8 then leave it as blank How to write if else conditions in pandas dataframe and derive columns? Expected Output For other column Text1 in my original question I have tried this one but the integer columns not converting the string when concatenation using astype(str) any other approach?,"import pandas as pd
import numpy as np
raw_data = {'student':['A','B','C','D','E'],
        'score': [100, 96, 80, 105,156], 
    'height': [7, 4,9,5,3],
    'trigger1' : [84,95,15,78,16],
    'trigger2' : [99,110,30,93,31],
    'trigger3' : [114,125,45,108,46]}

df2 = pd.DataFrame(raw_data, columns = ['student','score', 'height','trigger1','trigger2','trigger3'])

print(df2)
   student  score  height  trigger1  trigger2  trigger3    Flag
0       A    100       7        84        99       114  Yellow
1       B     96       4        95       110       125     Red
2       C     80       9        15        30        45     NaN
3       D    105       5        78        93       108  Yellow
4       E    156       3        16        31        46  Orange
 def text_df(df):

    if (df['trigger1'] &lt;= df['score'] &lt; df['trigger2']) and (df['height'] &lt; 8):
        return df['student'] + &quot; score &quot; + df['score'].astype(str) + &quot; greater than &quot; + df['trigger1'].astype(str) + &quot; and less than height 5&quot;
    elif (df['trigger2'] &lt;= df['score'] &lt; df['trigger3']) and (df['height'] &lt; 8):
        return df['student'] + &quot; score &quot; + df['score'].astype(str) + &quot; greater than &quot; + df['trigger2'].astype(str) + &quot; and less than height 5&quot;
    elif (df['trigger3'] &lt;= df['score']) and (df['height'] &lt; 8):
        return df['student'] + &quot; score &quot; + df['score'].astype(str) + &quot; greater than &quot; + df['trigger3'].astype(str) + &quot; and less than height 5&quot;
    elif (df['height'] &gt; 8):
        return np.nan
",25,48,0,0,
606,48140225,48140239,1622,How do you generalise the creation of a list with many variables and conditions of `if`?,4,<python><list><if-statement>,17,"<p>I create a list as follows:</p>

<pre><code>['v0' if x%4==0 else 'v1' if x%4==1 else 'v2' if x%4==2 else 'v3' for x in list_1]
</code></pre>

<p>How to generalize the creation of such a list, so that it can be easily expanded by a larger number of variables and subsequent conditions?</p>
",8820330,1127,07-01-2018 18:23,07-01-2018 18:25,0,1127,31,0,10,93,"{'badge_counts': {'bronze': 31, 'silver': 10, 'gold': 0}, 'account_id': 12061454, 'is_employee': False, 'last_modified_date': 1646445900, 'last_access_date': 1676558245, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1127, 'creation_date': 1508773491, 'user_type': 'registered', 'user_id': 8820330, 'accept_rate': 93, 'website_url': '', 'link': 'https://stackoverflow.com/users/8820330/tomasz-przemski', 'profile_image': 'https://i.stack.imgur.com/UUBoU.jpg?s=256&g=1', 'display_name': 'Tomasz Przemski'}","I create a list as follows: How to generalize the creation of such a list, so that it can be easily expanded by a larger number of variables and subsequent conditions?","['v0' if x%4==0 else 'v1' if x%4==1 else 'v2' if x%4==2 else 'v3' for x in list_1]
",0,6,0,0,
607,49560486,49562555,55316,How to convert PDF to CSV with tabula-py?,3,<python><csv><pdf><tabula>,11,"<p>In Python 3, I have a PDF file ""Ativos_Fevereiro_2018_servidores_rj.pdf"" with 6,041 pages. I'm on a machine with Ubuntu</p>

<p>On each page there is text at the top of the page, two lines. And below a table, with header and two columns. Each table in 36 rows, less on the last page</p>

<p>At the end of each page, after the tables, there is also a line of text</p>

<p>I want to create a CSV from this PDF, considering only the tables in the pages. And ignoring the texts before and after the tables</p>

<p>Initially I tested the tabula-py. But it generates an empty file:</p>

<pre><code>from tabula import convert_into

convert_into(""Ativos_Fevereiro_2018_servidores_rj.pdf"", ""test_s.csv"", output_format=""csv"")
</code></pre>

<p>Please, does anyone know of another method to use tabula-py for this type of demand?</p>

<p>Or another way to convert PDF to CSV in this file type?</p>
",8321427,975,29-03-2018 16:01,29-03-2018 18:07,0,985,46,4,17,81,"{'badge_counts': {'bronze': 46, 'silver': 17, 'gold': 4}, 'account_id': 8882257, 'is_employee': False, 'last_modified_date': 1666915500, 'last_access_date': 1708468730, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 985, 'creation_date': 1500318372, 'user_type': 'registered', 'user_id': 8321427, 'accept_rate': 81, 'location': 'S&#227;o Paulo, SP, Brasil', 'website_url': '', 'link': 'https://stackoverflow.com/users/8321427/reinaldo-chaves', 'profile_image': 'https://lh6.googleusercontent.com/-ONb_mSwNHKw/AAAAAAAAAAI/AAAAAAAAAhY/TDOgxKaK7lU/photo.jpg?sz=256', 'display_name': 'Reinaldo Chaves'}","In Python 3, I have a PDF file ""Ativos_Fevereiro_2018_servidores_rj.pdf"" with 6,041 pages. I'm on a machine with Ubuntu On each page there is text at the top of the page, two lines. And below a table, with header and two columns. Each table in 36 rows, less on the last page At the end of each page, after the tables, there is also a line of text I want to create a CSV from this PDF, considering only the tables in the pages. And ignoring the texts before and after the tables Initially I tested the tabula-py. But it generates an empty file: Please, does anyone know of another method to use tabula-py for this type of demand? Or another way to convert PDF to CSV in this file type?","from tabula import convert_into

convert_into(""Ativos_Fevereiro_2018_servidores_rj.pdf"", ""test_s.csv"", output_format=""csv"")
",2,18,0,0,
608,49348839,50343238,3323,How to unit test Neo4j in python ?,2,<python><unit-testing><neo4j>,11,"<p>I have an application backed by Neo4j database written in python. </p>

<p>Does anyone know what is the best approach to unit test my application ? </p>

<p>I have found this can be easily done in Java with usage of ImpermanentGraphDatabase. Is there a similar approach in python ? </p>

<p>Any tips are appreciated. </p>
",8337777,197,18-03-2018 14:14,15-05-2018 06:00,58,197,11,0,2,,"{'badge_counts': {'bronze': 11, 'silver': 2, 'gold': 0}, 'account_id': 11372576, 'is_employee': False, 'last_modified_date': 1573678580, 'last_access_date': 1688557070, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 197, 'creation_date': 1500548571, 'user_type': 'registered', 'user_id': 8337777, 'location': 'Brno, Czechia', 'link': 'https://stackoverflow.com/users/8337777/jakub-bartolomej-kosuth', 'profile_image': 'https://www.gravatar.com/avatar/eb1e3dcc889242ef61f1e6dab4c9f0cf?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jakub Bartolomej Kosuth'}",I have an application backed by Neo4j database written in python. Does anyone know what is the best approach to unit test my application ? I have found this can be easily done in Java with usage of ImpermanentGraphDatabase. Is there a similar approach in python ? Any tips are appreciated.,,0,7,0,0,
609,50008296,50011987,13423,Facebook JSON badly encoded,10,<python><python-3.x><unicode><mojibake>,55,"<p>I downloaded my Facebook messenger data (in your Facebook account, go to <em>settings,</em> then to <em>Your Facebook information</em>, then <em>Download your information</em>, then create a file with at least the <em>Messages</em> box checked) to do some cool statistics</p>

<p>However there is a small problem with encoding. I'm not sure, but it looks like Facebook used bad encoding for this data. When I open it with text editor I see something like this: <code>Rados\u00c5\u0082aw</code>. When I try to open it with python (UTF-8) I get <code>RadosÅ\x82aw</code>. However I should get: <code>Radosław</code>.</p>

<p>My python script:</p>

<pre><code>text = open(os.path.join(subdir, file), encoding='utf-8')
conversations.append(json.load(text))
</code></pre>

<p>I tried a few most common encodings. Example data is:</p>

<pre><code>{
  ""sender_name"": ""Rados\u00c5\u0082aw"",
  ""timestamp"": 1524558089,
  ""content"": ""No to trzeba ostatnie treningi zrobi\u00c4\u0087 xD"",
  ""type"": ""Generic""
}
</code></pre>
",8345375,619,24-04-2018 18:10,24-04-2018 23:23,0,619,6,1,5,,"{'badge_counts': {'bronze': 6, 'silver': 5, 'gold': 1}, 'account_id': 11382958, 'is_employee': False, 'last_modified_date': 1573678578, 'last_access_date': 1627665789, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 619, 'creation_date': 1500651003, 'user_type': 'registered', 'user_id': 8345375, 'link': 'https://stackoverflow.com/users/8345375/jakub-jendryka', 'profile_image': 'https://www.gravatar.com/avatar/18fa85e2ad439b7c29e369c7e41cda43?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jakub Jendryka'}","I downloaded my Facebook messenger data (in your Facebook account, go to settings, then to Your Facebook information, then Download your information, then create a file with at least the Messages box checked) to do some cool statistics However there is a small problem with encoding. I'm not sure, but it looks like Facebook used bad encoding for this data. When I open it with text editor I see something like this: . When I try to open it with python (UTF-8) I get . However I should get: . My python script: I tried a few most common encodings. Example data is:","Rados\u00c5\u0082aw RadosÅ\x82aw Radosław text = open(os.path.join(subdir, file), encoding='utf-8')
conversations.append(json.load(text))
 {
  ""sender_name"": ""Rados\u00c5\u0082aw"",
  ""timestamp"": 1524558089,
  ""content"": ""No to trzeba ostatnie treningi zrobi\u00c4\u0087 xD"",
  ""type"": ""Generic""
}
",3,19,0,0,
610,48997007,48997065,12816,"Check for string in ""response.content"" raising ""TypeError: a bytes-like object is required, not 'str'""",4,<python><python-3.x><string><python-requests>,12,"<p>I am trying to see if a sentence is present in the response back from a request. </p>

<pre><code>import requests

r = requests.get('https://www.eventbrite.co.uk/o/piers-test-16613670281')
text = 'Sorry, there are no upcoming events'

if text in r.content: 
   print('No Upcoming Events')
</code></pre>

<p>I am getting the following error:</p>

<pre><code>TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>I am not quite sure why this occurring and what the solution would be.</p>
",8840275,307,26-02-2018 21:01,26-02-2018 21:04,0,307,17,1,3,100,"{'badge_counts': {'bronze': 17, 'silver': 3, 'gold': 1}, 'account_id': 12097713, 'is_employee': False, 'last_modified_date': 1630740300, 'last_access_date': 1660225811, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 307, 'creation_date': 1509051893, 'user_type': 'registered', 'user_id': 8840275, 'accept_rate': 100, 'location': 'Oxford, United Kingdom', 'link': 'https://stackoverflow.com/users/8840275/piers-thomas', 'profile_image': 'https://graph.facebook.com/10212809421484187/picture?type=large', 'display_name': 'Piers Thomas'}",I am trying to see if a sentence is present in the response back from a request. I am getting the following error: I am not quite sure why this occurring and what the solution would be.,"import requests

r = requests.get('https://www.eventbrite.co.uk/o/piers-test-16613670281')
text = 'Sorry, there are no upcoming events'

if text in r.content: 
   print('No Upcoming Events')
 TypeError: a bytes-like object is required, not 'str'
",6,17,0,0,
611,49671990,49671991,5707,Nested lambda statements when sorting lists,8,<python><python-3.x><list><sorting><lambda>,15,"<p>I wish to sort the below list first by the number, then by the text.</p>

<pre><code>lst = ['b-3', 'a-2', 'c-4', 'd-2']

# result:
# ['a-2', 'd-2', 'b-3', 'c-4']
</code></pre>

<p><strong>Attempt 1</strong></p>

<pre><code>res = sorted(lst, key=lambda x: (int(x.split('-')[1]), x.split('-')[0]))
</code></pre>

<p>I was not happy with this since it required splitting a string twice, to extract the relevant components.</p>

<p><strong>Attempt 2</strong></p>

<p>I came up with the below solution. But I am hoping there is a more succinct solution via Pythonic <code>lambda</code> statements.</p>

<pre><code>def sorter_func(x):
    text, num = x.split('-')
    return int(num), text

res = sorted(lst, key=sorter_func)
</code></pre>

<p>I looked at <a href=""https://stackoverflow.com/questions/36391807/understanding-nested-lambda-function-behaviour-in-python"">Understanding nested lambda function behaviour in python</a> but couldn't adapt this solution directly. Is there a more succinct way to rewrite the above code?</p>
",9209546,161813,05-04-2018 12:07,05-04-2018 12:07,0,162051,348,35,286,100,"{'badge_counts': {'bronze': 348, 'silver': 286, 'gold': 35}, 'account_id': 12672477, 'is_employee': False, 'last_modified_date': 1710009300, 'last_access_date': 1710982305, 'reputation_change_year': 884, 'reputation_change_quarter': 884, 'reputation_change_month': 294, 'reputation_change_week': 120, 'reputation_change_day': 0, 'reputation': 162051, 'creation_date': 1515768442, 'user_type': 'registered', 'user_id': 9209546, 'accept_rate': 100, 'location': 'London, UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/9209546/jpp', 'profile_image': 'https://i.stack.imgur.com/qk9vC.jpg?s=256&g=1', 'display_name': 'jpp'}","I wish to sort the below list first by the number, then by the text. Attempt 1 I was not happy with this since it required splitting a string twice, to extract the relevant components. Attempt 2 I came up with the below solution. But I am hoping there is a more succinct solution via Pythonic statements. I looked at Understanding nested lambda function behaviour in python but couldn't adapt this solution directly. Is there a more succinct way to rewrite the above code?","lst = ['b-3', 'a-2', 'c-4', 'd-2']

# result:
# ['a-2', 'd-2', 'b-3', 'c-4']
 res = sorted(lst, key=lambda x: (int(x.split('-')[1]), x.split('-')[0]))
 lambda def sorter_func(x):
    text, num = x.split('-')
    return int(num), text

res = sorted(lst, key=sorter_func)
",6,27,0,1,
612,49087990,49088162,67054,Python - Request being blocked by Cloudflare,5,<python><python-3.x>,27,"<p>I am trying to log into a website. When I look at print(g.text) I am not getting back the web page I expect but instead a cloudflare page that says 'Checking your browser before accessing'</p>

<pre><code>import requests
import time

s = requests.Session()
s.get('https://www.off---white.com/en/GB/')

headers = {'Referer': 'https://www.off---white.com/en/GB/login'}

payload = {
    'utf8':'✓',
    'authenticity_token':'',
    'spree_user[email]': 'EMAIL@gmail.com',
    'spree_user[password]': 'PASSWORD',
    'spree_user[remember_me]': '0',
    'commit': 'Login'
}

r = s.post('https://www.off---white.com/en/GB/login', data=payload, headers=headers)

print(r.status_code)

g = s.get('https://www.off---white.com/en/GB/account')

print(g.status_code)
print(g.text)
</code></pre>

<p>Why is this occurring when I have set the session? </p>
",8853326,335,03-03-2018 18:56,03-03-2018 19:13,0,335,10,1,4,,"{'badge_counts': {'bronze': 10, 'silver': 4, 'gold': 1}, 'account_id': 12122445, 'is_employee': False, 'last_modified_date': 1573678487, 'last_access_date': 1528920740, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 335, 'creation_date': 1509312211, 'user_type': 'registered', 'user_id': 8853326, 'location': 'London, United Kingdom', 'link': 'https://stackoverflow.com/users/8853326/pthomas', 'profile_image': 'https://www.gravatar.com/avatar/6f6ebd196b298aaae5c7344e95e55e35?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Pthomas'}",I am trying to log into a website. When I look at print(g.text) I am not getting back the web page I expect but instead a cloudflare page that says 'Checking your browser before accessing' Why is this occurring when I have set the session?,"import requests
import time

s = requests.Session()
s.get('https://www.off---white.com/en/GB/')

headers = {'Referer': 'https://www.off---white.com/en/GB/login'}

payload = {
    'utf8':'✓',
    'authenticity_token':'',
    'spree_user[email]': 'EMAIL@gmail.com',
    'spree_user[password]': 'PASSWORD',
    'spree_user[remember_me]': '0',
    'commit': 'Login'
}

r = s.post('https://www.off---white.com/en/GB/login', data=payload, headers=headers)

print(r.status_code)

g = s.get('https://www.off---white.com/en/GB/account')

print(g.status_code)
print(g.text)
",24,30,0,0,
613,48299396,48330387,75702,Convert tkinter to EXE,2,<python><python-3.x><tkinter><cx-freeze>,12,"<p>Currently, I'm trying to convert my tkinter Python script to an EXE file using cx_freeze. It is somehow not working when I try to add another file. You can see the method I've used in the minimum example I'm using below.</p>
<pre><code>import tkinter as tk

import numpy.core._methods, numpy.lib.format 

class Main(tk.Tk):

    def __init__(self, *args, **kwargs):
        tk.Tk.__init__(self, *args, **kwargs)

        self.geometry(&quot;700x400&quot;)
        self.wm_iconbitmap('test.ico')

        container = tk.Frame(self)

        container.pack(side=&quot;top&quot;, fill=&quot;both&quot;, expand = True)

        container.grid_rowconfigure(0, weight=1)
        container.grid_columnconfigure(0, weight=1)

        self.frames = {}

        for F in (StartPage, PageOne):

            frame = F(container, self)
            self.frames[F] = frame
            frame.grid(row=0, column=0, sticky=&quot;nsew&quot;)

        self.show_frame(StartPage)

    def show_frame(self, cont):
        frame = self.frames[cont]
        frame.tkraise()        
        frame.update_page() # &lt;-- update data on page when you click button

    def get_page(self, page_class):
        return self.frames[page_class]


class StartPage(tk.Frame):

    def __init__(self, parent, controller):
        tk.Frame.__init__(self, parent)
        self.controller = controller 

        label1 = tk.Label(self, text=&quot;What are the sizes?&quot;)
        label1.pack()

        L1 = tk.Label(self, text=&quot;Length :&quot;)
        L1.pack()

        self.E1 = tk.Entry(self)
        self.E1.pack()

        button = tk.Button(self, text=&quot;Next&quot;, command=lambda: controller.show_frame(PageOne))
        button.pack()

    def update_page(self): # empty method but I need it
        pass   

class PageOne(tk.Frame):

    def __init__(self, parent, controller):
        tk.Frame.__init__(self, parent)
        self.controller = controller

        label1 = tk.Label(self, text=&quot;You have insert&quot;)
        label1.pack()

        # create empty label at start
        self.label2 = tk.Label(self, text=&quot;&quot;)
        self.label2.pack()

        button = tk.Button(self, text=&quot;Back&quot;, command=lambda: controller.show_frame(StartPage))
        button.pack()

    def update_page(self):
        # update label when page is changed
        page1 = self.controller.get_page(StartPage) 
        var = page1.E1.get()
        self.label2['text'] = var


app = Main()
app.mainloop() 
</code></pre>
<p>The second script:</p>
<pre><code>import cx_Freeze
import sys
import matplotlib 
import os 
import numpy.core._methods
import numpy.lib.format

base = None 

if sys.platform=='win32':
    base = &quot;Win32GUI&quot;
    

executables = [cx_Freeze.Executable(&quot;Show_file.py&quot;)]    

cx_Freeze.setup(
        name = &quot;Name&quot;,
        options = {
            &quot;build_exe&quot;: {
                &quot;packages&quot;: [&quot;tkinter&quot;,&quot;matplotlib&quot;],
                &quot;include_files&quot;: [&quot;test.ico&quot;]
            }
        },
        version=&quot;0.01&quot;,
        executables=executables) 
</code></pre>
<p>It works when I do not add an icon when I try to build the EXE file. However, the EXE does not open anymore when I try to add an icon. Furthermore, when I try to add a database Excel file, I get the message that such a file does not exist. All the files are in the correct folder. That is not the problem.</p>
",8899328,529,17-01-2018 10:56,18-01-2018 21:13,1,529,14,2,5,,"{'badge_counts': {'bronze': 14, 'silver': 5, 'gold': 2}, 'account_id': 12192309, 'is_employee': False, 'last_modified_date': 1711083600, 'last_access_date': 1685547621, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 529, 'creation_date': 1510048536, 'user_type': 'registered', 'user_id': 8899328, 'website_url': '', 'link': 'https://stackoverflow.com/users/8899328/beertje', 'profile_image': 'https://www.gravatar.com/avatar/ea20255478510c7578a3a6a0c802319e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Beertje'}","Currently, I'm trying to convert my tkinter Python script to an EXE file using cx_freeze. It is somehow not working when I try to add another file. You can see the method I've used in the minimum example I'm using below. The second script: It works when I do not add an icon when I try to build the EXE file. However, the EXE does not open anymore when I try to add an icon. Furthermore, when I try to add a database Excel file, I get the message that such a file does not exist. All the files are in the correct folder. That is not the problem.","import tkinter as tk

import numpy.core._methods, numpy.lib.format 

class Main(tk.Tk):

    def __init__(self, *args, **kwargs):
        tk.Tk.__init__(self, *args, **kwargs)

        self.geometry(&quot;700x400&quot;)
        self.wm_iconbitmap('test.ico')

        container = tk.Frame(self)

        container.pack(side=&quot;top&quot;, fill=&quot;both&quot;, expand = True)

        container.grid_rowconfigure(0, weight=1)
        container.grid_columnconfigure(0, weight=1)

        self.frames = {}

        for F in (StartPage, PageOne):

            frame = F(container, self)
            self.frames[F] = frame
            frame.grid(row=0, column=0, sticky=&quot;nsew&quot;)

        self.show_frame(StartPage)

    def show_frame(self, cont):
        frame = self.frames[cont]
        frame.tkraise()        
        frame.update_page() # &lt;-- update data on page when you click button

    def get_page(self, page_class):
        return self.frames[page_class]


class StartPage(tk.Frame):

    def __init__(self, parent, controller):
        tk.Frame.__init__(self, parent)
        self.controller = controller 

        label1 = tk.Label(self, text=&quot;What are the sizes?&quot;)
        label1.pack()

        L1 = tk.Label(self, text=&quot;Length :&quot;)
        L1.pack()

        self.E1 = tk.Entry(self)
        self.E1.pack()

        button = tk.Button(self, text=&quot;Next&quot;, command=lambda: controller.show_frame(PageOne))
        button.pack()

    def update_page(self): # empty method but I need it
        pass   

class PageOne(tk.Frame):

    def __init__(self, parent, controller):
        tk.Frame.__init__(self, parent)
        self.controller = controller

        label1 = tk.Label(self, text=&quot;You have insert&quot;)
        label1.pack()

        # create empty label at start
        self.label2 = tk.Label(self, text=&quot;&quot;)
        self.label2.pack()

        button = tk.Button(self, text=&quot;Back&quot;, command=lambda: controller.show_frame(StartPage))
        button.pack()

    def update_page(self):
        # update label when page is changed
        page1 = self.controller.get_page(StartPage) 
        var = page1.E1.get()
        self.label2['text'] = var


app = Main()
app.mainloop() 
 import cx_Freeze
import sys
import matplotlib 
import os 
import numpy.core._methods
import numpy.lib.format

base = None 

if sys.platform=='win32':
    base = &quot;Win32GUI&quot;
    

executables = [cx_Freeze.Executable(&quot;Show_file.py&quot;)]    

cx_Freeze.setup(
        name = &quot;Name&quot;,
        options = {
            &quot;build_exe&quot;: {
                &quot;packages&quot;: [&quot;tkinter&quot;,&quot;matplotlib&quot;],
                &quot;include_files&quot;: [&quot;test.ico&quot;]
            }
        },
        version=&quot;0.01&quot;,
        executables=executables) 
",107,114,0,0,
614,48252114,48252230,11798,"Equivalent of ""table"" of R in python",2,<python><r>,14,"<p>In <strong>R</strong> we can find the frequency of each item using <code>table</code>.
This is an example in <strong>R</strong>:</p>

<pre><code>x &lt;- c(1,1,1,1,2,2)
y &lt;- c(""a"",""a"",""b"",""a"",""a"",""b"")
table(x,y)
#   y
#x   a b
#  1 3 1
#  2 1 1
</code></pre>

<p>How can I implement it in python while x and y are as DataFrame?
I am totally new in Python and I searched a lot but I was unable to find my answer. I should mention that I read <a href=""https://stackoverflow.com/questions/25710875/python-equivalent-of-r-table"">this article</a> but I couldn't implement it in my case?</p>
",8899386,4072,14-01-2018 17:15,14-01-2018 17:27,0,4082,48,6,28,94,"{'badge_counts': {'bronze': 48, 'silver': 28, 'gold': 6}, 'account_id': 12192399, 'is_employee': False, 'last_modified_date': 1694346300, 'last_access_date': 1702650388, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4082, 'creation_date': 1510049215, 'user_type': 'registered', 'user_id': 8899386, 'accept_rate': 94, 'website_url': 'https://hadijahanshahi.com', 'link': 'https://stackoverflow.com/users/8899386/hadij', 'profile_image': 'https://i.stack.imgur.com/BpHdY.png?s=256&g=1', 'display_name': 'Hadij'}",In R we can find the frequency of each item using . This is an example in R: How can I implement it in python while x and y are as DataFrame? I am totally new in Python and I searched a lot but I was unable to find my answer. I should mention that I read this article but I couldn't implement it in my case?,"table x &lt;- c(1,1,1,1,2,2)
y &lt;- c(""a"",""a"",""b"",""a"",""a"",""b"")
table(x,y)
#   y
#x   a b
#  1 3 1
#  2 1 1
",5,14,0,1,
615,48519062,48519134,45685,R's which() and which.min() Equivalent in Python,6,<python><r><numpy>,36,"<p>I read the similar topic <a href=""https://stackoverflow.com/questions/25086060/pandas-equivalent-of-rs-which"">here</a>. I think the question is different or at least <code>.index()</code> could not solve my problem.</p>
<p>This is a simple code in R and its answer:</p>
<pre><code>x &lt;- c(1:4, 0:5, 11)
x
#[1]  1  2  3  4  0  1  2  3  4  5 11
which(x==2)
# [1] 2 7
min(which(x==2))
# [1] 2
which.min(x)
#[1] 5
</code></pre>
<p>Which simply returns the index of the item which meets the condition.</p>
<p>If <code>x</code> be the input for Python, how can I get the indeces for the elements which meet criteria <code>x==2</code> and the one which is the smallest in the array <code>which.min</code>.</p>
<pre><code>x = [1,2,3,4,0,1,2,3,4,11] 
x=np.array(x)
x[x&gt;2].index()
##'numpy.ndarray' object has no attribute 'index'
</code></pre>
",8899386,4072,30-01-2018 10:22,30-01-2018 10:26,0,4082,48,6,28,94,"{'badge_counts': {'bronze': 48, 'silver': 28, 'gold': 6}, 'account_id': 12192399, 'is_employee': False, 'last_modified_date': 1694346300, 'last_access_date': 1702650388, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4082, 'creation_date': 1510049215, 'user_type': 'registered', 'user_id': 8899386, 'accept_rate': 94, 'website_url': 'https://hadijahanshahi.com', 'link': 'https://stackoverflow.com/users/8899386/hadij', 'profile_image': 'https://i.stack.imgur.com/BpHdY.png?s=256&g=1', 'display_name': 'Hadij'}","I read the similar topic here. I think the question is different or at least could not solve my problem. This is a simple code in R and its answer: Which simply returns the index of the item which meets the condition. If be the input for Python, how can I get the indeces for the elements which meet criteria and the one which is the smallest in the array .",".index() x &lt;- c(1:4, 0:5, 11)
x
#[1]  1  2  3  4  0  1  2  3  4  5 11
which(x==2)
# [1] 2 7
min(which(x==2))
# [1] 2
which.min(x)
#[1] 5
 x x==2 which.min x = [1,2,3,4,0,1,2,3,4,11] 
x=np.array(x)
x[x&gt;2].index()
##'numpy.ndarray' object has no attribute 'index'
",7,19,0,1,
616,48420759,48427329,59541,Upload local files using Google Colab,6,<python><jupyter-notebook><google-colaboratory>,31,"<p>Trying to upload local files using Google Collaboratory this way:</p>

<pre><code>from google.colab import files
uploaded = files.upload()
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.</p>
</blockquote>

<p>I rerun the cell but it doesn't work...</p>
",9261603,313,24-01-2018 10:56,24-01-2018 16:31,0,313,5,1,3,,"{'badge_counts': {'bronze': 5, 'silver': 3, 'gold': 1}, 'account_id': 12799972, 'is_employee': False, 'last_modified_date': 1673514986, 'last_access_date': 1711108574, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 313, 'creation_date': 1516790818, 'user_type': 'registered', 'user_id': 9261603, 'location': 'Santiago de Compostela, Spain', 'website_url': '', 'link': 'https://stackoverflow.com/users/9261603/alexandre2r', 'profile_image': 'https://lh6.googleusercontent.com/-XHermJRnUsE/AAAAAAAAAAI/AAAAAAAAA6M/H9kk8OLIvK4/photo.jpg?sz=256', 'display_name': 'Alexandre2R'}",Trying to upload local files using Google Collaboratory this way: I get the following error: Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. I rerun the cell but it doesn't work...,"from google.colab import files
uploaded = files.upload()
",1,13,0,0,
617,49229610,49229702,16498,Difference between numpy.round and numpy.around,3,<python><arrays><numpy><rounding>,18,"<p>So, I was searching for ways to round off all the numbers in a numpy array. I found 2 similar functions, numpy.round and numpy.around. Both take seemingly same arguments for a beginner like me. </p>

<p>So what is the difference between these two in terms of:</p>

<ul>
<li>General difference</li>
<li>Speed</li>
<li>Accuracy</li>
<li>Being used in practice</li>
</ul>
",8560127,913,12-03-2018 06:55,12-03-2018 07:01,0,913,25,4,10,75,"{'badge_counts': {'bronze': 25, 'silver': 10, 'gold': 4}, 'account_id': 11692215, 'is_employee': False, 'last_modified_date': 1688471700, 'last_access_date': 1711044455, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 913, 'creation_date': 1504550673, 'user_type': 'registered', 'user_id': 8560127, 'accept_rate': 75, 'location': 'India', 'link': 'https://stackoverflow.com/users/8560127/duttaa', 'profile_image': 'https://i.stack.imgur.com/4JZNQ.jpg?s=256&g=1', 'display_name': 'DuttaA'}","So, I was searching for ways to round off all the numbers in a numpy array. I found 2 similar functions, numpy.round and numpy.around. Both take seemingly same arguments for a beginner like me. So what is the difference between these two in terms of: General difference Speed Accuracy Being used in practice",,0,10,0,0,
618,48898319,48898363,45795,"How to find the indices of items in a list, which are present in another list?",3,<python><list>,26,"<p>I want to essentially use one list ie.</p>
<pre><code>L = [10, 10, 100, 10, 17, 15]
</code></pre>
<p>and using another list</p>
<pre><code>R = [10, 15] 
</code></pre>
<p>want to return</p>
<pre><code>N = [0, 1, 3, 5] // indices of L that return the values in R
</code></pre>
<p>I tried using <code>L.index()</code> to get the indices but that only returns the first value. I then tried running a for loop over <code>L</code> and using <code>L.index(R[0])</code> every time, but similarly that only returns the first indices it finds at.</p>
<pre><code> for i in range(len(L)):
       j = R[i]
       N.append(L.index(j))
 return N
</code></pre>
<p>This would return index out of range which makes sense, but how do I get it to run through the <code>L</code>?</p>
",8606331,430,21-02-2018 04:44,21-02-2018 04:49,0,430,16,1,6,100,"{'badge_counts': {'bronze': 16, 'silver': 6, 'gold': 1}, 'account_id': 4801901, 'is_employee': False, 'last_modified_date': 1620814771, 'last_access_date': 1670179707, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 430, 'creation_date': 1505352266, 'user_type': 'registered', 'user_id': 8606331, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/8606331/andre-fu', 'profile_image': 'https://www.gravatar.com/avatar/e019a29e8801614d0df206307921451d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Andre Fu'}","I want to essentially use one list ie. and using another list want to return I tried using to get the indices but that only returns the first value. I then tried running a for loop over and using every time, but similarly that only returns the first indices it finds at. This would return index out of range which makes sense, but how do I get it to run through the ?","L = [10, 10, 100, 10, 17, 15]
 R = [10, 15] 
 N = [0, 1, 3, 5] // indices of L that return the values in R
 L.index() L L.index(R[0])  for i in range(len(L)):
       j = R[i]
       N.append(L.index(j))
 return N
 L",-1,16,0,0,
619,48612603,48613050,85192,Permission System for Discord.py Bot,4,<python><python-3.x><python-asyncio><discord><discord.py>,13,"<p>I am in the process of making a discord bot using discord.py and asyncio. The bot has commands like <code>kick</code> and <code>ban</code> which obviously should not be available to normal users.</p>

<p>I want to make a simple system which will detect what permissions the user's role has using <code>ctx.message.author</code> to get the user who sent the command.</p>

<p>I do not want the bot to detect a specific role name as these vary across servers. I also prefer not to have multiple files for the bot to keep it simple.</p>

<p>I have seen the discord.py documentation and various other sources but none contain examples of how to implement the various methods they talk about.</p>

<p>As an example, here is a single command from my bot:</p>

<pre><code>async def kick(ctx, userName: discord.User):
    if True: #ctx.message.author.Permissions.administrator
        await BSL.kick(userName)
    else:
        permission_error = str('Sorry ' + ctx.message.author + ' you do not have permissions to do that!')
        await BSL.send_message(ctx.message.channel, permission_error)
</code></pre>

<p>Where the <code>if else</code> statement is my attempt of doing this on my own. The <code>#ctx.message.author.Permissions.administrator</code> is commented out as it does not work and replaced with <code>True</code> for testing purposes.</p>

<p>Thank you for any help and suggestions in advance.</p>
",8623347,571,04-02-2018 20:18,04-02-2018 21:10,0,571,22,1,11,71,"{'badge_counts': {'bronze': 22, 'silver': 11, 'gold': 1}, 'account_id': 11785416, 'is_employee': False, 'last_modified_date': 1698761703, 'last_access_date': 1711166494, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 571, 'creation_date': 1505674280, 'user_type': 'registered', 'user_id': 8623347, 'accept_rate': 71, 'website_url': '', 'link': 'https://stackoverflow.com/users/8623347/user9123', 'profile_image': 'https://i.stack.imgur.com/QG7yn.jpg?s=256&g=1', 'display_name': 'user9123'}","I am in the process of making a discord bot using discord.py and asyncio. The bot has commands like and which obviously should not be available to normal users. I want to make a simple system which will detect what permissions the user's role has using to get the user who sent the command. I do not want the bot to detect a specific role name as these vary across servers. I also prefer not to have multiple files for the bot to keep it simple. I have seen the discord.py documentation and various other sources but none contain examples of how to implement the various methods they talk about. As an example, here is a single command from my bot: Where the statement is my attempt of doing this on my own. The is commented out as it does not work and replaced with for testing purposes. Thank you for any help and suggestions in advance.","kick ban ctx.message.author async def kick(ctx, userName: discord.User):
    if True: #ctx.message.author.Permissions.administrator
        await BSL.kick(userName)
    else:
        permission_error = str('Sorry ' + ctx.message.author + ' you do not have permissions to do that!')
        await BSL.send_message(ctx.message.channel, permission_error)
 if else #ctx.message.author.Permissions.administrator True",-1,21,0,0,
620,49121365,49121508,29339,Implementing retry for requests in Python,2,<python><python-3.x><python-requests>,24,"<p>How do I implement a retry count of 5 times, 10 seconds apart when sending a <code>POST</code> request using the <code>requests</code> package.
I have found plenty of examples for <code>GET</code> requests, just not <code>post</code>.</p>

<p>This is what I am working with at the moment, sometimes I get a 503 error. I just need to implement a retry if I get a bad response HTTP code.</p>

<pre><code>for x in final_payload:
    post_response = requests.post(url=endpoint, data=json.dumps(x), headers=headers)

#Email me the error
if str(post_response.status_code) not in [""201"",""200""]:
        email(str(post_response.status_code))
</code></pre>
",8624402,457,05-03-2018 23:56,06-03-2018 00:14,1,457,20,1,6,89,"{'badge_counts': {'bronze': 20, 'silver': 6, 'gold': 1}, 'account_id': 11787116, 'is_employee': False, 'last_modified_date': 1683339300, 'last_access_date': 1711137771, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 457, 'creation_date': 1505702932, 'user_type': 'registered', 'user_id': 8624402, 'accept_rate': 89, 'location': 'New Zealand', 'link': 'https://stackoverflow.com/users/8624402/phil-baines', 'profile_image': 'https://www.gravatar.com/avatar/b9662d23c13b38d5f392faf89254ba23?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Phil Baines'}","How do I implement a retry count of 5 times, 10 seconds apart when sending a request using the package. I have found plenty of examples for requests, just not . This is what I am working with at the moment, sometimes I get a 503 error. I just need to implement a retry if I get a bad response HTTP code.","POST requests GET post for x in final_payload:
    post_response = requests.post(url=endpoint, data=json.dumps(x), headers=headers)

#Email me the error
if str(post_response.status_code) not in [""201"",""200""]:
        email(str(post_response.status_code))
",1,12,0,0,
621,49868647,49868658,78848,How to slice a Pandas Dataframe based on datetime index,3,<python><pandas><slice>,36,"<p>This has been bothering me for ages now:</p>

<p>Given a simple pandas DataFrame</p>

<pre><code>&gt;&gt;&gt; df

Timestamp     Col1
2008-08-01    0.001373
2008-09-01    0.040192
2008-10-01    0.027794
2008-11-01    0.012590
2008-12-01    0.026394
2009-01-01    0.008564
2009-02-01    0.007714
2009-03-01   -0.019727
2009-04-01    0.008888
2009-05-01    0.039801
2009-06-01    0.010042
2009-07-01    0.020971
2009-08-01    0.011926
2009-09-01    0.024998
2009-10-01    0.005213
2009-11-01    0.016804
2009-12-01    0.020724
2010-01-01    0.006322
2010-02-01    0.008971
2010-03-01    0.003911
2010-04-01    0.013928
2010-05-01    0.004640
2010-06-01    0.000744
2010-07-01    0.004697
2010-08-01    0.002553
2010-09-01    0.002770
2010-10-01    0.002834
2010-11-01    0.002157
2010-12-01    0.001034
</code></pre>

<p>How do I separate it so that a new DataFrame equals the entries in df for the dates between <code>2009-05-01</code> and <code>2010-03-01</code></p>

<pre><code>&gt;&gt;&gt; df2

Timestamp     Col1
2009-05-01    0.039801
2009-06-01    0.010042
2009-07-01    0.020971
2009-08-01    0.011926
2009-09-01    0.024998
2009-10-01    0.005213
2009-11-01    0.016804
2009-12-01    0.020724
2010-01-01    0.006322
2010-02-01    0.008971
2010-03-01    0.003911
</code></pre>
",8629348,462,17-04-2018 01:49,17-04-2018 01:51,0,462,6,1,4,,"{'badge_counts': {'bronze': 6, 'silver': 4, 'gold': 1}, 'account_id': 11794265, 'is_employee': False, 'last_modified_date': 1601858652, 'last_access_date': 1711097870, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 462, 'creation_date': 1505777038, 'user_type': 'registered', 'user_id': 8629348, 'website_url': '', 'link': 'https://stackoverflow.com/users/8629348/0000', 'profile_image': 'https://www.gravatar.com/avatar/60d53434249b54fc7af5d31e3b4ef146?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': '0000'}",This has been bothering me for ages now: Given a simple pandas DataFrame How do I separate it so that a new DataFrame equals the entries in df for the dates between and,"&gt;&gt;&gt; df

Timestamp     Col1
2008-08-01    0.001373
2008-09-01    0.040192
2008-10-01    0.027794
2008-11-01    0.012590
2008-12-01    0.026394
2009-01-01    0.008564
2009-02-01    0.007714
2009-03-01   -0.019727
2009-04-01    0.008888
2009-05-01    0.039801
2009-06-01    0.010042
2009-07-01    0.020971
2009-08-01    0.011926
2009-09-01    0.024998
2009-10-01    0.005213
2009-11-01    0.016804
2009-12-01    0.020724
2010-01-01    0.006322
2010-02-01    0.008971
2010-03-01    0.003911
2010-04-01    0.013928
2010-05-01    0.004640
2010-06-01    0.000744
2010-07-01    0.004697
2010-08-01    0.002553
2010-09-01    0.002770
2010-10-01    0.002834
2010-11-01    0.002157
2010-12-01    0.001034
 2009-05-01 2010-03-01 &gt;&gt;&gt; df2

Timestamp     Col1
2009-05-01    0.039801
2009-06-01    0.010042
2009-07-01    0.020971
2009-08-01    0.011926
2009-09-01    0.024998
2009-10-01    0.005213
2009-11-01    0.016804
2009-12-01    0.020724
2010-01-01    0.006322
2010-02-01    0.008971
2010-03-01    0.003911
",42,55,0,0,
622,49179356,49179608,8990,Pybind11: Create and return numpy array from C++ side,1,<python><c++><pybind11>,11,"<p>How to create a numpy array from C++ side and give that to python?</p>

<p>I want Python to do the clean up when the returned array is no longer used by Python.</p>

<p>C++ side would not use <code>delete ret;</code> to free the memory allocated by <code>new double[size];</code>.</p>

<p>Is the following correct?</p>

<pre><code>#include ""pybind11/pybind11.h""
#include ""pybind11/numpy.h""

namespace py = pybind11;

py::array_t&lt;double&gt; make_array(const py::ssize_t size) {
    double* ret = new double[size];
    return py::array(size, ret);
}

PYBIND11_MODULE(my_module, m) {
    .def(""make_array"", &amp;make_array,
         py::return_value_policy::take_ownership);
}
</code></pre>
",8704463,2044,08-03-2018 17:45,08-03-2018 18:00,0,2044,30,0,13,93,"{'badge_counts': {'bronze': 30, 'silver': 13, 'gold': 0}, 'account_id': 11894685, 'is_employee': False, 'last_modified_date': 1654537500, 'last_access_date': 1571327572, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2044, 'creation_date': 1506872885, 'user_type': 'registered', 'user_id': 8704463, 'accept_rate': 93, 'link': 'https://stackoverflow.com/users/8704463/r-zu', 'profile_image': 'https://www.gravatar.com/avatar/ff85ea80675b0863e02820b4d7a6753f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'R zu'}",How to create a numpy array from C++ side and give that to python? I want Python to do the clean up when the returned array is no longer used by Python. C++ side would not use to free the memory allocated by . Is the following correct?,"delete ret; new double[size]; #include ""pybind11/pybind11.h""
#include ""pybind11/numpy.h""

namespace py = pybind11;

py::array_t&lt;double&gt; make_array(const py::ssize_t size) {
    double* ret = new double[size];
    return py::array(size, ret);
}

PYBIND11_MODULE(my_module, m) {
    .def(""make_array"", &amp;make_array,
         py::return_value_policy::take_ownership);
}
",11,23,0,0,
623,48999379,48999881,51001,"psycopg2.OperationalError: FATAL: password authentication failed for user ""<my UNIX user>""",8,<python><django><postgresql><ubuntu-16.04>,15,"<p>I am a fairly new to web developement.</p>

<p>First I deployed a static website on my vps (Ubuntu 16.04) without problem and then I tried to add a blog app to it.</p>

<p>It works well locally with PostgreSQL but I can't make it work on my server.
It seems like it tries to connect to Postgres with my Unix user. </p>

<p>Why would my server try to do that?</p>

<p>I did create a database and a owner via the postgres user, matching the login information in settings.py, I was expecting psycopg2 to try to connect to the database using these login informations:</p>

<p>Settings.py + python-decouple:</p>

<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': config ('NAME'),
        'USER': config ('USER'),
        'PASSWORD': config ('PASSWORD'),
        'HOST': 'localhost',
        'PORT': '',
    }
}
</code></pre>

<p>This is the error message I get each time I try to <code>./manage.py migrate</code></p>

<p>'myportfolio' is my Unix user name, the database username is different:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""./manage.py"", line 15, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 371, in execute_from_command_line
    utility.execute()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 365, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 288, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 335, in execute
    output = self.handle(*args, **options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/commands/migrate.py"", line 79, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/executor.py"", line 18, in __init__
    self.loader = MigrationLoader(self.connection)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 49, in __init__
    self.build_graph()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 206, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 61, in applied_migrations
    if self.has_table():
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 44, in has_table
    return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 255, in cursor
    return self._cursor()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 232, in _cursor
    self.ensure_connection()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
django.db.utils.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
</code></pre>

<p>I tried to: </p>

<ul>
<li>delete my django code, re install </li>
<li>delete/purge postgres and reinstall</li>
<li>modify pg_hba.conf local to trust</li>
</ul>

<p>At one point I did create a django superuser called 'myportfolio' as my unix user: could this have create a problem ?</p>
",8045609,1580,27-02-2018 00:41,27-02-2018 01:47,0,1580,27,4,18,62,"{'badge_counts': {'bronze': 27, 'silver': 18, 'gold': 4}, 'account_id': 10948464, 'is_employee': False, 'last_modified_date': 1666690500, 'last_access_date': 1710539011, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1580, 'creation_date': 1495414547, 'user_type': 'registered', 'user_id': 8045609, 'accept_rate': 62, 'location': 'San Diego, CA, USA', 'website_url': 'https://yannickleroux.com', 'link': 'https://stackoverflow.com/users/8045609/yannick', 'profile_image': 'https://i.stack.imgur.com/6thrF.jpg?s=256&g=1', 'display_name': 'Yannick'}","I am a fairly new to web developement. First I deployed a static website on my vps (Ubuntu 16.04) without problem and then I tried to add a blog app to it. It works well locally with PostgreSQL but I can't make it work on my server. It seems like it tries to connect to Postgres with my Unix user. Why would my server try to do that? I did create a database and a owner via the postgres user, matching the login information in settings.py, I was expecting psycopg2 to try to connect to the database using these login informations: Settings.py + python-decouple: This is the error message I get each time I try to 'myportfolio' is my Unix user name, the database username is different: I tried to: delete my django code, re install delete/purge postgres and reinstall modify pg_hba.conf local to trust At one point I did create a django superuser called 'myportfolio' as my unix user: could this have create a problem ?","DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': config ('NAME'),
        'USER': config ('USER'),
        'PASSWORD': config ('PASSWORD'),
        'HOST': 'localhost',
        'PORT': '',
    }
}
 ./manage.py migrate Traceback (most recent call last):
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""./manage.py"", line 15, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 371, in execute_from_command_line
    utility.execute()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/__init__.py"", line 365, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 288, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/base.py"", line 335, in execute
    output = self.handle(*args, **options)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/core/management/commands/migrate.py"", line 79, in handle
    executor = MigrationExecutor(connection, self.migration_progress_callback)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/executor.py"", line 18, in __init__
    self.loader = MigrationLoader(self.connection)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 49, in __init__
    self.build_graph()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/loader.py"", line 206, in build_graph
    self.applied_migrations = recorder.applied_migrations()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 61, in applied_migrations
    if self.has_table():
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/migrations/recorder.py"", line 44, in has_table
    return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 255, in cursor
    return self._cursor()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 232, in _cursor
    self.ensure_connection()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/utils.py"", line 89, in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 216, in ensure_connection
    self.connect()
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/base/base.py"", line 194, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/django/db/backends/postgresql/base.py"", line 168, in get_new_connection
    connection = Database.connect(**conn_params)
  File ""/home/myportfolio/lib/python3.5/site-packages/psycopg2/__init__.py"", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
django.db.utils.OperationalError: FATAL:  password authentication failed for user ""myportfolio""
FATAL:  password authentication failed for user ""myportfolio""
",63,96,0,0,
624,48171611,48171935,23996,difference between pandas read sql query and read sql table,4,<python><sql><pandas><dataframe><sqlite>,16,"<p>Is there a difference in relation to time execution between this two commands :</p>

<pre><code>import pandas as pd

df=pd.read_sql_query('SELECT * FROM TABLE',conn)
df=pd.read_sql_table(TABLE, conn)
</code></pre>

<p>Thank you for your help </p>
",8098361,684,09-01-2018 15:33,09-01-2018 15:49,0,684,18,1,7,80,"{'badge_counts': {'bronze': 18, 'silver': 7, 'gold': 1}, 'account_id': 11025259, 'is_employee': False, 'last_modified_date': 1645235100, 'last_access_date': 1701794098, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 684, 'creation_date': 1496327786, 'user_type': 'registered', 'user_id': 8098361, 'accept_rate': 80, 'website_url': '', 'link': 'https://stackoverflow.com/users/8098361/oussama-jabri', 'profile_image': 'https://www.gravatar.com/avatar/9a552920e856528d7a6b6da6700bc871?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Oussama Jabri'}",Is there a difference in relation to time execution between this two commands : Thank you for your help,"import pandas as pd

df=pd.read_sql_query('SELECT * FROM TABLE',conn)
df=pd.read_sql_table(TABLE, conn)
",3,9,0,0,
625,48272437,48300441,13680,Combining two heat maps in seaborn,3,<python><matplotlib><plot><graph><seaborn>,12,"<p>I have 2 data tables with the dimensions <code>4x25</code>. Each table is from a different point in time, but has exactly the same meta data, in essence the same column and row headers.</p>
<p>Given the large number of columns, I thought it best to represent this using a <code>heatmap</code> using the <code>seaborn</code> library for <code>Python</code>. However, I need to include both tables in the same plot. I am able to create a single heatmap representing a single data table as so.</p>
<pre><code>df = pd.DataFrame(raw_data)
ax = sns.heatmap(df)
ax.set(yticklabels=labels)
</code></pre>
<p>However, I'm not sure how to combine two data tables into the same heatmap. The only way I can think of is to just create a new <code>DataFrame</code> of dimension <code>4x50</code> and then fit both tables into that one and plot that using the heatmap. But then, I need help with the following issues:</p>
<ol>
<li>I'm not sure how I'd draw a line down the middle of the heatmap to differentiate the data from the 2 tables.</li>
<li>Another solution is to apply 2 different coloring schemes for the 2 sets of data <em>within</em> the same heatmap instead of simply just drawing a line down the middle.</li>
</ol>
",8100895,1906,16-01-2018 00:12,17-01-2018 11:48,1,1906,57,3,21,92,"{'badge_counts': {'bronze': 57, 'silver': 21, 'gold': 3}, 'account_id': 3119830, 'is_employee': False, 'last_modified_date': 1703009404, 'last_access_date': 1711135615, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1906, 'creation_date': 1496372638, 'user_type': 'registered', 'user_id': 8100895, 'accept_rate': 92, 'location': 'SD', 'link': 'https://stackoverflow.com/users/8100895/jonathan', 'profile_image': 'https://www.gravatar.com/avatar/0ff5f65bb0e493bb351bf33ed2bd6ef0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jonathan'}","I have 2 data tables with the dimensions . Each table is from a different point in time, but has exactly the same meta data, in essence the same column and row headers. Given the large number of columns, I thought it best to represent this using a using the library for . However, I need to include both tables in the same plot. I am able to create a single heatmap representing a single data table as so. However, I'm not sure how to combine two data tables into the same heatmap. The only way I can think of is to just create a new of dimension and then fit both tables into that one and plot that using the heatmap. But then, I need help with the following issues: I'm not sure how I'd draw a line down the middle of the heatmap to differentiate the data from the 2 tables. Another solution is to apply 2 different coloring schemes for the 2 sets of data within the same heatmap instead of simply just drawing a line down the middle.","4x25 heatmap seaborn Python df = pd.DataFrame(raw_data)
ax = sns.heatmap(df)
ax.set(yticklabels=labels)
 DataFrame 4x50",-4,11,0,0,
626,48105448,48105794,245367,Python HTTP Server/Client: Remote end closed connection without response error,2,<python><error-handling><python-requests><httpserver>,46,"<p>I made simple HTTP Server using <code>BaseHTTPRequestHandler</code>. The problem is, that when I want to post some data using requests from client, I get <code>ConnectionError</code>. I did simple request from <code>requests</code> lib documentation. Also interesting thing is, that HTTP Server will receive data from client and print it to console. I don't understand how its possible.</p>

<p>Client:</p>

<pre><code>def post_data():
    """"""Client method""""""
    json_data = {
        'sender': 'User',
        'receiver': 'MY_SERVER',
        'message': 'Hello server! Sending some data.'}
    data_headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}
    data_payload = json.dumps(json_data)

    try:
        post = requests.post('http://localhost:8080/post', data=data_payload,
                             headers=data_headers)
        print(post.status_code)
    except ConnectionError as e:
        print(""CONNECTION ERROR: "")
        print(e)
</code></pre>

<p>HTTP Server:</p>

<pre><code>def do_POST(self):
    """"""Server method""""""
    self.send_response(200)
    print(""Receiving new data ..."")
    content_length = int(self.headers['Content-Length'])
    post_data = self.rfile.read(content_length)
    print(post_data)
</code></pre>

<p>Server result:</p>

<pre><code>C:\Users\mypc\Projects\PythonFiles\httpserver&gt;python server.py

Fri Jan  5 01:09:12 2018: HTTP Server started on port 8080.
127.0.0.1 - - [05/Jan/2018 01:09:21] ""POST /post HTTP/1.1"" 200 -
Receiving new data ...
b'{""sender"": ""User"", ""receiver"": ""MY_SERVER"", ""message"": ""Hello server! Sending some data.""}'
</code></pre>

<p>Client result:</p>

<pre><code>C:\Users\mypc\Projects\PythonFiles\httpserver&gt;python client.py
CONNECTION ERROR:
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))

C:\Users\mypc\Projects\PythonFiles\httpserver&gt;
</code></pre>

<p>Error without exception block:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 601, in urlopen
    chunked=chunked)
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 387, in _make_request
    six.raise_from(e, None)
  File ""&lt;string&gt;"", line 2, in raise_from
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 383, in _make_request
    httplib_response = conn.getresponse()
  File ""C:\Python36\lib\http\client.py"", line 1331, in getresponse
    response.begin()
  File ""C:\Python36\lib\http\client.py"", line 297, in begin
    version, status, reason = self._read_status()
  File ""C:\Python36\lib\http\client.py"", line 266, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\requests\adapters.py"", line 440, in send
    timeout=timeout
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""C:\Python36\lib\site-packages\urllib3\util\retry.py"", line 357, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""C:\Python36\lib\site-packages\urllib3\packages\six.py"", line 685, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 601, in urlopen
    chunked=chunked)
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 387, in _make_request
    six.raise_from(e, None)
  File ""&lt;string&gt;"", line 2, in raise_from
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 383, in _make_request
    httplib_response = conn.getresponse()
  File ""C:\Python36\lib\http\client.py"", line 1331, in getresponse
    response.begin()
  File ""C:\Python36\lib\http\client.py"", line 297, in begin
    version, status, reason = self._read_status()
  File ""C:\Python36\lib\http\client.py"", line 266, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""client.py"", line 137, in &lt;module&gt;
    start_parser()
  File ""client.py"", line 101, in start_parser
    send_requests(args.get, args.post)
  File ""client.py"", line 51, in send_requests
    post_data()
  File ""client.py"", line 129, in post_data
    headers=data_headers)
  File ""C:\Python36\lib\site-packages\requests\api.py"", line 112, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""C:\Python36\lib\site-packages\requests\api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""C:\Python36\lib\site-packages\requests\sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python36\lib\site-packages\requests\sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""C:\Python36\lib\site-packages\requests\adapters.py"", line 490, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))
</code></pre>
",9046350,2668,05-01-2018 00:14,05-01-2018 01:03,0,2678,90,8,45,,"{'badge_counts': {'bronze': 90, 'silver': 45, 'gold': 8}, 'account_id': 12417864, 'is_employee': False, 'last_modified_date': 1710552600, 'last_access_date': 1711107934, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2678, 'creation_date': 1512303148, 'user_type': 'registered', 'user_id': 9046350, 'website_url': '', 'link': 'https://stackoverflow.com/users/9046350/martin', 'profile_image': 'https://www.gravatar.com/avatar/9ebef8887fd7698c9e03db906ed03a43?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Martin'}","I made simple HTTP Server using . The problem is, that when I want to post some data using requests from client, I get . I did simple request from lib documentation. Also interesting thing is, that HTTP Server will receive data from client and print it to console. I don't understand how its possible. Client: HTTP Server: Server result: Client result: Error without exception block:","BaseHTTPRequestHandler ConnectionError requests def post_data():
    """"""Client method""""""
    json_data = {
        'sender': 'User',
        'receiver': 'MY_SERVER',
        'message': 'Hello server! Sending some data.'}
    data_headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}
    data_payload = json.dumps(json_data)

    try:
        post = requests.post('http://localhost:8080/post', data=data_payload,
                             headers=data_headers)
        print(post.status_code)
    except ConnectionError as e:
        print(""CONNECTION ERROR: "")
        print(e)
 def do_POST(self):
    """"""Server method""""""
    self.send_response(200)
    print(""Receiving new data ..."")
    content_length = int(self.headers['Content-Length'])
    post_data = self.rfile.read(content_length)
    print(post_data)
 C:\Users\mypc\Projects\PythonFiles\httpserver&gt;python server.py

Fri Jan  5 01:09:12 2018: HTTP Server started on port 8080.
127.0.0.1 - - [05/Jan/2018 01:09:21] ""POST /post HTTP/1.1"" 200 -
Receiving new data ...
b'{""sender"": ""User"", ""receiver"": ""MY_SERVER"", ""message"": ""Hello server! Sending some data.""}'
 C:\Users\mypc\Projects\PythonFiles\httpserver&gt;python client.py
CONNECTION ERROR:
('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))

C:\Users\mypc\Projects\PythonFiles\httpserver&gt;
 Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 601, in urlopen
    chunked=chunked)
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 387, in _make_request
    six.raise_from(e, None)
  File ""&lt;string&gt;"", line 2, in raise_from
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 383, in _make_request
    httplib_response = conn.getresponse()
  File ""C:\Python36\lib\http\client.py"", line 1331, in getresponse
    response.begin()
  File ""C:\Python36\lib\http\client.py"", line 297, in begin
    version, status, reason = self._read_status()
  File ""C:\Python36\lib\http\client.py"", line 266, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python36\lib\site-packages\requests\adapters.py"", line 440, in send
    timeout=timeout
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""C:\Python36\lib\site-packages\urllib3\util\retry.py"", line 357, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""C:\Python36\lib\site-packages\urllib3\packages\six.py"", line 685, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 601, in urlopen
    chunked=chunked)
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 387, in _make_request
    six.raise_from(e, None)
  File ""&lt;string&gt;"", line 2, in raise_from
  File ""C:\Python36\lib\site-packages\urllib3\connectionpool.py"", line 383, in _make_request
    httplib_response = conn.getresponse()
  File ""C:\Python36\lib\http\client.py"", line 1331, in getresponse
    response.begin()
  File ""C:\Python36\lib\http\client.py"", line 297, in begin
    version, status, reason = self._read_status()
  File ""C:\Python36\lib\http\client.py"", line 266, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""client.py"", line 137, in &lt;module&gt;
    start_parser()
  File ""client.py"", line 101, in start_parser
    send_requests(args.get, args.post)
  File ""client.py"", line 51, in send_requests
    post_data()
  File ""client.py"", line 129, in post_data
    headers=data_headers)
  File ""C:\Python36\lib\site-packages\requests\api.py"", line 112, in post
    return request('post', url, data=data, json=json, **kwargs)
  File ""C:\Python36\lib\site-packages\requests\api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""C:\Python36\lib\site-packages\requests\sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python36\lib\site-packages\requests\sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""C:\Python36\lib\site-packages\requests\adapters.py"", line 490, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))
",90,119,0,0,
627,48757836,48758098,36872,Compute Date out of Timestamp from Binance-API (Python),3,<python><api><date><timestamp>,23,"<p>I received the servertime from the Binance-API,I try to work with and it looks like this:</p>

<pre><code>{
  ""serverTime"": 1518440400000
}
</code></pre>

<p>The question is, how can I compute the date out of this stamp?</p>

<p>I tried</p>

<pre><code>import datetime

print(datetime.datetime.fromtimestamp(
       int(""1518308894652"")).strftime('%Y-%m-%d %H:%M:%S'))
</code></pre>

<p>But the date wasn´t valid.</p>

<p>Do you have ideas, or is it to specific?
Thank you!</p>
",9058122,414,13-02-2018 00:43,13-02-2018 01:16,0,414,14,1,3,,"{'badge_counts': {'bronze': 14, 'silver': 3, 'gold': 1}, 'account_id': 12438979, 'is_employee': False, 'last_modified_date': 1638274517, 'last_access_date': 1670485792, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 414, 'creation_date': 1512500829, 'user_type': 'registered', 'user_id': 9058122, 'link': 'https://stackoverflow.com/users/9058122/cosmonaut', 'profile_image': 'https://lh6.googleusercontent.com/-KQRRIucPorA/AAAAAAAAAAI/AAAAAAAAAIY/07RQEvYGty8/photo.jpg?sz=256', 'display_name': 'cosmonaut'}","I received the servertime from the Binance-API,I try to work with and it looks like this: The question is, how can I compute the date out of this stamp? I tried But the date wasn´t valid. Do you have ideas, or is it to specific? Thank you!","{
  ""serverTime"": 1518440400000
}
 import datetime

print(datetime.datetime.fromtimestamp(
       int(""1518308894652"")).strftime('%Y-%m-%d %H:%M:%S'))
",5,21,0,0,
628,48083405,48097717,77626,What are the differences between feather and parquet?,2,<python><pandas><parquet><feather><pyarrow>,204,"<p>Both are <strong>columnar (disk-)storage formats</strong> for use in data analysis systems. 
Both are integrated within <a href=""https://arrow.apache.org/"" rel=""noreferrer"">Apache Arrow</a> (<a href=""https://pypi.python.org/pypi/pyarrow"" rel=""noreferrer"">pyarrow</a> package for python) and are
designed to correspond with <a href=""https://arrow.apache.org/docs/python/index.html"" rel=""noreferrer"">Arrow</a> as a columnar in-memory analytics layer.</p>

<p>How do both formats differ?</p>

<p>Should you always prefer feather when working with pandas when possible?</p>

<p>What are the use cases where <a href=""https://arrow.apache.org/docs/python/ipc.html#feather-format"" rel=""noreferrer"">feather</a> is more suitable than <a href=""https://arrow.apache.org/docs/python/parquet.html"" rel=""noreferrer"">parquet</a> and the 
other way round?</p>

<hr>

<p>Appendix</p>

<p>I found some hints here <a href=""https://github.com/wesm/feather/issues/188"" rel=""noreferrer"">https://github.com/wesm/feather/issues/188</a>,
but given the young age of this project, it's possibly a bit out of date.</p>

<p>Not a serious speed test because I'm just dumping and loading a whole
Dataframe but to give you some impression if you never
heard of the formats before:</p>

<pre><code> # IPython    
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.feather as feather
import pyarrow.parquet as pq
import fastparquet as fp


df = pd.DataFrame({'one': [-1, np.nan, 2.5],
                   'two': ['foo', 'bar', 'baz'],
                   'three': [True, False, True]})

print(""pandas df to disk ####################################################"")
print('example_feather:')
%timeit feather.write_feather(df, 'example_feather')
# 2.62 ms ± 35.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
print('example_parquet:')
%timeit pq.write_table(pa.Table.from_pandas(df), 'example.parquet')
# 3.19 ms ± 51 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
print()

print(""for comparison:"")
print('example_pickle:')
%timeit df.to_pickle('example_pickle')
# 2.75 ms ± 18.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
print('example_fp_parquet:')
%timeit fp.write('example_fp_parquet', df)
# 7.06 ms ± 205 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
print('example_hdf:')
%timeit df.to_hdf('example_hdf', 'key_to_store', mode='w', table=True)
# 24.6 ms ± 4.45 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
print()

print(""pandas df from disk ##################################################"")
print('example_feather:')
%timeit feather.read_feather('example_feather')
# 969 µs ± 1.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
print('example_parquet:')
%timeit pq.read_table('example.parquet').to_pandas()
# 1.9 ms ± 5.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

print(""for comparison:"")
print('example_pickle:')
%timeit pd.read_pickle('example_pickle')
# 1.07 ms ± 6.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
print('example_fp_parquet:')
%timeit fp.ParquetFile('example_fp_parquet').to_pandas()
# 4.53 ms ± 260 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
print('example_hdf:')
%timeit pd.read_hdf('example_hdf')
# 10 ms ± 43.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# pandas version: 0.22.0
# fastparquet version: 0.1.3
# numpy version: 1.13.3
# pandas version: 0.22.0
# pyarrow version: 0.8.0
# sys.version: 3.6.3
# example Dataframe taken from https://arrow.apache.org/docs/python/parquet.html
</code></pre>
",9059420,20906,03-01-2018 18:48,04-01-2018 14:47,1,20906,68,7,56,100,"{'badge_counts': {'bronze': 68, 'silver': 56, 'gold': 7}, 'account_id': 12441344, 'is_employee': False, 'last_modified_date': 1676318100, 'last_access_date': 1711154271, 'reputation_change_year': 270, 'reputation_change_quarter': 270, 'reputation_change_month': 50, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 20906, 'creation_date': 1512526231, 'user_type': 'registered', 'user_id': 9059420, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/9059420/darkonaut', 'profile_image': 'https://i.stack.imgur.com/qaO9g.jpg?s=256&g=1', 'display_name': 'Darkonaut'}","Both are columnar (disk-)storage formats for use in data analysis systems. Both are integrated within Apache Arrow (pyarrow package for python) and are designed to correspond with Arrow as a columnar in-memory analytics layer. How do both formats differ? Should you always prefer feather when working with pandas when possible? What are the use cases where feather is more suitable than parquet and the other way round? Appendix I found some hints here https://github.com/wesm/feather/issues/188, but given the young age of this project, it's possibly a bit out of date. Not a serious speed test because I'm just dumping and loading a whole Dataframe but to give you some impression if you never heard of the formats before:"," # IPython    
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.feather as feather
import pyarrow.parquet as pq
import fastparquet as fp


df = pd.DataFrame({'one': [-1, np.nan, 2.5],
                   'two': ['foo', 'bar', 'baz'],
                   'three': [True, False, True]})

print(""pandas df to disk ####################################################"")
print('example_feather:')
%timeit feather.write_feather(df, 'example_feather')
# 2.62 ms ± 35.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
print('example_parquet:')
%timeit pq.write_table(pa.Table.from_pandas(df), 'example.parquet')
# 3.19 ms ± 51 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
print()

print(""for comparison:"")
print('example_pickle:')
%timeit df.to_pickle('example_pickle')
# 2.75 ms ± 18.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
print('example_fp_parquet:')
%timeit fp.write('example_fp_parquet', df)
# 7.06 ms ± 205 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
print('example_hdf:')
%timeit df.to_hdf('example_hdf', 'key_to_store', mode='w', table=True)
# 24.6 ms ± 4.45 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)
print()

print(""pandas df from disk ##################################################"")
print('example_feather:')
%timeit feather.read_feather('example_feather')
# 969 µs ± 1.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
print('example_parquet:')
%timeit pq.read_table('example.parquet').to_pandas()
# 1.9 ms ± 5.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

print(""for comparison:"")
print('example_pickle:')
%timeit pd.read_pickle('example_pickle')
# 1.07 ms ± 6.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
print('example_fp_parquet:')
%timeit fp.ParquetFile('example_fp_parquet').to_pandas()
# 4.53 ms ± 260 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
print('example_hdf:')
%timeit pd.read_hdf('example_hdf')
# 10 ms ± 43.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

# pandas version: 0.22.0
# fastparquet version: 0.1.3
# numpy version: 1.13.3
# pandas version: 0.22.0
# pyarrow version: 0.8.0
# sys.version: 3.6.3
# example Dataframe taken from https://arrow.apache.org/docs/python/parquet.html
",59,83,0,6,
629,48554693,48555361,11798,"Python - ""No module named my_module"" in terminal, but not in PyCharm",3,<python><pycharm>,14,"<p>I have a python script that i'm trying to run.
When i run it from within PyCharm it runs without a problem, but when i run it through the terminal using:</p>

<p><code>python my_script.py</code></p>

<p>i get:</p>

<pre><code>Traceback (most recent call last):
  File ""folder/folder/my_script.py"", line 4, in &lt;module&gt;
    from my_module import me1, me2, me3
ImportError: No module named my_module
</code></pre>

<p>What could be the problem?</p>
",9064103,193,01-02-2018 03:03,01-02-2018 04:28,0,193,12,1,3,100,"{'badge_counts': {'bronze': 12, 'silver': 3, 'gold': 1}, 'account_id': 12448724, 'is_employee': False, 'last_modified_date': 1573678451, 'last_access_date': 1532458805, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 193, 'creation_date': 1512594240, 'user_type': 'registered', 'user_id': 9064103, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/9064103/user9064103', 'profile_image': 'https://www.gravatar.com/avatar/0ff5a8c0dd7b897e42ce84f974ac803c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user9064103'}","I have a python script that i'm trying to run. When i run it from within PyCharm it runs without a problem, but when i run it through the terminal using: i get: What could be the problem?","python my_script.py Traceback (most recent call last):
  File ""folder/folder/my_script.py"", line 4, in &lt;module&gt;
    from my_module import me1, me2, me3
ImportError: No module named my_module
",2,14,0,0,
630,48163539,48165615,24019,How to read gif from url using opencv (python),1,<python><opencv>,14,"<p>I can read jpg file using cv2 as</p>

<pre><code>import cv2
import numpy as np
import urllib
url = r'http://www.mywebsite.com/abc.jpg'
req = urllib.request.urlopen(url)
arr = np.asarray(bytearray(req.read()), dtype=np.uint8)
img = cv2.imdecode(arr,-1)
cv2.imshow('abc',img)
</code></pre>

<p>However, when I do it with gif file, it returns an error:</p>

<pre><code>error: (-215) size.width&gt;0 &amp;&amp; size.height&gt;0 in function cv::imshow
</code></pre>

<p>How to solve this problem?</p>
",9087866,3831,09-01-2018 07:50,09-01-2018 10:00,0,3861,63,9,30,65,"{'badge_counts': {'bronze': 63, 'silver': 30, 'gold': 9}, 'account_id': 12484109, 'is_employee': False, 'last_modified_date': 1658543400, 'last_access_date': 1636628951, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 3861, 'creation_date': 1513070855, 'user_type': 'registered', 'user_id': 9087866, 'accept_rate': 65, 'location': 'Hong Kong', 'link': 'https://stackoverflow.com/users/9087866/chan', 'profile_image': 'https://www.gravatar.com/avatar/2ac834ffde798a6efb617a1b6bf51faf?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Chan'}","I can read jpg file using cv2 as However, when I do it with gif file, it returns an error: How to solve this problem?","import cv2
import numpy as np
import urllib
url = r'http://www.mywebsite.com/abc.jpg'
req = urllib.request.urlopen(url)
arr = np.asarray(bytearray(req.read()), dtype=np.uint8)
img = cv2.imdecode(arr,-1)
cv2.imshow('abc',img)
 error: (-215) size.width&gt;0 &amp;&amp; size.height&gt;0 in function cv::imshow
",7,18,0,0,
631,49327296,49327656,40553,How to send bold text using Telegram Python bot,2,<python><bots><telegram>,17,"<p>I am writing a telegram bot in Python. I want to send messages with bold letters. I tried to inclose message inside both <code>*</code> and <code>**</code>, but it does not solve the problem.</p>
<p>Is there a function for mark up or HTML formatting or a way to do it?</p>
",9119822,365,16-03-2018 18:22,16-03-2018 18:47,0,365,11,1,2,,"{'badge_counts': {'bronze': 11, 'silver': 2, 'gold': 1}, 'account_id': 12531797, 'is_employee': False, 'last_modified_date': 1651779462, 'last_access_date': 1676302388, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 365, 'creation_date': 1513714262, 'user_type': 'registered', 'user_id': 9119822, 'website_url': '', 'link': 'https://stackoverflow.com/users/9119822/ramon-de-llano', 'profile_image': 'https://www.gravatar.com/avatar/dbd7d0a13a55b31df214d7be8c41aafc?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ramon de Llano'}","I am writing a telegram bot in Python. I want to send messages with bold letters. I tried to inclose message inside both and , but it does not solve the problem. Is there a function for mark up or HTML formatting or a way to do it?",* **,-2,2,0,0,
632,48245765,48245935,17008,Pass args for solve_ivp (new SciPy ODE API),6,<python><numpy><scipy><ode><odeint>,22,"<p>For solving simple ODEs using SciPy, I used to use the odeint function, with form:</p>

<pre><code>scipy.integrate.odeint(func, y0, t, args=(), Dfun=None, col_deriv=0, full_output=0, ml=None, mu=None, rtol=None, atol=None, tcrit=None, h0=0.0, hmax=0.0, hmin=0.0, ixpr=0, mxstep=0, mxhnil=0, mxordn=12, mxords=5, printmessg=0)[source]
</code></pre>

<p>where a simple function to be integrated could include additional arguments of the form:</p>

<pre><code>def dy_dt(t, y, arg1, arg2):
    # processing code here
</code></pre>

<p>In SciPy 1.0, it seems the <em>ode</em> and <em>odeint</em> funcs have been replaced by a newer <em>solve_ivp</em> method. </p>

<pre><code>scipy.integrate.solve_ivp(fun, t_span, y0, method='RK45', t_eval=None, dense_output=False, events=None, vectorized=False, **options)
</code></pre>

<p>However, this doesn't seem to offer an <em>args</em> parameter, nor any indication in the documentation as to implementing the passing of args.</p>

<p>Therefore, I wonder if arg passing is possible with the new API, or is this a feature that has yet to be added? (It would seem an oversight to me if this features has been intentionally removed?)</p>

<p>Reference:
<a href=""https://docs.scipy.org/doc/scipy/reference/integrate.html"" rel=""noreferrer"">https://docs.scipy.org/doc/scipy/reference/integrate.html</a></p>
",9136032,285,14-01-2018 00:53,14-01-2018 01:32,0,285,8,1,2,50,"{'badge_counts': {'bronze': 8, 'silver': 2, 'gold': 1}, 'account_id': 12556591, 'is_employee': False, 'last_modified_date': 1573678437, 'last_access_date': 1518697586, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 285, 'creation_date': 1514110405, 'user_type': 'registered', 'user_id': 9136032, 'accept_rate': 50, 'link': 'https://stackoverflow.com/users/9136032/sharkmas', 'profile_image': 'https://www.gravatar.com/avatar/5691c39e6dd2f8fc8bd128e0706a7249?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'sharkmas'}","For solving simple ODEs using SciPy, I used to use the odeint function, with form: where a simple function to be integrated could include additional arguments of the form: In SciPy 1.0, it seems the ode and odeint funcs have been replaced by a newer solve_ivp method. However, this doesn't seem to offer an args parameter, nor any indication in the documentation as to implementing the passing of args. Therefore, I wonder if arg passing is possible with the new API, or is this a feature that has yet to be added? (It would seem an oversight to me if this features has been intentionally removed?) Reference: https://docs.scipy.org/doc/scipy/reference/integrate.html","scipy.integrate.odeint(func, y0, t, args=(), Dfun=None, col_deriv=0, full_output=0, ml=None, mu=None, rtol=None, atol=None, tcrit=None, h0=0.0, hmax=0.0, hmin=0.0, ixpr=0, mxstep=0, mxhnil=0, mxordn=12, mxords=5, printmessg=0)[source]
 def dy_dt(t, y, arg1, arg2):
    # processing code here
 scipy.integrate.solve_ivp(fun, t_span, y0, method='RK45', t_eval=None, dense_output=False, events=None, vectorized=False, **options)
",1,22,0,1,
633,48126253,48126264,24818,How to check if a python varaible is of type pandas.core.series.Series,2,<python>,15,"<p>In Python 3.x, how can I check if my variable is of type <code>pandas.core.series.Series</code>?</p>
",9177379,443,06-01-2018 10:01,06-01-2018 10:03,0,443,20,3,12,,"{'badge_counts': {'bronze': 20, 'silver': 12, 'gold': 3}, 'account_id': 12622361, 'is_employee': False, 'last_modified_date': 1711097100, 'last_access_date': 1657540061, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 443, 'creation_date': 1515154091, 'user_type': 'registered', 'user_id': 9177379, 'link': 'https://stackoverflow.com/users/9177379/jdoe', 'profile_image': 'https://www.gravatar.com/avatar/193d1bbf4072a026c5599fa3d25e7f99?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'JDoe'}","In Python 3.x, how can I check if my variable is of type ?",pandas.core.series.Series,-1,1,0,0,
634,48620526,48630668,6688,How to use webdriver as context manager,2,<python><python-3.x><selenium><selenium-webdriver><web-scraping>,14,"<p>I'm trying to use <code>ChromeDriver</code> within with block to make the code look better and get rid of using <code>driver.quit()</code> command in the end. However, It doesn't seem to work. As soon as the browser opens, it throws the following error. Perhaps, I doing something wrong. Ain't there any way to do so? Thanks in advance.</p>

<p>This is what I've tried:</p>

<pre><code>from selenium import webdriver

with webdriver.Chrome() as wd:
    res = wd.get('https://stackoverflow.com/questions/')
    print(res.page_source)

#Another failure attempt with the same error

with webdriver.Chrome() as wd:
    wd.get('https://stackoverflow.com/questions/')
    print(wd.page_source)
</code></pre>

<p>This is the error I'm having:</p>

<pre><code>    with webdriver.Chrome() as wd:
AttributeError: __exit__
</code></pre>
",9189799,22227,05-02-2018 10:32,05-02-2018 20:09,0,22247,113,6,39,86,"{'badge_counts': {'bronze': 113, 'silver': 39, 'gold': 6}, 'account_id': 10394386, 'is_employee': False, 'last_modified_date': 1685006100, 'last_access_date': 1711179680, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 22247, 'creation_date': 1490527391, 'user_type': 'registered', 'user_id': 9189799, 'accept_rate': 86, 'location': 'Dhaka, Bangladesh', 'website_url': '', 'link': 'https://stackoverflow.com/users/9189799/sim', 'profile_image': 'https://i.stack.imgur.com/U2hEg.jpg?s=256&g=1', 'display_name': 'SIM'}","I'm trying to use within with block to make the code look better and get rid of using command in the end. However, It doesn't seem to work. As soon as the browser opens, it throws the following error. Perhaps, I doing something wrong. Ain't there any way to do so? Thanks in advance. This is what I've tried: This is the error I'm having:","ChromeDriver driver.quit() from selenium import webdriver

with webdriver.Chrome() as wd:
    res = wd.get('https://stackoverflow.com/questions/')
    print(res.page_source)

#Another failure attempt with the same error

with webdriver.Chrome() as wd:
    wd.get('https://stackoverflow.com/questions/')
    print(wd.page_source)
     with webdriver.Chrome() as wd:
AttributeError: __exit__
",9,22,0,0,
635,48648886,48649185,11758,How do I get the filename from a tempfile object?,1,<python><csv><temporary-files>,11,"<p>My goal is to create a temporary file that's written by csv.writer which will be used in an upload to a database. Once the upload completes, I want to delete the file. </p>

<p>The upload and deletion portions are not included here as I haven't fully built it out but I need to get past this part first.</p>

<pre><code>import csv, io, tempfile

filename = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)

file = io.StringIO(report_downloader.DownloadReportAsString(report, skip_report_header=False, skip_column_header=False, skip_report_summary=True))
reader = csv.reader(file)

with open(filename, 'w', encoding='utf8', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    for row in reader:
        writer.writerows([row])     
</code></pre>

<p>The resulting error from doing this is:</p>

<pre><code>Traceback (most recent call last):
File ""aw-ad-performance-tempfile.py"", line 99, in &lt;module&gt; get_api_report()
File ""aw-ad-performance-tempfile.py"", line 92, in get_api_report
with open(filename, 'w', encoding='utf8', newline='') as f:

OSError: [Errno 22] Invalid argument:'&lt;tempfile._TemporaryFileWrapper object at 0x00000274FBED0FD0&gt;'
</code></pre>

<p>I've tried a few different ways of addressing the problem and I've identified that it expects a string where filename is here:</p>

<pre><code>with open(filename, 'w', encoding='utf8', newline='') as f:
</code></pre>

<p>Is it possible to reference the temporary file as a string using the tempfile module so csv.writer recognizes it? What's the syntax to do so if it is?</p>
",9201029,147,06-02-2018 17:40,06-02-2018 17:58,0,147,8,1,1,,"{'badge_counts': {'bronze': 8, 'silver': 1, 'gold': 1}, 'account_id': 12659656, 'is_employee': False, 'last_modified_date': 1573678426, 'last_access_date': 1562178569, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 147, 'creation_date': 1515622643, 'user_type': 'registered', 'user_id': 9201029, 'website_url': '', 'link': 'https://stackoverflow.com/users/9201029/mnjt', 'profile_image': 'https://www.gravatar.com/avatar/667f41bd1678afa8c7a1d6f0c3be2515?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'mnjt'}","My goal is to create a temporary file that's written by csv.writer which will be used in an upload to a database. Once the upload completes, I want to delete the file. The upload and deletion portions are not included here as I haven't fully built it out but I need to get past this part first. The resulting error from doing this is: I've tried a few different ways of addressing the problem and I've identified that it expects a string where filename is here: Is it possible to reference the temporary file as a string using the tempfile module so csv.writer recognizes it? What's the syntax to do so if it is?","import csv, io, tempfile

filename = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)

file = io.StringIO(report_downloader.DownloadReportAsString(report, skip_report_header=False, skip_column_header=False, skip_report_summary=True))
reader = csv.reader(file)

with open(filename, 'w', encoding='utf8', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    for row in reader:
        writer.writerows([row])     
 Traceback (most recent call last):
File ""aw-ad-performance-tempfile.py"", line 99, in &lt;module&gt; get_api_report()
File ""aw-ad-performance-tempfile.py"", line 92, in get_api_report
with open(filename, 'w', encoding='utf8', newline='') as f:

OSError: [Errno 22] Invalid argument:'&lt;tempfile._TemporaryFileWrapper object at 0x00000274FBED0FD0&gt;'
 with open(filename, 'w', encoding='utf8', newline='') as f:
",15,33,0,0,
636,48750199,48803214,116091,Google Colaboratory: misleading information about its GPU (only 5% RAM available to some users),10,<python><machine-learning><gpu><ram><google-colaboratory>,133,"<p>update: this question is related to Google Colab's ""Notebook settings: Hardware accelerator: GPU"". This question was written before the ""TPU"" option was added.</p>

<p>Reading multiple excited announcements about Google Colaboratory providing free Tesla K80 GPU, I tried to run <a href=""http://course.fast.ai/"" rel=""noreferrer"">fast.ai</a> lesson on it for it to never complete - quickly running out of memory. I started investigating of why.</p>

<p>The bottom line is that “free Tesla K80” is not ""free"" for all - for some only a small slice of it is ""free"". </p>

<p>I connect to Google Colab from West Coast Canada and I get only 0.5GB of what supposed to be a 24GB GPU RAM. Other users get access to 11GB of GPU RAM.</p>

<p>Clearly 0.5GB GPU RAM is insufficient for most ML/DL work.</p>

<p>If you're not sure what you get, here is little debug function I scraped together (only works with the GPU setting of the notebook):</p>

<pre><code># memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isn’t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print(""Gen RAM Free: "" + humanize.naturalsize( psutil.virtual_memory().available ), "" | Proc size: "" + humanize.naturalsize( process.memory_info().rss))
 print(""GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB"".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()
</code></pre>

<p>Executing it in a jupyter notebook before running any other code gives me:</p>

<pre><code>Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB
GPU RAM Free: 566MB | Used: 10873MB | Util  95% | Total 11439MB
</code></pre>

<p>The lucky users who get access to the full card will see:</p>

<pre><code>Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB
GPU RAM Free: 11439MB | Used: 0MB | Util  0% | Total 11439MB
</code></pre>

<p>Do you see any flaw in my calculation of the GPU RAM availability, borrowed from GPUtil?</p>

<p>Can you confirm that you get similar results if you run this code on Google Colab notebook?</p>

<p>If my calculations are correct, is there any way to get more of that GPU RAM on the free box?</p>

<p>update: I'm not sure why some of us get 1/20th of what other users get. e.g. the person who helped me to debug this is from India and he gets the whole thing!</p>

<p><strong>note</strong>: please don't send any more suggestions on how to kill the potentially stuck/runaway/parallel notebooks that might be consuming parts of the GPU. No matter how you slice it, if you are in the same boat as I and were to run the debug code you'd see that you still get a total of 5% of GPU RAM (as of this update still).</p>
",9201239,5716,12-02-2018 15:44,15-02-2018 08:53,3,5746,50,5,38,,"{'badge_counts': {'bronze': 50, 'silver': 38, 'gold': 5}, 'account_id': 12482347, 'is_employee': False, 'last_modified_date': 1636804500, 'last_access_date': 1711158617, 'reputation_change_year': 170, 'reputation_change_quarter': 170, 'reputation_change_month': 40, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5746, 'creation_date': 1515627998, 'user_type': 'registered', 'user_id': 9201239, 'location': 'Victoria, BC, Canada', 'website_url': 'https://stasosphere.com/machine-learning/', 'link': 'https://stackoverflow.com/users/9201239/stason', 'profile_image': 'https://i.stack.imgur.com/weI7h.jpg?s=256&g=1', 'display_name': 'stason'}","update: this question is related to Google Colab's ""Notebook settings: Hardware accelerator: GPU"". This question was written before the ""TPU"" option was added. Reading multiple excited announcements about Google Colaboratory providing free Tesla K80 GPU, I tried to run fast.ai lesson on it for it to never complete - quickly running out of memory. I started investigating of why. The bottom line is that “free Tesla K80” is not ""free"" for all - for some only a small slice of it is ""free"". I connect to Google Colab from West Coast Canada and I get only 0.5GB of what supposed to be a 24GB GPU RAM. Other users get access to 11GB of GPU RAM. Clearly 0.5GB GPU RAM is insufficient for most ML/DL work. If you're not sure what you get, here is little debug function I scraped together (only works with the GPU setting of the notebook): Executing it in a jupyter notebook before running any other code gives me: The lucky users who get access to the full card will see: Do you see any flaw in my calculation of the GPU RAM availability, borrowed from GPUtil? Can you confirm that you get similar results if you run this code on Google Colab notebook? If my calculations are correct, is there any way to get more of that GPU RAM on the free box? update: I'm not sure why some of us get 1/20th of what other users get. e.g. the person who helped me to debug this is from India and he gets the whole thing! note: please don't send any more suggestions on how to kill the potentially stuck/runaway/parallel notebooks that might be consuming parts of the GPU. No matter how you slice it, if you are in the same boat as I and were to run the debug code you'd see that you still get a total of 5% of GPU RAM (as of this update still).","# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
import psutil
import humanize
import os
import GPUtil as GPU
GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isn’t guaranteed
gpu = GPUs[0]
def printm():
 process = psutil.Process(os.getpid())
 print(""Gen RAM Free: "" + humanize.naturalsize( psutil.virtual_memory().available ), "" | Proc size: "" + humanize.naturalsize( process.memory_info().rss))
 print(""GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB"".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()
 Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB
GPU RAM Free: 566MB | Used: 10873MB | Util  95% | Total 11439MB
 Gen RAM Free: 11.6 GB  | Proc size: 666.0 MB
GPU RAM Free: 11439MB | Used: 0MB | Util  0% | Total 11439MB
",18,52,0,1,
637,48311159,48313531,3225,Is 3-space indentation required in reST?,1,<python><python-sphinx><restructuredtext>,17,"<p>I'm documenting my Python code using Sphinx, and read <a href=""https://devguide.python.org/documenting/#style-guide"" rel=""noreferrer"">in the Python developer's guide</a> (and I think elsewhere as well) that reST files use an indentation of 3 spaces:</p>

<blockquote>
  <p>All reST files use an indentation of 3 spaces; no tabs are allowed.</p>
</blockquote>

<p>This is the case for the example I copied for my index file, and some other files where my IDE picked up the 3-space indentation and used it for the whole page. The <code>sphinx-apidoc</code> extension also uses 3 spaces for the <code>modules.rst</code> file it builds.</p>

<p>On the other hand, because Python uses 4-space indentation, all my docstrings are indented with 4 spaces. Moreover the <code>.. automodule::</code> directives generated by <code>sphinx-apidox</code> are indented with 4 spaces.</p>

<p>The point is, it all still <em>works</em>! So I'm left wondering whether the 3-space indentation thing is a requirement, or if it's good practice, but only in terms of style? (And if so, why, when all things Python are 4-space indented?)</p>

<p>Or are there cases where not having 3-space indentation will break my build?</p>

<h2>Other places I've looked</h2>

<ul>
<li>The <a href=""http://www.sphinx-doc.org/en/stable/rest.html#rst-primer"" rel=""noreferrer"">Sphinx reStructuredText Primer</a> doesn't mention a specific number of spaces, only:

<blockquote>
  <p>As in Python, indentation is significant in reST, so all lines of the same paragraph must be left-aligned to the same level of indentation.</p>
</blockquote></li>
<li>This <a href=""https://stackoverflow.com/questions/18876046/how-can-i-change-the-default-indentation-in-sphinx"">(unanswered) SO question</a>, which is about lists specifically, not spacing generally</li>
<li>The <a href=""http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#indentation"" rel=""noreferrer"">reStructuredText Markup Specification</a> only mentions 3 spaces in reference to <a href=""http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#footnotes"" rel=""noreferrer"">footnotes</a>.</li>
<li>This <a href=""https://github.com/benoitbryon/documentation-style-guide-sphinx/issues/9"" rel=""noreferrer"">issue on GitHub</a>, though I <em>think</em> this issue here is the mixture of indentation levels for different elements.</li>
</ul>

<hr>

<p>I'm beginning to think the Python developer's guide might be the anomaly, rather than everything else, especially since in all my searching I've come across basically no discussion of the ""3-or-4 space problem"" when working with Sphinx and Python.</p>
",9219425,1929,17-01-2018 22:38,18-01-2018 03:46,1,1929,18,0,13,,"{'badge_counts': {'bronze': 18, 'silver': 13, 'gold': 0}, 'account_id': 12688448, 'is_employee': False, 'last_modified_date': 1653705002, 'last_access_date': 1711126462, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1929, 'creation_date': 1516019845, 'user_type': 'registered', 'user_id': 9219425, 'website_url': '', 'link': 'https://stackoverflow.com/users/9219425/tim', 'profile_image': 'https://www.gravatar.com/avatar/5f98ebd2715e49b2133e461b4440527b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Tim'}","I'm documenting my Python code using Sphinx, and read in the Python developer's guide (and I think elsewhere as well) that reST files use an indentation of 3 spaces: All reST files use an indentation of 3 spaces; no tabs are allowed. This is the case for the example I copied for my index file, and some other files where my IDE picked up the 3-space indentation and used it for the whole page. The extension also uses 3 spaces for the file it builds. On the other hand, because Python uses 4-space indentation, all my docstrings are indented with 4 spaces. Moreover the directives generated by are indented with 4 spaces. The point is, it all still works! So I'm left wondering whether the 3-space indentation thing is a requirement, or if it's good practice, but only in terms of style? (And if so, why, when all things Python are 4-space indented?) Or are there cases where not having 3-space indentation will break my build? Other places I've looked The Sphinx reStructuredText Primer doesn't mention a specific number of spaces, only: As in Python, indentation is significant in reST, so all lines of the same paragraph must be left-aligned to the same level of indentation. This (unanswered) SO question, which is about lists specifically, not spacing generally The reStructuredText Markup Specification only mentions 3 spaces in reference to footnotes. This issue on GitHub, though I think this issue here is the mixture of indentation levels for different elements. I'm beginning to think the Python developer's guide might be the anomaly, rather than everything else, especially since in all my searching I've come across basically no discussion of the ""3-or-4 space problem"" when working with Sphinx and Python.",sphinx-apidoc modules.rst .. automodule:: sphinx-apidox,-4,30,0,6,
638,48635752,48635866,593,Dictionary in list,4,<python><python-3.x>,13,"<p>I want to print a dictionary inside a list like this :</p>

<pre><code>[{name : 'red', id : '1'}, {name : 'yellow', id : '2'}, {name : 'black', id : '3'}, {name : 'white', id : '4'}]`
</code></pre>

<p>I don't want quotations in <code>name</code> and <code>id</code>. However, I want them in the values portion of that dictionary.</p>
",9232907,173,06-02-2018 04:56,06-02-2018 05:09,0,173,6,0,0,,"{'badge_counts': {'bronze': 6, 'silver': 0, 'gold': 0}, 'account_id': 12708510, 'is_employee': False, 'last_modified_date': 1573678421, 'last_access_date': 1613064929, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 173, 'creation_date': 1516248623, 'user_type': 'registered', 'user_id': 9232907, 'link': 'https://stackoverflow.com/users/9232907/anmol-gautam', 'profile_image': 'https://lh5.googleusercontent.com/-tVOaMLIKm8I/AAAAAAAAAAI/AAAAAAAAD_Y/-oM1gGZQ7rg/photo.jpg?sz=256', 'display_name': 'Anmol Gautam'}","I want to print a dictionary inside a list like this : I don't want quotations in and . However, I want them in the values portion of that dictionary.","[{name : 'red', id : '1'}, {name : 'yellow', id : '2'}, {name : 'black', id : '3'}, {name : 'white', id : '4'}]`
 name id",-2,6,0,0,
639,48314971,48315086,38249,Pandas how can 'replace' work after 'loc'?,3,<python><pandas>,12,"<p>I have tried many times, but seems the 'replace' can NOT work well after use 'loc'.
For example I want to replace the 'conlumn_b' with an regex for the row that the 'conlumn_a' value is 'apple'.</p>

<p>Here is my sample code : </p>

<pre><code>df.loc[df['conlumn_a'] == 'apple', 'conlumn_b'].replace(r'^11*', 'XXX',inplace=True, regex=True)
</code></pre>

<p>Example:</p>

<pre><code>conlumn_a       conlumn_b
apple           123
banana          11
apple           11
orange          33
</code></pre>

<p>The result that I expected for the 'df' is:</p>

<pre><code>conlumn_a       conlumn_b
apple           123
banana          11
apple           XXX
orange          33
</code></pre>

<p>Anyone has meet this issue that needs 'replace' with regex after 'loc' ? </p>

<p>OR you guys has some other good solutions ?</p>

<p>Thank you so much for your help!</p>
",9233153,123,18-01-2018 06:19,18-01-2018 06:29,0,123,4,1,1,,"{'badge_counts': {'bronze': 4, 'silver': 1, 'gold': 1}, 'account_id': 12708909, 'is_employee': False, 'last_modified_date': 1573678421, 'last_access_date': 1522034442, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 123, 'creation_date': 1516254163, 'user_type': 'registered', 'user_id': 9233153, 'website_url': '', 'link': 'https://stackoverflow.com/users/9233153/jonathan-zhou', 'profile_image': 'https://www.gravatar.com/avatar/a02c434edeffe68ecd3e018efcf2970c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jonathan Zhou'}","I have tried many times, but seems the 'replace' can NOT work well after use 'loc'. For example I want to replace the 'conlumn_b' with an regex for the row that the 'conlumn_a' value is 'apple'. Here is my sample code : Example: The result that I expected for the 'df' is: Anyone has meet this issue that needs 'replace' with regex after 'loc' ? OR you guys has some other good solutions ? Thank you so much for your help!","df.loc[df['conlumn_a'] == 'apple', 'conlumn_b'].replace(r'^11*', 'XXX',inplace=True, regex=True)
 conlumn_a       conlumn_b
apple           123
banana          11
apple           11
orange          33
 conlumn_a       conlumn_b
apple           123
banana          11
apple           XXX
orange          33
",8,31,0,0,
640,48641632,48641886,231985,Extracting specific columns from pandas.dataframe,3,<python><pandas><spyder>,24,"<p>I'm trying to use python to read my csv file extract specific columns to a <code>pandas.dataframe</code> and show that dataframe. However, I don't see the data frame, I receive Series([], dtype: object) as an output. Below is the code that I'm working with:
My document consists of: 
     product    sub_product issue   sub_issue   consumer_complaint_narrative<br>
     company_public_response    company state   zipcode tags<br>
     consumer_consent_provided  submitted_via   date_sent_to_company<br>
     company_response_to_consumer   timely_response consumer_disputed?<br>
     complaint_id</p>

<p>I want to extract :
     sub_product    issue   sub_issue   consumer_complaint_narrative</p>

<pre><code>import pandas as pd

df=pd.read_csv(""C:\\....\\consumer_complaints.csv"")
df=df.stack(level=0)
df2 = df.filter(regex='[B-F]')
df[df2]
</code></pre>
",8495092,512,06-02-2018 11:10,06-02-2018 11:23,0,512,18,1,6,64,"{'badge_counts': {'bronze': 18, 'silver': 6, 'gold': 1}, 'account_id': 11596501, 'is_employee': False, 'last_modified_date': 1621642200, 'last_access_date': 1647968251, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 512, 'creation_date': 1503318170, 'user_type': 'registered', 'user_id': 8495092, 'accept_rate': 64, 'link': 'https://stackoverflow.com/users/8495092/yags', 'profile_image': 'https://www.gravatar.com/avatar/a50f9fcc7be4e1763c1d9c8719c206db?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Yags'}","I'm trying to use python to read my csv file extract specific columns to a and show that dataframe. However, I don't see the data frame, I receive Series([], dtype: object) as an output. Below is the code that I'm working with: My document consists of: product sub_product issue sub_issue consumer_complaint_narrative company_public_response company state zipcode tags consumer_consent_provided submitted_via date_sent_to_company company_response_to_consumer timely_response consumer_disputed? complaint_id I want to extract : sub_product issue sub_issue consumer_complaint_narrative","pandas.dataframe import pandas as pd

df=pd.read_csv(""C:\\....\\consumer_complaints.csv"")
df=df.stack(level=0)
df2 = df.filter(regex='[B-F]')
df[df2]
",4,18,0,0,
641,49784583,56018943,4030,Numpy import fails on multiarray extension library when called from embedded Python within a C++ application,2,<python><c++><python-3.x><numpy><python-embedding>,17,"<p>I'm running a C++ application which tries to run python using the <a href=""https://docs.python.org/3.5/extending/embedding.html"" rel=""noreferrer"">https://docs.python.org/3.5/extending/embedding.html</a> function calls. This is the error that the application error message pipes are giving me.</p>

<blockquote>
  <p>class 'ImportError': 
  Importing the multiarray numpy extension module failed.  Most
  likely you are trying to import a failed build of numpy.
  If you're working with a numpy git repo, try <code>git clean -xdf</code> (removes all
  files not under version control).  Otherwise reinstall numpy.</p>
  
  <p>Original error was: /usr/local/lib/python3.5/site-packages/numpy/core/multiarray.cpython-35m-x86_64-linux-gnu.so: undefined symbol: PyExc_UserWarning</p>
</blockquote>

<p>I'm quite puzzled as this only occurs when embedding Python in C++ as the import works when I use it through the interpreter. I'm more interested in an answer that adds to my understanding than a quick do this or do that fix. I list some system/problem information below, and some other questions that I'm considering posting about the same topic. Any guidance is appreciated!</p>

<p>System/Problem information:</p>

<ul>
<li>Ubuntu 16.04, 64 bit</li>
<li>Compiled Python 3.5.5 with enabled-shared</li>
<li>numpy import works in the interpreter (python3.exe, and python3.5.exe)</li>
<li>I have made sure that the  PySys_SetPath() sets the same sys.path as the output from the interpreter: <code>import sys</code>, <code>sys.path</code></li>
<li>I can import other modules like PIL, and datetimeutil; however, numpy and pandas are not importable (pandas uses numpy or seems to)</li>
<li>The embedded Python uses the following commands: <code>Py_Import_Import()</code>, <code>Py_Initialize()</code> (I made sure. It is only called once.), etc., but it does not get a global lock on the interpreter.</li>
<li>The application is built with a CMake build system which compiles to MakeFiles for my system.</li>
<li>Installed numpy-1.14.2 using pip 9.0.0 using the <code>pip3.5 install numpy</code> command</li>
<li>The python script that causes this error has one line: <code>import numpy</code>...</li>
<li>I do not have a .zip file that I'm importing files from.</li>
<li>The .exe used by the Python embedded in the C++ is located at /usr/local/bin/python3 (used Py_GetProgramName() to determine this). This .exe is linked to the libpython3.5m.so.1.0, and the missing symbol lives in libpython3.5m.so.1.0 (ran nm) </li>
<li><p>ldd on multiarray.cpython-35m-x86_64-linux-gnu.so shows:</p>

<p><code>ldd multiarray.cpython-35m-x86_64-linux-gnu.so</code></p>

<blockquote>
  <p>linux-vdso.so.1 =>  (0x00007ffd9e36b000)</p>
  
  <p>libopenblasp-r0-39a31c03.2.18.so => /usr/local/lib/python3.5/site-packages/numpy/core/./../.libs/libopenblasp-r0-39a31c03.2.18.so (0x00007fdbe149b000)</p>
  
  <p>libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fdbe1192000)</p>
  
  <p>libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fdbe0f75000)
  libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fdbe0bab000)
  /lib64/ld-linux-x86-64.so.2 (0x00007fdbe3ed5000)</p>
  
  <p>libgfortran-ed201abd.so.3.0.0 => /usr/local/lib/python3.5/site-packages/numpy/core/./../.libs/libgfortran-ed201abd.so.3.0.0 (0x00007fdbe08b1000)</p>
</blockquote></li>
</ul>

<p>I could/might try reinstalling numpy through different means, but I'm having trouble tracking why that might work. </p>

<p>At this point, I'm assuming some hole in my knowledge exists. I have looked at a lot of similar posts regarding not being able to import the multiarray component and numpy when embedding Python in C++; however, either none of them match my specific case or as I stated there exists a hole. Here are a list of sub-questions that I will probably be asking if no one sees anything in this setup that is obviously concerning. I'll probably update the questions with links when/if I ask them (After I polish them).</p>

<ul>
<li>How does the numpy multiarray.so link to the pythonX.X.so for symbol resolution? The ldd does not seem to suggest that it ever does. Asked this question at this <a href=""https://stackoverflow.com/questions/49866263/how-are-basic-python-symbols-linked-to-numpy-extension-dynamic-libraries"">link</a></li>
<li>CMake Question non-related issue resolved in this <a href=""https://stackoverflow.com/questions/49805429/cmake-findpackagepythonlibs-does-cmake-give-a-preference-to-dynamic-or-static"">question</a> asked on 4/12/18 and answered on 4/16/18.</li>
<li>Setting PYTHONPATH in .bashrc does not seem to update what Py_GetPath() returns, I had to add in the site-packages for imports through a different methodology to sys.path. It may only update the bash script environment variable which doesn't effect the C++. </li>
</ul>

<p>I'm not asking for an answer for the above question list at this point, rather I'm giving more clues to where my gap in knowledge may be.</p>

<p>Thank you for taking time from your day to read this question. Any help is appreciated. </p>

<h1><strong>Edit: 4/17/18:</strong></h1>

<p>Well, I found a work around, and I'm currently using it. Dunes question started making me think more closely about undefined symbols and how it could be a linker/compiler error or that the numpy import always expects an environment with those symbols already loaded into memory. This got me trying to install different versions of numpy to see if any of the older versions made a difference. They did not, but it did make the error thrown to be slightly different. When I googled that, this <a href=""https://stackoverflow.com/questions/29880931/importerror-and-pyexc-systemerror-while-embedding-python-script-within-c-for-pam"">question appeared</a>. The accepted answer gave me a work around by adding these two lines to the pythonInterface.cpp: </p>

<ul>
<li><code>#include &lt;dlfcn.h&gt;</code></li>
<li><code>dlopen(""libpython3.5m.so.1.0"", RTLD_LAZY | RTLD_GLOBAL)</code></li>
</ul>

<p>These commands add the shared library to be loaded in and available to the cpython.multiarray.so. </p>

<p>This is not an ideal solution as pointing to a specific .so which may be different from machine to machine. It resolves the issue for now, but it also could lead to errors where mismatches of shared libraries can occur during the python call process if the linked library to the pythonInterface.so changes, and this line does not get updated. I believe a better answer can be achieved if this <a href=""https://stackoverflow.com/questions/49866263/how-are-basic-python-symbols-linked-to-numpy-extension-dynamic-libraries?noredirect=1&amp;lq=1"">sub-question</a> is answered, so I'm currently holding out on submitting or accepting an answer until then. Thanks!</p>
",8525442,407,11-04-2018 21:14,07-05-2019 08:56,391,407,7,0,3,,"{'badge_counts': {'bronze': 7, 'silver': 3, 'gold': 0}, 'account_id': 11641035, 'is_employee': False, 'last_modified_date': 1633872128, 'last_access_date': 1693510375, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 407, 'creation_date': 1503892272, 'user_type': 'registered', 'user_id': 8525442, 'link': 'https://stackoverflow.com/users/8525442/skincell', 'profile_image': 'https://www.gravatar.com/avatar/4f5e133a4600749ae9e12ff24d7004f5?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'skincell'}","I'm running a C++ application which tries to run python using the https://docs.python.org/3.5/extending/embedding.html function calls. This is the error that the application error message pipes are giving me. class 'ImportError': Importing the multiarray numpy extension module failed. Most likely you are trying to import a failed build of numpy. If you're working with a numpy git repo, try (removes all files not under version control). Otherwise reinstall numpy. Original error was: /usr/local/lib/python3.5/site-packages/numpy/core/multiarray.cpython-35m-x86_64-linux-gnu.so: undefined symbol: PyExc_UserWarning I'm quite puzzled as this only occurs when embedding Python in C++ as the import works when I use it through the interpreter. I'm more interested in an answer that adds to my understanding than a quick do this or do that fix. I list some system/problem information below, and some other questions that I'm considering posting about the same topic. Any guidance is appreciated! System/Problem information: Ubuntu 16.04, 64 bit Compiled Python 3.5.5 with enabled-shared numpy import works in the interpreter (python3.exe, and python3.5.exe) I have made sure that the PySys_SetPath() sets the same sys.path as the output from the interpreter: , I can import other modules like PIL, and datetimeutil; however, numpy and pandas are not importable (pandas uses numpy or seems to) The embedded Python uses the following commands: , (I made sure. It is only called once.), etc., but it does not get a global lock on the interpreter. The application is built with a CMake build system which compiles to MakeFiles for my system. Installed numpy-1.14.2 using pip 9.0.0 using the command The python script that causes this error has one line: ... I do not have a .zip file that I'm importing files from. The .exe used by the Python embedded in the C++ is located at /usr/local/bin/python3 (used Py_GetProgramName() to determine this). This .exe is linked to the libpython3.5m.so.1.0, and the missing symbol lives in libpython3.5m.so.1.0 (ran nm) ldd on multiarray.cpython-35m-x86_64-linux-gnu.so shows: linux-vdso.so.1 => (0x00007ffd9e36b000) libopenblasp-r0-39a31c03.2.18.so => /usr/local/lib/python3.5/site-packages/numpy/core/./../.libs/libopenblasp-r0-39a31c03.2.18.so (0x00007fdbe149b000) libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fdbe1192000) libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fdbe0f75000) libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fdbe0bab000) /lib64/ld-linux-x86-64.so.2 (0x00007fdbe3ed5000) libgfortran-ed201abd.so.3.0.0 => /usr/local/lib/python3.5/site-packages/numpy/core/./../.libs/libgfortran-ed201abd.so.3.0.0 (0x00007fdbe08b1000) I could/might try reinstalling numpy through different means, but I'm having trouble tracking why that might work. At this point, I'm assuming some hole in my knowledge exists. I have looked at a lot of similar posts regarding not being able to import the multiarray component and numpy when embedding Python in C++; however, either none of them match my specific case or as I stated there exists a hole. Here are a list of sub-questions that I will probably be asking if no one sees anything in this setup that is obviously concerning. I'll probably update the questions with links when/if I ask them (After I polish them). How does the numpy multiarray.so link to the pythonX.X.so for symbol resolution? The ldd does not seem to suggest that it ever does. Asked this question at this link CMake Question non-related issue resolved in this question asked on 4/12/18 and answered on 4/16/18. Setting PYTHONPATH in .bashrc does not seem to update what Py_GetPath() returns, I had to add in the site-packages for imports through a different methodology to sys.path. It may only update the bash script environment variable which doesn't effect the C++. I'm not asking for an answer for the above question list at this point, rather I'm giving more clues to where my gap in knowledge may be. Thank you for taking time from your day to read this question. Any help is appreciated. Edit: 4/17/18: Well, I found a work around, and I'm currently using it. Dunes question started making me think more closely about undefined symbols and how it could be a linker/compiler error or that the numpy import always expects an environment with those symbols already loaded into memory. This got me trying to install different versions of numpy to see if any of the older versions made a difference. They did not, but it did make the error thrown to be slightly different. When I googled that, this question appeared. The accepted answer gave me a work around by adding these two lines to the pythonInterface.cpp: These commands add the shared library to be loaded in and available to the cpython.multiarray.so. This is not an ideal solution as pointing to a specific .so which may be different from machine to machine. It resolves the issue for now, but it also could lead to errors where mismatches of shared libraries can occur during the python call process if the linked library to the pythonInterface.so changes, and this line does not get updated. I believe a better answer can be achieved if this sub-question is answered, so I'm currently holding out on submitting or accepting an answer until then. Thanks!","git clean -xdf import sys sys.path Py_Import_Import() Py_Initialize() pip3.5 install numpy import numpy ldd multiarray.cpython-35m-x86_64-linux-gnu.so #include &lt;dlfcn.h&gt; dlopen(""libpython3.5m.so.1.0"", RTLD_LAZY | RTLD_GLOBAL)",-10,73,0,5,
642,49432081,49432187,24800,New column in Pandas dataframe based on boolean conditions,2,<python><pandas><dataframe>,12,"<p>I'd like to create a new column to a Pandas dataframe populated with True or False based on the other values in each specific row. My approach to solve this task was to apply a function checking boolean conditions across each row in the dataframe and populate the new column with either True or False.    </p>

<p>This is the dataframe:</p>

<pre><code>l={'DayTime':['2018-03-01','2018-03-02','2018-03-03'],'Pressure':
[9,10.5,10.5], 'Feed':[9,10.5,11], 'Temp':[9,10.5,11]}

df1=pd.DataFrame(l)
</code></pre>

<p>This is the function I wrote:</p>

<pre><code>def ops_on(row):
   return row[('Feed' &gt; 10)
              &amp; ('Pressure' &gt; 10)
              &amp; ('Temp' &gt; 10)
             ]
</code></pre>

<p>The function ops_on is used to create the new column ['ops_on']:</p>

<pre><code>df1['ops_on'] = df1.apply(ops_on, axis='columns')
</code></pre>

<p>Unfortunately, I get this error message:</p>

<p>TypeError: (""'>' not supported between instances of 'str' and 'int'"", 'occurred at index 0') </p>

<p>Thankful for help. </p>
",9259565,123,22-03-2018 15:06,22-03-2018 15:11,0,123,5,1,1,,"{'badge_counts': {'bronze': 5, 'silver': 1, 'gold': 1}, 'account_id': 12796946, 'is_employee': False, 'last_modified_date': 1573678416, 'last_access_date': 1527001501, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 123, 'creation_date': 1516753383, 'user_type': 'registered', 'user_id': 9259565, 'website_url': '', 'link': 'https://stackoverflow.com/users/9259565/tvdm', 'profile_image': 'https://www.gravatar.com/avatar/4f95fbf9d1954ebf8ce166f37eca3b5d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'TvdM'}","I'd like to create a new column to a Pandas dataframe populated with True or False based on the other values in each specific row. My approach to solve this task was to apply a function checking boolean conditions across each row in the dataframe and populate the new column with either True or False. This is the dataframe: This is the function I wrote: The function ops_on is used to create the new column ['ops_on']: Unfortunately, I get this error message: TypeError: (""'>' not supported between instances of 'str' and 'int'"", 'occurred at index 0') Thankful for help.","l={'DayTime':['2018-03-01','2018-03-02','2018-03-03'],'Pressure':
[9,10.5,10.5], 'Feed':[9,10.5,11], 'Temp':[9,10.5,11]}

df1=pd.DataFrame(l)
 def ops_on(row):
   return row[('Feed' &gt; 10)
              &amp; ('Pressure' &gt; 10)
              &amp; ('Temp' &gt; 10)
             ]
 df1['ops_on'] = df1.apply(ops_on, axis='columns')
",7,29,0,0,
643,48892772,48892802,32493,How to remove a directory? Is os.removedirs and os.rmdir only used to delete empty directories?,1,<python><python-3.x><python-2.7>,29,"<p>Whenever I try to use them to remove dirs with things in them I get this error message</p>

<pre><code>import os
os.chdir('/Users/mustafa/Desktop')
os.makedirs('new-file/sub-file')
os.removedirs('new-file') 
</code></pre>

<blockquote>
  <p>""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py"", line 170, in removedirs
      rmdir(name)
  OSError: [Errno 66] Directory not empty: 'new-file'</p>
</blockquote>

<p>However I think I saw people using those commands to delete dirs that weren't empty, so what am I doing wrong? Thanks</p>
",9291340,488,20-02-2018 19:26,20-02-2018 19:27,0,488,9,1,5,,"{'badge_counts': {'bronze': 9, 'silver': 5, 'gold': 1}, 'account_id': 12844392, 'is_employee': False, 'last_modified_date': 1704805800, 'last_access_date': 1708449528, 'reputation_change_year': -10, 'reputation_change_quarter': -10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 488, 'creation_date': 1517346721, 'user_type': 'registered', 'user_id': 9291340, 'link': 'https://stackoverflow.com/users/9291340/mustafa', 'profile_image': 'https://lh4.googleusercontent.com/-3ZCAWWhFtvA/AAAAAAAAAAI/AAAAAAAACSU/KMT67xVx2-w/photo.jpg?sz=256', 'display_name': 'Mustafa '}","Whenever I try to use them to remove dirs with things in them I get this error message ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py"", line 170, in removedirs rmdir(name) OSError: [Errno 66] Directory not empty: 'new-file' However I think I saw people using those commands to delete dirs that weren't empty, so what am I doing wrong? Thanks","import os
os.chdir('/Users/mustafa/Desktop')
os.makedirs('new-file/sub-file')
os.removedirs('new-file') 
",3,15,0,0,
644,48541040,49032887,66368,saving figures using plt.savefig on colaboratory,6,<python><matplotlib><jupyter-notebook>,27,"<p>I am using collaboratory(online jupyter notebook) 
I have the following code i am plotting some graphs using this functions and want to save plots locally 
how can I do this ?</p>

<pre><code>def make_plot_comparison(Xlabel,Ylabel,l1,l2,l1_title,l2_title,name): 
    plt.xlabel(Xlabel)
    plt.ylabel(Ylabel)
    plt.plot(l1,label=l1_title)
    plt.plot(l2,label=l2_title)
    plt.legend(loc='center right')
    plt.title(name)
    #plt.xlim(-5, 25)
    plt.savefig(""abc.png"")
    plt.show()
</code></pre>
",9294117,585,31-01-2018 11:26,28-02-2018 15:17,28,585,10,1,4,,"{'badge_counts': {'bronze': 10, 'silver': 4, 'gold': 1}, 'account_id': 11678535, 'is_employee': False, 'last_modified_date': 1648515300, 'last_access_date': 1711062676, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 585, 'creation_date': 1517397226, 'user_type': 'registered', 'user_id': 9294117, 'location': 'Karachi, Pakistan', 'website_url': '', 'link': 'https://stackoverflow.com/users/9294117/aqeel-altaf', 'profile_image': 'https://lh5.googleusercontent.com/-dpTQ2XXTl88/AAAAAAAAAAI/AAAAAAAAAGE/8al2PQ09NJ8/photo.jpg?sz=256', 'display_name': 'AQEEL ALTAF'}",I am using collaboratory(online jupyter notebook) I have the following code i am plotting some graphs using this functions and want to save plots locally how can I do this ?,"def make_plot_comparison(Xlabel,Ylabel,l1,l2,l1_title,l2_title,name): 
    plt.xlabel(Xlabel)
    plt.ylabel(Ylabel)
    plt.plot(l1,label=l1_title)
    plt.plot(l2,label=l2_title)
    plt.legend(loc='center right')
    plt.title(name)
    #plt.xlim(-5, 25)
    plt.savefig(""abc.png"")
    plt.show()
",9,15,0,0,
645,48589141,48594778,58436,Anaconda - UnsatisfiableError: The following specifications were found to be in conflict,1,<python><anaconda><pymc>,15,"<p>When I was trying to install a module 'pymc' through anaconda environments, it showed the error message as follows:</p>
<blockquote>
<p>UnsatisfiableError: The following specifications were found to be in
conflict:</p>
<ul>
<li><p>blaze -&gt; pyyaml -&gt; python[version='&gt;=2.7,&lt;2.8.0a0'] -&gt; vc=9</p>
</li>
<li><p>blaze -&gt; pyyaml -&gt; yaml -&gt; *[track_features=vc9]</p>
</li>
<li><p>pymc Use &quot;conda info &quot; to see the dependencies for each package.</p>
</li>
</ul>
</blockquote>
<p>I am using Python 2.7.14, and I installed anaconda 1.6.9 on a Windows. I am new to Python. I first tried to use cmd to install the module pymc and I ran into a lot of problems such as the requirement for install g77 compiler on windows. After I got the compiler from MinGW and also installed the Microsoft Visual C++ Compiler for Python, I still cannot install the module because there came new errors. That is when I found there is pymc module listed in anaconda environment that I can add manually, but it showed this conflict error.</p>
<p>I do not know whether the conflict comes from all those other stuff I installed above or not. Please HELP! Thanks!</p>
",9301534,153,02-02-2018 18:52,03-02-2018 06:51,1,153,4,1,1,,"{'badge_counts': {'bronze': 4, 'silver': 1, 'gold': 1}, 'account_id': 12859191, 'is_employee': False, 'last_modified_date': 1573678409, 'last_access_date': 1524532883, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 153, 'creation_date': 1517509296, 'user_type': 'registered', 'user_id': 9301534, 'link': 'https://stackoverflow.com/users/9301534/qismees', 'profile_image': 'https://lh6.googleusercontent.com/-bLrtqsaZ08o/AAAAAAAAAAI/AAAAAAAAAlI/d9oFGdaoJ5E/photo.jpg?sz=256', 'display_name': 'qismees'}","When I was trying to install a module 'pymc' through anaconda environments, it showed the error message as follows: UnsatisfiableError: The following specifications were found to be in conflict: blaze -&gt; pyyaml -&gt; python[version='&gt;=2.7,&lt;2.8.0a0'] -&gt; vc=9 blaze -&gt; pyyaml -&gt; yaml -&gt; *[track_features=vc9] pymc Use &quot;conda info &quot; to see the dependencies for each package. I am using Python 2.7.14, and I installed anaconda 1.6.9 on a Windows. I am new to Python. I first tried to use cmd to install the module pymc and I ran into a lot of problems such as the requirement for install g77 compiler on windows. After I got the compiler from MinGW and also installed the Microsoft Visual C++ Compiler for Python, I still cannot install the module because there came new errors. That is when I found there is pymc module listed in anaconda environment that I can add manually, but it showed this conflict error. I do not know whether the conflict comes from all those other stuff I installed above or not. Please HELP! Thanks!",,0,15,0,0,
646,48889914,48889934,19068,Pandas set_index doesn't drop the column,1,<python><pandas><csv><import>,22,"<p>I run the following code function on my dataframe:</p>

<pre><code>del dfname[""Unnamed: 0""]
dfname[""date""] = pd.to_datetime(dfname[""date""])
dfname.set_index(dfname[""date""], drop=True, inplace=True)
</code></pre>

<p>But the column does not drop (I know that the default is <code>drop=True</code>) </p>

<p><a href=""https://i.stack.imgur.com/apnc3.jpg"" rel=""noreferrer"">The output dataframe looks like this</a>. I'm using Python 3.6</p>
",8925363,257,20-02-2018 16:32,20-02-2018 16:32,0,257,10,1,3,,"{'badge_counts': {'bronze': 10, 'silver': 3, 'gold': 1}, 'account_id': 12228539, 'is_employee': False, 'last_modified_date': 1573678475, 'last_access_date': 1538159991, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 257, 'creation_date': 1510426218, 'user_type': 'registered', 'user_id': 8925363, 'link': 'https://stackoverflow.com/users/8925363/michael', 'profile_image': 'https://www.gravatar.com/avatar/7a711ebc251a7deda5ea47153099ce76?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Michael'}",I run the following code function on my dataframe: But the column does not drop (I know that the default is ) The output dataframe looks like this. I'm using Python 3.6,"del dfname[""Unnamed: 0""]
dfname[""date""] = pd.to_datetime(dfname[""date""])
dfname.set_index(dfname[""date""], drop=True, inplace=True)
 drop=True",1,10,0,1,
647,50098025,50098390,45202,Mapping ranges of values in pandas dataframe,2,<python><pandas><dataframe><categories><binning>,20,"<p>Apologies if this has been asked before, but I looked extensively without results.</p>

<pre><code>import pandas as pd    
import numpy as np    
df = pd.DataFrame(data = np.random.randint(1,10,10),columns=['a'])    

   a
0  7
1  8
2  8
3  3
4  1
5  1
6  2
7  8
8  6
9  6
</code></pre>

<p>I'd like to create a new column <code>b</code> that maps several values of <code>a</code> according to some rule, say a=[1,2,3] is 1, a = [4,5,6,7] is 2, a = [8,9,10] is 3. one-to-one mapping is clear to me, but what if I want to map by a list of values or a range?</p>

<p>I tought along these lines...</p>

<pre><code>df['b'] = df['a'].map({[1,2,3]:1,range(4,7):2,[8,9,10]:3})
</code></pre>
",8952565,730,30-04-2018 09:52,30-04-2018 10:14,0,740,29,1,8,,"{'badge_counts': {'bronze': 29, 'silver': 8, 'gold': 1}, 'account_id': 12266056, 'is_employee': False, 'last_modified_date': 1651861200, 'last_access_date': 1711099038, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 740, 'creation_date': 1510845983, 'user_type': 'registered', 'user_id': 8952565, 'location': 'Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/8952565/e-sommer', 'profile_image': 'https://i.stack.imgur.com/ufcwB.png?s=256&g=1', 'display_name': 'E. Sommer'}","Apologies if this has been asked before, but I looked extensively without results. I'd like to create a new column that maps several values of according to some rule, say a=[1,2,3] is 1, a = [4,5,6,7] is 2, a = [8,9,10] is 3. one-to-one mapping is clear to me, but what if I want to map by a list of values or a range? I tought along these lines...","import pandas as pd    
import numpy as np    
df = pd.DataFrame(data = np.random.randint(1,10,10),columns=['a'])    

   a
0  7
1  8
2  8
3  3
4  1
5  1
6  2
7  8
8  6
9  6
 b a df['b'] = df['a'].map({[1,2,3]:1,range(4,7):2,[8,9,10]:3})
",12,25,0,0,
648,48491839,48493537,30413,Any way to write files DIRECTLY to S3 using boto3?,2,<python><amazon-web-services><amazon-s3><boto3>,28,"<p>I wrote a python script to process very large files (few TB in total), which I'll run on an EC2 instance. Afterwards, I want to store the processed files in an S3 bucket. Currently, my script first saves the data to disk and then uploads it to S3. Unfortunately, this will be quite costly given the extra time spent waiting for the instance to first write to disk and then upload.</p>

<p>Is there any way to use boto3 to write files directly to an S3 bucket?</p>

<p>Edit: to clarify my question, I'm asking if I have an object in memory, writing that object directly to S3 without first saving the object onto disk.</p>
",8967919,385,28-01-2018 22:05,29-01-2018 02:23,1,385,6,1,3,,"{'badge_counts': {'bronze': 6, 'silver': 3, 'gold': 1}, 'account_id': 12290234, 'is_employee': False, 'last_modified_date': 1645091334, 'last_access_date': 1552920024, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 385, 'creation_date': 1511125276, 'user_type': 'registered', 'user_id': 8967919, 'link': 'https://stackoverflow.com/users/8967919/richard-sun', 'profile_image': 'https://www.gravatar.com/avatar/ce035f8225faf2763f908c0532262d7f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Richard Sun'}","I wrote a python script to process very large files (few TB in total), which I'll run on an EC2 instance. Afterwards, I want to store the processed files in an S3 bucket. Currently, my script first saves the data to disk and then uploads it to S3. Unfortunately, this will be quite costly given the extra time spent waiting for the instance to first write to disk and then upload. Is there any way to use boto3 to write files directly to an S3 bucket? Edit: to clarify my question, I'm asking if I have an object in memory, writing that object directly to S3 without first saving the object onto disk.",,0,5,0,0,
649,49474632,49475526,37706,TypeError: create_superuser() missing 1 required positional argument: 'profile_picture',10,<python><python-3.x><django><django-custom-user><django-custom-manager>,19,"<p>I get the following error after adding the <code>profile_picture</code> field:</p>

<pre><code>TypeError: create_superuser() missing 1 required positional argument: 'profile_picture'
</code></pre>

<p>This profile_picture field is an ""ImageField"" set as ""Null = True"". </p>

<p>I have tried the following: <code>def create_user(...., profile_picture=None, ....)</code>. It didn't work.</p>

<p>and <strong>the error occurs only in command prompt when i create superuser from there.</strong></p>

<p>Here is my models.py</p>

<pre><code>from django.db import models
from django.contrib.auth.models import BaseUserManager, AbstractBaseUser


class UserManager(BaseUserManager):
    def create_user(self, email, full_name, profile_picture=None, gender=None, password=None, is_admin=False, is_staff=False, is_active=True):
        if not email:
            raise ValueError(""User must have an email"")
        if not password:
            raise ValueError(""User must have a password"")
        if not full_name:
            raise ValueError(""User must have a full name"")

        user = self.model(
            email=self.normalize_email(email)
        )
        user.full_name = full_name
        user.set_password(password)  # change password to hash
        # user.profile_picture = profile_picture
        user.gender = gender
        user.admin = is_admin
        user.profile_picture = profile_picture
        user.staff = is_staff
        user.active = is_active
        user.save(using=self._db)
        return user

    def create_staffuser(self, email, profile_picture, gender, full_name, password=None):
        user = self.create_user(
            email,
            full_name,
            profile_picture,
            gender,
            password=password,
            is_staff=True,
        )
        return user

    def create_superuser(self, email, profile_picture, gender, full_name, password=None):
        user = self.create_user(
            email,
            full_name,
            profile_picture,
            gender,
            password=password,
            is_staff=True,
            is_admin=True,
        )
        return user


class User(AbstractBaseUser):
    username = models.CharField(max_length=255)
    full_name = models.CharField(max_length=255)
    email = models.EmailField(max_length=255, unique=True,)
    profile_picture = models.ImageField(upload_to='user_data/profile_picture', null=True, blank=True)
    gender = models.CharField(max_length=255, blank=True, default='rather_not_say')
    active = models.BooleanField(default=True)
    staff = models.BooleanField(default=False)  # a admin user; non super-user
    admin = models.BooleanField(default=False)  # a superuser
    # notice the absence of a ""Password field"", that's built in.

    USERNAME_FIELD = 'email'
    REQUIRED_FIELDS = ['full_name', 'gender']  # Email &amp; Password are required by default.

    objects = UserManager()

    def get_full_name(self):
        # The user is identified by their email address
        return self.email

    def get_short_name(self):
         # The user is identified by their email address
         return self.email

    def __str__(self):              # __unicode__ on Python 2
         return self.email

    @staticmethod
    def has_perm(perm, obj=None):
         # ""Does the user have a specific permission?""
         # Simplest possible answer: Yes, always
        return True

    @staticmethod
    def has_module_perms(app_label):
         # ""Does the user have permissions to view the app `app_label`?""
         # Simplest possible answer: Yes, always
         return True

    @property
    def is_staff(self):
         # ""Is the user a member of staff?""
         return self.staff

    @property
    def is_admin(self):
         # ""Is the user a admin member?""
         return self.admin

    @property
    def is_active(self):
         # ""Is the user active?""
         return self.active
</code></pre>
",8995580,247,25-03-2018 10:04,25-03-2018 11:52,0,247,12,1,3,,"{'badge_counts': {'bronze': 12, 'silver': 3, 'gold': 1}, 'account_id': 12328890, 'is_employee': False, 'last_modified_date': 1573678464, 'last_access_date': 1538133186, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 247, 'creation_date': 1511430494, 'user_type': 'registered', 'user_id': 8995580, 'website_url': '', 'link': 'https://stackoverflow.com/users/8995580/ankush-paul', 'profile_image': 'https://lh5.googleusercontent.com/-v7a6wdIKms0/AAAAAAAAAAI/AAAAAAAAAIg/lejpa7KfXWM/photo.jpg?sz=256', 'display_name': 'Ankush paul'}","I get the following error after adding the field: This profile_picture field is an ""ImageField"" set as ""Null = True"". I have tried the following: . It didn't work. and the error occurs only in command prompt when i create superuser from there. Here is my models.py","profile_picture TypeError: create_superuser() missing 1 required positional argument: 'profile_picture'
 def create_user(...., profile_picture=None, ....) from django.db import models
from django.contrib.auth.models import BaseUserManager, AbstractBaseUser


class UserManager(BaseUserManager):
    def create_user(self, email, full_name, profile_picture=None, gender=None, password=None, is_admin=False, is_staff=False, is_active=True):
        if not email:
            raise ValueError(""User must have an email"")
        if not password:
            raise ValueError(""User must have a password"")
        if not full_name:
            raise ValueError(""User must have a full name"")

        user = self.model(
            email=self.normalize_email(email)
        )
        user.full_name = full_name
        user.set_password(password)  # change password to hash
        # user.profile_picture = profile_picture
        user.gender = gender
        user.admin = is_admin
        user.profile_picture = profile_picture
        user.staff = is_staff
        user.active = is_active
        user.save(using=self._db)
        return user

    def create_staffuser(self, email, profile_picture, gender, full_name, password=None):
        user = self.create_user(
            email,
            full_name,
            profile_picture,
            gender,
            password=password,
            is_staff=True,
        )
        return user

    def create_superuser(self, email, profile_picture, gender, full_name, password=None):
        user = self.create_user(
            email,
            full_name,
            profile_picture,
            gender,
            password=password,
            is_staff=True,
            is_admin=True,
        )
        return user


class User(AbstractBaseUser):
    username = models.CharField(max_length=255)
    full_name = models.CharField(max_length=255)
    email = models.EmailField(max_length=255, unique=True,)
    profile_picture = models.ImageField(upload_to='user_data/profile_picture', null=True, blank=True)
    gender = models.CharField(max_length=255, blank=True, default='rather_not_say')
    active = models.BooleanField(default=True)
    staff = models.BooleanField(default=False)  # a admin user; non super-user
    admin = models.BooleanField(default=False)  # a superuser
    # notice the absence of a ""Password field"", that's built in.

    USERNAME_FIELD = 'email'
    REQUIRED_FIELDS = ['full_name', 'gender']  # Email &amp; Password are required by default.

    objects = UserManager()

    def get_full_name(self):
        # The user is identified by their email address
        return self.email

    def get_short_name(self):
         # The user is identified by their email address
         return self.email

    def __str__(self):              # __unicode__ on Python 2
         return self.email

    @staticmethod
    def has_perm(perm, obj=None):
         # ""Does the user have a specific permission?""
         # Simplest possible answer: Yes, always
        return True

    @staticmethod
    def has_module_perms(app_label):
         # ""Does the user have permissions to view the app `app_label`?""
         # Simplest possible answer: Yes, always
         return True

    @property
    def is_staff(self):
         # ""Is the user a member of staff?""
         return self.staff

    @property
    def is_admin(self):
         # ""Is the user a admin member?""
         return self.admin

    @property
    def is_active(self):
         # ""Is the user active?""
         return self.active
",101,118,0,0,
650,50113683,50148096,121544,ModuleNotFoundError: No module named 'object_detection',8,<python>,28,"<p>i try to train.py in object_detection in under git url</p>

<p><a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""noreferrer"">https://github.com/tensorflow/models/tree/master/research/object_detection</a></p>

<p>However, the following error occurs.</p>

<blockquote>
  <p>ModuleNotFoundError: No module named 'object_detection'</p>
</blockquote>

<p>So I tried to solve the problem by writing the following code.</p>

<pre><code>import sys

sys.path.append('/home/user/Documents/imgmlreport/inception/models/research/object_detection')
from object_detection.builders import dataset_builder
</code></pre>

<p>This problem has not been solved yet.</p>

<p>The directory structure is shown below.</p>

<pre><code>~/object_detection/train.py

~/object_detection/builders/dataset_bulider.py
</code></pre>

<p>and here is full error massage</p>

<blockquote>
  <p>/home/user/anaconda3/lib/python3.6/site-packages/h5py/<strong>init</strong>.py:34: FutureWarning: Conversion of the second argument of issubdtype from <code>float</code> to <code>np.floating</code> is deprecated.</p>
  
  <p>In future, it will be treated as <code>np.float64 == np.dtype(float).type</code>.
    from ._conv import register_converters as _register_converters</p>
  
  <p>Traceback (most recent call last):</p>
  
  <p>File ""train.py"", line 52, in 
      import trainer</p>
  
  <p>File""/home/user/Documents/imgmlreport/inception/models/research/object_detection/trainer.py"", line 26, in 
      from object_detection.builders import optimizer_builder</p>
  
  <p>ModuleNotFoundError: No module named 'object_detection'</p>
</blockquote>

<p>how can i import modules?</p>
",9019755,1011,01-05-2018 08:49,03-05-2018 06:13,2,1011,32,2,17,,"{'badge_counts': {'bronze': 32, 'silver': 17, 'gold': 2}, 'account_id': 12368697, 'is_employee': False, 'last_modified_date': 1646995800, 'last_access_date': 1704546058, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1011, 'creation_date': 1511867541, 'user_type': 'registered', 'user_id': 9019755, 'location': 'Seoul, 대한민국', 'website_url': 'http://None', 'link': 'https://stackoverflow.com/users/9019755/%ec%86%a1%ec%a4%80%ec%84%9d', 'profile_image': 'https://lh5.googleusercontent.com/-W-DGq5ck66k/AAAAAAAAAAI/AAAAAAAAAEE/N0PpIlFCPA0/photo.jpg?sz=256', 'display_name': '송준석'}","i try to train.py in object_detection in under git url https://github.com/tensorflow/models/tree/master/research/object_detection However, the following error occurs. ModuleNotFoundError: No module named 'object_detection' So I tried to solve the problem by writing the following code. This problem has not been solved yet. The directory structure is shown below. and here is full error massage /home/user/anaconda3/lib/python3.6/site-packages/h5py/init.py:34: FutureWarning: Conversion of the second argument of issubdtype from to is deprecated. In future, it will be treated as . from ._conv import register_converters as _register_converters Traceback (most recent call last): File ""train.py"", line 52, in import trainer File""/home/user/Documents/imgmlreport/inception/models/research/object_detection/trainer.py"", line 26, in from object_detection.builders import optimizer_builder ModuleNotFoundError: No module named 'object_detection' how can i import modules?","import sys

sys.path.append('/home/user/Documents/imgmlreport/inception/models/research/object_detection')
from object_detection.builders import dataset_builder
 ~/object_detection/train.py

~/object_detection/builders/dataset_bulider.py
 float np.floating np.float64 == np.dtype(float).type",2,47,0,1,
651,50304156,51367588,137343,Tensorflow Allocation Memory: Allocation of 38535168 exceeds 10% of system memory,10,<python><tensorflow><memory><keras-layer><resnet>,59,"<p>Using ResNet50 pre-trained Weights I am trying to build a classifier. The code base is fully implemented in Keras high-level Tensorflow API. The complete code is posted in the below GitHub Link.</p>
<p>Source Code: <a href=""https://gist.github.com/Madhivarman/676650f71ec35a5f2802631fcfa0ff73"" rel=""noreferrer"">Classification Using RestNet50 Architecture</a></p>
<p>The file size of the pre-trained model is <strong>94.7mb</strong>.</p>
<p>I loaded the pre-trained file</p>
<pre><code>new_model = Sequential()

new_model.add(ResNet50(include_top=False,
                pooling='avg',
                weights=resnet_weight_paths))
</code></pre>
<p>and fit the model</p>
<pre><code>train_generator = data_generator.flow_from_directory(
    'path_to_the_training_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    batch_size = 12,
    class_mode = 'categorical'
    )

validation_generator = data_generator.flow_from_directory(
    'path_to_the_validation_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    class_mode = 'categorical'
    )

#compile the model

new_model.fit_generator(
    train_generator,
    steps_per_epoch = 3,
    validation_data = validation_generator,
    validation_steps = 1
)
</code></pre>
<p>and in the Training dataset, I have two folders dog and cat, each holder almost 10,000 images. When  I compiled the script, I get the following error</p>
<blockquote>
<p>Epoch 1/1 2018-05-12 13:04:45.847298: W
tensorflow/core/framework/allocator.cc:101] Allocation of 38535168
exceeds 10% of system memory. 2018-05-12 13:04:46.845021: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:47.552176: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:48.199240: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:48.918930: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:49.274137: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:49.647061: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:50.028839: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:50.413735: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory.</p>
</blockquote>
<p>Any ideas to optimize the way to load the pre-trained model (or) get rid of this warning message?</p>
<p>Thanks!</p>
",9044016,1206,12-05-2018 08:09,16-07-2018 17:55,65,1206,27,3,18,60,"{'badge_counts': {'bronze': 27, 'silver': 18, 'gold': 3}, 'account_id': 12412739, 'is_employee': False, 'last_modified_date': 1652364339, 'last_access_date': 1710398335, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1206, 'creation_date': 1512237334, 'user_type': 'registered', 'user_id': 9044016, 'accept_rate': 60, 'location': 'Bangalore, Karnataka, India', 'website_url': 'https://medium.com/@iamvarman', 'link': 'https://stackoverflow.com/users/9044016/madhi', 'profile_image': 'https://www.gravatar.com/avatar/0e54c6c7365d307261cc2af3d5b027e0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Madhi'}","Using ResNet50 pre-trained Weights I am trying to build a classifier. The code base is fully implemented in Keras high-level Tensorflow API. The complete code is posted in the below GitHub Link. Source Code: Classification Using RestNet50 Architecture The file size of the pre-trained model is 94.7mb. I loaded the pre-trained file and fit the model and in the Training dataset, I have two folders dog and cat, each holder almost 10,000 images. When I compiled the script, I get the following error Epoch 1/1 2018-05-12 13:04:45.847298: W tensorflow/core/framework/allocator.cc:101] Allocation of 38535168 exceeds 10% of system memory. 2018-05-12 13:04:46.845021: W tensorflow/core/framework/allocator.cc:101] Allocation of 37171200 exceeds 10% of system memory. 2018-05-12 13:04:47.552176: W tensorflow/core/framework/allocator.cc:101] Allocation of 37171200 exceeds 10% of system memory. 2018-05-12 13:04:48.199240: W tensorflow/core/framework/allocator.cc:101] Allocation of 37171200 exceeds 10% of system memory. 2018-05-12 13:04:48.918930: W tensorflow/core/framework/allocator.cc:101] Allocation of 37171200 exceeds 10% of system memory. 2018-05-12 13:04:49.274137: W tensorflow/core/framework/allocator.cc:101] Allocation of 19267584 exceeds 10% of system memory. 2018-05-12 13:04:49.647061: W tensorflow/core/framework/allocator.cc:101] Allocation of 19267584 exceeds 10% of system memory. 2018-05-12 13:04:50.028839: W tensorflow/core/framework/allocator.cc:101] Allocation of 19267584 exceeds 10% of system memory. 2018-05-12 13:04:50.413735: W tensorflow/core/framework/allocator.cc:101] Allocation of 19267584 exceeds 10% of system memory. Any ideas to optimize the way to load the pre-trained model (or) get rid of this warning message? Thanks!","new_model = Sequential()

new_model.add(ResNet50(include_top=False,
                pooling='avg',
                weights=resnet_weight_paths))
 train_generator = data_generator.flow_from_directory(
    'path_to_the_training_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    batch_size = 12,
    class_mode = 'categorical'
    )

validation_generator = data_generator.flow_from_directory(
    'path_to_the_validation_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    class_mode = 'categorical'
    )

#compile the model

new_model.fit_generator(
    train_generator,
    steps_per_epoch = 3,
    validation_data = validation_generator,
    validation_steps = 1
)
",24,57,0,1,
652,49785904,49793781,33310,how to set threshold to scikit learn random forest model,3,<python><scikit-learn>,11,"<p>After seeing the precision_recall_curve, if I want to set threshold = 0.4, how to implement 0.4 into my random forest model (binary classification),  for any probability &lt;0.4, label it as 0, for any >=0.4, label it as 1.</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
  random_forest = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=12)
  random_forest.fit(X_train, y_train)
from sklearn.metrics import accuracy_score
  predicted = random_forest.predict(X_test)
accuracy = accuracy_score(y_test, predicted)
</code></pre>

<p>Documentation <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html"" rel=""noreferrer"">Precision recall</a></p>
",9362220,407,11-04-2018 23:40,12-04-2018 10:07,1,407,13,2,4,,"{'badge_counts': {'bronze': 13, 'silver': 4, 'gold': 2}, 'account_id': 12948689, 'is_employee': False, 'last_modified_date': 1573678399, 'last_access_date': 1543470557, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 407, 'creation_date': 1518643938, 'user_type': 'registered', 'user_id': 9362220, 'website_url': '', 'link': 'https://stackoverflow.com/users/9362220/bigdata', 'profile_image': 'https://www.gravatar.com/avatar/dfab12849b3f25ad1ca76f6845a9f96e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'BigData'}","After seeing the precision_recall_curve, if I want to set threshold = 0.4, how to implement 0.4 into my random forest model (binary classification), for any probability &lt;0.4, label it as 0, for any >=0.4, label it as 1. Documentation Precision recall","from sklearn.ensemble import RandomForestClassifier
  random_forest = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=12)
  random_forest.fit(X_train, y_train)
from sklearn.metrics import accuracy_score
  predicted = random_forest.predict(X_test)
accuracy = accuracy_score(y_test, predicted)
",5,11,0,1,
653,49641707,49641723,40695,Standardize some columns in Python Pandas dataframe?,4,<python><pandas><sklearn-pandas><standardized>,13,"<p>Python code below only return me an array, but I want the scaled data to replace the original data.</p>

<pre><code>from sklearn.preprocessing import StandardScaler
df = StandardScaler().fit_transform(df[['cost', 'sales']])
df
</code></pre>

<p>output</p>

<pre><code>array([[ 1.99987622, -0.55900276],
       [-0.49786658, -0.45658181],
       [-0.5146864 , -0.505097  ],
       [-0.48104676, -0.47814412],
       [-0.50627649,  1.9988257 ]])
</code></pre>

<p>original data</p>

<pre><code>id  cost    sales   item
1   300       50    pen
2   3         88    bottle
3   1         70    drink
4   5         80    cup
5   2        999    ink
</code></pre>
",9362220,407,04-04-2018 02:14,04-04-2018 02:16,0,407,13,2,4,,"{'badge_counts': {'bronze': 13, 'silver': 4, 'gold': 2}, 'account_id': 12948689, 'is_employee': False, 'last_modified_date': 1573678399, 'last_access_date': 1543470557, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 407, 'creation_date': 1518643938, 'user_type': 'registered', 'user_id': 9362220, 'website_url': '', 'link': 'https://stackoverflow.com/users/9362220/bigdata', 'profile_image': 'https://www.gravatar.com/avatar/dfab12849b3f25ad1ca76f6845a9f96e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'BigData'}","Python code below only return me an array, but I want the scaled data to replace the original data. output original data","from sklearn.preprocessing import StandardScaler
df = StandardScaler().fit_transform(df[['cost', 'sales']])
df
 array([[ 1.99987622, -0.55900276],
       [-0.49786658, -0.45658181],
       [-0.5146864 , -0.505097  ],
       [-0.48104676, -0.47814412],
       [-0.50627649,  1.9988257 ]])
 id  cost    sales   item
1   300       50    pen
2   3         88    bottle
3   1         70    drink
4   5         80    cup
5   2        999    ink
",11,25,0,0,
654,48217449,48217551,8714,Getting index of currently executing input in python multiprocessing,8,<python><multiprocessing>,24,"<pre><code>    from multiprocessing import Pool
    with Pool(processes=5) as p:
        p.starmap(name_of_function, all_inputs)
</code></pre>

<p>I have a piece of code like above that executes a function in parallel. Assuming that <code>all_inputs</code> has 10,000 elements, I would like to know which one is currently executing e.g. 100 out of 10,000... Is there a way to get that index?</p>
",308827,21597,11-01-2018 23:31,11-01-2018 23:44,0,21707,431,93,265,96,"{'badge_counts': {'bronze': 431, 'silver': 265, 'gold': 93}, 'account_id': 118069, 'is_employee': False, 'last_modified_date': 1695033300, 'last_access_date': 1710715265, 'reputation_change_year': 538, 'reputation_change_quarter': 538, 'reputation_change_month': 140, 'reputation_change_week': 60, 'reputation_change_day': 0, 'reputation': 21707, 'creation_date': 1270407253, 'user_type': 'registered', 'user_id': 308827, 'accept_rate': 96, 'link': 'https://stackoverflow.com/users/308827/user308827', 'profile_image': 'https://www.gravatar.com/avatar/28f523030f3ecd9c9f2ad23ca745cc65?s=256&d=identicon&r=PG', 'display_name': 'user308827'}","I have a piece of code like above that executes a function in parallel. Assuming that has 10,000 elements, I would like to know which one is currently executing e.g. 100 out of 10,000... Is there a way to get that index?","    from multiprocessing import Pool
    with Pool(processes=5) as p:
        p.starmap(name_of_function, all_inputs)
 all_inputs",1,6,0,0,
655,50123923,50124135,26598,replace values in xarray dataset with None,1,<python><python-xarray>,21,"<p>I want to replace values in a variable in an xarray dataset with None. I tried this approach but it did not work:</p>

<pre><code>da[da['var'] == -9999.]['var'] = None
</code></pre>

<p>I get this error: <code>*** TypeError: unhashable type: 'numpy.ndarray'</code></p>

<p>Is there something like numpy replace that I could use here? <code>da</code> is xarray dataset. here is what <code>da</code> looks like:</p>

<pre><code>&lt;xarray.Dataset&gt;
Dimensions:  (band: 1, time: 3, x: 4258, y: 2334)
Coordinates:
  * band     (band) int32 1
  * y        (y) float64 4.406e+06 4.406e+06 4.406e+06 4.406e+06 4.406e+06 ...
  * x        (x) float64 1.125e+05 1.126e+05 1.127e+05 1.128e+05 1.129e+05 ...
  * time     (time) datetime64[ns] 2005-12-31 2006-12-31 2007-12-31
Data variables:
    var      (time, band, y, x) float32 dask.array&lt;shape=(3, 1, 2334, 4258), chunksize=(1, 1, 2334, 4258)&gt;
</code></pre>

<p>Here is what da.var looks like:</p>

<pre><code>&lt;xarray.DataArray 'var' (time: 3, band: 1, y: 2334, x: 4258)&gt;
dask.array&lt;shape=(3, 1, 2334, 4258), dtype=float32, chunksize=(1, 1, 2334, 4258)&gt;
Coordinates:
  * band     (band) int32 1
  * y        (y) float64 4.406e+06 4.406e+06 4.406e+06 4.406e+06 4.406e+06 ...
  * x        (x) float64 1.125e+05 1.126e+05 1.127e+05 1.128e+05 1.129e+05 ...
  * time     (time) datetime64[ns] 2005-12-31 2006-12-31 2007-12-31
Attributes:
    transform:   (90.0, 0.0, 112500.0, 0.0, -90.0, 4406400.0, 0.0, 0.0, 1.0)
    crs:         +ellps=GRS80 +no_defs +proj=utm +towgs84=0,0,0,0,0,0,0 +unit...
    res:         (90.0, 90.0)
    is_tiled:    1
    nodatavals:  (-9999.0,)
</code></pre>
",308827,21597,01-05-2018 21:14,01-05-2018 21:31,0,21707,431,93,265,96,"{'badge_counts': {'bronze': 431, 'silver': 265, 'gold': 93}, 'account_id': 118069, 'is_employee': False, 'last_modified_date': 1695033300, 'last_access_date': 1710715265, 'reputation_change_year': 538, 'reputation_change_quarter': 538, 'reputation_change_month': 140, 'reputation_change_week': 60, 'reputation_change_day': 0, 'reputation': 21707, 'creation_date': 1270407253, 'user_type': 'registered', 'user_id': 308827, 'accept_rate': 96, 'link': 'https://stackoverflow.com/users/308827/user308827', 'profile_image': 'https://www.gravatar.com/avatar/28f523030f3ecd9c9f2ad23ca745cc65?s=256&d=identicon&r=PG', 'display_name': 'user308827'}",I want to replace values in a variable in an xarray dataset with None. I tried this approach but it did not work: I get this error: Is there something like numpy replace that I could use here? is xarray dataset. here is what looks like: Here is what da.var looks like:,"da[da['var'] == -9999.]['var'] = None
 *** TypeError: unhashable type: 'numpy.ndarray' da da &lt;xarray.Dataset&gt;
Dimensions:  (band: 1, time: 3, x: 4258, y: 2334)
Coordinates:
  * band     (band) int32 1
  * y        (y) float64 4.406e+06 4.406e+06 4.406e+06 4.406e+06 4.406e+06 ...
  * x        (x) float64 1.125e+05 1.126e+05 1.127e+05 1.128e+05 1.129e+05 ...
  * time     (time) datetime64[ns] 2005-12-31 2006-12-31 2007-12-31
Data variables:
    var      (time, band, y, x) float32 dask.array&lt;shape=(3, 1, 2334, 4258), chunksize=(1, 1, 2334, 4258)&gt;
 &lt;xarray.DataArray 'var' (time: 3, band: 1, y: 2334, x: 4258)&gt;
dask.array&lt;shape=(3, 1, 2334, 4258), dtype=float32, chunksize=(1, 1, 2334, 4258)&gt;
Coordinates:
  * band     (band) int32 1
  * y        (y) float64 4.406e+06 4.406e+06 4.406e+06 4.406e+06 4.406e+06 ...
  * x        (x) float64 1.125e+05 1.126e+05 1.127e+05 1.128e+05 1.129e+05 ...
  * time     (time) datetime64[ns] 2005-12-31 2006-12-31 2007-12-31
Attributes:
    transform:   (90.0, 0.0, 112500.0, 0.0, -90.0, 4406400.0, 0.0, 0.0, 1.0)
    crs:         +ellps=GRS80 +no_defs +proj=utm +towgs84=0,0,0,0,0,0,0 +unit...
    res:         (90.0, 90.0)
    is_tiled:    1
    nodatavals:  (-9999.0,)
",17,36,0,0,
656,48178884,48178960,98165,Min-max normalisation of a NumPy array,4,<python><arrays><numpy>,17,"<p>I have the following numpy array:</p>

<pre><code>foo = np.array([[0.0, 10.0], [0.13216, 12.11837], [0.25379, 42.05027], [0.30874, 13.11784]])
</code></pre>

<p>which yields:</p>

<pre><code>[[  0.       10.     ]
 [  0.13216  12.11837]
 [  0.25379  42.05027]
 [  0.30874  13.11784]]
</code></pre>

<p>How can I normalize the Y component of this array. So it gives me something like:</p>

<pre><code>[[  0.       0.   ]
 [  0.13216  0.06 ]
 [  0.25379  1    ]
 [  0.30874  0.097]]
</code></pre>
",513449,2383,10-01-2018 01:02,10-01-2018 01:14,0,2383,51,6,30,67,"{'badge_counts': {'bronze': 51, 'silver': 30, 'gold': 6}, 'account_id': 242134, 'is_employee': False, 'last_modified_date': 1662336001, 'last_access_date': 1709714715, 'reputation_change_year': 28, 'reputation_change_quarter': 28, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2383, 'creation_date': 1290169286, 'user_type': 'registered', 'user_id': 513449, 'accept_rate': 67, 'location': 'London, United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/513449/mbilyanov', 'profile_image': 'https://i.stack.imgur.com/20Eb7.png?s=256&g=1', 'display_name': 'mbilyanov'}",I have the following numpy array: which yields: How can I normalize the Y component of this array. So it gives me something like:,"foo = np.array([[0.0, 10.0], [0.13216, 12.11837], [0.25379, 42.05027], [0.30874, 13.11784]])
 [[  0.       10.     ]
 [  0.13216  12.11837]
 [  0.25379  42.05027]
 [  0.30874  13.11784]]
 [[  0.       0.   ]
 [  0.13216  0.06 ]
 [  0.25379  1    ]
 [  0.30874  0.097]]
",6,20,0,0,
657,48082138,48082201,17645,Is it possible to disable the zoom/pan window on a plotly.py candlestick chart?,4,<python><plotly>,12,"<p>I am creating a candlestick plot using plotly.py. I would like to have a horizontal split and place the candlestick data in the top split and some data curves in the bottom bottom split. I do not need panning and zooming and that lower section of the candlestick chart with the zoom/pan controls is getting in my way.</p>

<p><a href=""https://i.stack.imgur.com/gXbAA.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gXbAA.png"" alt=""enter image description here""></a></p>
",513449,2383,03-01-2018 17:20,03-01-2018 17:24,0,2383,51,6,30,67,"{'badge_counts': {'bronze': 51, 'silver': 30, 'gold': 6}, 'account_id': 242134, 'is_employee': False, 'last_modified_date': 1662336001, 'last_access_date': 1709714715, 'reputation_change_year': 28, 'reputation_change_quarter': 28, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2383, 'creation_date': 1290169286, 'user_type': 'registered', 'user_id': 513449, 'accept_rate': 67, 'location': 'London, United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/513449/mbilyanov', 'profile_image': 'https://i.stack.imgur.com/20Eb7.png?s=256&g=1', 'display_name': 'mbilyanov'}",I am creating a candlestick plot using plotly.py. I would like to have a horizontal split and place the candlestick data in the top split and some data curves in the bottom bottom split. I do not need panning and zooming and that lower section of the candlestick chart with the zoom/pan controls is getting in my way.,,0,3,1,1,
658,49201236,49201237,148054,Check the total number of parameters in a PyTorch model,11,<python><pytorch>,185,"<p>How do I count the total number of parameters in a PyTorch model? Something similar to <code>model.count_params()</code> in Keras.</p>
",604734,24850,09-03-2018 19:55,09-03-2018 19:55,0,24878,100,22,78,100,"{'badge_counts': {'bronze': 100, 'silver': 78, 'gold': 22}, 'collectives': [{'collective': {'tags': ['word2vec', 'nlp', 'gensim', 'nlp-question-answering', 'spacy-3', 'opennlp', 'tf-idf', 'stanford-nlp', 'topic-modeling', 'named-entity-recognition', 'bert-language-model', 'spacy', 'huggingface-transformers', 'sentiment-analysis', 'word-embedding', 'nltk'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective focused on NLP (natural language processing), the transformation or extraction of useful information from natural language data.', 'link': '/collectives/nlp', 'name': 'NLP', 'slug': 'nlp'}, 'role': 'member'}], 'account_id': 298619, 'is_employee': False, 'last_modified_date': 1705929000, 'last_access_date': 1709236086, 'reputation_change_year': 298, 'reputation_change_quarter': 298, 'reputation_change_month': 58, 'reputation_change_week': 28, 'reputation_change_day': 0, 'reputation': 24878, 'creation_date': 1296940209, 'user_type': 'registered', 'user_id': 604734, 'accept_rate': 100, 'location': 'S&#227;o Paulo, State of S&#227;o Paulo, Brazil', 'website_url': 'http://www.fabioperez.com.br/', 'link': 'https://stackoverflow.com/users/604734/f%c3%a1bio-perez', 'profile_image': 'https://www.gravatar.com/avatar/b7b889367f06fa0457298888c3402ee9?s=256&d=identicon&r=PG', 'display_name': 'F&#225;bio Perez'}",How do I count the total number of parameters in a PyTorch model? Something similar to in Keras.,model.count_params(),-1,1,0,0,
659,49030629,49036254,689,Statistical Profiling in Python,3,<python><profiling><usage-statistics>,12,"<p>I would like to know that the Python interpreter is doing in my production environments.</p>

<p>Some time ago I wrote a simple tool called <a href=""https://github.com/guettli/live-trace"" rel=""noreferrer"">live-trace</a> which runs a daemon thread which collects stacktraces every N milliseconds. </p>

<p>But signal handling in the interpreter itself has one disadvantage:</p>

<blockquote>
  <p>Although Python signal handlers are called asynchronously as far as the Python user is concerned, they can only occur between the “atomic” instructions of the Python interpreter. This means that signals arriving during long calculations implemented purely in C (such as regular expression matches on large bodies of text) may be delayed for an arbitrary amount of time.</p>
</blockquote>

<p>Source: <a href=""https://docs.python.org/2/library/signal.html"" rel=""noreferrer"">https://docs.python.org/2/library/signal.html</a></p>

<p>How could I work around above constraint and get a stacktrace, even if the interpreter is in some C code for several seconds?</p>

<p>Related: <a href=""https://github.com/23andMe/djdt-flamegraph/issues/5"" rel=""noreferrer"">https://github.com/23andMe/djdt-flamegraph/issues/5</a></p>
",633961,26880,28-02-2018 13:22,28-02-2018 18:29,0,27092,706,89,373,66,"{'badge_counts': {'bronze': 706, 'silver': 373, 'gold': 89}, 'account_id': 317335, 'is_employee': False, 'last_modified_date': 1709346000, 'last_access_date': 1711173143, 'reputation_change_year': 947, 'reputation_change_quarter': 947, 'reputation_change_month': 302, 'reputation_change_week': 80, 'reputation_change_day': 0, 'reputation': 27092, 'creation_date': 1298630183, 'user_type': 'registered', 'user_id': 633961, 'accept_rate': 66, 'location': 'Chemnitz, Germany', 'website_url': 'http://www.thomas-guettler.de/', 'link': 'https://stackoverflow.com/users/633961/guettli', 'profile_image': 'https://i.stack.imgur.com/j9Krc.jpg?s=256&g=1', 'display_name': 'guettli'}","I would like to know that the Python interpreter is doing in my production environments. Some time ago I wrote a simple tool called live-trace which runs a daemon thread which collects stacktraces every N milliseconds. But signal handling in the interpreter itself has one disadvantage: Although Python signal handlers are called asynchronously as far as the Python user is concerned, they can only occur between the “atomic” instructions of the Python interpreter. This means that signals arriving during long calculations implemented purely in C (such as regular expression matches on large bodies of text) may be delayed for an arbitrary amount of time. Source: https://docs.python.org/2/library/signal.html How could I work around above constraint and get a stacktrace, even if the interpreter is in some C code for several seconds? Related: https://github.com/23andMe/djdt-flamegraph/issues/5",,0,15,0,3,
660,49691298,49691659,948,Chunking big datasets in PyRFC. Possible?,2,<python><saprfc><pyrfc>,11,"<p>Is there a way to do ""chunking"" of big results into several smaller parts with SAP-RFC?</p>

<p>According to these links it seems like you need to implement chunking yourself :-(</p>

<ul>
<li><a href=""https://archive.sap.com/discussions/thread/1416684"" rel=""noreferrer"">https://archive.sap.com/discussions/thread/1416684</a></li>
<li><a href=""https://github.com/SAP/PyRFC/issues/20"" rel=""noreferrer"">https://github.com/SAP/PyRFC/issues/20</a></li>
</ul>

<p>I would like to avoid this, and I hope that there is a way let SAP-RFC library do the chunking.</p>

<p>Use case:</p>

<blockquote>
  <p>The result are 100k rows. I would like to fetch 1k rows until all rows are received.</p>
</blockquote>

<p>I guess it does not matter much, but I will use <a href=""https://github.com/SAP/PyRFC"" rel=""noreferrer"">PyRFC</a> for my code.</p>
",633961,26880,06-04-2018 11:02,06-04-2018 11:21,0,27092,706,89,373,66,"{'badge_counts': {'bronze': 706, 'silver': 373, 'gold': 89}, 'account_id': 317335, 'is_employee': False, 'last_modified_date': 1709346000, 'last_access_date': 1711173143, 'reputation_change_year': 947, 'reputation_change_quarter': 947, 'reputation_change_month': 302, 'reputation_change_week': 80, 'reputation_change_day': 0, 'reputation': 27092, 'creation_date': 1298630183, 'user_type': 'registered', 'user_id': 633961, 'accept_rate': 66, 'location': 'Chemnitz, Germany', 'website_url': 'http://www.thomas-guettler.de/', 'link': 'https://stackoverflow.com/users/633961/guettli', 'profile_image': 'https://i.stack.imgur.com/j9Krc.jpg?s=256&g=1', 'display_name': 'guettli'}","Is there a way to do ""chunking"" of big results into several smaller parts with SAP-RFC? According to these links it seems like you need to implement chunking yourself :-( https://archive.sap.com/discussions/thread/1416684 https://github.com/SAP/PyRFC/issues/20 I would like to avoid this, and I hope that there is a way let SAP-RFC library do the chunking. Use case: The result are 100k rows. I would like to fetch 1k rows until all rows are received. I guess it does not matter much, but I will use PyRFC for my code.",,0,18,0,3,
661,48403207,48403263,3234,Python: Stacktrace vs Traceback,2,<python><terminology>,29,"<p>In the Python world there are two terms which seem to be equal:</p>

<ul>
<li>Stacktrace</li>
<li>Traceback</li>
</ul>

<p>Is there any difference between the two?</p>
",633961,26880,23-01-2018 13:42,23-01-2018 13:45,0,27092,706,89,373,66,"{'badge_counts': {'bronze': 706, 'silver': 373, 'gold': 89}, 'account_id': 317335, 'is_employee': False, 'last_modified_date': 1709346000, 'last_access_date': 1711173143, 'reputation_change_year': 947, 'reputation_change_quarter': 947, 'reputation_change_month': 302, 'reputation_change_week': 80, 'reputation_change_day': 0, 'reputation': 27092, 'creation_date': 1298630183, 'user_type': 'registered', 'user_id': 633961, 'accept_rate': 66, 'location': 'Chemnitz, Germany', 'website_url': 'http://www.thomas-guettler.de/', 'link': 'https://stackoverflow.com/users/633961/guettli', 'profile_image': 'https://i.stack.imgur.com/j9Krc.jpg?s=256&g=1', 'display_name': 'guettli'}",In the Python world there are two terms which seem to be equal: Stacktrace Traceback Is there any difference between the two?,,0,8,0,0,
662,48265926,48267194,21303,Keras: find out the number of layers,3,<python><machine-learning><keras><deep-learning><keras-layer>,27,"<p>Is there a way to get the number of layers (not parameters) in a Keras model?</p>

<p><code>model.summary()</code> is very informative, but it is not straightforward to get the number of layers from it.</p>
",673592,2090,15-01-2018 15:25,15-01-2018 16:38,0,2090,38,1,23,75,"{'badge_counts': {'bronze': 38, 'silver': 23, 'gold': 1}, 'account_id': 342434, 'is_employee': False, 'last_modified_date': 1616199600, 'last_access_date': 1680011521, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2090, 'creation_date': 1300902886, 'user_type': 'registered', 'user_id': 673592, 'accept_rate': 75, 'link': 'https://stackoverflow.com/users/673592/user673592', 'profile_image': 'https://www.gravatar.com/avatar/eb7267c67d9b7bec58fd5a0f28b4669b?s=256&d=identicon&r=PG', 'display_name': 'user673592'}","Is there a way to get the number of layers (not parameters) in a Keras model? is very informative, but it is not straightforward to get the number of layers from it.",model.summary(),-1,3,0,0,
663,49159238,49159268,1729,Why does the **kwargs mapping compare equal with a differently ordered OrderedDict?,4,<python><function><python-3.6><ordereddictionary><keyword-argument>,12,"<p>According to <a href=""https://www.python.org/dev/peps/pep-0468/"" rel=""nofollow noreferrer"">PEP 468</a>:</p>

<blockquote>
  <p>Starting in version 3.6 Python will preserve the order of keyword arguments as passed to a function. To accomplish this the collected <strong>kwargs will now be an ordered mapping</strong>. Note that this does not necessarily mean <code>OrderedDict</code>.</p>
</blockquote>

<p>In that case, why does this ordered mapping fail to respect equality comparison with Python's canonical ordered mapping type, the <code>collections.OrderedDict</code>:</p>

<pre><code>&gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt; data = OrderedDict(zip('xy', 'xy'))
&gt;&gt;&gt; def foo(**kwargs):
...     return kwargs == data
... 
&gt;&gt;&gt; foo(x='x', y='y')  # expected result: True
True
&gt;&gt;&gt; foo(y='y', x='x')  # expected result: False
True
</code></pre>

<p>Although iteration order is now preserved, <code>kwargs</code> seems to be behaving just like a normal dict for the comparisons. Python has a C implemented ordered dict <a href=""https://bugs.python.org/issue16991"" rel=""nofollow noreferrer"">since 3.5</a>, so it could conceivably have been used directly (or, if performance was still a concern, a faster implementation using a thin subclass of the 3.6 compact dict).</p>

<p><strong>Why doesn't the ordered mapping received by a function respect ordering in equality comparisons?</strong></p>
",674039,348162,07-03-2018 18:50,07-03-2018 18:52,0,348698,767,105,631,94,"{'badge_counts': {'bronze': 767, 'silver': 631, 'gold': 105}, 'account_id': 342731, 'is_employee': False, 'last_modified_date': 1710872103, 'last_access_date': 1711143082, 'reputation_change_year': 4383, 'reputation_change_quarter': 4383, 'reputation_change_month': 1127, 'reputation_change_week': 227, 'reputation_change_day': 10, 'reputation': 348698, 'creation_date': 1300923627, 'user_type': 'registered', 'user_id': 674039, 'accept_rate': 94, 'location': 'ℂ&#120153;&#120154;&#120148;&#120146;&#120152;&#120160;, &#120128;&#120131;', 'website_url': 'http://www.wimglenn.com', 'link': 'https://stackoverflow.com/users/674039/wim', 'profile_image': 'https://i.stack.imgur.com/leoFi.gif?s=256&g=1', 'display_name': 'wim'}","According to PEP 468: Starting in version 3.6 Python will preserve the order of keyword arguments as passed to a function. To accomplish this the collected kwargs will now be an ordered mapping. Note that this does not necessarily mean . In that case, why does this ordered mapping fail to respect equality comparison with Python's canonical ordered mapping type, the : Although iteration order is now preserved, seems to be behaving just like a normal dict for the comparisons. Python has a C implemented ordered dict since 3.5, so it could conceivably have been used directly (or, if performance was still a concern, a faster implementation using a thin subclass of the 3.6 compact dict). Why doesn't the ordered mapping received by a function respect ordering in equality comparisons?","OrderedDict collections.OrderedDict &gt;&gt;&gt; from collections import OrderedDict
&gt;&gt;&gt; data = OrderedDict(zip('xy', 'xy'))
&gt;&gt;&gt; def foo(**kwargs):
...     return kwargs == data
... 
&gt;&gt;&gt; foo(x='x', y='y')  # expected result: True
True
&gt;&gt;&gt; foo(y='y', x='x')  # expected result: False
True
 kwargs",5,22,0,2,
664,49062365,49062478,1745,What is the root of the distinction in the meaning of equality between C++ and Python?,3,<python><c++>,15,"<p>There appears to be an almost philosophical difference in the meaning of ""equality"" between C++ and Python. I <a href=""https://stackoverflow.com/q/49060812/704972"">became aware</a> of this distinction through an attempt to do in Python something that is quite difficult in C++: distinguishing between two enum types when they are both just a wrapper for a set of integers, <em>but the issue is broader than enums</em>, hence the present question.</p>

<p>If I write in C++ code such as the following</p>

<pre><code>#include &lt;iostream&gt;

struct Foo {
    bool operator==(const Foo&amp; foo) const { return this == &amp;foo; }
};

struct Bar {};

int main() {
    Foo foo = Foo();
    Bar bar = Bar();
    if (foo == bar) std::cout &lt;&lt; ""ok"" &lt;&lt; std::endl;
}
</code></pre>

<p>I fully expect that the equality comparison will fail. Indeed, it's a compilation error. You can't even compare two objects unless they're, just to get going, of the same type.</p>

<p>And yet it appears that 
""<a href=""https://stackoverflow.com/a/49060836/704972""><em>there is little (no?) precedent in Python for equality comparisons raising errors</em></a>"".</p>

<p>and ""<a href=""https://stackoverflow.com/a/49061174/704972""><em>if [an object] raises every time it is compared to a [object of a different type], it will break any container it is added to</em></a>"".</p>

<p>Indeed, writing</p>

<pre><code>class Foo(object):
    pass


class Bar(object):
    pass


foo = Foo()
bar = Bar()

if (foo == bar):
    print(""equal"")
</code></pre>

<p>reveals that there is no problem in comparing objects that should otherwise be incomparable.</p>

<p>What, philosophically, is the root of this distinction in the meaning of equality between the two languages?</p>

<p><strong>Update</strong></p>

<p>Part of my puzzlement at finding this out about Python is that so far every feature appears to have been designed with the intent of being ""natural"", ""intuitive"", even ""human""—not that these can be defined in the first place.</p>

<p>But consider that you are at the fruit section of a grocery shop and ask one of the aproned chaps: ""Could you tell me whether these oranges are Fuji or Red Delicious?"" Surely no one could make sense of the question to venture an answer one way or the other. So the question is how to provide a response of ""incredulous"" in bits and bytes.</p>

<p><strong>Update 2</strong></p>

<p>(Too long to be a comment to @GiacomoAlzetta's comment) I respect your opinion. Still, from this point on, I will not respect a book on Python that does not dedicate a chapter, or at least a section, to pointing out that <code>3 &lt; [1]</code> is True, and that explains the background (whether historical or philosophical) for this. Being dynamically typed does not mean that one is so very cornered (because, e.g., one has only a handful of available names 'a', 'b', and 'c') to reuse a name for a very different meaning. Ultimately, it's not even a philosophical, but an engineering, issue: How do you remove, or at least reduce, the chance that one among multiple people collaborating on a software project will introduce bugs in the system? Bugs that remain dormant (because the language is dynamically typed—and we cannot predict computation paths) are far worse than bugs that scream ""error"".</p>
",704972,10403,02-03-2018 03:23,02-03-2018 03:38,0,10423,122,15,59,77,"{'badge_counts': {'bronze': 122, 'silver': 59, 'gold': 15}, 'account_id': 362010, 'is_employee': False, 'last_modified_date': 1703299800, 'last_access_date': 1698411166, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 10423, 'creation_date': 1302644661, 'user_type': 'registered', 'user_id': 704972, 'accept_rate': 77, 'location': 'Toronto, ON, Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/704972/calaf', 'profile_image': 'https://www.gravatar.com/avatar/c176240cfcbeda075c655197a2cc8544?s=256&d=identicon&r=PG', 'display_name': 'Calaf'}","There appears to be an almost philosophical difference in the meaning of ""equality"" between C++ and Python. I became aware of this distinction through an attempt to do in Python something that is quite difficult in C++: distinguishing between two enum types when they are both just a wrapper for a set of integers, but the issue is broader than enums, hence the present question. If I write in C++ code such as the following I fully expect that the equality comparison will fail. Indeed, it's a compilation error. You can't even compare two objects unless they're, just to get going, of the same type. And yet it appears that ""there is little (no?) precedent in Python for equality comparisons raising errors"". and ""if [an object] raises every time it is compared to a [object of a different type], it will break any container it is added to"". Indeed, writing reveals that there is no problem in comparing objects that should otherwise be incomparable. What, philosophically, is the root of this distinction in the meaning of equality between the two languages? Update Part of my puzzlement at finding this out about Python is that so far every feature appears to have been designed with the intent of being ""natural"", ""intuitive"", even ""human""—not that these can be defined in the first place. But consider that you are at the fruit section of a grocery shop and ask one of the aproned chaps: ""Could you tell me whether these oranges are Fuji or Red Delicious?"" Surely no one could make sense of the question to venture an answer one way or the other. So the question is how to provide a response of ""incredulous"" in bits and bytes. Update 2 (Too long to be a comment to @GiacomoAlzetta's comment) I respect your opinion. Still, from this point on, I will not respect a book on Python that does not dedicate a chapter, or at least a section, to pointing out that is True, and that explains the background (whether historical or philosophical) for this. Being dynamically typed does not mean that one is so very cornered (because, e.g., one has only a handful of available names 'a', 'b', and 'c') to reuse a name for a very different meaning. Ultimately, it's not even a philosophical, but an engineering, issue: How do you remove, or at least reduce, the chance that one among multiple people collaborating on a software project will introduce bugs in the system? Bugs that remain dormant (because the language is dynamically typed—and we cannot predict computation paths) are far worse than bugs that scream ""error"".","#include &lt;iostream&gt;

struct Foo {
    bool operator==(const Foo&amp; foo) const { return this == &amp;foo; }
};

struct Bar {};

int main() {
    Foo foo = Foo();
    Bar bar = Bar();
    if (foo == bar) std::cout &lt;&lt; ""ok"" &lt;&lt; std::endl;
}
 class Foo(object):
    pass


class Bar(object):
    pass


foo = Foo()
bar = Bar()

if (foo == bar):
    print(""equal"")
 3 &lt; [1]",23,56,0,3,
665,48439159,48444260,40497,failed to create anaconda environment ResolvePackageNotFound,4,<python><anaconda><conda><virtual-environment>,12,"<p>I've been trying to use this yml file to create an environment (I created the yml):</p>

<pre><code>name: testenv
channels:
- esri
- scitools
- obspy
- conda-forge
- defaults
dependencies:
- appnope=0.1.0=py36_0
- libgfortran=3.0.0=0
- pip=9.0.1=py36_0
- python=3.6.2=0
- wheel=0.30.0=py_1
- pip:
  - ipython-genutils==0.2.0
  - jupyter-client==5.1.0
  - jupyter-console==5.1.0
  - jupyter-core==4.3.0
  - prompt-toolkit==1.0.15
</code></pre>

<p>however it always fails with the following error message:</p>

<pre><code>Using Anaconda API: https://api.anaconda.org
Solving environment: failed

ResolvePackageNotFound:
  - wheel==0.30.0=py_1
  - appnope==0.1.0=py36_0
</code></pre>

<p>Is it searching the wrong channels for it? I can find these packages if I simply install them in the base conda install. Thanks for your help.</p>
",845888,1183,25-01-2018 09:07,25-01-2018 13:39,0,1183,33,2,16,85,"{'badge_counts': {'bronze': 33, 'silver': 16, 'gold': 2}, 'account_id': 450515, 'is_employee': False, 'last_modified_date': 1681521300, 'last_access_date': 1687340181, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1183, 'creation_date': 1310709081, 'user_type': 'registered', 'user_id': 845888, 'accept_rate': 85, 'location': 'Oslo, Norway', 'website_url': 'http://johnmaiken.com', 'link': 'https://stackoverflow.com/users/845888/mnky9800n', 'profile_image': 'https://www.gravatar.com/avatar/62a278879009d837b1d64fa011046183?s=256&d=identicon&r=PG', 'display_name': 'mnky9800n'}",I've been trying to use this yml file to create an environment (I created the yml): however it always fails with the following error message: Is it searching the wrong channels for it? I can find these packages if I simply install them in the base conda install. Thanks for your help.,"name: testenv
channels:
- esri
- scitools
- obspy
- conda-forge
- defaults
dependencies:
- appnope=0.1.0=py36_0
- libgfortran=3.0.0=0
- pip=9.0.1=py36_0
- python=3.6.2=0
- wheel=0.30.0=py_1
- pip:
  - ipython-genutils==0.2.0
  - jupyter-client==5.1.0
  - jupyter-console==5.1.0
  - jupyter-core==4.3.0
  - prompt-toolkit==1.0.15
 Using Anaconda API: https://api.anaconda.org
Solving environment: failed

ResolvePackageNotFound:
  - wheel==0.30.0=py_1
  - appnope==0.1.0=py36_0
",23,34,0,0,
666,49228596,49228646,106583,Pandas - Case when & default in pandas,4,<python><pandas><dataframe><conditional-statements>,51,"<p>I have the below case statement in python,</p>
<pre class=""lang-py prettyprint-override""><code>pd_df['difficulty'] = 'Unknown'
pd_df['difficulty'][(pd_df['Time']&lt;30) &amp; (pd_df['Time']&gt;0)] = 'Easy'
pd_df['difficulty'][(pd_df['Time']&gt;=30) &amp; (pd_df['Time']&lt;=60)] = 'Medium'
pd_df['difficulty'][pd_df['Time']&gt;60] = 'Hard'
</code></pre>
<p>But when I run the code, it throws an error.</p>
<pre class=""lang-none prettyprint-override""><code>A value is trying to be set on a copy of a slice from a DataFrame
</code></pre>
",848510,3168,12-03-2018 05:23,12-03-2018 05:27,0,3178,66,7,42,87,"{'badge_counts': {'bronze': 66, 'silver': 42, 'gold': 7}, 'collectives': [{'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 452204, 'is_employee': False, 'last_modified_date': 1710552000, 'last_access_date': 1711004154, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3178, 'creation_date': 1310893185, 'user_type': 'registered', 'user_id': 848510, 'accept_rate': 87, 'location': 'Thiruvananthapuram, Kerala, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/848510/tom-j-muthirenthi', 'profile_image': 'https://i.stack.imgur.com/9FcPS.jpg?s=256&g=1', 'display_name': 'Tom J Muthirenthi'}","I have the below case statement in python, But when I run the code, it throws an error.","pd_df['difficulty'] = 'Unknown'
pd_df['difficulty'][(pd_df['Time']&lt;30) &amp; (pd_df['Time']&gt;0)] = 'Easy'
pd_df['difficulty'][(pd_df['Time']&gt;=30) &amp; (pd_df['Time']&lt;=60)] = 'Medium'
pd_df['difficulty'][pd_df['Time']&gt;60] = 'Hard'
 A value is trying to be set on a copy of a slice from a DataFrame
",3,9,0,0,
667,49322477,49323470,12710,concurrent.futures.ThreadPoolExecutor swallowing exceptions (Python 3.6),3,<python><python-3.x><windows-7-x64><python-multithreading><cpython>,20,"<p>I'm trying to use <code>ThreadPoolExecutor</code> in Python 3.6 on Windows 7 and it seems that the exceptions are silently ignored or stop program execution. Example code:</p>

<pre><code>#!/usr/bin/env python3

from time import sleep

from concurrent.futures import ThreadPoolExecutor

EXECUTOR = ThreadPoolExecutor(2)


def run_jobs():
    EXECUTOR.submit(some_long_task1)
    EXECUTOR.submit(some_long_task2, 'hello', 123)
    return 'Two jobs was launched in background!'


def some_long_task1():
    print(""Task #1 started!"")
    for i in range(10000000):
        j = i + 1
    1/0
    print(""Task #1 is done!"")


def some_long_task2(arg1, arg2):
    print(""Task #2 started with args: %s %s!"" % (arg1, arg2))
    for i in range(10000000):
        j = i + 1
    print(""Task #2 is done!"")


if __name__ == '__main__':
    run_jobs()
    while True:
        sleep(1)
</code></pre>

<p>The output:</p>

<pre><code>Task #1 started!
Task #2 started with args: hello 123!
Task #2 is done!
</code></pre>

<p>It's hanging there until I kill it with <kbd>Ctrl</kbd>+<kbd>C</kbd>. </p>

<p>However, when I remove <code>1/0</code> from <code>some_long_task1</code>, Task #1 completes without problem:</p>

<pre><code>Task #1 started!
Task #2 started with args: hello 123!
Task #1 is done!
Task #2 is done!
</code></pre>

<p>I need to capture the exceptions raised in functions running in <code>ThreadPoolExecutor</code> <em>somehow</em>.</p>

<p>Python 3.6 (Minconda), Windows 7 x64.</p>
",857741,6573,16-03-2018 13:56,16-03-2018 14:45,0,6574,96,10,55,80,"{'badge_counts': {'bronze': 96, 'silver': 55, 'gold': 10}, 'account_id': 785941, 'is_employee': False, 'last_modified_date': 1710768000, 'last_access_date': 1710866174, 'reputation_change_year': 51, 'reputation_change_quarter': 51, 'reputation_change_month': 31, 'reputation_change_week': 1, 'reputation_change_day': 0, 'reputation': 6574, 'creation_date': 1311330892, 'user_type': 'registered', 'user_id': 857741, 'accept_rate': 80, 'website_url': '', 'link': 'https://stackoverflow.com/users/857741/letmesothat4u', 'profile_image': 'https://www.gravatar.com/avatar/1aebaf4fe7c5b3aa038d52f8c6c3301a?s=256&d=identicon&r=PG', 'display_name': 'LetMeSOThat4U'}","I'm trying to use in Python 3.6 on Windows 7 and it seems that the exceptions are silently ignored or stop program execution. Example code: The output: It's hanging there until I kill it with Ctrl+C. However, when I remove from , Task #1 completes without problem: I need to capture the exceptions raised in functions running in somehow. Python 3.6 (Minconda), Windows 7 x64.","ThreadPoolExecutor #!/usr/bin/env python3

from time import sleep

from concurrent.futures import ThreadPoolExecutor

EXECUTOR = ThreadPoolExecutor(2)


def run_jobs():
    EXECUTOR.submit(some_long_task1)
    EXECUTOR.submit(some_long_task2, 'hello', 123)
    return 'Two jobs was launched in background!'


def some_long_task1():
    print(""Task #1 started!"")
    for i in range(10000000):
        j = i + 1
    1/0
    print(""Task #1 is done!"")


def some_long_task2(arg1, arg2):
    print(""Task #2 started with args: %s %s!"" % (arg1, arg2))
    for i in range(10000000):
        j = i + 1
    print(""Task #2 is done!"")


if __name__ == '__main__':
    run_jobs()
    while True:
        sleep(1)
 Task #1 started!
Task #2 started with args: hello 123!
Task #2 is done!
 1/0 some_long_task1 Task #1 started!
Task #2 started with args: hello 123!
Task #1 is done!
Task #2 is done!
 ThreadPoolExecutor",34,58,0,0,
668,49053741,49070017,7873,Accessing the API logger in Flask-Restful's resources,3,<python><rest><flask>,11,"<p>I'm currently using flask-restful (<a href=""http://flask-restful.readthedocs.io/en/0.3.5/index.html"" rel=""noreferrer"">http://flask-restful.readthedocs.io/en/0.3.5/index.html</a>) to deploy resources as endpoints and I'm wondering if there's a way to access the API logger from within then resources classes. I've skimmed through the docs and couldn't find the appropriate answer. </p>

<p>Basically I want to do that : </p>

<pre><code>from flask_restful import Resource 

class SomeEndpoint(Resource):

    def get(self):

        try:

            ... something throws an exception

        except SomeException as se:

            ... send custom message to API logger         &lt;----- Here!

        return response
</code></pre>

<p>What I though of doing was passing the logger from the API through the constructor of the Resource like that : </p>

<pre><code>App = Flask(__name__)
api = Api(App)
api.add_resource(SomeEndpoint, '/', resource_class_kwargs={'logger': App.logger})
</code></pre>

<p>Is this the most appropriate way to access the logger inside flask-restful resource endpoints ?</p>

<p>Thanks a lot</p>
",942696,840,01-03-2018 15:59,02-03-2018 13:23,1,840,24,2,11,74,"{'badge_counts': {'bronze': 24, 'silver': 11, 'gold': 2}, 'account_id': 908239, 'is_employee': False, 'last_modified_date': 1705713300, 'last_access_date': 1696598990, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 840, 'creation_date': 1315922059, 'user_type': 'registered', 'user_id': 942696, 'accept_rate': 74, 'location': 'Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/942696/elcapitaine', 'profile_image': 'https://www.gravatar.com/avatar/e48d0aa3c85edbd4a4d12109a06f9ed6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ElCapitaine'}",I'm currently using flask-restful (http://flask-restful.readthedocs.io/en/0.3.5/index.html) to deploy resources as endpoints and I'm wondering if there's a way to access the API logger from within then resources classes. I've skimmed through the docs and couldn't find the appropriate answer. Basically I want to do that : What I though of doing was passing the logger from the API through the constructor of the Resource like that : Is this the most appropriate way to access the logger inside flask-restful resource endpoints ? Thanks a lot,"from flask_restful import Resource 

class SomeEndpoint(Resource):

    def get(self):

        try:

            ... something throws an exception

        except SomeException as se:

            ... send custom message to API logger         &lt;----- Here!

        return response
 App = Flask(__name__)
api = Api(App)
api.add_resource(SomeEndpoint, '/', resource_class_kwargs={'logger': App.logger})
",16,31,0,1,
669,48404881,48405016,1038,"Unicode subscripts and superscripts in identifiers, why does Python consider XU == Xᵘ == Xᵤ?",2,<python><unicode><syntax><identifier>,19,"<p>Python allows unicode identifiers.  I defined <code>Xᵘ = 42</code>, expecting <code>XU</code> and <code>Xᵤ</code> to result in a <code>NameError</code>.  But in reality, when I define <code>Xᵘ</code>, Python (silently?) turns <code>Xᵘ</code> into <code>Xu</code>, which strikes me as somewhat of an unpythonic thing to do.  Why is this happening?</p>

<pre><code>&gt;&gt;&gt; Xᵘ = 42
&gt;&gt;&gt; print((Xu, Xᵘ, Xᵤ))
(42, 42, 42)
</code></pre>
",974555,25066,23-01-2018 15:08,23-01-2018 15:14,0,25096,177,17,104,67,"{'badge_counts': {'bronze': 177, 'silver': 104, 'gold': 17}, 'account_id': 935589, 'is_employee': False, 'last_modified_date': 1705324800, 'last_access_date': 1710926000, 'reputation_change_year': 451, 'reputation_change_quarter': 451, 'reputation_change_month': 110, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 25096, 'creation_date': 1317479623, 'user_type': 'registered', 'user_id': 974555, 'accept_rate': 67, 'location': 'Heigenbr&#252;cken, Germany', 'website_url': 'http://www.topjaklont.org/', 'link': 'https://stackoverflow.com/users/974555/gerrit', 'profile_image': 'https://i.stack.imgur.com/vAAS0.png?s=256&g=1', 'display_name': 'gerrit'}","Python allows unicode identifiers. I defined , expecting and to result in a . But in reality, when I define , Python (silently?) turns into , which strikes me as somewhat of an unpythonic thing to do. Why is this happening?","Xᵘ = 42 XU Xᵤ NameError Xᵘ Xᵘ Xu &gt;&gt;&gt; Xᵘ = 42
&gt;&gt;&gt; print((Xu, Xᵘ, Xᵤ))
(42, 42, 42)
",-5,6,0,0,
670,49416042,49416074,29879,How to write an f-string on multiple lines without introducing unintended whitespace?,1,<python><f-string>,12,"<p>Consider the following code snippet:</p>

<pre><code>name1 = ""Nadya""
name2 = ""Jim""

def print_string():
    string = f""{name1}\n\
{name2}""
    print(string)

print_string()
</code></pre>

<p>which produces</p>

<pre><code>Nadya
Jim
</code></pre>

<p>This works, but the 'break' in indentation on the second line of the <code>string</code> definition looks ugly. I've found that if I indent the <code>{name2}</code> line, this indentation shows up in the final string.</p>

<p>I'm trying to find a way to continue the f-string on a new line and indent it without the indentation showing up in the final string. Following something similar I've seen for ordinary strings, I've tried</p>

<pre><code>name1 = ""Nadya""
name2 = ""Jim""

def print_string():
    string = f""{name1}\n""
             f""{name2}""
    print(string)

print_string()
</code></pre>

<p>but this leads to an <code>IndentationError: unexpected indent</code>. Is what I am trying possible in another way?</p>
",995862,54143,21-03-2018 20:36,21-03-2018 20:39,0,54393,547,95,320,89,"{'badge_counts': {'bronze': 547, 'silver': 320, 'gold': 95}, 'account_id': 973980, 'is_employee': False, 'last_modified_date': 1703313301, 'last_access_date': 1710970031, 'reputation_change_year': 958, 'reputation_change_quarter': 958, 'reputation_change_month': 320, 'reputation_change_week': 170, 'reputation_change_day': 10, 'reputation': 54393, 'creation_date': 1318612149, 'user_type': 'registered', 'user_id': 995862, 'accept_rate': 89, 'location': 'San Francisco, CA, USA', 'website_url': 'http://www.kurtpeek.com', 'link': 'https://stackoverflow.com/users/995862/kurt-peek', 'profile_image': 'https://www.gravatar.com/avatar/d5efb6c1054808f7a79aaefbc64cce76?s=256&d=identicon&r=PG', 'display_name': 'Kurt Peek'}","Consider the following code snippet: which produces This works, but the 'break' in indentation on the second line of the definition looks ugly. I've found that if I indent the line, this indentation shows up in the final string. I'm trying to find a way to continue the f-string on a new line and indent it without the indentation showing up in the final string. Following something similar I've seen for ordinary strings, I've tried but this leads to an . Is what I am trying possible in another way?","name1 = ""Nadya""
name2 = ""Jim""

def print_string():
    string = f""{name1}\n\
{name2}""
    print(string)

print_string()
 Nadya
Jim
 string {name2} name1 = ""Nadya""
name2 = ""Jim""

def print_string():
    string = f""{name1}\n""
             f""{name2}""
    print(string)

print_string()
 IndentationError: unexpected indent",14,35,0,0,
671,48106347,48106434,17613,"""Apps aren't loaded yet"" when trying to run pytest-django",10,<python><django><pytest><pytest-django>,17,"<p>Using the (partial) polls app from the <a href=""https://docs.djangoproject.com/en/2.0/intro/tutorial01/"" rel=""noreferrer"">Django tutorial</a> as an example, I'm trying to get <a href=""https://pytest-django.readthedocs.io/en/latest/"" rel=""noreferrer"">pytest-django</a> to run.</p>

<p>Using the command <code>django-admin startproject mysite2</code>, I've created a project directory with the following structure:</p>

<pre><code>.
├── db.sqlite3
├── manage.py
├── mysite2
│   ├── __init__.py
│   ├── settings.py
│   ├── urls.py
│   └── wsgi.py
├── polls
│   ├── __init__.py
│   ├── admin.py
│   ├── apps.py
│   ├── migrations
│   │   ├── 0001_initial.py
│   │   └── __init__.py
│   ├── models.py
│   ├── tests.py
│   ├── urls.py
│   └── views.py
└── pytest.ini
</code></pre>

<p>My <code>pytest.ini</code> looks like</p>

<pre><code>[pytest]
DJANGO_SETTINGS_MODULE = mysite2.settings
python_files = tests.py test_*.py *_tests.py
</code></pre>

<p>Following the tutorial, in <code>polls/models.py</code> I've created <code>Question</code> and <code>Choice</code> models:</p>

<pre><code>import datetime

from django.db import models
from django.utils import timezone

class Question(models.Model):
    question_text = models.CharField(max_length=200)
    pub_date = models.DateTimeField('date published')

    def __str__(self):
        return self.question_text

    def was_published_recently(self):
        return self.pub_date &gt;= timezone.now() - datetime.timedelta(days=1)

class Choice(models.Model):
    question = models.ForeignKey(Question, on_delete=models.CASCADE)
    choice_text = models.CharField(max_length=200)
    votes = models.IntegerField(default=0)

    def __str__(self):
        return self.choice_text
</code></pre>

<p>Now, if I make <code>tests.py</code> as described in the tutorial, which is based on Python's built-in <code>unittest</code> module,</p>

<pre><code>import datetime

from django.utils import timezone
from django.test import TestCase

from .models import Question

class QuestionModelTests(TestCase):
    def test_was_published_recently_with_future_question(self):
        time = timezone.now() + datetime.timedelta(days=30)
        future_question = Question(pub_date=time)
        self.assertIs(future_question.was_published_recently(), False)
</code></pre>

<p>and I run <code>python manage.py test</code> from the command line, the test fails expected:</p>

<pre><code>Creating test database for alias 'default'...
System check identified no issues (0 silenced).
F
======================================================================
FAIL: test_was_published_recently_with_future_question (polls.tests.QuestionModelTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/kurtpeek/Documents/Scratch/mysite2/polls/tests.py"", line 23, in test_was_published_recently_with_future_question
    self.assertIs(future_question.was_published_recently(), False)
AssertionError: True is not False

----------------------------------------------------------------------
Ran 1 test in 0.001s

FAILED (failures=1)
Destroying test database for alias 'default'...
</code></pre>

<p>However, if I change the testing code to the (attempted) <code>pytest</code> equivalent (that is, without having to subclass <code>TestCase</code> and with ordinary assertions):</p>

<pre><code>def test_was_published_recently_with_future_question():
    time = timezone.now() + datetime.timedelta(days=30)
    future_question = Question(pub_date=time)
    assert future_question.was_published_recently() is False
</code></pre>

<p>and run the <code>pytest</code> command, I get the following error:</p>

<pre><code>================================= test session starts ==================================
platform darwin -- Python 3.6.3, pytest-3.2.3, py-1.4.34, pluggy-0.4.0
rootdir: /Users/kurtpeek/Documents/Scratch/mysite2, inifile: pytest.ini
plugins: timeout-1.2.1
collected 0 items / 1 errors                                                            

======================================== ERRORS ========================================
___________________________ ERROR collecting polls/tests.py ____________________________
polls/tests.py:10: in &lt;module&gt;
    from .models import Question
polls/models.py:6: in &lt;module&gt;
    class Question(models.Model):
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/django/db/models/base.py:100: in __new__
    app_config = apps.get_containing_app_config(module)
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/django/apps/registry.py:244: in get_containing_app_config
    self.check_apps_ready()
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/django/apps/registry.py:127: in check_apps_ready
    raise AppRegistryNotReady(""Apps aren't loaded yet."")
E   django.core.exceptions.AppRegistryNotReady: Apps aren't loaded yet.
!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!
=============================== 1 error in 0.64 seconds ================================
</code></pre>

<p>So far I haven't been able to find a way to fix this. Any ideas on how to get the test to run?</p>
",995862,54143,05-01-2018 02:19,05-01-2018 02:32,0,54393,547,95,320,89,"{'badge_counts': {'bronze': 547, 'silver': 320, 'gold': 95}, 'account_id': 973980, 'is_employee': False, 'last_modified_date': 1703313301, 'last_access_date': 1710970031, 'reputation_change_year': 958, 'reputation_change_quarter': 958, 'reputation_change_month': 320, 'reputation_change_week': 170, 'reputation_change_day': 10, 'reputation': 54393, 'creation_date': 1318612149, 'user_type': 'registered', 'user_id': 995862, 'accept_rate': 89, 'location': 'San Francisco, CA, USA', 'website_url': 'http://www.kurtpeek.com', 'link': 'https://stackoverflow.com/users/995862/kurt-peek', 'profile_image': 'https://www.gravatar.com/avatar/d5efb6c1054808f7a79aaefbc64cce76?s=256&d=identicon&r=PG', 'display_name': 'Kurt Peek'}","Using the (partial) polls app from the Django tutorial as an example, I'm trying to get pytest-django to run. Using the command , I've created a project directory with the following structure: My looks like Following the tutorial, in I've created and models: Now, if I make as described in the tutorial, which is based on Python's built-in module, and I run from the command line, the test fails expected: However, if I change the testing code to the (attempted) equivalent (that is, without having to subclass and with ordinary assertions): and run the command, I get the following error: So far I haven't been able to find a way to fix this. Any ideas on how to get the test to run?","django-admin startproject mysite2 .
├── db.sqlite3
├── manage.py
├── mysite2
│   ├── __init__.py
│   ├── settings.py
│   ├── urls.py
│   └── wsgi.py
├── polls
│   ├── __init__.py
│   ├── admin.py
│   ├── apps.py
│   ├── migrations
│   │   ├── 0001_initial.py
│   │   └── __init__.py
│   ├── models.py
│   ├── tests.py
│   ├── urls.py
│   └── views.py
└── pytest.ini
 pytest.ini [pytest]
DJANGO_SETTINGS_MODULE = mysite2.settings
python_files = tests.py test_*.py *_tests.py
 polls/models.py Question Choice import datetime

from django.db import models
from django.utils import timezone

class Question(models.Model):
    question_text = models.CharField(max_length=200)
    pub_date = models.DateTimeField('date published')

    def __str__(self):
        return self.question_text

    def was_published_recently(self):
        return self.pub_date &gt;= timezone.now() - datetime.timedelta(days=1)

class Choice(models.Model):
    question = models.ForeignKey(Question, on_delete=models.CASCADE)
    choice_text = models.CharField(max_length=200)
    votes = models.IntegerField(default=0)

    def __str__(self):
        return self.choice_text
 tests.py unittest import datetime

from django.utils import timezone
from django.test import TestCase

from .models import Question

class QuestionModelTests(TestCase):
    def test_was_published_recently_with_future_question(self):
        time = timezone.now() + datetime.timedelta(days=30)
        future_question = Question(pub_date=time)
        self.assertIs(future_question.was_published_recently(), False)
 python manage.py test Creating test database for alias 'default'...
System check identified no issues (0 silenced).
F
======================================================================
FAIL: test_was_published_recently_with_future_question (polls.tests.QuestionModelTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/kurtpeek/Documents/Scratch/mysite2/polls/tests.py"", line 23, in test_was_published_recently_with_future_question
    self.assertIs(future_question.was_published_recently(), False)
AssertionError: True is not False

----------------------------------------------------------------------
Ran 1 test in 0.001s

FAILED (failures=1)
Destroying test database for alias 'default'...
 pytest TestCase def test_was_published_recently_with_future_question():
    time = timezone.now() + datetime.timedelta(days=30)
    future_question = Question(pub_date=time)
    assert future_question.was_published_recently() is False
 pytest ================================= test session starts ==================================
platform darwin -- Python 3.6.3, pytest-3.2.3, py-1.4.34, pluggy-0.4.0
rootdir: /Users/kurtpeek/Documents/Scratch/mysite2, inifile: pytest.ini
plugins: timeout-1.2.1
collected 0 items / 1 errors                                                            

======================================== ERRORS ========================================
___________________________ ERROR collecting polls/tests.py ____________________________
polls/tests.py:10: in &lt;module&gt;
    from .models import Question
polls/models.py:6: in &lt;module&gt;
    class Question(models.Model):
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/django/db/models/base.py:100: in __new__
    app_config = apps.get_containing_app_config(module)
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/django/apps/registry.py:244: in get_containing_app_config
    self.check_apps_ready()
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/django/apps/registry.py:127: in check_apps_ready
    raise AppRegistryNotReady(""Apps aren't loaded yet."")
E   django.core.exceptions.AppRegistryNotReady: Apps aren't loaded yet.
!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!
=============================== 1 error in 0.64 seconds ================================
",80,129,0,2,
672,48814830,48815002,5592,How to test Django's UpdateView?,3,<python><django>,21,"<p>As a simplified example, I've written an <code>UpdateView</code> for a <code>Book</code> model, as well as a <code>ListView</code> to redirect to upon success:</p>

<pre><code>from django.urls import reverse
from django.views.generic import ListView
from django.views.generic.edit import UpdateView
from .models import Book


class BookUpdate(UpdateView):
    model = Book
    fields = ['title', 'author']


class BookList(ListView):
    model = Book
</code></pre>

<p>The <code>Book</code> model is defined as</p>

<pre><code>class Book(models.Model):
    title = models.CharField(max_length=100)
    author = models.CharField(max_length=100, blank=True)

    def get_absolute_url(self):
        return reverse('books-list')
</code></pre>

<p>where <code>urls.py</code> is</p>

<pre><code>from django.urls import path
from books.views import BookUpdate, BookList


urlpatterns = [
    path('books/', BookList.as_view(), name='books-list'),
    path('book/&lt;int:pk&gt;/', BookUpdate.as_view(), name='book-update')
]
</code></pre>

<p>In <code>books/tests.py</code> I've tried to write the following test:</p>

<pre><code>class BookUpdateTest(TestCase):
    def test_update_book(self):
        book = Book.objects.create(title='The Catcher in the Rye')

        response = self.client.post(
            reverse('book-update', kwargs={'pk': book.id}), 
            {'author': 'J.D. Salinger'})

        self.assertEqual(response.status_code, 200)

        book.refresh_from_db()
        self.assertEqual(book.author, 'J.D. Salinger')
</code></pre>

<p>However, this test fails because the <code>book</code>'s <code>author</code> appears not to be updated after the <code>POST</code> request, even after refreshing from the database:</p>

<pre><code>FAIL: test_update_book (books.tests.BookUpdateTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/kurtpeek/Documents/Scratch/book_project/books/tests.py"", line 46, in test_update_book
    self.assertEqual(book.author, 'J.D. Salinger')
AssertionError: '' != 'J.D. Salinger'
+ J.D. Salinger
</code></pre>

<p>On the other hand, if I run the development server and fill out the fields manually, everything seems to work as expected. How can I write a unit test for the <code>UpdateView</code> which captures the user updating the fields, submitting the form, and making changes to the corresponding objects?</p>
",995862,54143,15-02-2018 19:25,15-02-2018 19:37,0,54393,547,95,320,89,"{'badge_counts': {'bronze': 547, 'silver': 320, 'gold': 95}, 'account_id': 973980, 'is_employee': False, 'last_modified_date': 1703313301, 'last_access_date': 1710970031, 'reputation_change_year': 958, 'reputation_change_quarter': 958, 'reputation_change_month': 320, 'reputation_change_week': 170, 'reputation_change_day': 10, 'reputation': 54393, 'creation_date': 1318612149, 'user_type': 'registered', 'user_id': 995862, 'accept_rate': 89, 'location': 'San Francisco, CA, USA', 'website_url': 'http://www.kurtpeek.com', 'link': 'https://stackoverflow.com/users/995862/kurt-peek', 'profile_image': 'https://www.gravatar.com/avatar/d5efb6c1054808f7a79aaefbc64cce76?s=256&d=identicon&r=PG', 'display_name': 'Kurt Peek'}","As a simplified example, I've written an for a model, as well as a to redirect to upon success: The model is defined as where is In I've tried to write the following test: However, this test fails because the 's appears not to be updated after the request, even after refreshing from the database: On the other hand, if I run the development server and fill out the fields manually, everything seems to work as expected. How can I write a unit test for the which captures the user updating the fields, submitting the form, and making changes to the corresponding objects?","UpdateView Book ListView from django.urls import reverse
from django.views.generic import ListView
from django.views.generic.edit import UpdateView
from .models import Book


class BookUpdate(UpdateView):
    model = Book
    fields = ['title', 'author']


class BookList(ListView):
    model = Book
 Book class Book(models.Model):
    title = models.CharField(max_length=100)
    author = models.CharField(max_length=100, blank=True)

    def get_absolute_url(self):
        return reverse('books-list')
 urls.py from django.urls import path
from books.views import BookUpdate, BookList


urlpatterns = [
    path('books/', BookList.as_view(), name='books-list'),
    path('book/&lt;int:pk&gt;/', BookUpdate.as_view(), name='book-update')
]
 books/tests.py class BookUpdateTest(TestCase):
    def test_update_book(self):
        book = Book.objects.create(title='The Catcher in the Rye')

        response = self.client.post(
            reverse('book-update', kwargs={'pk': book.id}), 
            {'author': 'J.D. Salinger'})

        self.assertEqual(response.status_code, 200)

        book.refresh_from_db()
        self.assertEqual(book.author, 'J.D. Salinger')
 book author POST FAIL: test_update_book (books.tests.BookUpdateTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/kurtpeek/Documents/Scratch/book_project/books/tests.py"", line 46, in test_update_book
    self.assertEqual(book.author, 'J.D. Salinger')
AssertionError: '' != 'J.D. Salinger'
+ J.D. Salinger
 UpdateView",31,67,0,0,
673,48837006,48837030,30820,Add uuid to a new column in a pandas DataFrame,5,<python><python-3.x><pandas><dataframe><uuid>,37,"<p>I'm looking to add a uuid for every row in a single new column in a pandas DataFrame. This obviously fills the column with the same uuid:</p>

<pre><code>import uuid
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(4,3), columns=list('abc'),
                  index=['apple', 'banana', 'cherry', 'date'])
df['uuid'] = uuid.uuid4()
print(df)

               a         b         c                                  uuid
apple   0.687601 -1.332904 -0.166018  34115445-c4b8-4e64-bc96-e120abda1653
banana -2.252191 -0.844470  0.384140  34115445-c4b8-4e64-bc96-e120abda1653
cherry -0.470388  0.642342  0.692454  34115445-c4b8-4e64-bc96-e120abda1653
date   -0.943255  1.450051 -0.296499  34115445-c4b8-4e64-bc96-e120abda1653
</code></pre>

<p>What I am looking for is a new uuid in each row of the 'uuid' column. I have also tried using .apply() and .map() without success. </p>
",1141844,1157,17-02-2018 01:18,17-02-2018 01:22,0,1157,23,2,14,100,"{'badge_counts': {'bronze': 23, 'silver': 14, 'gold': 2}, 'account_id': 1162645, 'is_employee': False, 'last_modified_date': 1607614969, 'last_access_date': 1701119820, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1157, 'creation_date': 1326232165, 'user_type': 'registered', 'user_id': 1141844, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/1141844/tankofvines', 'profile_image': 'https://www.gravatar.com/avatar/5445a17d0dc90d14a98c9cd9523ab24d?s=256&d=identicon&r=PG', 'display_name': 'TankofVines'}",I'm looking to add a uuid for every row in a single new column in a pandas DataFrame. This obviously fills the column with the same uuid: What I am looking for is a new uuid in each row of the 'uuid' column. I have also tried using .apply() and .map() without success.,"import uuid
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(4,3), columns=list('abc'),
                  index=['apple', 'banana', 'cherry', 'date'])
df['uuid'] = uuid.uuid4()
print(df)

               a         b         c                                  uuid
apple   0.687601 -1.332904 -0.166018  34115445-c4b8-4e64-bc96-e120abda1653
banana -2.252191 -0.844470  0.384140  34115445-c4b8-4e64-bc96-e120abda1653
cherry -0.470388  0.642342  0.692454  34115445-c4b8-4e64-bc96-e120abda1653
date   -0.943255  1.450051 -0.296499  34115445-c4b8-4e64-bc96-e120abda1653
",13,19,0,0,
674,49862648,49862912,4124,Why do glob.glob and pathlib.Path.glob treat hidden files differently?,1,<python><glob><hidden-files>,13,"<p>Consider this folder containing two files:</p>

<pre><code>test/
    foo
    .bar
</code></pre>

<p>Calling <a href=""https://docs.python.org/3/library/glob.html#glob.glob"" rel=""noreferrer""><code>glob.glob('*')</code></a> on this folder won't list the hidden <code>.bar</code> file:</p>

<pre><code>&gt;&gt;&gt; glob.glob('test/*')
['test/foo']
</code></pre>

<p>But <a href=""https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob"" rel=""noreferrer""><code>pathlib.Path.glob('*')</code></a> will:</p>

<pre><code>&gt;&gt;&gt; list(Path('test').glob('*'))
[PosixPath('test/.bar'), PosixPath('test/foo')]
</code></pre>

<p>I'd like to know if this is intended or possibly a bug/oversight.</p>

<hr>

<p>The <a href=""https://docs.python.org/3/library/glob.html"" rel=""noreferrer""><code>glob</code> module documentation</a> states that files starting with a dot are special cased:</p>

<blockquote>
  <p>glob treats filenames beginning with a dot (.) as special cases</p>
</blockquote>

<p>Meaning that the result given by <code>glob.glob('*')</code> is intended. But what about pathlib's <code>glob</code>? I couldn't find any relevant information in the docs. Is this the intended behavior? Shouldn't both functions produce the same results?</p>
",1222951,41267,16-04-2018 17:00,16-04-2018 17:16,0,41387,156,13,105,88,"{'badge_counts': {'bronze': 156, 'silver': 105, 'gold': 13}, 'account_id': 1265583, 'is_employee': False, 'last_modified_date': 1703300400, 'last_access_date': 1707255249, 'reputation_change_year': 660, 'reputation_change_quarter': 660, 'reputation_change_month': 190, 'reputation_change_week': 70, 'reputation_change_day': 0, 'reputation': 41387, 'creation_date': 1329815496, 'user_type': 'registered', 'user_id': 1222951, 'accept_rate': 88, 'location': 'Austria', 'website_url': '', 'link': 'https://stackoverflow.com/users/1222951/aran-fey', 'profile_image': 'https://i.stack.imgur.com/s64JD.png?s=256&g=1', 'display_name': 'Aran-Fey'}",Consider this folder containing two files: Calling on this folder won't list the hidden file: But will: I'd like to know if this is intended or possibly a bug/oversight. The module documentation states that files starting with a dot are special cased: glob treats filenames beginning with a dot (.) as special cases Meaning that the result given by is intended. But what about pathlib's ? I couldn't find any relevant information in the docs. Is this the intended behavior? Shouldn't both functions produce the same results?,"test/
    foo
    .bar
 glob.glob('*') .bar &gt;&gt;&gt; glob.glob('test/*')
['test/foo']
 pathlib.Path.glob('*') &gt;&gt;&gt; list(Path('test').glob('*'))
[PosixPath('test/.bar'), PosixPath('test/foo')]
 glob glob.glob('*') glob",-2,30,0,3,
675,49260393,49261966,14318,Django: Filter a Queryset made of unions not working,3,<python><django>,20,"<p>I defined 3 models related with M2M relationsships</p>

<pre><code>class Suite(models.Model):
    name = models.CharField(max_length=250)
    title = models.CharField(max_length=250)
    icon = models.CharField(max_length=250)

    def __str__(self):
        return self.title



class Role(models.Model):
    name = models.CharField(max_length=250)
    title = models.CharField(max_length=250)
    suites = models.ManyToManyField(Suite)
    services = models.ManyToManyField(Service)
    Actions = models.ManyToManyField(Action)
    users = models.ManyToManyField(User)

    def __str__(self):
        return self.title
</code></pre>

<p>In one of my views I tried to collect all the <strong>Suite</strong>s related to an specific <strong>User</strong>. The user may be related to several <strong>Role</strong>s that can contain many <strong>Suite</strong>s. And then filter <strong>Suite</strong>s by name. But the filter seem to have no effects</p>

<pre><code>queryset = Suite.objects.union(*(role.suites.all() for role in 
self.get_user().role_set.all()))
repr(self.queryset)

'&lt;QuerySet [&lt;Suite: energia&gt;, &lt;Suite: waste 4 thing&gt;]&gt;'

self.queryset = self.queryset.filter(name=""energia"")
repr(self.queryset)

'&lt;QuerySet [&lt;Suite: energia&gt;, &lt;Suite: waste 4 thing&gt;]&gt;'
</code></pre>

<p>The query atribute inside the queryset  not alter its content before executin the filter:</p>

<pre><code>(SELECT ""navbar_suite"".""id"", ""navbar_suite"".""name"", ""navbar_suite"".""title"", ""navbar_suite"".""icon"" FROM ""navbar_suite"") UNION (SELECT ""navbar_suite"".""id"", ""navbar_suite"".""name"", ""navbar_suite"".""title"", ""navbar_suite"".""icon"" FROM ""navbar_suite"" INNER JOIN ""navbar_role_suites"" ON (""navbar_suite"".""id"" = ""navbar_role_suites"".""suite_id"") WHERE ""navbar_role_suites"".""role_id"" = 1)

(SELECT ""navbar_suite"".""id"", ""navbar_suite"".""name"", ""navbar_suite"".""title"", ""navbar_suite"".""icon"" FROM ""navbar_suite"") UNION (SELECT ""navbar_suite"".""id"", ""navbar_suite"".""name"", ""navbar_suite"".""title"", ""navbar_suite"".""icon"" FROM ""navbar_suite"" INNER JOIN ""navbar_role_suites"" ON (""navbar_suite"".""id"" = ""navbar_role_suites"".""suite_id"") WHERE ""navbar_role_suites"".""role_id"" = 1)
</code></pre>
",1075163,909,13-03-2018 15:48,13-03-2018 17:08,0,909,27,2,12,89,"{'badge_counts': {'bronze': 27, 'silver': 12, 'gold': 2}, 'account_id': 1077310, 'is_employee': False, 'last_modified_date': 1700269800, 'last_access_date': 1710847264, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 909, 'creation_date': 1322733747, 'user_type': 'registered', 'user_id': 1075163, 'accept_rate': 89, 'website_url': '', 'link': 'https://stackoverflow.com/users/1075163/nasgar', 'profile_image': 'https://www.gravatar.com/avatar/770ceeca851ad03aa4fd822678d75f2b?s=256&d=identicon&r=PG', 'display_name': 'Nasgar'}",I defined 3 models related with M2M relationsships In one of my views I tried to collect all the Suites related to an specific User. The user may be related to several Roles that can contain many Suites. And then filter Suites by name. But the filter seem to have no effects The query atribute inside the queryset not alter its content before executin the filter:,"class Suite(models.Model):
    name = models.CharField(max_length=250)
    title = models.CharField(max_length=250)
    icon = models.CharField(max_length=250)

    def __str__(self):
        return self.title



class Role(models.Model):
    name = models.CharField(max_length=250)
    title = models.CharField(max_length=250)
    suites = models.ManyToManyField(Suite)
    services = models.ManyToManyField(Service)
    Actions = models.ManyToManyField(Action)
    users = models.ManyToManyField(User)

    def __str__(self):
        return self.title
 queryset = Suite.objects.union(*(role.suites.all() for role in 
self.get_user().role_set.all()))
repr(self.queryset)

'&lt;QuerySet [&lt;Suite: energia&gt;, &lt;Suite: waste 4 thing&gt;]&gt;'

self.queryset = self.queryset.filter(name=""energia"")
repr(self.queryset)

'&lt;QuerySet [&lt;Suite: energia&gt;, &lt;Suite: waste 4 thing&gt;]&gt;'
 (SELECT ""navbar_suite"".""id"", ""navbar_suite"".""name"", ""navbar_suite"".""title"", ""navbar_suite"".""icon"" FROM ""navbar_suite"") UNION (SELECT ""navbar_suite"".""id"", ""navbar_suite"".""name"", ""navbar_suite"".""title"", ""navbar_suite"".""icon"" FROM ""navbar_suite"" INNER JOIN ""navbar_role_suites"" ON (""navbar_suite"".""id"" = ""navbar_role_suites"".""suite_id"") WHERE ""navbar_role_suites"".""role_id"" = 1)

(SELECT ""navbar_suite"".""id"", ""navbar_suite"".""name"", ""navbar_suite"".""title"", ""navbar_suite"".""icon"" FROM ""navbar_suite"") UNION (SELECT ""navbar_suite"".""id"", ""navbar_suite"".""name"", ""navbar_suite"".""title"", ""navbar_suite"".""icon"" FROM ""navbar_suite"" INNER JOIN ""navbar_role_suites"" ON (""navbar_suite"".""id"" = ""navbar_role_suites"".""suite_id"") WHERE ""navbar_role_suites"".""role_id"" = 1)
",30,44,0,0,
676,49692157,49749764,1666,Boost.Python return python object which references to existing c++ objects,1,<python><c++><boost><boost-python>,11,"<p>Suppose I want to get the reference to some global / internal c++ object, one method is to declare function with <code>boost::python::return_value_policy&lt;reference_existing_object&gt;()</code>.</p>

<p>Both <code>GetGlobalObjectA</code> and <code>GetGlobalObjectB</code> return the reference to the original c++ object without create a new copy;</p>

<p>But how to make <code>GetGlobalObjectByID</code> return a ref to the existing c++ object?</p>

<pre>
struct A { uint32_t value; };
struct B { uint64_t value; };

A globalA;
B globalB;

boost::python::object GetGlobalObjectByID(int id)
{
    // boost::python::object will return a new copy of C++ object, not the global one.

    if (id == 1)
        return boost::python::object(&globalA);
    else if (id == 2)
        return boost::python::object(&globalB);
    else
        return boost::python::object(nullptr);
}

A& GetGlobalObjectA() { return globalA; }
B& GetGlobalObjectB() { return globalB; }

BOOST_PYTHON_MODULE(myModule)
{
    using namespace boost::python;

    class_&ltA&gt(""A"");
    class_&ltB&gt(""B"");

    def(""GetGlobalObjectByID"", GetGlobalObjectByID);

    def(""GetGlobalObjectA"", GetGlobalObjectA, return_value_policy&ltreference_existing_object&gt());
    def(""GetGlobalObjectB"", GetGlobalObjectB, return_value_policy&ltreference_existing_object&gt());

}

</pre>
",979013,4515,06-04-2018 11:49,10-04-2018 09:14,4,4515,35,3,20,67,"{'badge_counts': {'bronze': 35, 'silver': 20, 'gold': 3}, 'account_id': 553743, 'is_employee': False, 'last_modified_date': 1700470806, 'last_access_date': 1659525080, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4515, 'creation_date': 1317750315, 'user_type': 'registered', 'user_id': 979013, 'accept_rate': 67, 'link': 'https://stackoverflow.com/users/979013/lostyzd', 'profile_image': 'https://www.gravatar.com/avatar/2830359dd0a7f03e41e01f99a9c6be95?s=256&d=identicon&r=PG', 'display_name': 'lostyzd'}","Suppose I want to get the reference to some global / internal c++ object, one method is to declare function with . Both and return the reference to the original c++ object without create a new copy; But how to make return a ref to the existing c++ object? struct A { uint32_t value; }; struct B { uint64_t value; }; A globalA; B globalB; boost::python::object GetGlobalObjectByID(int id) { // boost::python::object will return a new copy of C++ object, not the global one. if (id == 1) return boost::python::object(&globalA); else if (id == 2) return boost::python::object(&globalB); else return boost::python::object(nullptr); } A& GetGlobalObjectA() { return globalA; } B& GetGlobalObjectB() { return globalB; } BOOST_PYTHON_MODULE(myModule) { using namespace boost::python; class_&ltA&gt(""A""); class_&ltB&gt(""B""); def(""GetGlobalObjectByID"", GetGlobalObjectByID); def(""GetGlobalObjectA"", GetGlobalObjectA, return_value_policy&ltreference_existing_object&gt()); def(""GetGlobalObjectB"", GetGlobalObjectB, return_value_policy&ltreference_existing_object&gt()); }",boost::python::return_value_policy&lt;reference_existing_object&gt;() GetGlobalObjectA GetGlobalObjectB GetGlobalObjectByID,-4,43,0,0,
677,50292460,50292552,39511,How to apply a condition to pandas iloc,2,<python><pandas><dataframe>,11,"<p>I select columns <code>2 - end</code> from a pandas DataFrame with <code>iloc</code> as</p>

<pre><code>d=c.iloc[:,2:]
</code></pre>

<p>now how can I apply a condition to this selection? For example, if <code>column1==1</code>.</p>
",943340,15417,11-05-2018 12:33,11-05-2018 12:38,0,15427,238,44,136,84,"{'badge_counts': {'bronze': 238, 'silver': 136, 'gold': 44}, 'account_id': 909063, 'is_employee': False, 'last_modified_date': 1694433600, 'last_access_date': 1711070095, 'reputation_change_year': 140, 'reputation_change_quarter': 140, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 15427, 'creation_date': 1315945144, 'user_type': 'registered', 'user_id': 943340, 'accept_rate': 84, 'location': 'United States', 'website_url': 'http://www.phpfreaks.com/', 'link': 'https://stackoverflow.com/users/943340/googlebot', 'profile_image': 'https://www.gravatar.com/avatar/f7b5b9f0164c4854a8c9f148ddc97c8e?s=256&d=identicon&r=PG', 'display_name': 'Googlebot'}","I select columns from a pandas DataFrame with as now how can I apply a condition to this selection? For example, if .","2 - end iloc d=c.iloc[:,2:]
 column1==1",-3,6,0,0,
678,49133234,49156417,30790,Docker ENTRYPOINT with ENV variable and optional arguments,2,<python><docker><dockerfile>,18,"<p>I have a Dockerfile with an ENTRYPOINT that uses an ENV variable. I can't get the ENTRYPOINT structured so the container can also accept additional command line arguments. Here is the relevant part of the Dockerfile:</p>

<pre><code>ARG MODULE_NAME
ENV MODULE_NAME=$MODULE_NAME
ENTRYPOINT /usr/bin/python3 -m ${MODULE_NAME}
</code></pre>

<p>That works fine if I just want to launch the container without additional arguments:</p>

<pre><code>docker run my-image
</code></pre>

<p>But I need to be able to pass additional command line arguments (e.g., a ""--debug"" flag) to the python process like this:</p>

<pre><code>docker run my-image --debug
</code></pre>

<p>With the form of ENTRYPOINT above, the ""--debug"" arg is not passed to the python process. I've tried both the exec form and the shell form of ENTRYPOINT but can't get it to work with both the ENV variable <em>and</em> command line args. A few other forms I tried:</p>

<p>This runs but doesn't accept additional args:</p>

<pre><code>ENTRYPOINT [""/bin/bash"", ""-c"", ""/usr/bin/python3 -m ${MODULE_NAME}""]
</code></pre>

<p>This gives ""/usr/bin/python3: No module named  ${MODULE_NAME}"":</p>

<pre><code>ENTRYPOINT [""/usr/bin/python3"", ""-m ${MODULE_NAME}""]
</code></pre>

<p>This gives ""/usr/bin/python3: No module named  ${MODULE_NAME}"":</p>

<pre><code>ENTRYPOINT [""/usr/bin/python3"", ""-m"", ""${MODULE_NAME}""]
</code></pre>
",1361822,18919,06-03-2018 14:32,07-03-2018 16:07,1,18969,49,6,54,78,"{'badge_counts': {'bronze': 49, 'silver': 54, 'gold': 6}, 'account_id': 1441946, 'is_employee': False, 'last_modified_date': 1710246000, 'last_access_date': 1711131406, 'reputation_change_year': 160, 'reputation_change_quarter': 160, 'reputation_change_month': 60, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 18969, 'creation_date': 1335552542, 'user_type': 'registered', 'user_id': 1361822, 'accept_rate': 78, 'website_url': '', 'link': 'https://stackoverflow.com/users/1361822/bogatron', 'profile_image': 'https://www.gravatar.com/avatar/273deb3577b8c3aa4d669528784b3e12?s=256&d=identicon&r=PG', 'display_name': 'bogatron'}","I have a Dockerfile with an ENTRYPOINT that uses an ENV variable. I can't get the ENTRYPOINT structured so the container can also accept additional command line arguments. Here is the relevant part of the Dockerfile: That works fine if I just want to launch the container without additional arguments: But I need to be able to pass additional command line arguments (e.g., a ""--debug"" flag) to the python process like this: With the form of ENTRYPOINT above, the ""--debug"" arg is not passed to the python process. I've tried both the exec form and the shell form of ENTRYPOINT but can't get it to work with both the ENV variable and command line args. A few other forms I tried: This runs but doesn't accept additional args: This gives ""/usr/bin/python3: No module named ${MODULE_NAME}"": This gives ""/usr/bin/python3: No module named ${MODULE_NAME}"":","ARG MODULE_NAME
ENV MODULE_NAME=$MODULE_NAME
ENTRYPOINT /usr/bin/python3 -m ${MODULE_NAME}
 docker run my-image
 docker run my-image --debug
 ENTRYPOINT [""/bin/bash"", ""-c"", ""/usr/bin/python3 -m ${MODULE_NAME}""]
 ENTRYPOINT [""/usr/bin/python3"", ""-m ${MODULE_NAME}""]
 ENTRYPOINT [""/usr/bin/python3"", ""-m"", ""${MODULE_NAME}""]
",2,33,0,0,
679,48887912,48888152,35383,Find minimum distance between points of two lists in Python,5,<python><optimization>,15,"<p>I have two lists of coordinates:</p>

<pre><code>s1 = [(0,0), (0,1), (1,0), (1,1)]
s2 = [(3,2), (1,9)]
</code></pre>

<p>I want to calculate the minimum distance of each point in s1 to any point in s2. e.g. the results should be as follows. </p>

<pre><code>result = [3.60, 3.16, 2.82, 2.23]
</code></pre>

<p><strong>Question:</strong> What is the most optimized way in terms of execution time, to achieve this result?</p>

<p>So far I've tried this but the execution time is not promising:</p>

<pre><code>import math
def nearestDistance(boundary, p):
    minDistList = map(lambda b: (b[0] - p[0])**2 + (b[1] - p[1])**2, boundary)
    minDist2 = min(minDistList)
    return math.sqrt(float(minDist2))

d = []
for p in s1:
    d.append(nearestDistance(s2, p))
</code></pre>

<p>Should I change the structure of s1 and s2 (instead of points use 2d arrays for example)? </p>
",911391,2409,20-02-2018 14:48,20-02-2018 15:02,0,2409,55,7,31,65,"{'badge_counts': {'bronze': 55, 'silver': 31, 'gold': 7}, 'account_id': 542524, 'is_employee': False, 'last_modified_date': 1710554100, 'last_access_date': 1705168906, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2409, 'creation_date': 1314255780, 'user_type': 'registered', 'user_id': 911391, 'accept_rate': 65, 'website_url': '', 'link': 'https://stackoverflow.com/users/911391/orak', 'profile_image': 'https://www.gravatar.com/avatar/961538ec11ff8946111330094a9945ce?s=256&d=identicon&r=PG', 'display_name': 'orak'}","I have two lists of coordinates: I want to calculate the minimum distance of each point in s1 to any point in s2. e.g. the results should be as follows. Question: What is the most optimized way in terms of execution time, to achieve this result? So far I've tried this but the execution time is not promising: Should I change the structure of s1 and s2 (instead of points use 2d arrays for example)?","s1 = [(0,0), (0,1), (1,0), (1,1)]
s2 = [(3,2), (1,9)]
 result = [3.60, 3.16, 2.82, 2.23]
 import math
def nearestDistance(boundary, p):
    minDistList = map(lambda b: (b[0] - p[0])**2 + (b[1] - p[1])**2, boundary)
    minDist2 = min(minDistList)
    return math.sqrt(float(minDist2))

d = []
for p in s1:
    d.append(nearestDistance(s2, p))
",9,27,0,0,
680,48997644,48997741,19598,How to describe columns as categorical values?,3,<python><pandas><dataframe><describe>,13,"<p>I have a pandas dataframe that contains a mix of categorical and numeric columns. By default, <code>df.describe()</code> returns only a summary of the numerical data (describing those columns with <code>count</code>, <code>mean</code>, <code>std</code>, <code>min</code>, <code>quantiles</code>, <code>max</code>)</p>

<p>when iterating through all the columns in the df and describing them individually as <code>[df[c].describe() for c in df.columns]</code>  the description is returned based off of specific column dtype; i.e. numerical summary for <code>int</code> and <code>float</code> and categoric summary for <code>object</code></p>

<p>Does any one know of a succinct way of describing all columns as categorical with <code>count</code>, <code>unique</code>, <code>top</code>, <code>freq</code>?</p>
",1434041,6978,26-02-2018 21:46,26-02-2018 21:54,0,6998,77,9,53,65,"{'badge_counts': {'bronze': 77, 'silver': 53, 'gold': 9}, 'account_id': 1538845, 'is_employee': False, 'last_modified_date': 1632766200, 'last_access_date': 1711078623, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 6998, 'creation_date': 1338758051, 'user_type': 'registered', 'user_id': 1434041, 'accept_rate': 65, 'location': 'New York, NY, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1434041/zahra', 'profile_image': 'https://i.stack.imgur.com/8dMvM.jpg?s=256&g=1', 'display_name': 'Zahra'}","I have a pandas dataframe that contains a mix of categorical and numeric columns. By default, returns only a summary of the numerical data (describing those columns with , , , , , ) when iterating through all the columns in the df and describing them individually as the description is returned based off of specific column dtype; i.e. numerical summary for and and categoric summary for Does any one know of a succinct way of describing all columns as categorical with , , , ?",df.describe() count mean std min quantiles max [df[c].describe() for c in df.columns] int float object count unique top freq,-15,5,0,0,
681,48903470,48912748,5070,Why find_packages(exclude=xxx) does not work when doing setup.py sdist?,2,<python><setup.py>,12,"<p>I am packaging my source code, but I do not want to include tests and docs because it will be too big.</p>

<p>To do that I include in my setup.py:</p>

<pre><code>setup(...
      packages=find_packages(exclude=['tests.*','tests','docs.*','docs']),
      ...
)
</code></pre>

<p>When doing a </p>

<pre><code>python setup.py sdist
</code></pre>

<p>I can see that my root tests/ and docs/ dirs and everything inside are still included in the generated distribution.</p>

<p>It seems that only</p>

<pre><code>python setup.py bdist
</code></pre>

<p>is sensible to the exclude parameter.</p>

<p>Why ? is it possible to exclude dirs for 'setup.py sdist' ?</p>
",755371,4961,21-02-2018 10:24,21-02-2018 18:11,0,4961,62,6,33,44,"{'badge_counts': {'bronze': 62, 'silver': 33, 'gold': 6}, 'account_id': 393493, 'is_employee': False, 'last_modified_date': 1684016100, 'last_access_date': 1710940068, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4961, 'creation_date': 1305535885, 'user_type': 'registered', 'user_id': 755371, 'accept_rate': 44, 'location': 'Paris, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/755371/eric', 'profile_image': 'https://i.stack.imgur.com/Cmp2K.png?s=256&g=1', 'display_name': 'Eric'}","I am packaging my source code, but I do not want to include tests and docs because it will be too big. To do that I include in my setup.py: When doing a I can see that my root tests/ and docs/ dirs and everything inside are still included in the generated distribution. It seems that only is sensible to the exclude parameter. Why ? is it possible to exclude dirs for 'setup.py sdist' ?","setup(...
      packages=find_packages(exclude=['tests.*','tests','docs.*','docs']),
      ...
)
 python setup.py sdist
 python setup.py bdist
",3,25,0,0,
682,49576657,49576950,39609,Is there anyway I can download the file in Google colaboratory?,2,<python><python-3.x><jupyter-notebook><google-colaboratory>,28,"<p>I am trying tensorflow in Google Colaboratory from this codelab, 
I need to download '<a href=""http://download.tensorflow.org/example_images/flower_photos.tgz"" rel=""noreferrer"">http://download.tensorflow.org/example_images/flower_photos.tgz</a>' this file to complete the lab.</p>

<p>How I can download the file. Is there anyway to upload the tar file without downloading it on my machine. </p>

<p>I tried this method</p>

<pre><code>import urllib
testfile = urllib.URLopener()
testfile.retrieve(""http://randomsite.com/file.gz"", ""file.gz"")
</code></pre>

<p>This doesn't work, wget is not found also. Anyone please tell me how to do this.</p>
",713789,9141,30-03-2018 14:57,30-03-2018 15:18,0,9151,81,9,56,90,"{'badge_counts': {'bronze': 81, 'silver': 56, 'gold': 9}, 'collectives': [{'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 99491, 'is_employee': False, 'last_modified_date': 1700525400, 'last_access_date': 1711077754, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 40, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 9151, 'creation_date': 1265638450, 'user_type': 'registered', 'user_id': 713789, 'accept_rate': 90, 'website_url': 'http://anirudhagupta.com/', 'link': 'https://stackoverflow.com/users/713789/anirudha-gupta', 'profile_image': 'https://www.gravatar.com/avatar/f179c89b80f1ed1f62f8a1d65523d6c6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Anirudha Gupta'}","I am trying tensorflow in Google Colaboratory from this codelab, I need to download 'http://download.tensorflow.org/example_images/flower_photos.tgz' this file to complete the lab. How I can download the file. Is there anyway to upload the tar file without downloading it on my machine. I tried this method This doesn't work, wget is not found also. Anyone please tell me how to do this.","import urllib
testfile = urllib.URLopener()
testfile.retrieve(""http://randomsite.com/file.gz"", ""file.gz"")
",2,13,0,1,
683,49277932,49277956,4857,Difference pandas.DateTimeIndex without a frequency,2,<python><pandas><time-series><data-science>,16,"<p>An irregular time series <code>data</code> is stored in a <code>pandas.DataFrame</code>. A <code>DatetimeIndex</code> has been set. I need the time difference between consecutive entries in the index. </p>

<p>I thought it would be as simple as</p>

<pre><code>data.index.diff()
</code></pre>

<p>but got</p>

<pre><code>AttributeError: 'DatetimeIndex' object has no attribute 'diff'
</code></pre>

<p>I tried</p>

<pre><code>data.index - data.index.shift(1)
</code></pre>

<p>but got</p>

<pre><code>ValueError: Cannot shift with no freq
</code></pre>

<p>I do not want to infer or enforce a frequency first before doing this operation. There are large gaps in the time series that would be expanded to large runs of <code>nan</code>. The point is to find these gaps first.</p>

<p>So, what is a clean way to do this seemingly simple operation? </p>
",626537,21920,14-03-2018 12:34,14-03-2018 12:35,0,21960,246,46,158,88,"{'badge_counts': {'bronze': 246, 'silver': 158, 'gold': 46}, 'account_id': 312459, 'is_employee': False, 'last_modified_date': 1694905801, 'last_access_date': 1701387479, 'reputation_change_year': 280, 'reputation_change_quarter': 280, 'reputation_change_month': 70, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 21960, 'creation_date': 1298290528, 'user_type': 'registered', 'user_id': 626537, 'accept_rate': 88, 'website_url': 'http://clstaudt.me', 'link': 'https://stackoverflow.com/users/626537/clstaudt', 'profile_image': 'https://i.stack.imgur.com/SZIeu.jpg?s=256&g=1', 'display_name': 'clstaudt'}","An irregular time series is stored in a . A has been set. I need the time difference between consecutive entries in the index. I thought it would be as simple as but got I tried but got I do not want to infer or enforce a frequency first before doing this operation. There are large gaps in the time series that would be expanded to large runs of . The point is to find these gaps first. So, what is a clean way to do this seemingly simple operation?","data pandas.DataFrame DatetimeIndex data.index.diff()
 AttributeError: 'DatetimeIndex' object has no attribute 'diff'
 data.index - data.index.shift(1)
 ValueError: Cannot shift with no freq
 nan",-4,25,0,0,
684,50322001,50327323,16396,How to save/load a tensorflow hub module to/from a custom path?,6,<python><tensorflow><deep-learning><pre-trained-model><tensorflow-hub>,11,"<p>The <code>tensorflow_hub</code> library maintainers has made it every easy for users to download and use the pre-trained tensorflow modules, e.g.:</p>

<pre><code>import tensorflow_hub as hub

embed = hub.Module(""https://tfhub.dev/google/universal-sentence-encoder/1"")
</code></pre>

<p>But from the <code>sys.stderr</code> it seemed like it was saving the module locally to a temporary directory, i.e.</p>

<blockquote>
  <p>INFO:tensorflow:Using
  /var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules to
  cache modules. INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_0:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_1:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_10:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_11:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_12:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_13:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_14:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_15:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_16:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_2:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_3:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_4:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_5:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_6:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_7:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_8:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_9:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9 INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights
  INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights
  INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights
  INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection
  INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from
  checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0
  from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from
  checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0
  from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from
  checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0
  from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights
  INFO:tensorflow:Initialize variable
  module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias INFO:tensorflow:Initialize
  variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights INFO:tensorflow:Initialize
  variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias INFO:tensorflow:Initialize
  variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights INFO:tensorflow:Initialize
  variable module/global_step:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step</p>
</blockquote>

<p>After a machine reboot, the module gets deleted and running the <code>hub.Module('...')</code> code again would re-download the module. </p>

<p><strong>Is is possible to save the module to a custom directory and afterwards load from the custom directory?</strong></p>

<p>If it's possible, <strong>how to save/load a tensorflow hub module to/from a custom path?</strong></p>
",610569,118118,14-05-2018 01:07,14-05-2018 09:42,0,118218,764,110,475,90,"{'badge_counts': {'bronze': 764, 'silver': 475, 'gold': 110}, 'collectives': [{'collective': {'tags': ['word2vec', 'nlp', 'gensim', 'nlp-question-answering', 'spacy-3', 'opennlp', 'tf-idf', 'stanford-nlp', 'topic-modeling', 'named-entity-recognition', 'bert-language-model', 'spacy', 'huggingface-transformers', 'sentiment-analysis', 'word-embedding', 'nltk'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective focused on NLP (natural language processing), the transformation or extraction of useful information from natural language data.', 'link': '/collectives/nlp', 'name': 'NLP', 'slug': 'nlp'}, 'role': 'recognized_member'}], 'account_id': 302279, 'is_employee': False, 'last_modified_date': 1710277800, 'last_access_date': 1711180307, 'reputation_change_year': 1146, 'reputation_change_quarter': 1146, 'reputation_change_month': 274, 'reputation_change_week': -120, 'reputation_change_day': 0, 'reputation': 118218, 'creation_date': 1297292814, 'user_type': 'registered', 'user_id': 610569, 'accept_rate': 90, 'location': 'Singapore', 'website_url': 'http://en.wikipedia.org/wiki/User:Alvations', 'link': 'https://stackoverflow.com/users/610569/alvas', 'profile_image': 'https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG', 'display_name': 'alvas'}","The library maintainers has made it every easy for users to download and use the pre-trained tensorflow modules, e.g.: But from the it seemed like it was saving the module locally to a temporary directory, i.e. INFO:tensorflow:Using /var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules to cache modules. INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8 INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9 INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step After a machine reboot, the module gets deleted and running the code again would re-download the module. Is is possible to save the module to a custom directory and afterwards load from the custom directory? If it's possible, how to save/load a tensorflow hub module to/from a custom path?","tensorflow_hub import tensorflow_hub as hub

embed = hub.Module(""https://tfhub.dev/google/universal-sentence-encoder/1"")
 sys.stderr hub.Module('...')",-1,103,0,0,
685,49186905,49188683,177178,Loading images in google colab,11,<python><google-colaboratory>,25,"<p>My Jupyter Notebook has the following code to upload an image to Colab:</p>
<pre><code>from google.colab import files
uploaded = files.upload()
</code></pre>
<p>I get prompted for the file. Which gets uploaded.</p>
<p>I verify that the file upload was successful using:</p>
<pre><code>!ls
</code></pre>
<p>and I see that it's there.</p>
<p>I check the current working directory using:</p>
<pre><code>import os
os.getcwd()
</code></pre>
<p>and it tells me that it is /content</p>
<p>now, the following call fails:</p>
<pre><code>assert os.path.exists(img_path)
</code></pre>
<p>It also fails whether i'm using just the file name or the full path.</p>
<p>Any thoughts on what is going on?</p>
",550116,1827,09-03-2018 05:03,09-03-2018 07:31,0,1827,36,7,26,64,"{'badge_counts': {'bronze': 36, 'silver': 26, 'gold': 7}, 'account_id': 264084, 'is_employee': False, 'last_modified_date': 1666937700, 'last_access_date': 1689252970, 'reputation_change_year': 8, 'reputation_change_quarter': 8, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1827, 'creation_date': 1292944930, 'user_type': 'registered', 'user_id': 550116, 'accept_rate': 64, 'link': 'https://stackoverflow.com/users/550116/djeetee', 'profile_image': 'https://www.gravatar.com/avatar/f58c4f2f03e43725c9cc7c8f3552b735?s=256&d=identicon&r=PG', 'display_name': 'djeetee'}","My Jupyter Notebook has the following code to upload an image to Colab: I get prompted for the file. Which gets uploaded. I verify that the file upload was successful using: and I see that it's there. I check the current working directory using: and it tells me that it is /content now, the following call fails: It also fails whether i'm using just the file name or the full path. Any thoughts on what is going on?","from google.colab import files
uploaded = files.upload()
 !ls
 import os
os.getcwd()
 assert os.path.exists(img_path)
",2,19,0,0,
686,49820173,49929345,20376,"""RecursionError: maximum recursion depth exceeded"" from ssl.py: `super(SSLContext, SSLContext).options.__set__(self, value)`",5,<python><python-requests><pentaho><gevent>,26,"<p>I am using Python 3.6.5 on the following remote server setup:</p>

<blockquote>
  <p>Server: Windows 10 </p>
  
  <p>Python: 3.6.5 </p>
  
  <p>Requests: 2.18.4 </p>
  
  <p>Pentaho: 8.0</p>
</blockquote>

<p>When I run <code>request.get</code> against URLs in the server's command prompt, it gets the JSON as expected:</p>

<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; response = requests.get(url, headers=headers)
&gt;&gt;&gt; json = response.json()
&gt;&gt;&gt; print(json)
{'d': {'results': [{'_ ... 
</code></pre>

<p>However when I run the same script in <a href=""https://wiki.pentaho.com/display/EAI/CPython+Script+Executor"" rel=""noreferrer"">CPython</a> for Pentaho 8.0, I get</p>

<blockquote>
  <p>RecursionError: maximum recursion depth exceeded </p>
</blockquote>

<p><strong>Full log:</strong></p>

<pre><code>2018/04/13 15:02:17 - Get SP Doc List.0 - ERROR (version 8.0.0.0-28, build 8.0.0.0-28 from 2017-11-05 07.27.50 by buildguy) : Unexpected error
    2018/04/13 15:02:17 - Get SP Doc List.0 - ERROR (version 8.0.0.0-28, build 8.0.0.0-28 from 2017-11-05 07.27.50 by buildguy) : org.pentaho.di.core.exception.KettleException: 
    2018/04/13 15:02:17 - Get SP Doc List.0 - Traceback (most recent call last):
      File ""C:\Users\ADMINI~1\AppData\Local\Temp\2\pyServer.py"", line 299, in execute_script
        exec (script, _global_env)
      File ""&lt;string&gt;"", line 16, in &lt;module&gt;
      File ""C:\Program Files\Python36\lib\site-packages\requests\api.py"", line 72, in get
        return request('get', url, params=params, **kwargs)
      File ""C:\Program Files\Python36\lib\site-packages\requests\api.py"", line 58, in request
        return session.request(method=method, url=url, **kwargs)
      File ""C:\Program Files\Python36\lib\site-packages\requests\sessions.py"", line 508, in request
        resp = self.send(prep, **send_kwargs)
      File ""C:\Program Files\Python36\lib\site-packages\requests\sessions.py"", line 618, in send
        r = adapter.send(request, **kwargs)
      File ""C:\Program Files\Python36\lib\site-packages\requests\adapters.py"", line 440, in send
        timeout=timeout
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py"", line 601, in urlopen
        chunked=chunked)
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py"", line 346, in _make_request
        self._validate_conn(conn)
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py"", line 850, in _validate_conn
        conn.connect()
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\connection.py"", line 314, in connect
        cert_reqs=resolve_cert_reqs(self.cert_reqs),
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\util\ssl_.py"", line 269, in create_urllib3_context
        context.options |= options
      File ""C:\Program Files\Python36\lib\ssl.py"", line 465, in options
        super(SSLContext, SSLContext).options.__set__(self, value)
      File ""C:\Program Files\Python36\lib\ssl.py"", line 465, in options
        super(SSLContext, SSLContext).options.__set__(self, value)
      File ""C:\Program Files\Python36\lib\ssl.py"", line 465, in options
        super(SSLContext, SSLContext).options.__set__(self, value)
      [Previous line repeated 322 more times]
    RecursionError: maximum recursion depth exceeded
</code></pre>

<p>Script:</p>

<pre><code>import requests
import json


# By Filename
url = ""https://myco.sharepoint.com/teams/dg/l/_api/web/lists/GetByTitle('eRetail%20Data%20Sources')/items?...""

authtoken = ""Bearer eyJ...""

headers = {
    ""Content-Type"": ""application/json;odata=verbose"",
    ""Accept"": ""application/json;odata=verbose"",
    ""Authorization"": authtoken
}

response = requests.get(url, headers=headers)

json = response.json()
print('===========================')
print(json)
</code></pre>
",507624,12542,13-04-2018 15:11,19-04-2018 20:03,6,12552,273,34,135,67,"{'badge_counts': {'bronze': 273, 'silver': 135, 'gold': 34}, 'account_id': 238615, 'is_employee': False, 'last_modified_date': 1704226230, 'last_access_date': 1711130574, 'reputation_change_year': 42, 'reputation_change_quarter': 42, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 12552, 'creation_date': 1289771280, 'user_type': 'registered', 'user_id': 507624, 'accept_rate': 67, 'link': 'https://stackoverflow.com/users/507624/user3871', 'profile_image': 'https://www.gravatar.com/avatar/0a44ddbe33d59f92f84f978de1dd8c42?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user3871'}","I am using Python 3.6.5 on the following remote server setup: Server: Windows 10 Python: 3.6.5 Requests: 2.18.4 Pentaho: 8.0 When I run against URLs in the server's command prompt, it gets the JSON as expected: However when I run the same script in CPython for Pentaho 8.0, I get RecursionError: maximum recursion depth exceeded Full log: Script:","request.get &gt;&gt;&gt; import requests
&gt;&gt;&gt; response = requests.get(url, headers=headers)
&gt;&gt;&gt; json = response.json()
&gt;&gt;&gt; print(json)
{'d': {'results': [{'_ ... 
 2018/04/13 15:02:17 - Get SP Doc List.0 - ERROR (version 8.0.0.0-28, build 8.0.0.0-28 from 2017-11-05 07.27.50 by buildguy) : Unexpected error
    2018/04/13 15:02:17 - Get SP Doc List.0 - ERROR (version 8.0.0.0-28, build 8.0.0.0-28 from 2017-11-05 07.27.50 by buildguy) : org.pentaho.di.core.exception.KettleException: 
    2018/04/13 15:02:17 - Get SP Doc List.0 - Traceback (most recent call last):
      File ""C:\Users\ADMINI~1\AppData\Local\Temp\2\pyServer.py"", line 299, in execute_script
        exec (script, _global_env)
      File ""&lt;string&gt;"", line 16, in &lt;module&gt;
      File ""C:\Program Files\Python36\lib\site-packages\requests\api.py"", line 72, in get
        return request('get', url, params=params, **kwargs)
      File ""C:\Program Files\Python36\lib\site-packages\requests\api.py"", line 58, in request
        return session.request(method=method, url=url, **kwargs)
      File ""C:\Program Files\Python36\lib\site-packages\requests\sessions.py"", line 508, in request
        resp = self.send(prep, **send_kwargs)
      File ""C:\Program Files\Python36\lib\site-packages\requests\sessions.py"", line 618, in send
        r = adapter.send(request, **kwargs)
      File ""C:\Program Files\Python36\lib\site-packages\requests\adapters.py"", line 440, in send
        timeout=timeout
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py"", line 601, in urlopen
        chunked=chunked)
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py"", line 346, in _make_request
        self._validate_conn(conn)
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\connectionpool.py"", line 850, in _validate_conn
        conn.connect()
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\connection.py"", line 314, in connect
        cert_reqs=resolve_cert_reqs(self.cert_reqs),
      File ""C:\Program Files\Python36\lib\site-packages\urllib3\util\ssl_.py"", line 269, in create_urllib3_context
        context.options |= options
      File ""C:\Program Files\Python36\lib\ssl.py"", line 465, in options
        super(SSLContext, SSLContext).options.__set__(self, value)
      File ""C:\Program Files\Python36\lib\ssl.py"", line 465, in options
        super(SSLContext, SSLContext).options.__set__(self, value)
      File ""C:\Program Files\Python36\lib\ssl.py"", line 465, in options
        super(SSLContext, SSLContext).options.__set__(self, value)
      [Previous line repeated 322 more times]
    RecursionError: maximum recursion depth exceeded
 import requests
import json


# By Filename
url = ""https://myco.sharepoint.com/teams/dg/l/_api/web/lists/GetByTitle('eRetail%20Data%20Sources')/items?...""

authtoken = ""Bearer eyJ...""

headers = {
    ""Content-Type"": ""application/json;odata=verbose"",
    ""Accept"": ""application/json;odata=verbose"",
    ""Authorization"": authtoken
}

response = requests.get(url, headers=headers)

json = response.json()
print('===========================')
print(json)
",55,88,0,1,
687,50040470,50041860,16520,How to parse a pandas column of JSON content efficiently?,3,<python><json><pandas><performance>,15,"<p>Let's say I have the following DataFrame, where the <code>data</code> column contains a nested JSON string that I want to parse into separate columns:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({
    'bank_account': [101, 102, 201, 301],
    'data': [
        '{""uid"": 100, ""account_type"": 1, ""account_data"": {""currency"": {""current"": 1000, ""minimum"": -500}, ""fees"": {""monthly"": 13.5}}, ""user_name"": ""Alice""}',
        '{""uid"": 100, ""account_type"": 2, ""account_data"": {""currency"": {""current"": 2000, ""minimum"": 0},  ""fees"": {""monthly"": 0}}, ""user_name"": ""Alice""}',
        '{""uid"": 200, ""account_type"": 1, ""account_data"": {""currency"": {""current"": 3000, ""minimum"": 0},  ""fees"": {""monthly"": 13.5}}, ""user_name"": ""Bob""}',        
        '{""uid"": 300, ""account_type"": 1, ""account_data"": {""currency"": {""current"": 4000, ""minimum"": 0},  ""fees"": {""monthly"": 13.5}}, ""user_name"": ""Carol""}'        
    ]},
    index = ['Alice', 'Alice', 'Bob', 'Carol']
)


df
</code></pre>

<p>I've found the <code>json_normalize</code> function, and am currently parsing the JSON in a list comprehension; the result is correct, but this takes <em>long</em>. 1000 rows take 1-2 seconds, and I have about a million rows in my real run:</p>

<pre><code>import json
from pandas.io.json import json_normalize

parsed_df = pd.concat([json_normalize(json.loads(js)) for js in df['data']])

parsed_df['bank_account'] = df['bank_account'].values
parsed_df.index = parsed_df['user_id']

parsed_df
</code></pre>

<p>Is there a faster way to parse this data into a nice-looking DataFrame?</p>
",477883,1702,26-04-2018 10:18,26-04-2018 11:30,0,1702,33,3,16,80,"{'badge_counts': {'bronze': 33, 'silver': 16, 'gold': 3}, 'account_id': 220466, 'is_employee': False, 'last_modified_date': 1607615192, 'last_access_date': 1711107460, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1702, 'creation_date': 1287230000, 'user_type': 'registered', 'user_id': 477883, 'accept_rate': 80, 'location': 'M&#252;nchen, Deutschland', 'website_url': 'http://www.alpha-epsilon.de', 'link': 'https://stackoverflow.com/users/477883/alexander-engelhardt', 'profile_image': 'https://www.gravatar.com/avatar/d65bd960fd357b1a004ee52526dd9456?s=256&d=identicon&r=PG', 'display_name': 'Alexander Engelhardt'}","Let's say I have the following DataFrame, where the column contains a nested JSON string that I want to parse into separate columns: I've found the function, and am currently parsing the JSON in a list comprehension; the result is correct, but this takes long. 1000 rows take 1-2 seconds, and I have about a million rows in my real run: Is there a faster way to parse this data into a nice-looking DataFrame?","data import pandas as pd

df = pd.DataFrame({
    'bank_account': [101, 102, 201, 301],
    'data': [
        '{""uid"": 100, ""account_type"": 1, ""account_data"": {""currency"": {""current"": 1000, ""minimum"": -500}, ""fees"": {""monthly"": 13.5}}, ""user_name"": ""Alice""}',
        '{""uid"": 100, ""account_type"": 2, ""account_data"": {""currency"": {""current"": 2000, ""minimum"": 0},  ""fees"": {""monthly"": 0}}, ""user_name"": ""Alice""}',
        '{""uid"": 200, ""account_type"": 1, ""account_data"": {""currency"": {""current"": 3000, ""minimum"": 0},  ""fees"": {""monthly"": 13.5}}, ""user_name"": ""Bob""}',        
        '{""uid"": 300, ""account_type"": 1, ""account_data"": {""currency"": {""current"": 4000, ""minimum"": 0},  ""fees"": {""monthly"": 13.5}}, ""user_name"": ""Carol""}'        
    ]},
    index = ['Alice', 'Alice', 'Bob', 'Carol']
)


df
 json_normalize import json
from pandas.io.json import json_normalize

parsed_df = pd.concat([json_normalize(json.loads(js)) for js in df['data']])

parsed_df['bank_account'] = df['bank_account'].values
parsed_df.index = parsed_df['user_id']

parsed_df
",20,33,0,0,
688,48813097,48813117,16351,Does Python have the Elvis operator?,4,<python><conditional-operator><elvis-operator>,27,"<p>The ternary operator in many languages works like so:</p>

<pre><code>x = f() ? f() : g()
</code></pre>

<p>Where if <code>f()</code> is truthy then <code>x</code> is assigned the value of <code>f()</code>, otherwise it is assigned the value of <code>g()</code>. However, some languages have a more succinct <a href=""https://en.wikipedia.org/wiki/Elvis_operator"" rel=""noreferrer""><em>elvis</em> operator</a> that is functionally equivalent:</p>

<pre><code>x = f() ?: g()
</code></pre>

<p>In python, the <em>ternary</em> operator is expressed like so:</p>

<pre><code>x = f() if f() else g()
</code></pre>

<p>But does python have the more succinct <em>elvis</em> operator?</p>

<p>Maybe something like:</p>

<pre><code>x = f() else g() # Not actually valid python
</code></pre>
",446554,53208,15-02-2018 17:31,15-02-2018 17:32,0,53318,252,45,194,73,"{'badge_counts': {'bronze': 252, 'silver': 194, 'gold': 45}, 'account_id': 200610, 'is_employee': False, 'last_modified_date': 1708189502, 'last_access_date': 1711059447, 'reputation_change_year': 820, 'reputation_change_quarter': 820, 'reputation_change_month': 190, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 53318, 'creation_date': 1284396394, 'user_type': 'registered', 'user_id': 446554, 'accept_rate': 73, 'location': 'Pleasant Grove, UT', 'website_url': 'http://www.coryklein.com', 'link': 'https://stackoverflow.com/users/446554/cory-klein', 'profile_image': 'https://www.gravatar.com/avatar/2fe5bee1c3e113262b71d6d336821b13?s=256&d=identicon&r=PG', 'display_name': 'Cory Klein'}","The ternary operator in many languages works like so: Where if is truthy then is assigned the value of , otherwise it is assigned the value of . However, some languages have a more succinct elvis operator that is functionally equivalent: In python, the ternary operator is expressed like so: But does python have the more succinct elvis operator? Maybe something like:","x = f() ? f() : g()
 f() x f() g() x = f() ?: g()
 x = f() if f() else g()
 x = f() else g() # Not actually valid python
",-4,21,0,1,
689,48391075,48391232,24119,Is it possible to visualize a tensorflow graph without a training op?,1,<python><tensorflow><machine-learning><visualization><tensorboard>,15,"<p>I know how to visualize a tensorflow graph after training with tensorboard. Now, is it possible to visualize just the forward part of the graph, i.e., with no training operator defined?</p>

<p>The reason I'm asking this is that I'm getting this error:</p>

<pre><code>No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [ ... list of model variables here ... ] and loss Tensor(""Mean:0"", dtype=float32).
</code></pre>

<p>I'd like to inspect the graph to find out where the gradient tensor flow (pun intended) is broken.</p>
",348412,2076,22-01-2018 22:12,22-01-2018 22:23,0,2076,44,5,21,79,"{'badge_counts': {'bronze': 44, 'silver': 21, 'gold': 5}, 'account_id': 141412, 'is_employee': False, 'last_modified_date': 1684545900, 'last_access_date': 1711013357, 'reputation_change_year': -2, 'reputation_change_quarter': -2, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2076, 'creation_date': 1274638289, 'user_type': 'registered', 'user_id': 348412, 'accept_rate': 79, 'location': 'Cologne, Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/348412/erickrf', 'profile_image': 'https://www.gravatar.com/avatar/2b4c4233cc40179d8d26cc10c30e0a34?s=256&d=identicon&r=PG', 'display_name': 'erickrf'}","I know how to visualize a tensorflow graph after training with tensorboard. Now, is it possible to visualize just the forward part of the graph, i.e., with no training operator defined? The reason I'm asking this is that I'm getting this error: I'd like to inspect the graph to find out where the gradient tensor flow (pun intended) is broken.","No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [ ... list of model variables here ... ] and loss Tensor(""Mean:0"", dtype=float32).
",0,8,0,0,
690,48910590,48913749,3358,How to bulk write TFRecords?,1,<python><tensorflow><file-writing>,13,"<p>I have a CSV with approximately 40 million rows. Each row is a training instance. As per <a href=""https://www.tensorflow.org/programmers_guide/datasets#reading_input_data"" rel=""noreferrer"">the documentation on consuming TFRecords</a> I am trying to encode and save the data in a TFRecord file. </p>

<p>All the examples I have found (<a href=""https://github.com/tensorflow/tensorflow/blob/3c3c0481ec087aca4fa875d6d936f19b31191fc1/tensorflow/examples/how_tos/reading_data/convert_to_records.py"" rel=""noreferrer"">even the ones in the TensorFlow repo</a>) show the process of creating a TFRecord is dependant on the class TFRecordWriter. This class has a method <code>write</code> that takes as input a serialised string representation of the data and writes it to disk. However, this appears to be done one training instance at a time.</p>

<p><strong>How do I write a batch of the serialised data?</strong></p>

<p>Let's say I have a funtion:</p>

<pre><code>  def write_row(sentiment, text, encoded):
    feature = {""one_hot"": _float_feature(encoded),
               ""label"": _int64_feature([sentiment]),
               ""text"": _bytes_feature([text.encode()])}

    example = tf.train.Example(features=tf.train.Features(feature=feature))
    writer.write(example.SerializeToString())
</code></pre>

<p>Writing to disk 40 million times (once for each example) is going to be incredibly slow. It would be far more efficient to batch this data and write 50k or 100k examples at a time (as far as the machine's resources will allow). However there does not appear to be any method to do this inside <code>TFRecordWriter</code>. </p>

<p>Something along the lines of:</p>

<pre><code>class MyRecordWriter:

  def __init__(self, writer):
    self.records = []
    self.counter = 0
    self.writer = writer

  def write_row_batched(self, sentiment, text, encoded):
    feature = {""one_hot"": _float_feature(encoded),
               ""label"": _int64_feature([sentiment]),
               ""text"": _bytes_feature([text.encode()])}

    example = tf.train.Example(features=tf.train.Features(feature=feature))
    self.records.append(example.SerializeToString())
    self.counter += 1
    if self.counter &gt;= 10000:
      self.writer.write(os.linesep.join(self.records))
      self.counter = 0
      self.records = []
</code></pre>

<p>But when reading the file created by this method I get the following error:</p>

<pre><code>tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Could not parse example input, value: '
��

label

��
one_hot����
��
</code></pre>

<p>Note: I could change the encoding process so that each <code>example</code> proto contains several thousand examples instead of just one but 
I don't want to pre-batch the data when writing to the TFrecord file in this way as it will introduce extra overhead in my training pipeline when I want to use the file for training with different batch sizes.</p>
",281089,1335,21-02-2018 16:16,21-02-2018 19:16,0,1335,29,3,14,83,"{'badge_counts': {'bronze': 29, 'silver': 14, 'gold': 3}, 'account_id': 105304, 'is_employee': False, 'last_modified_date': 1648565386, 'last_access_date': 1710951413, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1335, 'creation_date': 1267090019, 'user_type': 'registered', 'user_id': 281089, 'accept_rate': 83, 'location': 'London, United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/281089/insectatorious', 'profile_image': 'https://www.gravatar.com/avatar/bd482f22fd414cfb076b45af6c5524a7?s=256&d=identicon&r=PG', 'display_name': 'Insectatorious'}","I have a CSV with approximately 40 million rows. Each row is a training instance. As per the documentation on consuming TFRecords I am trying to encode and save the data in a TFRecord file. All the examples I have found (even the ones in the TensorFlow repo) show the process of creating a TFRecord is dependant on the class TFRecordWriter. This class has a method that takes as input a serialised string representation of the data and writes it to disk. However, this appears to be done one training instance at a time. How do I write a batch of the serialised data? Let's say I have a funtion: Writing to disk 40 million times (once for each example) is going to be incredibly slow. It would be far more efficient to batch this data and write 50k or 100k examples at a time (as far as the machine's resources will allow). However there does not appear to be any method to do this inside . Something along the lines of: But when reading the file created by this method I get the following error: Note: I could change the encoding process so that each proto contains several thousand examples instead of just one but I don't want to pre-batch the data when writing to the TFrecord file in this way as it will introduce extra overhead in my training pipeline when I want to use the file for training with different batch sizes.","write   def write_row(sentiment, text, encoded):
    feature = {""one_hot"": _float_feature(encoded),
               ""label"": _int64_feature([sentiment]),
               ""text"": _bytes_feature([text.encode()])}

    example = tf.train.Example(features=tf.train.Features(feature=feature))
    writer.write(example.SerializeToString())
 TFRecordWriter class MyRecordWriter:

  def __init__(self, writer):
    self.records = []
    self.counter = 0
    self.writer = writer

  def write_row_batched(self, sentiment, text, encoded):
    feature = {""one_hot"": _float_feature(encoded),
               ""label"": _int64_feature([sentiment]),
               ""text"": _bytes_feature([text.encode()])}

    example = tf.train.Example(features=tf.train.Features(feature=feature))
    self.records.append(example.SerializeToString())
    self.counter += 1
    if self.counter &gt;= 10000:
      self.writer.write(os.linesep.join(self.records))
      self.counter = 0
      self.records = []
 tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Could not parse example input, value: '
��

label

��
one_hot����
��
 example",28,56,0,2,
691,48503151,48503355,4820,Is there a maximum character limit to random seed?,3,<python><python-2.7><random><seeding>,11,"<p>Is there a maximum number of characters (and therfore value) to the seed in Python?</p>

<pre><code>import random
random.seed(13) # fine
random.seed(1234567890) # also fine
random.seed(31415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989)
# also fine
</code></pre>

<p>I'm not sure why someone would pick such a high value, but I just want to know if has a limit.</p>
",1654143,6722,29-01-2018 14:07,29-01-2018 14:18,0,6757,131,11,70,71,"{'badge_counts': {'bronze': 131, 'silver': 70, 'gold': 11}, 'account_id': 1817615, 'is_employee': False, 'last_modified_date': 1705932301, 'last_access_date': 1711115731, 'reputation_change_year': 110, 'reputation_change_quarter': 110, 'reputation_change_month': 45, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6757, 'creation_date': 1347005571, 'user_type': 'registered', 'user_id': 1654143, 'accept_rate': 71, 'location': 'South Coast', 'website_url': '', 'link': 'https://stackoverflow.com/users/1654143/ghoul-fool', 'profile_image': 'https://i.stack.imgur.com/DiNAr.png?s=256&g=1', 'display_name': 'Ghoul Fool'}","Is there a maximum number of characters (and therfore value) to the seed in Python? I'm not sure why someone would pick such a high value, but I just want to know if has a limit.","import random
random.seed(13) # fine
random.seed(1234567890) # also fine
random.seed(31415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989)
# also fine
",4,10,0,0,
692,49302516,49594324,5775,where to get and install crypto.dll on 64 bit Windows,3,<python><python-3.x><dll><openssl><python-import>,14,"<p>Note: This is not an issue with Visual Studio, but rather with incompatible dll versions. The steps below replicate the problem since running in Visual Studio in debug mode breaks on exceptions being thrown. If you just run, the thrown exception is handled elsewhere and the program works fine. But since I spend a lot of time in debug mode, I would prefer to fix this problem.</p>

<p>When debugging, I want to be able to step into modules I have added to my Virtual Environment in Visual Studio. I get a 'library not found' error that I am not able to fix. Here are the steps:</p>

<ol>
<li>In Visual Studio create a new Python Application.</li>
<li>Create a virtual environment for that application (Python 3.6 64
bit).</li>
<li><code>pip install twilio</code> into your virtual environment. You get the
    following output.</li>
</ol>

<p>...</p>

<pre><code>   ----- Installing 'twilio' -----
Collecting twilio
  Using cached twilio-6.10.5-py2.py3-none-any.whl
Collecting pytz (from twilio)
  Using cached pytz-2018.3-py2.py3-none-any.whl
Collecting six (from twilio)
  Using cached six-1.11.0-py2.py3-none-any.whl
Collecting PyJWT&gt;=1.4.2 (from twilio)
  Using cached PyJWT-1.6.0-py2.py3-none-any.whl
Collecting requests&gt;=2.0.0; python_version &gt;= ""3.0"" (from twilio)
  Using cached requests-2.18.4-py2.py3-none-any.whl
Collecting pysocks; python_version &gt;= ""3.0"" (from twilio)
  Using cached PySocks-1.6.8.tar.gz
Collecting certifi&gt;=2017.4.17 (from requests&gt;=2.0.0; python_version &gt;= ""3.0""-&gt;twilio)
  Using cached certifi-2018.1.18-py2.py3-none-any.whl
Collecting chardet&lt;3.1.0,&gt;=3.0.2 (from requests&gt;=2.0.0; python_version &gt;= ""3.0""-&gt;twilio)
  Using cached chardet-3.0.4-py2.py3-none-any.whl
Collecting urllib3&lt;1.23,&gt;=1.21.1 (from requests&gt;=2.0.0; python_version &gt;= ""3.0""-&gt;twilio)
  Using cached urllib3-1.22-py2.py3-none-any.whl
Collecting idna&lt;2.7,&gt;=2.5 (from requests&gt;=2.0.0; python_version &gt;= ""3.0""-&gt;twilio)
  Using cached idna-2.6-py2.py3-none-any.whl
Installing collected packages: pytz, six, PyJWT, certifi, chardet, urllib3, idna, requests, pysocks, twilio
  Running setup.py install for pysocks: started
    Running setup.py install for pysocks: finished with status 'done'
Successfully installed PyJWT-1.6.0 certifi-2018.1.18 chardet-3.0.4 idna-2.6 pysocks-1.6.8 pytz-2018.3 requests-2.18.4 six-1.11.0 twilio-6.10.5 urllib3-1.22
----- Successfully installed 'twilio' -----
</code></pre>

<ol start=""4"">
<li><p>Add the following line to the top of your .py file:</p>

<p><code>from twilio.rest import Client</code></p></li>
<li><p>In Visual Studio go to tools > options > python > debugging. Make
sure 'Enable debugging of Python standard library' is checked</p></li>
<li><p>Run the application. You get the following error:</p></li>
</ol>

<blockquote>
  <p>ModuleNotFoundError: No module named 'OpenSSL'</p>
</blockquote>

<ol start=""7"">
<li><code>pip install pyopenssl</code> You get the following output:</li>
</ol>

<p>...</p>

<pre><code>----- Installing 'pyopenssl' -----
Collecting pyopenssl
  Using cached pyOpenSSL-17.5.0-py2.py3-none-any.whl
Requirement already satisfied: six&gt;=1.5.2 in c:\users\x\source\repos\pythonapplication9\pythonapplication9\env\lib\site-packages (from pyopenssl)
Collecting cryptography&gt;=2.1.4 (from pyopenssl)
  Using cached cryptography-2.1.4-cp36-cp36m-win_amd64.whl
Requirement already satisfied: idna&gt;=2.1 in c:\users\x\source\repos\pythonapplication9\pythonapplication9\env\lib\site-packages (from cryptography&gt;=2.1.4-&gt;pyopenssl)
Collecting cffi&gt;=1.7; platform_python_implementation != ""PyPy"" (from cryptography&gt;=2.1.4-&gt;pyopenssl)
  Using cached cffi-1.11.5-cp36-cp36m-win_amd64.whl
Collecting asn1crypto&gt;=0.21.0 (from cryptography&gt;=2.1.4-&gt;pyopenssl)
  Using cached asn1crypto-0.24.0-py2.py3-none-any.whl
Collecting pycparser (from cffi&gt;=1.7; platform_python_implementation != ""PyPy""-&gt;cryptography&gt;=2.1.4-&gt;pyopenssl)
  Using cached pycparser-2.18.tar.gz
Installing collected packages: pycparser, cffi, asn1crypto, cryptography, pyopenssl
  Running setup.py install for pycparser: started
    Running setup.py install for pycparser: finished with status 'done'
Successfully installed asn1crypto-0.24.0 cffi-1.11.5 cryptography-2.1.4 pycparser-2.18 pyopenssl-17.5.0
----- Successfully installed 'pyopenssl' -----
</code></pre>

<ol start=""8"">
<li><p>Run the application. You get the following error:</p>

<p><code>asn1crypto._ffi.LibraryNotFoundError: The library libcrypto could not be found</code></p></li>
</ol>

<p>The error is thrown in the file named <code>_big_num_ctypes.py</code> in <code>asn1crypto</code>. The code line where this is thrown is:</p>

<pre><code>libcrypto_path = find_library(b'crypto' if sys.version_info &lt; (3,) else 'crypto')
if not libcrypto_path:
    raise LibraryNotFoundError('The library libcrypto could not be found')
</code></pre>

<p><strong>Update:</strong> I was asked to provide the full backtrace. I modified the code in this way to print it:</p>

<pre><code>import unittest
import traceback

class Test_test1(unittest.TestCase):
    def test_A(self):
        try:
            from twilio.rest import Client
        except Exception as e:
            print('foo')
            foo = traceback.extract_stack()
            traceback.print_exc(e)

if __name__ == '__main__':
    unittest.main()
</code></pre>

<p>As before the import line throws the exception but the exception is not caught and the lines in 'except' clause are never executed
from twilio.rest import Client</p>

<p>update 2: I somehow had gotten this to work following @Prateek and @user8212173. But now it is not working again. As both suggested, the problem is that crypto.dll is not there. So I went thru the steps below to add it with no success:</p>

<ol>
<li>I installed Win64 OpenSSL v1.1.0j from <a href=""https://slproweb.com/products/Win32OpenSSL.html"" rel=""noreferrer"">https://slproweb.com/products/Win32OpenSSL.html</a>  (pointed to from <a href=""https://wiki.openssl.org/index.php/Binaries"" rel=""noreferrer"">https://wiki.openssl.org/index.php/Binaries</a>). It does not contain crypto.dll. </li>
<li>I then installed crypto.dll from  <a href=""http://www.dlldownloader.com/crypto-dll/"" rel=""noreferrer"">http://www.dlldownloader.com/crypto-dll/</a> (as @user8212173 suggested) (there is only a 32 bit version) and followed the instructions. I then got a new error message ""ImportError: DLL load failed: %1 is not a valid Win32 application"" which means that the crypto.dll I installed has a version conflict (I am running 64bit python on a 64bit computer). I remember installing it from <a href=""https://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""noreferrer"">Unofficial Windows Binaries for Python Extension Packages</a> I can't find it there. So where do I get a working 64bit version of crypto.dll?</li>
</ol>
",277498,8832,15-03-2018 14:47,01-04-2018 02:47,17,8832,93,15,67,77,"{'badge_counts': {'bronze': 93, 'silver': 67, 'gold': 15}, 'account_id': 103647, 'is_employee': False, 'last_modified_date': 1703300100, 'last_access_date': 1674511493, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 8832, 'creation_date': 1266638451, 'user_type': 'registered', 'user_id': 277498, 'accept_rate': 77, 'website_url': '', 'link': 'https://stackoverflow.com/users/277498/barka', 'profile_image': 'https://i.stack.imgur.com/u3a0H.jpg?s=256&g=1', 'display_name': 'Barka'}","Note: This is not an issue with Visual Studio, but rather with incompatible dll versions. The steps below replicate the problem since running in Visual Studio in debug mode breaks on exceptions being thrown. If you just run, the thrown exception is handled elsewhere and the program works fine. But since I spend a lot of time in debug mode, I would prefer to fix this problem. When debugging, I want to be able to step into modules I have added to my Virtual Environment in Visual Studio. I get a 'library not found' error that I am not able to fix. Here are the steps: In Visual Studio create a new Python Application. Create a virtual environment for that application (Python 3.6 64 bit). into your virtual environment. You get the following output. ... Add the following line to the top of your .py file: In Visual Studio go to tools > options > python > debugging. Make sure 'Enable debugging of Python standard library' is checked Run the application. You get the following error: ModuleNotFoundError: No module named 'OpenSSL' You get the following output: ... Run the application. You get the following error: The error is thrown in the file named in . The code line where this is thrown is: Update: I was asked to provide the full backtrace. I modified the code in this way to print it: As before the import line throws the exception but the exception is not caught and the lines in 'except' clause are never executed from twilio.rest import Client update 2: I somehow had gotten this to work following @Prateek and @user8212173. But now it is not working again. As both suggested, the problem is that crypto.dll is not there. So I went thru the steps below to add it with no success: I installed Win64 OpenSSL v1.1.0j from https://slproweb.com/products/Win32OpenSSL.html (pointed to from https://wiki.openssl.org/index.php/Binaries). It does not contain crypto.dll. I then installed crypto.dll from http://www.dlldownloader.com/crypto-dll/ (as @user8212173 suggested) (there is only a 32 bit version) and followed the instructions. I then got a new error message ""ImportError: DLL load failed: %1 is not a valid Win32 application"" which means that the crypto.dll I installed has a version conflict (I am running 64bit python on a 64bit computer). I remember installing it from Unofficial Windows Binaries for Python Extension Packages I can't find it there. So where do I get a working 64bit version of crypto.dll?","pip install twilio    ----- Installing 'twilio' -----
Collecting twilio
  Using cached twilio-6.10.5-py2.py3-none-any.whl
Collecting pytz (from twilio)
  Using cached pytz-2018.3-py2.py3-none-any.whl
Collecting six (from twilio)
  Using cached six-1.11.0-py2.py3-none-any.whl
Collecting PyJWT&gt;=1.4.2 (from twilio)
  Using cached PyJWT-1.6.0-py2.py3-none-any.whl
Collecting requests&gt;=2.0.0; python_version &gt;= ""3.0"" (from twilio)
  Using cached requests-2.18.4-py2.py3-none-any.whl
Collecting pysocks; python_version &gt;= ""3.0"" (from twilio)
  Using cached PySocks-1.6.8.tar.gz
Collecting certifi&gt;=2017.4.17 (from requests&gt;=2.0.0; python_version &gt;= ""3.0""-&gt;twilio)
  Using cached certifi-2018.1.18-py2.py3-none-any.whl
Collecting chardet&lt;3.1.0,&gt;=3.0.2 (from requests&gt;=2.0.0; python_version &gt;= ""3.0""-&gt;twilio)
  Using cached chardet-3.0.4-py2.py3-none-any.whl
Collecting urllib3&lt;1.23,&gt;=1.21.1 (from requests&gt;=2.0.0; python_version &gt;= ""3.0""-&gt;twilio)
  Using cached urllib3-1.22-py2.py3-none-any.whl
Collecting idna&lt;2.7,&gt;=2.5 (from requests&gt;=2.0.0; python_version &gt;= ""3.0""-&gt;twilio)
  Using cached idna-2.6-py2.py3-none-any.whl
Installing collected packages: pytz, six, PyJWT, certifi, chardet, urllib3, idna, requests, pysocks, twilio
  Running setup.py install for pysocks: started
    Running setup.py install for pysocks: finished with status 'done'
Successfully installed PyJWT-1.6.0 certifi-2018.1.18 chardet-3.0.4 idna-2.6 pysocks-1.6.8 pytz-2018.3 requests-2.18.4 six-1.11.0 twilio-6.10.5 urllib3-1.22
----- Successfully installed 'twilio' -----
 from twilio.rest import Client pip install pyopenssl ----- Installing 'pyopenssl' -----
Collecting pyopenssl
  Using cached pyOpenSSL-17.5.0-py2.py3-none-any.whl
Requirement already satisfied: six&gt;=1.5.2 in c:\users\x\source\repos\pythonapplication9\pythonapplication9\env\lib\site-packages (from pyopenssl)
Collecting cryptography&gt;=2.1.4 (from pyopenssl)
  Using cached cryptography-2.1.4-cp36-cp36m-win_amd64.whl
Requirement already satisfied: idna&gt;=2.1 in c:\users\x\source\repos\pythonapplication9\pythonapplication9\env\lib\site-packages (from cryptography&gt;=2.1.4-&gt;pyopenssl)
Collecting cffi&gt;=1.7; platform_python_implementation != ""PyPy"" (from cryptography&gt;=2.1.4-&gt;pyopenssl)
  Using cached cffi-1.11.5-cp36-cp36m-win_amd64.whl
Collecting asn1crypto&gt;=0.21.0 (from cryptography&gt;=2.1.4-&gt;pyopenssl)
  Using cached asn1crypto-0.24.0-py2.py3-none-any.whl
Collecting pycparser (from cffi&gt;=1.7; platform_python_implementation != ""PyPy""-&gt;cryptography&gt;=2.1.4-&gt;pyopenssl)
  Using cached pycparser-2.18.tar.gz
Installing collected packages: pycparser, cffi, asn1crypto, cryptography, pyopenssl
  Running setup.py install for pycparser: started
    Running setup.py install for pycparser: finished with status 'done'
Successfully installed asn1crypto-0.24.0 cffi-1.11.5 cryptography-2.1.4 pycparser-2.18 pyopenssl-17.5.0
----- Successfully installed 'pyopenssl' -----
 asn1crypto._ffi.LibraryNotFoundError: The library libcrypto could not be found _big_num_ctypes.py asn1crypto libcrypto_path = find_library(b'crypto' if sys.version_info &lt; (3,) else 'crypto')
if not libcrypto_path:
    raise LibraryNotFoundError('The library libcrypto could not be found')
 import unittest
import traceback

class Test_test1(unittest.TestCase):
    def test_A(self):
        try:
            from twilio.rest import Client
        except Exception as e:
            print('foo')
            foo = traceback.extract_stack()
            traceback.print_exc(e)

if __name__ == '__main__':
    unittest.main()
",51,121,0,4,
693,49651815,49652477,23762,setUpClass() missing 1 required positional argument: 'cls',2,<python><python-3.x><unit-testing>,41,"<p>I tried to use <code>setUpClass()</code> method for the first time in my life and wrote:</p>

<pre><code>class TestDownload(unittest.TestCase):

    def setUpClass(cls):
        config.fs = True
</code></pre>

<p>and got: </p>

<pre><code>Ran 0 tests in 0.004s

FAILED (errors=1)

Failure
Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.5/unittest/suite.py"", line 163, in _handleClassSetUp
    setUpClass()
TypeError: setUpClass() missing 1 required positional argument: 'cls'
</code></pre>

<p>What does it mean and how to satisfy it?</p>
",258483,49049,04-04-2018 13:04,04-04-2018 13:36,0,49109,621,123,341,46,"{'badge_counts': {'bronze': 621, 'silver': 341, 'gold': 123}, 'collectives': [{'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 94736, 'is_employee': False, 'last_modified_date': 1711020600, 'last_access_date': 1710974198, 'reputation_change_year': 498, 'reputation_change_quarter': 498, 'reputation_change_month': 90, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 49109, 'creation_date': 1264429148, 'user_type': 'registered', 'user_id': 258483, 'accept_rate': 46, 'location': 'Moscow, Russia', 'website_url': 'https://inthemoon.net/', 'link': 'https://stackoverflow.com/users/258483/dims', 'profile_image': 'https://www.gravatar.com/avatar/dd4dee2094dcd427aaec8ee329d9cfd6?s=256&d=identicon&r=PG', 'display_name': 'Dims'}",I tried to use method for the first time in my life and wrote: and got: What does it mean and how to satisfy it?,"setUpClass() class TestDownload(unittest.TestCase):

    def setUpClass(cls):
        config.fs = True
 Ran 0 tests in 0.004s

FAILED (errors=1)

Failure
Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.5/unittest/suite.py"", line 163, in _handleClassSetUp
    setUpClass()
TypeError: setUpClass() missing 1 required positional argument: 'cls'
",10,22,0,0,
694,49348340,49348341,19064,How to remove multiple elements from a set?,1,<python><python-3.x><set>,30,"<p>Say I have a set <code>s = {1, 2, 3, 4, 5}</code>. Can I remove the subset <code>{1, 2, 3}</code> from the set in just one statement (as opposed to calling <code>s.remove(elem)</code> in a loop)?</p>
",244297,145777,18-03-2018 13:22,18-03-2018 13:22,0,145997,382,42,335,94,"{'badge_counts': {'bronze': 382, 'silver': 335, 'gold': 42}, 'account_id': 88287, 'is_employee': False, 'last_modified_date': 1703301600, 'last_access_date': 1711181119, 'reputation_change_year': 1307, 'reputation_change_quarter': 1307, 'reputation_change_month': 350, 'reputation_change_week': 180, 'reputation_change_day': 0, 'reputation': 145997, 'creation_date': 1262729362, 'user_type': 'registered', 'user_id': 244297, 'accept_rate': 94, 'location': 'Minsk, Belarus', 'website_url': '', 'link': 'https://stackoverflow.com/users/244297/eugene-yarmash', 'profile_image': 'https://www.gravatar.com/avatar/0e726125e86b084ffec695e845b360b7?s=256&d=identicon&r=PG', 'display_name': 'Eugene Yarmash'}",Say I have a set . Can I remove the subset from the set in just one statement (as opposed to calling in a loop)?,"s = {1, 2, 3, 4, 5} {1, 2, 3} s.remove(elem)",-3,1,0,0,
695,50045775,50046915,30224,lxml / BeautifulSoup parser warning,3,<python><python-3.x><beautifulsoup><lxml>,15,"<p>Using Python 3, I'm trying to parse ugly HTML (which is not under my control) by using <code>lxml</code> with BeautifulSoup as explained here: <a href=""http://lxml.de/elementsoup.html"" rel=""noreferrer"">http://lxml.de/elementsoup.html</a></p>

<p>Specifically, I want to use <code>lxml</code>, but I'd like to use BeautifulSoup because like I said, it's ugly HTML and <code>lxml</code> will reject it on its own.</p>

<p>The link above says: ""All you need to do is pass it to the fromstring() function:""</p>

<pre><code>from lxml.html.soupparser import fromstring
root = fromstring(tag_soup)
</code></pre>

<p>So that's what I'm doing:</p>

<pre><code>URL = 'http://some-place-on-the-internet.com'
html_goo = requests.get(URL).text
root = fromstring(html_goo)
</code></pre>

<p>It <strong>works</strong> in the sense that I can manipulate the HTML just fine after that. My problem is that every time I run the script, I receive this annoying warning:</p>

<pre><code>/usr/lib/python3/dist-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (""html.parser""). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

To get rid of this warning, change this:

 BeautifulSoup([your markup])

to this:

 BeautifulSoup([your markup], ""html.parser"")

  markup_type=markup_type))
</code></pre>

<p>My problem is perhaps obvious: I'm not instantiating BeautifulSoup myself. I've tried adding the proposed parameter to the <code>fromstring</code> function, but that just gives me the error: <code>TypeError: 'str' object is not callable</code>. Searches online have proven fruitless so far.</p>

<p>I'd like to get rid of that warning message. Help appreciated, thanks in advance.</p>
",232220,12811,26-04-2018 14:37,26-04-2018 15:32,0,12831,67,16,56,74,"{'badge_counts': {'bronze': 67, 'silver': 56, 'gold': 16}, 'account_id': 82649, 'is_employee': False, 'last_modified_date': 1701914700, 'last_access_date': 1711025633, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 12831, 'creation_date': 1260893381, 'user_type': 'registered', 'user_id': 232220, 'accept_rate': 74, 'website_url': '', 'link': 'https://stackoverflow.com/users/232220/teekin', 'profile_image': 'https://www.gravatar.com/avatar/aaeeac6d94f8135b2f677590fc88634f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Teekin'}","Using Python 3, I'm trying to parse ugly HTML (which is not under my control) by using with BeautifulSoup as explained here: http://lxml.de/elementsoup.html Specifically, I want to use , but I'd like to use BeautifulSoup because like I said, it's ugly HTML and will reject it on its own. The link above says: ""All you need to do is pass it to the fromstring() function:"" So that's what I'm doing: It works in the sense that I can manipulate the HTML just fine after that. My problem is that every time I run the script, I receive this annoying warning: My problem is perhaps obvious: I'm not instantiating BeautifulSoup myself. I've tried adding the proposed parameter to the function, but that just gives me the error: . Searches online have proven fruitless so far. I'd like to get rid of that warning message. Help appreciated, thanks in advance.","lxml lxml lxml from lxml.html.soupparser import fromstring
root = fromstring(tag_soup)
 URL = 'http://some-place-on-the-internet.com'
html_goo = requests.get(URL).text
root = fromstring(html_goo)
 /usr/lib/python3/dist-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (""html.parser""). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

To get rid of this warning, change this:

 BeautifulSoup([your markup])

to this:

 BeautifulSoup([your markup], ""html.parser"")

  markup_type=markup_type))
 fromstring TypeError: 'str' object is not callable",8,35,0,1,
696,49672621,49673284,21982,"What are the valid values for --platform, --abi, and --implementation for pip download?",3,<python><pip>,27,"<p><a href=""https://pip.pypa.io/en/stable/reference/pip_download/"" rel=""noreferrer"">pip download</a> has several flags that I would like to play with <code>--platform</code>, <code>--abi</code>, and <code>--implementation</code>. </p>

<p>Where can I find the complete list of valid values for these flags?</p>
",198301,8562,05-04-2018 12:37,05-04-2018 13:12,0,8572,79,9,45,79,"{'badge_counts': {'bronze': 79, 'silver': 45, 'gold': 9}, 'account_id': 67850, 'is_employee': False, 'last_modified_date': 1708821900, 'last_access_date': 1710937228, 'reputation_change_year': 49, 'reputation_change_quarter': 49, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 8572, 'creation_date': 1256750677, 'user_type': 'registered', 'user_id': 198301, 'accept_rate': 79, 'website_url': '', 'link': 'https://stackoverflow.com/users/198301/ericg', 'profile_image': 'https://www.gravatar.com/avatar/b85858c8092862d570cf864a2c1d2a03?s=256&d=identicon&r=PG', 'display_name': 'ericg'}","pip download has several flags that I would like to play with , , and . Where can I find the complete list of valid values for these flags?",#NAME?,-3,3,0,1,
697,50081909,50144898,2587,Convert virtualenv instance/`requirements.txt` to pipenv,1,<python><virtualenv><pipenv>,13,"<p>I've got a lot of projects using the <code>virtualenv</code> and <code>requirements.txt</code> or <code>./requirements/</code> pattern, but now using <code>pipenv</code> is obviously many times superior.</p>

<p>It seems to be easy to generate <code>requirements.txt</code> from <code>pipenv</code>, but going the other way seems more confusing.</p>

<p>There doesn't seem to be an obvious way to ""convert"" existing projects to <code>pipenv</code>. </p>

<p>I'm writing a script to execute <code>pipenv</code> on everything in a large <code>requirements.txt</code> but this can't be right -- is there a way to apply an existing requirements.txt to a <code>pipenv</code>?</p>
",163567,4180,28-04-2018 23:15,02-05-2018 23:26,4,4190,54,1,39,58,"{'badge_counts': {'bronze': 54, 'silver': 39, 'gold': 1}, 'account_id': 54647, 'is_employee': False, 'last_modified_date': 1652217902, 'last_access_date': 1711098299, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4190, 'creation_date': 1251297017, 'user_type': 'registered', 'user_id': 163567, 'accept_rate': 58, 'location': 'Australia', 'website_url': '', 'link': 'https://stackoverflow.com/users/163567/williams', 'profile_image': 'https://i.stack.imgur.com/PWxxR.jpg?s=256&g=1', 'display_name': 'Williams'}","I've got a lot of projects using the and or pattern, but now using is obviously many times superior. It seems to be easy to generate from , but going the other way seems more confusing. There doesn't seem to be an obvious way to ""convert"" existing projects to . I'm writing a script to execute on everything in a large but this can't be right -- is there a way to apply an existing requirements.txt to a ?",virtualenv requirements.txt ./requirements/ pipenv requirements.txt pipenv pipenv pipenv requirements.txt pipenv,-10,7,0,0,
698,48836604,48836605,24517,Avoiding pylint complaints when importing Python packages from submodules,2,<python><python-import><git-submodules><conventions><pylint>,18,"<p><strong>Background</strong></p>

<p>I have a Python application dependent upon another package which is provided as a git submodule, yielding a directory structure similar to the following:</p>

<pre><code>foo/
    bar/
        bar/
            __init__.py
            eggs.py
        test/
        setup.py
    foo/
        __init__.py
        ham.py
    main.py
</code></pre>

<p>Accessing the <code>foo</code> package is simple enough, as <code>main.py</code> is executed from the toplevel <code>foo/</code> directory; but the <code>bar</code> package is nested within another <code>bar</code> directory and is not directly importable.</p>

<p>This is solvable readily enough, by modifying <code>sys.path</code> at the beginning of <code>main.py</code>:</p>

<pre><code>import sys

# Or sys.path.append()
sys.path.insert(0, './bar')

from bar.eggs import Eggs
from foo.ham import Ham
</code></pre>

<p>(Note: this code example assumes that <code>main.py</code> will always be invoked from <code>foo/</code>; in cases where this may not be the case, <code>'.bar'</code> could be replaced with <code>os.path.join(os.path.dirname(__file__), 'bar')</code> though this is clearly more unwieldy.)</p>

<p><strong>The Problem</strong></p>

<p>Unfortunately, pylint doesn't like this solution. While the code works, the linter considers the <code>sys.path</code> modifications to be a block of code ending the ""top of the module"" and gives an undesirable <code>wrong-import-position</code> warning:</p>

<pre><code>C: 6, 0: Import ""from bar.eggs import Eggs"" should be placed at the top of the module (wrong-import-position)
C: 7, 0: Import ""from foo.ham import Ham"" should be placed at the top of the module (wrong-import-position)
</code></pre>

<hr>

<p><strong>Similar questions</strong></p>

<p><a href=""https://stackoverflow.com/questions/44732819/adding-a-path-to-sys-path-in-python-and-pylint"">Adding a path to sys.path in python and pylint</a></p>

<p>This questioner has an issue with pylint failing to correctly parse the imports altogether. The lone answer to the this question suggests adding to pylint's internal path; this does nothing to avoid complaints about an interleaved <code>sys.path</code> modification.</p>
",1795505,2805,17-02-2018 00:08,17-02-2018 00:08,0,2815,34,1,21,,"{'badge_counts': {'bronze': 34, 'silver': 21, 'gold': 1}, 'account_id': 2005475, 'is_employee': False, 'last_modified_date': 1706010901, 'last_access_date': 1706412356, 'reputation_change_year': 21, 'reputation_change_quarter': 21, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2815, 'creation_date': 1351892211, 'user_type': 'registered', 'user_id': 1795505, 'location': 'Midwestern United States', 'website_url': 'https://www.d48.io', 'link': 'https://stackoverflow.com/users/1795505/pydsigner', 'profile_image': 'https://i.stack.imgur.com/7mj1s.png?s=256&g=1', 'display_name': 'pydsigner'}","Background I have a Python application dependent upon another package which is provided as a git submodule, yielding a directory structure similar to the following: Accessing the package is simple enough, as is executed from the toplevel directory; but the package is nested within another directory and is not directly importable. This is solvable readily enough, by modifying at the beginning of : (Note: this code example assumes that will always be invoked from ; in cases where this may not be the case, could be replaced with though this is clearly more unwieldy.) The Problem Unfortunately, pylint doesn't like this solution. While the code works, the linter considers the modifications to be a block of code ending the ""top of the module"" and gives an undesirable warning: Similar questions Adding a path to sys.path in python and pylint This questioner has an issue with pylint failing to correctly parse the imports altogether. The lone answer to the this question suggests adding to pylint's internal path; this does nothing to avoid complaints about an interleaved modification.","foo/
    bar/
        bar/
            __init__.py
            eggs.py
        test/
        setup.py
    foo/
        __init__.py
        ham.py
    main.py
 foo main.py foo/ bar bar sys.path main.py import sys

# Or sys.path.append()
sys.path.insert(0, './bar')

from bar.eggs import Eggs
from foo.ham import Ham
 main.py foo/ '.bar' os.path.join(os.path.dirname(__file__), 'bar') sys.path wrong-import-position C: 6, 0: Import ""from bar.eggs import Eggs"" should be placed at the top of the module (wrong-import-position)
C: 7, 0: Import ""from foo.ham import Ham"" should be placed at the top of the module (wrong-import-position)
 sys.path",3,47,0,1,
699,49083680,69079804,4936,How are the new tf.contrib.summary summaries in TensorFlow evaluated?,1,<python><tensorflow><tensorboard>,150,"<p>I'm having a bit of trouble understanding the new <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/summary"" rel=""nofollow noreferrer""><code>tf.contrib.summary</code></a> API. In the old one, it seemed that all one was supposed to do was to run <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/merge_all"" rel=""nofollow noreferrer""><code>tf.summary.merge_all()</code></a> and run that as an op.</p>
<p>But now we have things like <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/summary/record_summaries_every_n_global_steps"" rel=""nofollow noreferrer""><code>tf.contrib.summary.record_summaries_every_n_global_steps</code></a>, which can be used like this:</p>
<pre><code>import tensorflow.contrib.summary as tfsum

summary_writer = tfsum.create_file_writer(logdir, flush_millis=3000)
summaries = []

# First we create one summary which runs every n global steps
with summary_writer.as_default(), tfsum.record_summaries_every_n_global_steps(30):
    summaries.append(tfsum.scalar(&quot;train/loss&quot;, loss))

# And then one that runs every single time?
with summary_writer.as_default(), tfsum.always_record_summaries():
    summaries.append(tfsum.scalar(&quot;train/accuracy&quot;, accuracy))

# Then create an optimizer which uses a global step
step = tf.create_global_step()
train = tf.train.AdamOptimizer().minimize(loss, global_step=step)
</code></pre>
<p>And now come a few questions:</p>
<ol>
<li>If we just run <code>session.run(summaries)</code> in a loop, I assume that the accuracy summary would get written every single time, while the loss one wouldn't, because it only gets written if the global step is divisible by 30?</li>
<li>Assuming the summaries automatically evaluate their dependencies, I never need to run <code>session.run([accuracy, summaries])</code> but can just run, <code>session.run(summaries)</code> since they have a dependency in the graph, right?</li>
<li>If 2) is true, can't I just add a control dependency to the training step so that the summaries are written on every train run? Or is this a bad practice?</li>
<li>Is there any downside to using control dependencies in general for things that are going to be evaluated at the same time anyway?</li>
<li>Why does <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/summary/scalar"" rel=""nofollow noreferrer""><code>tf.contrib.summary.scalar</code></a> (and others) take in a <code>step</code> parameter?</li>
</ol>
<p>By adding a control dependency in 3) I mean doing this:</p>
<pre><code>tf.control_dependencies(summaries):
    train = tf.train.AdamOptimizer().minimize(loss, global_step=step)
</code></pre>
",72583,86262,03-03-2018 11:34,06-09-2021 20:06,1283,86322,329,91,232,78,"{'badge_counts': {'bronze': 329, 'silver': 232, 'gold': 91}, 'account_id': 27479, 'is_employee': False, 'last_modified_date': 1706684421, 'last_access_date': 1711140113, 'reputation_change_year': 310, 'reputation_change_quarter': 310, 'reputation_change_month': 120, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 86322, 'creation_date': 1235952284, 'user_type': 'registered', 'user_id': 72583, 'accept_rate': 78, 'location': 'Prague, Czech Republic', 'website_url': 'http://blog.jakuba.net', 'link': 'https://stackoverflow.com/users/72583/jakub-arnold', 'profile_image': 'https://www.gravatar.com/avatar/16a5eec5eca4550033c0e19dba3b32b8?s=256&d=identicon&r=PG', 'display_name': 'Jakub Arnold'}","I'm having a bit of trouble understanding the new API. In the old one, it seemed that all one was supposed to do was to run and run that as an op. But now we have things like , which can be used like this: And now come a few questions: If we just run in a loop, I assume that the accuracy summary would get written every single time, while the loss one wouldn't, because it only gets written if the global step is divisible by 30? Assuming the summaries automatically evaluate their dependencies, I never need to run but can just run, since they have a dependency in the graph, right? If 2) is true, can't I just add a control dependency to the training step so that the summaries are written on every train run? Or is this a bad practice? Is there any downside to using control dependencies in general for things that are going to be evaluated at the same time anyway? Why does (and others) take in a parameter? By adding a control dependency in 3) I mean doing this:","tf.contrib.summary tf.summary.merge_all() tf.contrib.summary.record_summaries_every_n_global_steps import tensorflow.contrib.summary as tfsum

summary_writer = tfsum.create_file_writer(logdir, flush_millis=3000)
summaries = []

# First we create one summary which runs every n global steps
with summary_writer.as_default(), tfsum.record_summaries_every_n_global_steps(30):
    summaries.append(tfsum.scalar(&quot;train/loss&quot;, loss))

# And then one that runs every single time?
with summary_writer.as_default(), tfsum.always_record_summaries():
    summaries.append(tfsum.scalar(&quot;train/accuracy&quot;, accuracy))

# Then create an optimizer which uses a global step
step = tf.create_global_step()
train = tf.train.AdamOptimizer().minimize(loss, global_step=step)
 session.run(summaries) session.run([accuracy, summaries]) session.run(summaries) tf.contrib.summary.scalar step tf.control_dependencies(summaries):
    train = tf.train.AdamOptimizer().minimize(loss, global_step=step)
",8,31,0,4,
700,49220022,49220098,234732,How can mypy ignore a single line in a source file?,5,<python><types><mypy>,244,"<p>I'm using <a href=""http://mypy-lang.org/"" rel=""noreferrer"">mypy</a> in my python project for type checking. I'm also using PyYAML for reading and writing the project configuration files. Unfortunately, when using the <a href=""http://pyyaml.org/wiki/PyYAMLDocumentation"" rel=""noreferrer"">recommended import mechanism from the PyYAML documentation</a> this generates a spurious error in a try/except clause that attempts to import native libraries:</p>

<pre><code>from yaml import load, dump
try:
    from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
    from yaml import Loader, Dumper
</code></pre>

<p>On my system <code>CLoader</code> and <code>CDumper</code> aren't present, which results in the errors <code>error: Module 'yaml' has no attribute 'CLoader'</code> and <code>error: Module 'yaml' has no attribute 'CDumper'</code>.</p>

<p>Is there a way to have mypy ignore errors on this line? I was hoping that I could do something like this to have mypy skip that line:</p>

<pre><code>from yaml import load, dump
try:
    from yaml import CLoader as Loader, CDumper as Dumper  # nomypy
except ImportError:
    from yaml import Loader, Dumper
</code></pre>
",57626,5073,11-03-2018 12:32,11-03-2018 12:42,0,5073,48,4,31,78,"{'badge_counts': {'bronze': 48, 'silver': 31, 'gold': 4}, 'account_id': 23095, 'is_employee': False, 'last_modified_date': 1633615733, 'last_access_date': 1709866535, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5073, 'creation_date': 1228340969, 'user_type': 'registered', 'user_id': 57626, 'accept_rate': 78, 'location': 'New England', 'website_url': 'http://patrick.wagstrom.net/weblog/', 'link': 'https://stackoverflow.com/users/57626/pridkett', 'profile_image': 'https://www.gravatar.com/avatar/3b2319bbc1b3531865eed255a5f34a33?s=256&d=identicon&r=PG', 'display_name': 'Pridkett'}","I'm using mypy in my python project for type checking. I'm also using PyYAML for reading and writing the project configuration files. Unfortunately, when using the recommended import mechanism from the PyYAML documentation this generates a spurious error in a try/except clause that attempts to import native libraries: On my system and aren't present, which results in the errors and . Is there a way to have mypy ignore errors on this line? I was hoping that I could do something like this to have mypy skip that line:","from yaml import load, dump
try:
    from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
    from yaml import Loader, Dumper
 CLoader CDumper error: Module 'yaml' has no attribute 'CLoader' error: Module 'yaml' has no attribute 'CDumper' from yaml import load, dump
try:
    from yaml import CLoader as Loader, CDumper as Dumper  # nomypy
except ImportError:
    from yaml import Loader, Dumper
",4,19,0,2,
701,50296097,50296116,13665,How to initialize a SimpleNamespace from a dict,1,<python><python-3.x><dictionary>,18,"<p>I'm sure this must be simple, but I could not find the answer.</p>

<p>I have a dictionary like:</p>

<pre><code>d = {'a': 1, 'b':2}
</code></pre>

<p>I'd like to access that via dot notation, like: <code>d.a</code></p>

<p>The <a href=""https://docs.python.org/3/library/types.html"" rel=""noreferrer""><code>SimpleNamespace</code></a> is designed for this, but I cannot just pass the dict into the SimpleNamespace constructor. 
I get the error: <code>TypeError: no positional arguments expected</code></p>

<p>How do I initialize the SimpleNamespace from a dictionary?</p>
",18044,22692,11-05-2018 15:58,11-05-2018 15:59,0,22742,96,9,69,82,"{'badge_counts': {'bronze': 96, 'silver': 69, 'gold': 9}, 'account_id': 9706, 'is_employee': False, 'last_modified_date': 1704631501, 'last_access_date': 1710477071, 'reputation_change_year': 250, 'reputation_change_quarter': 250, 'reputation_change_month': 70, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 22742, 'creation_date': 1221761004, 'user_type': 'registered', 'user_id': 18044, 'accept_rate': 82, 'location': 'Redmond, WA', 'website_url': '', 'link': 'https://stackoverflow.com/users/18044/mvdd', 'profile_image': 'https://www.gravatar.com/avatar/1f0136186ca27b7b070b49cf1973885e?s=256&d=identicon&r=PG', 'display_name': 'MvdD'}","I'm sure this must be simple, but I could not find the answer. I have a dictionary like: I'd like to access that via dot notation, like: The is designed for this, but I cannot just pass the dict into the SimpleNamespace constructor. I get the error: How do I initialize the SimpleNamespace from a dictionary?","d = {'a': 1, 'b':2}
 d.a SimpleNamespace TypeError: no positional arguments expected",-3,13,0,1,
702,48572648,48627770,4098,How to monitor python's concurrent.futures.ProcessPoolExecutor?,2,<python><process><monitoring><concurrent.futures><capacity>,14,"<p>We are using the <a href=""https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor"" rel=""noreferrer"">ProcessPoolExecutor</a> from <code>concurrent.futures</code> in a service that asynchronously receives requests, and does the actual, synchronous processing in the process pool.</p>

<p>Once we ran into the case that the process pool was exhausted, so new requests had to wait until some other processes were finished.</p>

<p>Is there a way to interrogate the process pool for its current usage? That would allow us to monitor their state and do proper capacity planning.</p>

<p>If there isn't, is there any good alternative process pool implementation with an asynchronous interface that supports such monitoring/capacity planning?</p>
",14132,12760,01-02-2018 22:15,05-02-2018 17:05,4,12760,63,1,42,,"{'badge_counts': {'bronze': 63, 'silver': 42, 'gold': 1}, 'account_id': 8008, 'is_employee': False, 'last_modified_date': 1607615578, 'last_access_date': 1711005914, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 12760, 'creation_date': 1221602832, 'user_type': 'registered', 'user_id': 14132, 'location': 'Germany', 'website_url': 'http://perlgeek.de/', 'link': 'https://stackoverflow.com/users/14132/moritz', 'profile_image': 'https://www.gravatar.com/avatar/579c95ecf03cf9f61e5e6637e57feebe?s=256&d=identicon&r=PG', 'display_name': 'moritz'}","We are using the ProcessPoolExecutor from in a service that asynchronously receives requests, and does the actual, synchronous processing in the process pool. Once we ran into the case that the process pool was exhausted, so new requests had to wait until some other processes were finished. Is there a way to interrogate the process pool for its current usage? That would allow us to monitor their state and do proper capacity planning. If there isn't, is there any good alternative process pool implementation with an asynchronous interface that supports such monitoring/capacity planning?",concurrent.futures,-1,7,0,1,
703,49348851,49521560,16906,celery shutdown worker after particular task,3,<python><celery>,29,"<p>I'm using celery (solo pool with concurrency=1) and I want to be able to shut down the worker after a particular task has run. A caveat is that I want to avoid any possibility of the worker picking up any further tasks after that one.</p>

<p>Here's my attempt in the outline:</p>

<pre><code>from __future__ import absolute_import, unicode_literals
from celery import Celery
from celery.exceptions import WorkerShutdown
from celery.signals import task_postrun

app = Celery()
app.config_from_object('celeryconfig')

@app.task
def add(x, y):
    return x + y

@task_postrun.connect(sender=add)
def shutdown(*args, **kwargs):
    raise WorkerShutdown()
</code></pre>

<p>However, when I run the worker</p>

<pre><code>celery -A celeryapp  worker --concurrency=1 --pool=solo
</code></pre>

<p>and run the task</p>

<pre><code>add.delay(1,4)
</code></pre>

<p>I get the following:</p>

<pre><code> -------------- celery@sam-APOLLO-2000 v4.0.2 (latentcall)
---- **** ----- 
--- * ***  * -- Linux-4.4.0-116-generic-x86_64-with-Ubuntu-16.04-xenial 2018-03-18 14:08:37
-- * - **** --- 
- ** ---------- [config]
- ** ---------- .&gt; app:         __main__:0x7f596896ce90
- ** ---------- .&gt; transport:   redis://localhost:6379/0
- ** ---------- .&gt; results:     redis://localhost/
- *** --- * --- .&gt; concurrency: 4 (solo)
-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)
--- ***** ----- 
 -------------- [queues]
                .&gt; celery           exchange=celery(direct) key=celery


[2018-03-18 14:08:39,892: WARNING/MainProcess] Restoring 1 unacknowledged message(s)
</code></pre>

<p>The task is re-queued and will be run again on another worker, leading to a loop.</p>

<p>This also happens when I move the <code>WorkerShutdown</code> exception within the task itself.</p>

<pre><code>@app.task
def add(x, y):
    print(x + y)
    raise WorkerShutdown()
</code></pre>

<p>Is there a way I can shut down the worker after a particular task, while avoiding this unfortunate side-effect?</p>
",1256529,3535,18-03-2018 14:15,27-03-2018 20:04,9,3535,44,3,26,64,"{'badge_counts': {'bronze': 44, 'silver': 26, 'gold': 3}, 'account_id': 1307961, 'is_employee': False, 'last_modified_date': 1703322001, 'last_access_date': 1676991330, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3535, 'creation_date': 1331194925, 'user_type': 'registered', 'user_id': 1256529, 'accept_rate': 64, 'location': 'London, United Kingdom', 'website_url': 'http://www.samfrances.co.uk', 'link': 'https://stackoverflow.com/users/1256529/samfrances', 'profile_image': 'https://i.stack.imgur.com/G5zlw.jpg?s=256&g=1', 'display_name': 'samfrances'}","I'm using celery (solo pool with concurrency=1) and I want to be able to shut down the worker after a particular task has run. A caveat is that I want to avoid any possibility of the worker picking up any further tasks after that one. Here's my attempt in the outline: However, when I run the worker and run the task I get the following: The task is re-queued and will be run again on another worker, leading to a loop. This also happens when I move the exception within the task itself. Is there a way I can shut down the worker after a particular task, while avoiding this unfortunate side-effect?","from __future__ import absolute_import, unicode_literals
from celery import Celery
from celery.exceptions import WorkerShutdown
from celery.signals import task_postrun

app = Celery()
app.config_from_object('celeryconfig')

@app.task
def add(x, y):
    return x + y

@task_postrun.connect(sender=add)
def shutdown(*args, **kwargs):
    raise WorkerShutdown()
 celery -A celeryapp  worker --concurrency=1 --pool=solo
 add.delay(1,4)
  -------------- celery@sam-APOLLO-2000 v4.0.2 (latentcall)
---- **** ----- 
--- * ***  * -- Linux-4.4.0-116-generic-x86_64-with-Ubuntu-16.04-xenial 2018-03-18 14:08:37
-- * - **** --- 
- ** ---------- [config]
- ** ---------- .&gt; app:         __main__:0x7f596896ce90
- ** ---------- .&gt; transport:   redis://localhost:6379/0
- ** ---------- .&gt; results:     redis://localhost/
- *** --- * --- .&gt; concurrency: 4 (solo)
-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)
--- ***** ----- 
 -------------- [queues]
                .&gt; celery           exchange=celery(direct) key=celery


[2018-03-18 14:08:39,892: WARNING/MainProcess] Restoring 1 unacknowledged message(s)
 WorkerShutdown @app.task
def add(x, y):
    print(x + y)
    raise WorkerShutdown()
",31,62,0,0,
704,49768306,49768500,523684,Pytorch tensor to numpy array,6,<python><numpy><pytorch>,169,"<p>I have a pytorch <code>Tensor</code> of shape <code>[4, 3, 966, 1296]</code>. I want to convert it to <code>numpy</code> array using the following code:</p>
<pre><code>imgs = imgs.numpy()[:, ::-1, :, :]
</code></pre>
<p>How does that code work?</p>
",1249184,2269,11-04-2018 06:49,11-04-2018 07:00,0,2279,27,2,20,62,"{'badge_counts': {'bronze': 27, 'silver': 20, 'gold': 2}, 'collectives': [{'collective': {'tags': ['azure-timeseries-insights', 'azure-storage', 'azure-mobile-services', 'azure-authentication', 'azure-storage-emulator', 'azurerm-app-service', 'azure-qna-maker', 'azure-http-trigger', 'azure-compliance-policy', 'azure-cloud-shell', 'azure-ml-component', 'azure-signalr', 'microsoft-custom-vision', 'azure-dashboard', 'azure-blob-trigger', 'azure-ad-b2b', 'azure-regions', 'azure-webjobs-continuous', 'azure-mapping-data-flow', 'azure-rest-api', 'azure-private-dns-zone', 'azure-devtest-labs', 'azure-functions-core-tools', 'microsoft-entra-id', 'azure-webjobs-triggered', 'azure-cost-calculation', 'azure-nsg', 'azure-waf', 'azure-ddos', 'azure-hub', 'azure-storage-account', 'azure-managed-grafana', 'kitchen-azurerm', 'azure-app-service-plans', 'azure-stream-analytics', 'azure-digital-twins', 'azure-management-portal', 'azure-china', 'azure-sdk', 'azure-managed-database', 'azure-python-sdk', 'azure-compute-emulator', 'azure-function-app-proxy', 'azure-sdk-go', 'sitecore-azure', 'azure-storage-explorer', 'azure-service-principal', 'azure-cdn', 'azure-api-apps', 'azure-resource-graph', 'azure-communication-services', 'azure-app-api', 'azure-iot-suite', 'azure-container-instances', 'azure-synapse', 'azure-runbook', 'azure-analysis-services', 'azure-lab-services', 'azure-rm', 'azure-eventhub', 'azure-emulator', 'azure-cosmosdb-mongoapi', 'azure-update-management-center', 'fhir-server-for-azure', 'azure-mcd', 'sql-azure-alerts', 'azure-sql-server', 'azure-ilb', 'azure-servicebus-subscriptions', 'azure-redis-cache', 'azure-application-roles', 'azure-keyvault', 'azure-database-mysql', 'azure-sql-reporting', 'azure-eventgrid', 'azure-availability-set', 'azure-cosmosdb-gremlinapi', 'azure-packaging', 'azure-file-copy', 'azure-cosmosdb-sqlapi', 'azure-availability-zones', 'azure-sql-managed-instance', 'azure-functions-proxies', 'microsoft-entra-private-access', 'azure-static-website-hosting', 'azure-webjobs', 'azure-virtual-machine', 'azure-data-factory', 'azure-static-web-app-routing', 'azure-web-roles', 'azure-java-tools', 'azure-worker-roles', 'azure-industrial-iot', 'azure-site-recovery', 'azure-mobile-engagement', 'azure-servicebus-queues', 'azure-node-sdk', 'azure-log-analytics', 'azure-vpn', 'azure-cloud-services', 'azure-ai-translator', 'azure-ad-b2c-custom-policy', 'azureml-python-sdk', 'azure-private-dns', 'azure-function-async', 'django-pyodbc-azure', 'azure-text-translation', 'azure-maps', 'azure-policy', 'spring-cloud-azure', 'azure-storage-files', 'azure-cosmosdb', 'azure-data-lake-gen2', 'azure-iot-hub', 'azure-advisor', 'azure-application-insights-profiler', 'azure-service-hooks', 'azure-pack', 'azure-application-gateway', 'azure-relay', 'azure-app-registration', 'azure-ad-role', 'azure-billing', 'azure-purview', 'azureportal', 'azure-front-door', 'azure-logic-apps', 'azure-servicebusrelay', 'azure-cli2', 'azure-caching', 'azure-iot-central', 'azure-sdk-js', 'azure-in-role-cache', 'azure-agent', 'azure-ad-domain-services', 'azure-resource-manager', 'azure-anomaly-detection', 'azure-iot-sdk', 'azure-cognitive-search', 'azure-acr', 'azure-traffic-manager', 'passport-azure-ad', 'azure-synapse-link', 'azure-data-share', 'azure-configuration', 'azure-active-directory', 'azure-resource-group', 'defaultazurecredential', 'pulumi-azure', 'azure-zulu', 'azure-files', 'azure-dsvm', 'azure-sdk-for-java', 'azure-security-center', 'azure-dev-spaces', 'azure-application-insights', 'azure-metrics-advisor', 'azure-billing-api', 'azure-defender', 'azure-marketplace', 'azureshell', 'azure-promptflow', 'azure-image-builder', 'azure-pipelines-release-pipeline', 'azure-deployment', 'azure-backup-vault', 'azure-static-web-app', 'azure-service-fabric', 'azure-rm-template', 'azure-role-environment', 'azure-git-deployment', 'azure-ad-verifiable-credentials', 'azure', 'azure-machine-learning-service', 'azure-sql-edge', 'azure-cosmosdb-tables', 'azure-sas', 'azure-migrate', 'azure-triggers', 'azure-management', 'azure-web-app-for-containers', 'azure-video-indexer', 'azure-disk', 'azure-security', 'azure-anomaly-detector', 'azure-media-services', 'azure-analytics', 'azure-ad-powershell-v2', 'azure-container-apps', 'azure-store', 'microsoft-entra-internet-access', 'azure-alerts', 'azure-private-link', 'adal.js', 'azure-ad-b2c', 'azure-sdk-for-go', 'azure-application-proxy', 'azure-hybrid-connections', 'azure-rbac', 'azure.data.tables', 'azure-tablequery', 'azure-application-registration', 'azure-subscription', 'azure-mysql-database', 'azure-data-explorer', 'sql-azure-federations', 'azure-managed-identity', 'azure-feature-manager', 'azure-form-recognizer', 'azure-xplat-cli', 'azure-diagnostics', 'azure-remote-rendering', 'azure-web-pubsub', 'azure-managed-app', 'azure-notebooks', 'spark-bash-azure-databricks', 'azure-spring-cloud', 'azure-bastion', 'azure-hdinsight', 'azure-appfabric', 'azure-blueprints', 'azure-cosmosdb-changefeed', 'azure-aks', 'azure-iot-hub-device-management', 'azure-virtual-network', 'azure-ai', 'azure-elastic-scale', 'azure-web-app-firewall', 'azure-linux', 'azurekinect', 'azure-notificationhub', 'azure-oms', 'azure-functions-docker', 'azure-gov', 'azure-android-sdk', 'azure-stack', 'azure-functions', 'azure-monitor-workbooks', 'azure-function-queue', 'azure-load-balancer', 'azure-affinity-group', 'azure-load-testing', 'azure-identity', 'azure-sdk-python', 'azure-postgresql', 'azure-batch-account', 'azure-cosmosdb-cassandra-api', 'azure-autoscaling-block', 'azure-clouddrive', 'azure-service-plan', 'azure-ad-graph-api', 'azure-adf', 'azure-vm-scale-set', 'azure-durable-functions', 'azure-monitoring', 'azure-log-analytics-workspace', 'azure-monitor', 'azure-container-registry', 'azure-calculator', 'azure-auto-ml', 'azuremlsdk', 'azure-sql', 'azure-data-sync', 'azure-bot-service', 'azure-ase', 'azure-debugger', 'azure-ad-v2', 'azure-function-http', 'azure-object-anchors', 'azure-container-service', 'terraform-provider-azure', 'azure-data-lake', 'azure-dns', 'azure-blockchain-workbench', 'azure-information-protection', 'azure-media-player', 'azure-public-ip', 'azure-blockchain-service', 'azure-sdk-.net', 'sql-server-azure', 'azure-java-sdk', 'azureservicebus', 'azure-webapps', 'azure-data-studio', 'azure-cognitive-services', 'azure-rtos', 'azure-data-catalog', 'azure-sphere', 'azure-secrets', 'azure-connect', 'azure-webjobssdk', 'azure-sdk-php', 'microsoft-entra-external-id', 'azure-arc', 'azure-static-website-routing', 'azure-ad-msal', 'azure-service-runtime', 'azure-search-.net-sdk', 'azure-management-groups', 'azure-deployment-slots', 'azure-sdk-ruby', 'azure-elasticpool', 'azure-vm-templates', 'azure-appservice', 'azure-spring-boot', 'azure-functions-runtime', 'azure-workflow-automation', 'azure-managed-disk', 'azure-service-fabric-mesh', 'kql', 'azure-spatial-anchors', 'rebus-azureservicebus', 'azure-blob-storage', 'azure-custom-providers', 'azure-storage-queues', 'azure-acs', 'adal', 'azure-sentinel', 'azure-iot-dps', 'azure-webhooks', 'azure-databoxfamily', 'azure-powershell', 'azure-web-app-service', 'azureclicredential', 'azure-language-understanding', 'azure-free-services', 'azure-adal-deprecation', 'azure-integration-account', 'azure-management-api', 'azure-application-settings', 'azure-app-service-envrmnt', 'azure-fluent-api', 'azure-batch', 'azure-sdk-for-ruby', 'azureadgraph-deprecation', 'azure-quantum', 'azure-performancecounters', 'azure-iot-edge', 'azure-scheduler', 'azure-custom-domain', 'azure-logic-app-standard', 'azure-vm', 'azure-api-management', 'azure-ml-pipelines', 'azure-queues', 'azure-database-postgresql', 'azure-servicebus-topics', 'azure-sql-database', 'azure-oauth', 'azure-iot-hub-device-update', 'azure-cosmosdb-emulator', 'azure-function-app', 'azure-app-configuration', 'azure-cli', 'azure-bicep', 'azure-speech', 'azure-table-storage', 'azure-elastic-sharding', 'azure-automation', 'azure-cosmosdb-mongovcore', 'azure-databricks'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}], 'account_id': 1298864, 'is_employee': False, 'last_modified_date': 1698784800, 'last_access_date': 1711164888, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2279, 'creation_date': 1330928719, 'user_type': 'registered', 'user_id': 1249184, 'accept_rate': 62, 'website_url': '', 'link': 'https://stackoverflow.com/users/1249184/dukelover', 'profile_image': 'https://i.stack.imgur.com/UkPsU.jpg?s=256&g=1', 'display_name': 'DukeLover'}",I have a pytorch of shape . I want to convert it to array using the following code: How does that code work?,"Tensor [4, 3, 966, 1296] numpy imgs = imgs.numpy()[:, ::-1, :, :]
",-3,4,0,0,
705,48255244,48255245,24357,Python — check if a string contains Cyrillic characters,4,<python><string><cyrillic>,22,"<p>How to check whether a string contains Cyrillic characters?</p>

<p>E.g.</p>

<pre><code>&gt;&gt;&gt; has_cyrillic('Hello, world!')
False
&gt;&gt;&gt; has_cyrillic('Привет, world!')
True
</code></pre>
",1977620,30436,14-01-2018 23:34,14-01-2018 23:34,0,30526,121,20,114,54,"{'badge_counts': {'bronze': 121, 'silver': 114, 'gold': 20}, 'account_id': 2239560, 'is_employee': False, 'last_modified_date': 1703299800, 'last_access_date': 1711157652, 'reputation_change_year': 462, 'reputation_change_quarter': 462, 'reputation_change_month': 136, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 30526, 'creation_date': 1358175312, 'user_type': 'registered', 'user_id': 1977620, 'accept_rate': 54, 'location': 'Dubai', 'website_url': 'https://maxmalysh.com', 'link': 'https://stackoverflow.com/users/1977620/max-malysh', 'profile_image': 'https://i.stack.imgur.com/lvWfj.jpg?s=256&g=1', 'display_name': 'Max Malysh'}",How to check whether a string contains Cyrillic characters? E.g.,"&gt;&gt;&gt; has_cyrillic('Hello, world!')
False
&gt;&gt;&gt; has_cyrillic('Привет, world!')
True
",3,9,0,0,
706,49834883,49835113,51787,Scatter plot form dataframe with index on x-axis,3,<python><pandas><matplotlib>,41,"<p>I've got pandas <code>DataFrame</code>, <code>df</code>, with <code>index</code> named <code>date</code> and the columns <code>columnA</code>, <code>columnB</code> and <code>columnC</code></p>

<p>I am trying to scatter plot <code>index</code> on a x-axis and <code>columnA</code> on a y-axis using the <code>DataFrame</code> syntax.</p>

<p>When I try:</p>

<pre><code>df.plot(kind='scatter', x='date', y='columnA')
</code></pre>

<p>I ma getting an error <code>KeyError: 'date'</code> probably because the <code>date</code> is not column</p>

<pre><code>df.plot(kind='scatter', y='columnA')
</code></pre>

<p>I am getting an error:</p>

<pre class=""lang-none prettyprint-override""><code>ValueError: scatter requires and x and y column
</code></pre>

<p>so no default index on x-axis.</p>

<pre><code>df.plot(kind='scatter', x=df.index, y='columnA')
</code></pre>

<p>I am getting error </p>

<pre class=""lang-none prettyprint-override""><code>KeyError: ""DatetimeIndex(['1818-01-01', '1818-01-02', '1818-01-03', '1818-01-04',\n
                          '1818-01-05', '1818-01-06', '1818-01-07', '1818-01-08',\n
                          '1818-01-09', '1818-01-10',\n               ...\n  
                          '2018-03-22', '2018-03-23', '2018-03-24', '2018-03-25',\n
                          '2018-03-26', '2018-03-27', '2018-03-28', '2018-03-29',\n 
                          '2018-03-30', '2018-03-31'],\n  
dtype='datetime64[ns]', name='date', length=73139, freq=None) not in index""
</code></pre>

<hr>

<p>I can plot it if I use <code>matplotlib.pyplot</code> directly</p>

<pre><code>plt.scatter(df.index, df['columnA'])
</code></pre>

<p><strong>Is there a way to plot index as <code>x-axis</code> using the <code>DataFrame</code> <code>kind</code> syntax?</strong></p>
",1214289,6761,14-04-2018 18:36,14-04-2018 19:02,0,6771,53,8,38,59,"{'badge_counts': {'bronze': 53, 'silver': 38, 'gold': 8}, 'account_id': 1240383, 'is_employee': False, 'last_modified_date': 1631201104, 'last_access_date': 1683623645, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 6771, 'creation_date': 1328915973, 'user_type': 'registered', 'user_id': 1214289, 'accept_rate': 59, 'website_url': '', 'link': 'https://stackoverflow.com/users/1214289/kocur4d', 'profile_image': 'https://www.gravatar.com/avatar/12267beae3c0d1e532fae7e9cc539941?s=256&d=identicon&r=PG', 'display_name': 'Kocur4d'}","I've got pandas , , with named and the columns , and I am trying to scatter plot on a x-axis and on a y-axis using the syntax. When I try: I ma getting an error probably because the is not column I am getting an error: so no default index on x-axis. I am getting error I can plot it if I use directly Is there a way to plot index as using the syntax?","DataFrame df index date columnA columnB columnC index columnA DataFrame df.plot(kind='scatter', x='date', y='columnA')
 KeyError: 'date' date df.plot(kind='scatter', y='columnA')
 ValueError: scatter requires and x and y column
 df.plot(kind='scatter', x=df.index, y='columnA')
 KeyError: ""DatetimeIndex(['1818-01-01', '1818-01-02', '1818-01-03', '1818-01-04',\n
                          '1818-01-05', '1818-01-06', '1818-01-07', '1818-01-08',\n
                          '1818-01-09', '1818-01-10',\n               ...\n  
                          '2018-03-22', '2018-03-23', '2018-03-24', '2018-03-25',\n
                          '2018-03-26', '2018-03-27', '2018-03-28', '2018-03-29',\n 
                          '2018-03-30', '2018-03-31'],\n  
dtype='datetime64[ns]', name='date', length=73139, freq=None) not in index""
 matplotlib.pyplot plt.scatter(df.index, df['columnA'])
 x-axis DataFrame kind",-10,43,0,0,
707,50356572,50356696,24773,Python datetime to epoch,4,<python><python-3.x><python-2.7>,13,"<pre><code>t='20180515102500'
d=datetime.strptime(t, ""%Y%m%d%H%M%S"")
millis_since_epoch = int(time.mktime(d.timetuple())) * 1000
print(millis_since_epoch)
</code></pre>

<p>gives me: 15263<strong>799</strong>00000 on repl.it(python 3.6.1)
and 
on my local: 15263<strong>979</strong>00000 (python 2.7.14)</p>

<p>Why?
What is the recommended way to convert datetime object to epoch?</p>
",1179299,356,15-05-2018 17:58,15-05-2018 18:07,0,356,16,1,4,86,"{'badge_counts': {'bronze': 16, 'silver': 4, 'gold': 1}, 'account_id': 1210709, 'is_employee': False, 'last_modified_date': 1573682838, 'last_access_date': 1544120525, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 356, 'creation_date': 1327972168, 'user_type': 'registered', 'user_id': 1179299, 'accept_rate': 86, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1179299/user1179299', 'profile_image': 'https://www.gravatar.com/avatar/f5ec0e42b4547f02709672696a86bb5a?s=256&d=identicon&r=PG', 'display_name': 'user1179299'}",gives me: 1526379900000 on repl.it(python 3.6.1) and on my local: 1526397900000 (python 2.7.14) Why? What is the recommended way to convert datetime object to epoch?,"t='20180515102500'
d=datetime.strptime(t, ""%Y%m%d%H%M%S"")
millis_since_epoch = int(time.mktime(d.timetuple())) * 1000
print(millis_since_epoch)
",3,12,0,0,
708,49755565,49760137,22979,How to generate python class files from protobuf,4,<python><protocol-buffers><grpc>,18,"<p>I am trying to transfer large amounts of structured data from Java to Python. That includes many objects that are related to each other in some form or another. When I receive them in my Python code, it's quiet ugly to work with the types that are provided by protobuf. My VIM IDE crashed when trying to use autocomplete on the types, PyCharm doesn't complete <em>anything</em> and generally it just seems absurd that they don't provide some clean <code>class</code> definition for the different types.</p>

<p>Is there a way to get IDE support while working with protobuf messages in python? I'm looking at 20+ methods handling complex messages and without IDE support I might as well code with notepad. </p>

<p><a href=""https://i.stack.imgur.com/7vmz1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7vmz1.png"" alt=""enter image description here""></a></p>

<p>I understand that protobuf is using metaclasses (although I don't know why they do that). Maybe there is a way to generate python class files from that data or maybe there is something similar to <a href=""https://www.npmjs.com/%7Etypes"" rel=""noreferrer"">typescript typing files</a>.</p>

<p>Did I maybe misuse protobuf? I believed I would describe my domain model in a way that may be used across languages. In Java I am happy with the generated classes and I can use them easily. Should I maybe have used something like <a href=""http://swagger.io"" rel=""noreferrer"">swagger.io</a> instead?</p>
",1170940,3074,10-04-2018 13:57,10-04-2018 17:54,0,3074,42,3,27,43,"{'badge_counts': {'bronze': 42, 'silver': 27, 'gold': 3}, 'account_id': 1200108, 'is_employee': False, 'last_modified_date': 1670634600, 'last_access_date': 1674724679, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3074, 'creation_date': 1327570314, 'user_type': 'registered', 'user_id': 1170940, 'accept_rate': 43, 'location': 'Berlin, Germany', 'website_url': 'http://pascalbrokmeier.de', 'link': 'https://stackoverflow.com/users/1170940/pascalwhoop', 'profile_image': 'https://i.stack.imgur.com/n7mDL.jpg?s=256&g=1', 'display_name': 'pascalwhoop'}","I am trying to transfer large amounts of structured data from Java to Python. That includes many objects that are related to each other in some form or another. When I receive them in my Python code, it's quiet ugly to work with the types that are provided by protobuf. My VIM IDE crashed when trying to use autocomplete on the types, PyCharm doesn't complete anything and generally it just seems absurd that they don't provide some clean definition for the different types. Is there a way to get IDE support while working with protobuf messages in python? I'm looking at 20+ methods handling complex messages and without IDE support I might as well code with notepad. I understand that protobuf is using metaclasses (although I don't know why they do that). Maybe there is a way to generate python class files from that data or maybe there is something similar to typescript typing files. Did I maybe misuse protobuf? I believed I would describe my domain model in a way that may be used across languages. In Java I am happy with the generated classes and I can use them easily. Should I maybe have used something like swagger.io instead?",class,-1,9,1,3,
709,48967021,50138992,4551,"Compile protobuf to python, cmake command",1,<python><cmake><protocol-buffers>,12,"<p>I have some proto definitions that I compile to cpp.<br>
To generate the corresponding make target I use cmake like this:<br>
<code>protobuf_generate_cpp(CPP_SOURCES PROTO_HEADERS ${PROTO_FILES})</code><br>
And I use the <code>CPP_SOURCES</code> to build my lib.</p>

<p>Now I need to compile the same proto files for python also and I added this:<br>
<code>protobuf_generate_python(PY_SOURCES ${PROTO_FILES})</code><br>
This alone has no effect, and I am not sure what I should / can add more in order to have some make target that will trigger also the <em>protoc</em> for <em>python</em></p>
",1156145,1073,24-02-2018 19:59,02-05-2018 16:06,67,1083,34,1,16,,"{'badge_counts': {'bronze': 34, 'silver': 16, 'gold': 1}, 'account_id': 1181259, 'is_employee': False, 'last_modified_date': 1707531600, 'last_access_date': 1710672183, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1083, 'creation_date': 1326887823, 'user_type': 'registered', 'user_id': 1156145, 'website_url': '', 'link': 'https://stackoverflow.com/users/1156145/codentary', 'profile_image': 'https://i.stack.imgur.com/mNDqY.png?s=256&g=1', 'display_name': 'codentary'}","I have some proto definitions that I compile to cpp. To generate the corresponding make target I use cmake like this: And I use the to build my lib. Now I need to compile the same proto files for python also and I added this: This alone has no effect, and I am not sure what I should / can add more in order to have some make target that will trigger also the protoc for python",protobuf_generate_cpp(CPP_SOURCES PROTO_HEADERS ${PROTO_FILES}) CPP_SOURCES protobuf_generate_python(PY_SOURCES ${PROTO_FILES}),-3,8,0,0,
710,49866283,49876768,31300,How to run Google gsutil using Python,2,<python><google-cloud-platform><gcloud><gsutil>,14,"<p>After installing and configuring Google Cloud SDK <code>gsutil</code> command can be run by simply typing its name and the argument(-s) using Windows cmd.</p>

<p>Here is the example:</p>

<p><code>""C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\bin\gcloud"" version</code></p>

<p><a href=""https://i.stack.imgur.com/rheCS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rheCS.png"" alt=""enter image description here""></a></p>

<p>But the same command fails if run using Python subprocess. 
With subprocess's <code>shell</code> argument set to True the <code>ImportError</code> occurs:</p>

<pre><code>import subprocess

cmd = '""C:/Program Files (x86)/Google/Cloud SDK/google-cloud-sdk/bin/gsutil"" version'

p = subprocess.Popen(cmd, shell=True)
</code></pre>

<p>.....</p>

<pre><code>ImportError: No module named site
</code></pre>

<p>With subprocess's <code>shell</code> argument set to False then the <code>WindowsError: [Error 2] The system cannot find the file specified</code> occurs:</p>

<pre><code>p = subprocess.Popen(cmd, shell=False)
</code></pre>

<p>Is there a way to run <code>gsutil</code> on Windows using Python?</p>
",1107049,18577,16-04-2018 21:10,17-04-2018 11:21,1,18617,408,67,258,97,"{'badge_counts': {'bronze': 408, 'silver': 258, 'gold': 67}, 'account_id': 1118195, 'is_employee': False, 'last_modified_date': 1695086400, 'last_access_date': 1711134717, 'reputation_change_year': 240, 'reputation_change_quarter': 240, 'reputation_change_month': 60, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 18617, 'creation_date': 1324349536, 'user_type': 'registered', 'user_id': 1107049, 'accept_rate': 97, 'location': 'Santa Monica, CA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/1107049/alphanumeric', 'profile_image': 'https://www.gravatar.com/avatar/8f94b3bb695b3bbd83be44eb151aa63e?s=256&d=identicon&r=PG', 'display_name': 'alphanumeric'}",After installing and configuring Google Cloud SDK command can be run by simply typing its name and the argument(-s) using Windows cmd. Here is the example: But the same command fails if run using Python subprocess. With subprocess's argument set to True the occurs: ..... With subprocess's argument set to False then the occurs: Is there a way to run on Windows using Python?,"gsutil ""C:\Program Files (x86)\Google\Cloud SDK\google-cloud-sdk\bin\gcloud"" version shell ImportError import subprocess

cmd = '""C:/Program Files (x86)/Google/Cloud SDK/google-cloud-sdk/bin/gsutil"" version'

p = subprocess.Popen(cmd, shell=True)
 ImportError: No module named site
 shell WindowsError: [Error 2] The system cannot find the file specified p = subprocess.Popen(cmd, shell=False)
 gsutil",-3,29,1,1,
711,49934598,49934693,11531,How can I get the size of a TemporaryFile in python?,2,<python>,11,"<p>Currently when I write to a temporary:</p>

<pre><code>import tempfile
a = tempfile.TemporaryFile()

a.write(...)

# The only way I know to get the size
a.seek(0)
len(a.read())
</code></pre>

<p>Is there a better way?</p>
",1103966,10290,20-04-2018 05:28,20-04-2018 05:36,0,10280,133,23,84,76,"{'badge_counts': {'bronze': 133, 'silver': 84, 'gold': 23}, 'account_id': 1114198, 'is_employee': False, 'last_modified_date': 1710101100, 'last_access_date': 1568661338, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 10280, 'creation_date': 1324161647, 'user_type': 'registered', 'user_id': 1103966, 'accept_rate': 76, 'location': 'San Diego, CA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1103966/dr-knowitall', 'profile_image': 'https://www.gravatar.com/avatar/0fb92143734ac26c56fca34c669cd99a?s=256&d=identicon&r=PG', 'display_name': 'Dr.Knowitall'}",Currently when I write to a temporary: Is there a better way?,"import tempfile
a = tempfile.TemporaryFile()

a.write(...)

# The only way I know to get the size
a.seek(0)
len(a.read())
",7,13,0,0,
712,48532411,48534215,23795,How do I wait for ThreadPoolExecutor.map to finish,3,<python><python-3.x><python-multithreading>,17,"<p>I have the following code, which has been simplified:</p>

<pre><code>import concurrent.futures

pool = concurrent.futures.ThreadPoolExecutor(8)

def _exec(x):
    return x + x

myfuturelist = pool.map(_exec,[x for x in range(5)])

# How do I wait for my futures to finish?

for result in myfuturelist:
    # Is this how it's done?
    print(result)

#... stuff that should happen only after myfuturelist is
#completely resolved.
# Documentation says pool.map is asynchronous
</code></pre>

<p>The documentation is weak regarding ThreadPoolExecutor.map. Help would be great.</p>

<p>Thanks! </p>
",1103966,10290,31-01-2018 00:01,31-01-2018 03:55,0,10280,133,23,84,76,"{'badge_counts': {'bronze': 133, 'silver': 84, 'gold': 23}, 'account_id': 1114198, 'is_employee': False, 'last_modified_date': 1710101100, 'last_access_date': 1568661338, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 10280, 'creation_date': 1324161647, 'user_type': 'registered', 'user_id': 1103966, 'accept_rate': 76, 'location': 'San Diego, CA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1103966/dr-knowitall', 'profile_image': 'https://www.gravatar.com/avatar/0fb92143734ac26c56fca34c669cd99a?s=256&d=identicon&r=PG', 'display_name': 'Dr.Knowitall'}","I have the following code, which has been simplified: The documentation is weak regarding ThreadPoolExecutor.map. Help would be great. Thanks!","import concurrent.futures

pool = concurrent.futures.ThreadPoolExecutor(8)

def _exec(x):
    return x + x

myfuturelist = pool.map(_exec,[x for x in range(5)])

# How do I wait for my futures to finish?

for result in myfuturelist:
    # Is this how it's done?
    print(result)

#... stuff that should happen only after myfuturelist is
#completely resolved.
# Documentation says pool.map is asynchronous
",17,25,0,0,
713,48719893,48760481,2023,Why is the block size for Python httplib's reads hard coded as 8192 bytes,2,<python><http><httplib>,14,"<p>I'm looking to make a fast streaming download -> upload to move large files via HTTP from one server to another.</p>

<p>During this, I've noticed that httplib, that is used by urllib3 and therefore also requests, seems to hard code how much it fetches from a stream at a time to 8192 bytes</p>

<p><a href=""https://github.com/python/cpython/blob/28453feaa8d88bbcbf6d834b1d5ca396d17265f2/Lib/http/client.py#L970"" rel=""noreferrer"">https://github.com/python/cpython/blob/28453feaa8d88bbcbf6d834b1d5ca396d17265f2/Lib/http/client.py#L970</a></p>

<p>Why is this? What is the benefit of 8192 over other sizes?</p>
",1319998,26332,10-02-2018 10:46,13-02-2018 06:10,3,26362,170,14,100,58,"{'badge_counts': {'bronze': 170, 'silver': 100, 'gold': 14}, 'account_id': 1388614, 'is_employee': False, 'last_modified_date': 1708135800, 'last_access_date': 1711180332, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 30, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 26362, 'creation_date': 1333869109, 'user_type': 'registered', 'user_id': 1319998, 'accept_rate': 58, 'location': 'London, United Kingdom', 'website_url': 'https://charemza.name', 'link': 'https://stackoverflow.com/users/1319998/michal-charemza', 'profile_image': 'https://i.stack.imgur.com/EP486.jpg?s=256&g=1', 'display_name': 'Michal Charemza'}","I'm looking to make a fast streaming download -> upload to move large files via HTTP from one server to another. During this, I've noticed that httplib, that is used by urllib3 and therefore also requests, seems to hard code how much it fetches from a stream at a time to 8192 bytes https://github.com/python/cpython/blob/28453feaa8d88bbcbf6d834b1d5ca396d17265f2/Lib/http/client.py#L970 Why is this? What is the benefit of 8192 over other sizes?",,0,7,0,1,
714,48249253,48301522,660,How to efficiently pass function through?,2,<python><algorithm><python-2.7><numpy><scipy>,16,"<p><strong>Motivation</strong></p>

<p>Take a look at the following picture.</p>

<p><a href=""https://i.stack.imgur.com/NXtiX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NXtiX.png"" alt=""enter image description here""></a></p>

<p>Given are the red, blue, and green curve. I would like to find at each point on the <code>x</code> axis the dominating curve. This is shown as the black graph in the picture. From the properties of the red, green, and blue curve (increasing and constant after a while) this boils down to find the dominating curve on the very right hand side and then move towards the left hand side finding all intersection points and update the dominating curve.</p>

<p>This outlined problem should be solved <code>T</code> times. There is one final twist in the problem. The blue, green, and red curve of the next iteration are constructed via the dominating solution from the previous iteration plus some varying parameters. As an example in the picture above: The solution is the black function. This function is used to generate a new blue, green, and red curve. Then the problem start again to find the dominating one for these new curves etc.</p>

<p><strong>Question in a nutshell</strong><br>
In each iteration I start at the fixed very right hand side and evaluate all three functions to see which is the dominating one. This evaluations are taking longer and longer over iteration. <em>My feeling is that I don't pass optimally the old dominating function to construct the new blue, green, and red curve. Reason: I got in an earlier version a maximum recursion depth error.</em> Other parts of the code where the value of the current dominating function (which is essential either the green, red, or blue curve) is required are also taking longer and longer with iteration.</p>

<p>For 5 iterations just evaluating the functions on one point on the very right hand side grows:</p>

<p>The results were produced via</p>

<pre><code>test = A(5, 120000, 100000) 
</code></pre>

<p>And then running</p>

<pre><code>test.find_all_intersections()

&gt;&gt;&gt; test.find_all_intersections()
iteration 4
to compute function values it took
0.0102479457855
iteration 3
to compute function values it took
0.0134601593018
iteration 2
to compute function values it took
0.0294270515442
iteration 1
to compute function values it took
0.109843969345
iteration 0
to compute function values it took
0.823768854141
</code></pre>

<p>I would like to know why is this the case and if one can program it more efficiently.</p>

<p><strong>Detailed Code explanation</strong></p>

<p>I quickly summarize the most important functions. The complete code can be found further below. If there are any other questions regarding the code I'm more than happy to elaborate / clarify.</p>

<ol>
<li><p>Method <code>u</code>: For the recurring task of generating a new batch of
the green, red, and blue curve above we need the old dominating one.
<code>u</code> is the initialization to be used in the very first iteration.</p></li>
<li><p>Method <code>_function_template</code>: The function generates versions of the
green, blue, and red curve by using different parameters. It returns
a function of a single input.</p></li>
<li><p>Method <code>eval</code>: This is the core function to generate the blue, green, and red versions every time. It takes three varying parameters each iteration: <code>vfunction</code> which is the dominating function from the previous step, <code>m</code>, and <code>s</code> which are two parameters (flaots) affecting the shape of the resulting curve. The other parameters are the same in each iteration. In the code there are sample values for <code>m</code> and <code>s</code> for each iteration. For the more geeky ones: It's to approximate an integral where <code>m</code> and <code>s</code> are the expected mean and standard deviation of the underlying normal distribution. The approximation is done via Gauss-Hermite nodes / weights.</p></li>
<li><p>Method <code>find_all_intersections</code>: This is the core method finding in
each iteration the dominating one. It constructs a dominating
function via a piece wise concatenation of the blue, green, and red
curve. This is achieved via the function <code>piecewise</code>.</p></li>
</ol>

<p>Here is the complete code</p>

<pre><code>import numpy as np
import pandas as pd
from scipy.optimize import brentq
import multiprocessing as mp
import pathos as pt
import timeit
import math
class A(object):
    def u(self, w):
        _w = np.asarray(w).copy()
        _w[_w &gt;= 120000] = 120000
        _p = np.maximum(0, 100000 - _w)
        return _w - 1000*_p**2

    def __init__(self, T, upper_bound, lower_bound):
        self.T = T
        self.upper_bound = upper_bound
        self.lower_bound = lower_bound

    def _function_template(self, *args):
        def _f(x):
            return self.evalv(x, *args)
        return _f

    def evalv(self, w, c, vfunction, g, m, s, gauss_weights, gauss_nodes):
        _A = np.tile(1 + m + math.sqrt(2) * s * gauss_nodes, (np.size(w), 1))
        _W = (_A.T * w).T
        _W = gauss_weights * vfunction(np.ravel(_W)).reshape(np.size(w),
                                                             len(gauss_nodes))
        evalue = g*1/math.sqrt(math.pi)*np.sum(_W, axis=1)
        return c + evalue

    def find_all_intersections(self):

        # the hermite gauss weights and nodes for integration
        # and additional paramters used for function generation

        gauss = np.polynomial.hermite.hermgauss(10)
        gauss_nodes = gauss[0]
        gauss_weights = gauss[1]
        r = np.asarray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                        1., 1., 1., 1., 1., 1., 1., 1., 1.])
        m = [[0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.03836174909668277, 0.08543620707856969, 0.15548297423808233],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.03836174909668277, 0.08543620707856969, 0.15548297423808233],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.03836174909668277, 0.08543620707856969, 0.15548297423808233],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.03836174909668277, 0.08543620707856969, 0.15548297423808233],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624]]

        s = [[0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019529101011406914, 0.04708607140891122, 0.20089341636351565],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.019529101011406914, 0.04708607140891122, 0.20089341636351565],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019529101011406914, 0.04708607140891122, 0.20089341636351565],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019529101011406914, 0.04708607140891122, 0.20089341636351565],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142]]

        self.solution = []

        n_cpu = mp.cpu_count()
        pool = pt.multiprocessing.ProcessPool(n_cpu)

        # this function is used for multiprocessing
        def call_f(f, x):
            return f(x)

        # this function takes differences for getting cross points
        def _diff(f_dom, f_other):
            def h(x):
                return f_dom(x) - f_other(x)
            return h

        # finds the root of two function
        def find_roots(F, u_bound, l_bound):
                try:
                    sol = brentq(F, a=l_bound,
                                 b=u_bound)
                    if np.absolute(sol - u_bound) &gt; 1:
                        return sol
                    else:
                        return l_bound
                except ValueError:
                    return l_bound

        # piecewise function
        def piecewise(l_comp, l_f):
            def f(x):
                _ind_f = np.digitize(x, l_comp) - 1
                if np.isscalar(x):
                    return l_f[_ind_f](x)
                else:
                    return np.asarray([l_f[_ind_f[i]](x[i])
                                       for i in range(0, len(x))]).ravel()
            return f

        _u = self.u

        for t in range(self.T-1, -1, -1):
            print('iteration' + ' ' + str(t))

            l_bound, u_bound = 0.5*self.lower_bound, self.upper_bound
            l_ordered_functions = []
            l_roots = []
            l_solution = []

            # build all function variations

            l_functions = [self._function_template(0, _u, r[t], m[t][i], s[t][i],
                                                   gauss_weights, gauss_nodes)
                           for i in range(0, len(m[t]))]

            # get the best solution for the upper bound on the very
            # right hand side of wealth interval

            array_functions = np.asarray(l_functions)
            start_time = timeit.default_timer()
            functions_values = pool.map(call_f, array_functions.tolist(),
                                        len(m[t]) * [u_bound])
            elapsed = timeit.default_timer() - start_time
            print('to compute function values it took')
            print(elapsed)

            ind = np.argmax(functions_values)
            cross_points = len(m[t]) * [u_bound]
            l_roots.insert(0, u_bound)
            max_m = m[t][ind]
            l_solution.insert(0, max_m)

            # move from the upper bound twoards the lower bound
            # and find the dominating solution by exploring all cross
            # points.

            test = True

            while test:
                l_ordered_functions.insert(0, array_functions[ind])
                current_max = l_ordered_functions[0]

                l_c_max = len(m[t]) * [current_max]
                l_u_cross = len(m[t]) * [cross_points[ind]]

                # Find new cross points on the smaller interval

                diff = pool.map(_diff, l_c_max, array_functions.tolist())
                cross_points = pool.map(find_roots, diff,
                                        l_u_cross, len(m[t]) * [l_bound])

                # update the solution, cross points and current
                # dominating function.

                ind = np.argmax(cross_points)
                l_roots.insert(0, cross_points[ind])
                max_m = m[t][ind]
                l_solution.insert(0, max_m)

                if cross_points[ind] &lt;= l_bound:
                    test = False

            l_ordered_functions.insert(0, l_functions[0])
            l_roots.insert(0, 0)
            l_roots[-1] = np.inf

            l_comp = l_roots[:]
            l_f = l_ordered_functions[:]

            # build piecewise function which is used for next
            # iteration.

            _u = piecewise(l_comp, l_f)
            _sol = pd.DataFrame(data=l_solution,
                                index=np.asarray(l_roots)[0:-1])
            self.solution.insert(0, _sol)
        return self.solution
</code></pre>
",1355634,1908,14-01-2018 11:49,17-01-2018 12:45,3,1928,61,4,28,84,"{'badge_counts': {'bronze': 61, 'silver': 28, 'gold': 4}, 'account_id': 556532, 'is_employee': False, 'last_modified_date': 1675472700, 'last_access_date': 1697969792, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1928, 'creation_date': 1335342031, 'user_type': 'registered', 'user_id': 1355634, 'accept_rate': 84, 'link': 'https://stackoverflow.com/users/1355634/math', 'profile_image': 'https://www.gravatar.com/avatar/d8b4ab7cfa6273819da5bbe387550ecd?s=256&d=identicon&r=PG', 'display_name': 'math'}","Motivation Take a look at the following picture. Given are the red, blue, and green curve. I would like to find at each point on the axis the dominating curve. This is shown as the black graph in the picture. From the properties of the red, green, and blue curve (increasing and constant after a while) this boils down to find the dominating curve on the very right hand side and then move towards the left hand side finding all intersection points and update the dominating curve. This outlined problem should be solved times. There is one final twist in the problem. The blue, green, and red curve of the next iteration are constructed via the dominating solution from the previous iteration plus some varying parameters. As an example in the picture above: The solution is the black function. This function is used to generate a new blue, green, and red curve. Then the problem start again to find the dominating one for these new curves etc. Question in a nutshell In each iteration I start at the fixed very right hand side and evaluate all three functions to see which is the dominating one. This evaluations are taking longer and longer over iteration. My feeling is that I don't pass optimally the old dominating function to construct the new blue, green, and red curve. Reason: I got in an earlier version a maximum recursion depth error. Other parts of the code where the value of the current dominating function (which is essential either the green, red, or blue curve) is required are also taking longer and longer with iteration. For 5 iterations just evaluating the functions on one point on the very right hand side grows: The results were produced via And then running I would like to know why is this the case and if one can program it more efficiently. Detailed Code explanation I quickly summarize the most important functions. The complete code can be found further below. If there are any other questions regarding the code I'm more than happy to elaborate / clarify. Method : For the recurring task of generating a new batch of the green, red, and blue curve above we need the old dominating one. is the initialization to be used in the very first iteration. Method : The function generates versions of the green, blue, and red curve by using different parameters. It returns a function of a single input. Method : This is the core function to generate the blue, green, and red versions every time. It takes three varying parameters each iteration: which is the dominating function from the previous step, , and which are two parameters (flaots) affecting the shape of the resulting curve. The other parameters are the same in each iteration. In the code there are sample values for and for each iteration. For the more geeky ones: It's to approximate an integral where and are the expected mean and standard deviation of the underlying normal distribution. The approximation is done via Gauss-Hermite nodes / weights. Method : This is the core method finding in each iteration the dominating one. It constructs a dominating function via a piece wise concatenation of the blue, green, and red curve. This is achieved via the function . Here is the complete code","x T test = A(5, 120000, 100000) 
 test.find_all_intersections()

&gt;&gt;&gt; test.find_all_intersections()
iteration 4
to compute function values it took
0.0102479457855
iteration 3
to compute function values it took
0.0134601593018
iteration 2
to compute function values it took
0.0294270515442
iteration 1
to compute function values it took
0.109843969345
iteration 0
to compute function values it took
0.823768854141
 u u _function_template eval vfunction m s m s m s find_all_intersections piecewise import numpy as np
import pandas as pd
from scipy.optimize import brentq
import multiprocessing as mp
import pathos as pt
import timeit
import math
class A(object):
    def u(self, w):
        _w = np.asarray(w).copy()
        _w[_w &gt;= 120000] = 120000
        _p = np.maximum(0, 100000 - _w)
        return _w - 1000*_p**2

    def __init__(self, T, upper_bound, lower_bound):
        self.T = T
        self.upper_bound = upper_bound
        self.lower_bound = lower_bound

    def _function_template(self, *args):
        def _f(x):
            return self.evalv(x, *args)
        return _f

    def evalv(self, w, c, vfunction, g, m, s, gauss_weights, gauss_nodes):
        _A = np.tile(1 + m + math.sqrt(2) * s * gauss_nodes, (np.size(w), 1))
        _W = (_A.T * w).T
        _W = gauss_weights * vfunction(np.ravel(_W)).reshape(np.size(w),
                                                             len(gauss_nodes))
        evalue = g*1/math.sqrt(math.pi)*np.sum(_W, axis=1)
        return c + evalue

    def find_all_intersections(self):

        # the hermite gauss weights and nodes for integration
        # and additional paramters used for function generation

        gauss = np.polynomial.hermite.hermgauss(10)
        gauss_nodes = gauss[0]
        gauss_weights = gauss[1]
        r = np.asarray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                        1., 1., 1., 1., 1., 1., 1., 1., 1.])
        m = [[0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.03836174909668277, 0.08543620707856969, 0.15548297423808233],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.03836174909668277, 0.08543620707856969, 0.15548297423808233],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.03836174909668277, 0.08543620707856969, 0.15548297423808233],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038063407778193614, 0.08475713587463352, 0.15420895520972322],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.03836174909668277, 0.08543620707856969, 0.15548297423808233],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624],
             [0.038212567720998125, 0.08509661835487026, 0.15484578903763624]]

        s = [[0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019529101011406914, 0.04708607140891122, 0.20089341636351565],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.019529101011406914, 0.04708607140891122, 0.20089341636351565],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019529101011406914, 0.04708607140891122, 0.20089341636351565],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.01945441966324046, 0.04690600929081242, 0.200125178687699],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019529101011406914, 0.04708607140891122, 0.20089341636351565],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142],
             [0.019491796104351332, 0.04699612658674578, 0.20050966545654142]]

        self.solution = []

        n_cpu = mp.cpu_count()
        pool = pt.multiprocessing.ProcessPool(n_cpu)

        # this function is used for multiprocessing
        def call_f(f, x):
            return f(x)

        # this function takes differences for getting cross points
        def _diff(f_dom, f_other):
            def h(x):
                return f_dom(x) - f_other(x)
            return h

        # finds the root of two function
        def find_roots(F, u_bound, l_bound):
                try:
                    sol = brentq(F, a=l_bound,
                                 b=u_bound)
                    if np.absolute(sol - u_bound) &gt; 1:
                        return sol
                    else:
                        return l_bound
                except ValueError:
                    return l_bound

        # piecewise function
        def piecewise(l_comp, l_f):
            def f(x):
                _ind_f = np.digitize(x, l_comp) - 1
                if np.isscalar(x):
                    return l_f[_ind_f](x)
                else:
                    return np.asarray([l_f[_ind_f[i]](x[i])
                                       for i in range(0, len(x))]).ravel()
            return f

        _u = self.u

        for t in range(self.T-1, -1, -1):
            print('iteration' + ' ' + str(t))

            l_bound, u_bound = 0.5*self.lower_bound, self.upper_bound
            l_ordered_functions = []
            l_roots = []
            l_solution = []

            # build all function variations

            l_functions = [self._function_template(0, _u, r[t], m[t][i], s[t][i],
                                                   gauss_weights, gauss_nodes)
                           for i in range(0, len(m[t]))]

            # get the best solution for the upper bound on the very
            # right hand side of wealth interval

            array_functions = np.asarray(l_functions)
            start_time = timeit.default_timer()
            functions_values = pool.map(call_f, array_functions.tolist(),
                                        len(m[t]) * [u_bound])
            elapsed = timeit.default_timer() - start_time
            print('to compute function values it took')
            print(elapsed)

            ind = np.argmax(functions_values)
            cross_points = len(m[t]) * [u_bound]
            l_roots.insert(0, u_bound)
            max_m = m[t][ind]
            l_solution.insert(0, max_m)

            # move from the upper bound twoards the lower bound
            # and find the dominating solution by exploring all cross
            # points.

            test = True

            while test:
                l_ordered_functions.insert(0, array_functions[ind])
                current_max = l_ordered_functions[0]

                l_c_max = len(m[t]) * [current_max]
                l_u_cross = len(m[t]) * [cross_points[ind]]

                # Find new cross points on the smaller interval

                diff = pool.map(_diff, l_c_max, array_functions.tolist())
                cross_points = pool.map(find_roots, diff,
                                        l_u_cross, len(m[t]) * [l_bound])

                # update the solution, cross points and current
                # dominating function.

                ind = np.argmax(cross_points)
                l_roots.insert(0, cross_points[ind])
                max_m = m[t][ind]
                l_solution.insert(0, max_m)

                if cross_points[ind] &lt;= l_bound:
                    test = False

            l_ordered_functions.insert(0, l_functions[0])
            l_roots.insert(0, 0)
            l_roots[-1] = np.inf

            l_comp = l_roots[:]
            l_f = l_ordered_functions[:]

            # build piecewise function which is used for next
            # iteration.

            _u = piecewise(l_comp, l_f)
            _sol = pd.DataFrame(data=l_solution,
                                index=np.asarray(l_roots)[0:-1])
            self.solution.insert(0, _sol)
        return self.solution
",203,267,1,1,
715,49736531,50462691,6016,Implement a C# Interface in Python for .NET,1,<c#><python><interface><python.net><pythonnet>,11,"<p>I have been given a library written in C# that I'm trying to call using Python for .NET.</p>

<p>The primary class I need an instance of has a constructor like:</p>

<pre><code>GDhuClient(IGDhuSettings)
</code></pre>

<p>There are no (exposed) classes that implement the <code>IGDhuSettings</code> interface. When I create a Python class to implement it, e.g.,</p>

<pre><code>class PyGDhuSettings(IGDhuSettings):
    ...
</code></pre>

<p>I get <code>TypeError: interface takes exactly one argument</code> if I don't have a <code>__new__</code> method or if I define one the normal way:</p>

<pre><code>def __new__(cls):
    return super().__new__(cls)
</code></pre>

<p>If I try to instantiate the interface as if it were a class, I either get the same error (with no or >1 arguments) or <code>&lt;whatever&gt; does not implement IGDhuSettings</code> if I pass it a single argument.</p>

<p>Looking at the <a href=""https://github.com/pythonnet/pythonnet/blob/master/src/runtime/interfaceobject.cs"" rel=""noreferrer"">Python for .NET source</a>,</p>

<pre><code>using System;
using System.Reflection;
using System.Runtime.InteropServices;

namespace Python.Runtime
{
    /// &lt;summary&gt;
    /// Provides the implementation for reflected interface types. Managed
    /// interfaces are represented in Python by actual Python type objects.
    /// Each of those type objects is associated with an instance of this
    /// class, which provides the implementation for the Python type.
    /// &lt;/summary&gt;
    internal class InterfaceObject : ClassBase
    {
        internal ConstructorInfo ctor;

        internal InterfaceObject(Type tp) : base(tp)
        {
            var coclass = (CoClassAttribute)Attribute.GetCustomAttribute(tp, cc_attr);
            if (coclass != null)
            {
                ctor = coclass.CoClass.GetConstructor(Type.EmptyTypes);
            }
        }

        private static Type cc_attr;

        static InterfaceObject()
        {
            cc_attr = typeof(CoClassAttribute);
        }

        /// &lt;summary&gt;
        /// Implements __new__ for reflected interface types.
        /// &lt;/summary&gt;
        public static IntPtr tp_new(IntPtr tp, IntPtr args, IntPtr kw)
        {
            var self = (InterfaceObject)GetManagedObject(tp);
            int nargs = Runtime.PyTuple_Size(args);
            Type type = self.type;
            object obj;

            if (nargs == 1)
            {
                IntPtr inst = Runtime.PyTuple_GetItem(args, 0);
                var co = GetManagedObject(inst) as CLRObject;

                if (co == null || !type.IsInstanceOfType(co.inst))
                {
                    Exceptions.SetError(Exceptions.TypeError, $""object does not implement {type.Name}"");
                    return IntPtr.Zero;
                }

                obj = co.inst;
            }

            else if (nargs == 0 &amp;&amp; self.ctor != null)
            {
                obj = self.ctor.Invoke(null);

                if (obj == null || !type.IsInstanceOfType(obj))
                {
                    Exceptions.SetError(Exceptions.TypeError, ""CoClass default constructor failed"");
                    return IntPtr.Zero;
                }
            }

            else
            {
                Exceptions.SetError(Exceptions.TypeError, ""interface takes exactly one argument"");
                return IntPtr.Zero;
            }

            return CLRObject.GetInstHandle(obj, self.pyHandle);
        }
    }
}
</code></pre>

<p>I don't see a means of implementing a C# interface in Python without either a CoClass (there isn't one defined) or already having a class that implements it.</p>

<p>Is there some nuance that I'm missing here, or is this a limitation of Python for .NET?</p>

<p>Discussion on GitHub: <a href=""https://github.com/pythonnet/pythonnet/issues/674"" rel=""noreferrer"">https://github.com/pythonnet/pythonnet/issues/674</a></p>
",1362205,223,09-04-2018 15:27,22-05-2018 08:03,43,223,10,0,2,,"{'badge_counts': {'bronze': 10, 'silver': 2, 'gold': 0}, 'account_id': 154537, 'is_employee': False, 'last_modified_date': 1573682530, 'last_access_date': 1711147871, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 223, 'creation_date': 1335572513, 'user_type': 'registered', 'user_id': 1362205, 'location': 'Seattle, WA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1362205/ghillie-dhu', 'profile_image': 'https://www.gravatar.com/avatar/e9e933afcc71a10d1f8eb4c5f9be7f6d?s=256&d=identicon&r=PG', 'display_name': 'Ghillie Dhu'}","I have been given a library written in C# that I'm trying to call using Python for .NET. The primary class I need an instance of has a constructor like: There are no (exposed) classes that implement the interface. When I create a Python class to implement it, e.g., I get if I don't have a method or if I define one the normal way: If I try to instantiate the interface as if it were a class, I either get the same error (with no or >1 arguments) or if I pass it a single argument. Looking at the Python for .NET source, I don't see a means of implementing a C# interface in Python without either a CoClass (there isn't one defined) or already having a class that implements it. Is there some nuance that I'm missing here, or is this a limitation of Python for .NET? Discussion on GitHub: https://github.com/pythonnet/pythonnet/issues/674","GDhuClient(IGDhuSettings)
 IGDhuSettings class PyGDhuSettings(IGDhuSettings):
    ...
 TypeError: interface takes exactly one argument __new__ def __new__(cls):
    return super().__new__(cls)
 &lt;whatever&gt; does not implement IGDhuSettings using System;
using System.Reflection;
using System.Runtime.InteropServices;

namespace Python.Runtime
{
    /// &lt;summary&gt;
    /// Provides the implementation for reflected interface types. Managed
    /// interfaces are represented in Python by actual Python type objects.
    /// Each of those type objects is associated with an instance of this
    /// class, which provides the implementation for the Python type.
    /// &lt;/summary&gt;
    internal class InterfaceObject : ClassBase
    {
        internal ConstructorInfo ctor;

        internal InterfaceObject(Type tp) : base(tp)
        {
            var coclass = (CoClassAttribute)Attribute.GetCustomAttribute(tp, cc_attr);
            if (coclass != null)
            {
                ctor = coclass.CoClass.GetConstructor(Type.EmptyTypes);
            }
        }

        private static Type cc_attr;

        static InterfaceObject()
        {
            cc_attr = typeof(CoClassAttribute);
        }

        /// &lt;summary&gt;
        /// Implements __new__ for reflected interface types.
        /// &lt;/summary&gt;
        public static IntPtr tp_new(IntPtr tp, IntPtr args, IntPtr kw)
        {
            var self = (InterfaceObject)GetManagedObject(tp);
            int nargs = Runtime.PyTuple_Size(args);
            Type type = self.type;
            object obj;

            if (nargs == 1)
            {
                IntPtr inst = Runtime.PyTuple_GetItem(args, 0);
                var co = GetManagedObject(inst) as CLRObject;

                if (co == null || !type.IsInstanceOfType(co.inst))
                {
                    Exceptions.SetError(Exceptions.TypeError, $""object does not implement {type.Name}"");
                    return IntPtr.Zero;
                }

                obj = co.inst;
            }

            else if (nargs == 0 &amp;&amp; self.ctor != null)
            {
                obj = self.ctor.Invoke(null);

                if (obj == null || !type.IsInstanceOfType(obj))
                {
                    Exceptions.SetError(Exceptions.TypeError, ""CoClass default constructor failed"");
                    return IntPtr.Zero;
                }
            }

            else
            {
                Exceptions.SetError(Exceptions.TypeError, ""interface takes exactly one argument"");
                return IntPtr.Zero;
            }

            return CLRObject.GetInstHandle(obj, self.pyHandle);
        }
    }
}
",74,107,0,2,
716,49620538,49620539,32030,"What are the 'levels', 'keys', and names arguments for in Pandas' concat function?",1,<python><pandas>,126,"<h3>Questions</h3>

<ul>
<li>How do I use <code>pd.concat</code>?</li>
<li>What is the <code>levels</code> argument for?</li>
<li>What is the <code>keys</code> argument for?</li>
<li>Are there a bunch of examples to help explain how to use all the arguments?</li>
</ul>

<p>Pandas' <code>concat</code> function is the <a href=""https://en.wikipedia.org/wiki/Swiss_Army_knife"" rel=""noreferrer"">Swiss Army knife</a> of the merging utilities.  The variety of situations in which it is useful are numerous. The existing documentation leaves out a few details on some of the optional arguments. Among them are the <code>levels</code> and <code>keys</code> arguments. I set out to figure out what those arguments do.</p>

<p>I'll pose a question that will act as a gateway into many aspects of <code>pd.concat</code>.</p>

<p>Consider the data frames <code>d1</code>, <code>d2</code>, and <code>d3</code>:</p>

<pre><code>import pandas as pd

d1 = pd.DataFrame(dict(A=.1, B=.2, C=.3), [2, 3])
d2 = pd.DataFrame(dict(B=.4, C=.5, D=.6), [1, 2])
d3 = pd.DataFrame(dict(A=.7, B=.8, D=.9), [1, 3])
</code></pre>

<p>If I were to concatenate these together with</p>

<pre><code>pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'])
</code></pre>

<p>I get the expected result with a <code>pandas.MultiIndex</code> for my <code>columns</code> object:</p>

<pre><code>        A    B    C    D
d1 2  0.1  0.2  0.3  NaN
   3  0.1  0.2  0.3  NaN
d2 1  NaN  0.4  0.5  0.6
   2  NaN  0.4  0.5  0.6
d3 1  0.7  0.8  NaN  0.9
   3  0.7  0.8  NaN  0.9
</code></pre>

<p>However, I wanted to use the <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html#pandas-concat"" rel=""noreferrer""><code>levels</code> argument documentation</a>:</p>

<blockquote>
  <p><strong>levels</strong>: list of sequences, default None.
  Specific levels (unique values) to use for constructing a MultiIndex. Otherwise, they will be inferred from the keys.</p>
</blockquote>

<p>So I passed</p>

<pre><code>pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2']])
</code></pre>

<p>And get a <code>KeyError</code></p>

<blockquote>
  <p><code>ValueError: Key d3 not in level Index(['d1', 'd2'], dtype='object')</code></p>
</blockquote>

<p>This made sense. The levels I passed were inadequate to describe the necessary levels indicated by the keys. Had I not passed anything, as I did above, the levels are inferred (as stated in the documentation). But how else can I use this argument to better effect?</p>

<p>If I tried this instead:</p>

<pre><code>pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2', 'd3']])
</code></pre>

<p>I and got the same results as above. But when I add one more value to the levels,</p>

<pre><code>df = pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2', 'd3', 'd4']])
</code></pre>

<p>I end up with the same looking data frame, but the resulting <code>MultiIndex</code> has an unused level.</p>

<pre><code>df.index.levels[0]

Index(['d1', 'd2', 'd3', 'd4'], dtype='object')
</code></pre>

<p>So what is the point of the <code>level</code> argument and should I be using <code>keys</code> differently?</p>

<p>I'm using Python 3.6 and Pandas 0.22.</p>
",2336654,290224,03-04-2018 00:25,03-04-2018 00:25,0,290572,634,60,489,96,"{'badge_counts': {'bronze': 634, 'silver': 489, 'gold': 60}, 'account_id': 2707337, 'is_employee': False, 'last_modified_date': 1711112700, 'last_access_date': 1709261486, 'reputation_change_year': 2134, 'reputation_change_quarter': 2134, 'reputation_change_month': 598, 'reputation_change_week': 198, 'reputation_change_day': 0, 'reputation': 290572, 'creation_date': 1367338176, 'user_type': 'registered', 'user_id': 2336654, 'accept_rate': 96, 'location': 'San Diego, CA, USA', 'website_url': 'https://www.linkedin.com/in/piRSqrd', 'link': 'https://stackoverflow.com/users/2336654/pirsquared', 'profile_image': 'https://i.stack.imgur.com/kWBRw.png?s=256&g=1', 'display_name': 'piRSquared'}","Questions How do I use ? What is the argument for? What is the argument for? Are there a bunch of examples to help explain how to use all the arguments? Pandas' function is the Swiss Army knife of the merging utilities. The variety of situations in which it is useful are numerous. The existing documentation leaves out a few details on some of the optional arguments. Among them are the and arguments. I set out to figure out what those arguments do. I'll pose a question that will act as a gateway into many aspects of . Consider the data frames , , and : If I were to concatenate these together with I get the expected result with a for my object: However, I wanted to use the argument documentation: levels: list of sequences, default None. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise, they will be inferred from the keys. So I passed And get a This made sense. The levels I passed were inadequate to describe the necessary levels indicated by the keys. Had I not passed anything, as I did above, the levels are inferred (as stated in the documentation). But how else can I use this argument to better effect? If I tried this instead: I and got the same results as above. But when I add one more value to the levels, I end up with the same looking data frame, but the resulting has an unused level. So what is the point of the argument and should I be using differently? I'm using Python 3.6 and Pandas 0.22.","pd.concat levels keys concat levels keys pd.concat d1 d2 d3 import pandas as pd

d1 = pd.DataFrame(dict(A=.1, B=.2, C=.3), [2, 3])
d2 = pd.DataFrame(dict(B=.4, C=.5, D=.6), [1, 2])
d3 = pd.DataFrame(dict(A=.7, B=.8, D=.9), [1, 3])
 pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'])
 pandas.MultiIndex columns         A    B    C    D
d1 2  0.1  0.2  0.3  NaN
   3  0.1  0.2  0.3  NaN
d2 1  NaN  0.4  0.5  0.6
   2  NaN  0.4  0.5  0.6
d3 1  0.7  0.8  NaN  0.9
   3  0.7  0.8  NaN  0.9
 levels pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2']])
 KeyError ValueError: Key d3 not in level Index(['d1', 'd2'], dtype='object') pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2', 'd3']])
 df = pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2', 'd3', 'd4']])
 MultiIndex df.index.levels[0]

Index(['d1', 'd2', 'd3', 'd4'], dtype='object')
 level keys",-6,78,0,2,
717,48770263,48997214,3119,Bundling Python3 packages for PySpark results in missing imports,2,<python><python-3.x><numpy><apache-spark><pyspark>,11,"<p>I'm trying to run a PySpark job that depends on certain python3 libraries.
I know I can install these libraries on the Spark Cluster, but since I'm reusing the cluster for multiple jobs, I'd like to rather bundle all dependencies and pass them to each job via the <code>--py-files</code> directive. </p>

<p>To do this I use:</p>



<pre><code>pip3 install -r requirements.txt --target ./build/dependencies
cd ./build/dependencies
zip -qrm . ../dependencies.zip
</code></pre>

<p>Which effectively zips all code from the required packages to be used at root level.</p>

<p><a href=""https://i.stack.imgur.com/UgEoY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UgEoY.png"" alt=""enter image description here""></a></p>

<p>In my <code>main.py</code> I can import the dependencies </p>



<pre><code>if os.path.exists('dependencies.zip'):
    sys.path.insert(0, 'dependencies.zip')
</code></pre>

<p>And also add the .zip to my Spark Context</p>



<pre><code>sc.addPyFile('dependencies.zip')
</code></pre>

<p>So far so good.</p>

<p>But for some reason this will devolve in some kind of dependency hell on the Spark Cluster</p>

<p>Eg running </p>

<p><code>spark-submit --py-files dependencies.zip main.py</code></p>

<p>Where in <code>main.py</code> (or class)  I want to use a panda. The code it will trigger this error:</p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File ""/Users/tomlous/Development/Python/enrichers/build/main.py"", line 53, in 
      job_module = importlib.import_module('spark.jobs.%s' % args.job_name)
  ...</p>
  
  <p>File """", line 978, in _gcd_import</p>
  
  <p>File """", line 961, in _find_and_load</p>
  
  <p>File """", line 950, in _find_and_load_unlocked</p>
  
  <p>File """", line 646, in _load_unlocked</p>
  
  <p>File """", line 616, in _load_backward_compatible</p>
  
  <p>File ""dependencies.zip/spark/jobs/classify_existence.py"", line 9, in </p>
  
  <p>File ""dependencies.zip/enrich/existence.py"", line 3, in </p>
  
  <p>File ""dependencies.zip/pandas/<strong>init</strong>.py"", line 19, in </p>
  
  <p><strong>ImportError: Missing required dependencies ['numpy']</strong></p>
</blockquote>

<p>Looking at panda's <code>__init__.py</code>  I see something like <code>__import__(numpy)</code></p>

<p>So I assume numpy is not loaded.</p>

<p>But if I change my code to explicitly call numpy functions, it actually finds numpy, but not some of it's dependecies</p>



<pre><code>import numpy as np
a = np.array([1, 2, 3])
</code></pre>

<p>The code returns </p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File ""dependencies.zip/numpy/core/<strong>init</strong>.py"", line 16, in </p>
  
  <p><strong>ImportError: cannot import name 'multiarray'</strong></p>
</blockquote>

<p>So my question is:</p>

<p>How should I bundle python3 libraries with my spark job in a way that I don't have to pip3 install all possible libraries on a Spark cluster?</p>
",1444286,2869,13-02-2018 15:31,26-02-2018 21:15,13,2879,49,2,26,71,"{'badge_counts': {'bronze': 49, 'silver': 26, 'gold': 2}, 'account_id': 1552121, 'is_employee': False, 'last_modified_date': 1607614882, 'last_access_date': 1711012016, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 2879, 'creation_date': 1339150363, 'user_type': 'registered', 'user_id': 1444286, 'accept_rate': 71, 'location': 'Rotterdam, Netherlands', 'website_url': 'https://lous.info/', 'link': 'https://stackoverflow.com/users/1444286/tom-lous', 'profile_image': 'https://www.gravatar.com/avatar/f9557bb4f3896e474bbb88089330c5d6?s=256&d=identicon&r=PG', 'display_name': 'Tom Lous'}","I'm trying to run a PySpark job that depends on certain python3 libraries. I know I can install these libraries on the Spark Cluster, but since I'm reusing the cluster for multiple jobs, I'd like to rather bundle all dependencies and pass them to each job via the directive. To do this I use: Which effectively zips all code from the required packages to be used at root level. In my I can import the dependencies And also add the .zip to my Spark Context So far so good. But for some reason this will devolve in some kind of dependency hell on the Spark Cluster Eg running Where in (or class) I want to use a panda. The code it will trigger this error: Traceback (most recent call last): File ""/Users/tomlous/Development/Python/enrichers/build/main.py"", line 53, in job_module = importlib.import_module('spark.jobs.%s' % args.job_name) ... File """", line 978, in _gcd_import File """", line 961, in _find_and_load File """", line 950, in _find_and_load_unlocked File """", line 646, in _load_unlocked File """", line 616, in _load_backward_compatible File ""dependencies.zip/spark/jobs/classify_existence.py"", line 9, in File ""dependencies.zip/enrich/existence.py"", line 3, in File ""dependencies.zip/pandas/init.py"", line 19, in ImportError: Missing required dependencies ['numpy'] Looking at panda's I see something like So I assume numpy is not loaded. But if I change my code to explicitly call numpy functions, it actually finds numpy, but not some of it's dependecies The code returns Traceback (most recent call last): File ""dependencies.zip/numpy/core/init.py"", line 16, in ImportError: cannot import name 'multiarray' So my question is: How should I bundle python3 libraries with my spark job in a way that I don't have to pip3 install all possible libraries on a Spark cluster?","--py-files pip3 install -r requirements.txt --target ./build/dependencies
cd ./build/dependencies
zip -qrm . ../dependencies.zip
 main.py if os.path.exists('dependencies.zip'):
    sys.path.insert(0, 'dependencies.zip')
 sc.addPyFile('dependencies.zip')
 spark-submit --py-files dependencies.zip main.py main.py __init__.py __import__(numpy) import numpy as np
a = np.array([1, 2, 3])
",-2,92,1,1,
718,49199164,49199453,53999,"Increasing pie chart size with matplotlib, radius parameter appears to do nothing",1,<python><python-2.7><matplotlib><pie-chart>,14,"<p>Trying to make the pie larger. Looking at the docs, and other places, it says to set the radius. It seems no matter which value I put in the radius there's no increase. I'm posting the full code and the image it generates.</p>

<pre><code>import matplotlib.pyplot as plt


def autopct_generator(limit):
    """"""Remove percent on small slices.""""""
    def inner_autopct(pct):
        return ('%.2f%%' % pct) if pct &gt; limit else ''
    return inner_autopct

labels = 'Frogs', 'Hogs', 'Dogs', 'Logs', 'Test', 'Test2', 'Test3', \
    'Test4', 'Test5', 'Test6', 'Test7', 'Test8', 'Test9', 'Test10', \
    'Test11', 'Test12', 'Test13', 'Test14'
sizes = [15, 30, 45, 10, 10, 24, 13, 18, 28, 20, 13, 15, 5, 1, 18, 10,
         10, 10]
NUM_COLORS = len(sizes)

fig1, ax1 = plt.subplots(figsize=(6, 5))

# set color theme
# https://matplotlib.org/api/pyplot_summary.html#colors-in-matplotlib
theme = plt.get_cmap('bwr')
ax1.set_color_cycle([theme(
    1. * i / NUM_COLORS) for i in range(NUM_COLORS)])

box = ax1.get_position()
ax1.set_position([box.x0, box.y0, box.width * 1.3, box.height])

_, _, autotexts = ax1.pie(
    sizes, autopct=autopct_generator(7), startangle=90, radius=1.8 * 1000)
for autotext in autotexts:
    autotext.set_weight('bold')
ax1.axis('equal')
total = sum(sizes)
plt.legend(
    loc='upper left',
    labels=['%s, %1.1f%%' % (
        l, (float(s) / total) * 100) for l, s in zip(labels, sizes)],
    prop={'size': 12},
    bbox_to_anchor=(0.0, 1),
    bbox_transform=fig1.transFigure
)
# fig1.set_size_inches(18.5, 10.5)
fig1.savefig('chart.png')
</code></pre>

<p><a href=""https://i.stack.imgur.com/1RJmF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1RJmF.png"" alt=""enter image description here""></a></p>
",1447974,3266,09-03-2018 17:32,09-03-2018 17:50,0,3276,37,5,23,96,"{'badge_counts': {'bronze': 37, 'silver': 23, 'gold': 5}, 'account_id': 1557270, 'is_employee': False, 'last_modified_date': 1696681800, 'last_access_date': 1654720765, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3276, 'creation_date': 1339375836, 'user_type': 'registered', 'user_id': 1447974, 'accept_rate': 96, 'location': 'Sacramento, CA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1447974/avoliva', 'profile_image': 'https://www.gravatar.com/avatar/f382ae0131419f89106feb89bbacd881?s=256&d=identicon&r=PG', 'display_name': 'avoliva'}","Trying to make the pie larger. Looking at the docs, and other places, it says to set the radius. It seems no matter which value I put in the radius there's no increase. I'm posting the full code and the image it generates.","import matplotlib.pyplot as plt


def autopct_generator(limit):
    """"""Remove percent on small slices.""""""
    def inner_autopct(pct):
        return ('%.2f%%' % pct) if pct &gt; limit else ''
    return inner_autopct

labels = 'Frogs', 'Hogs', 'Dogs', 'Logs', 'Test', 'Test2', 'Test3', \
    'Test4', 'Test5', 'Test6', 'Test7', 'Test8', 'Test9', 'Test10', \
    'Test11', 'Test12', 'Test13', 'Test14'
sizes = [15, 30, 45, 10, 10, 24, 13, 18, 28, 20, 13, 15, 5, 1, 18, 10,
         10, 10]
NUM_COLORS = len(sizes)

fig1, ax1 = plt.subplots(figsize=(6, 5))

# set color theme
# https://matplotlib.org/api/pyplot_summary.html#colors-in-matplotlib
theme = plt.get_cmap('bwr')
ax1.set_color_cycle([theme(
    1. * i / NUM_COLORS) for i in range(NUM_COLORS)])

box = ax1.get_position()
ax1.set_position([box.x0, box.y0, box.width * 1.3, box.height])

_, _, autotexts = ax1.pie(
    sizes, autopct=autopct_generator(7), startangle=90, radius=1.8 * 1000)
for autotext in autotexts:
    autotext.set_weight('bold')
ax1.axis('equal')
total = sum(sizes)
plt.legend(
    loc='upper left',
    labels=['%s, %1.1f%%' % (
        l, (float(s) / total) * 100) for l, s in zip(labels, sizes)],
    prop={'size': 12},
    bbox_to_anchor=(0.0, 1),
    bbox_transform=fig1.transFigure
)
# fig1.set_size_inches(18.5, 10.5)
fig1.savefig('chart.png')
",42,48,1,1,
719,48622486,48622911,13413,How to display a pandas dataframe as datatable?,2,<python><jquery><pandas><flask><datatables>,13,"<p>I want to display a table - which is a pandas dataframe - as a <a href=""https://datatables.net/examples/data_sources/dom.html"" rel=""noreferrer"">DataTable</a>. In the simplified example below, I read two numbers provided by a user, that determine the row and column number of the table. The number of elements of this table is then displayed correctly, however, the table does not appear.</p>

<p>The problem is, I think, that I pass the table in the wrong way. When I try</p>

<pre><code>return jsonify(number_elements=a * b,
                   my_table=df)
</code></pre>

<p>I get the error</p>

<blockquote>
  <p>anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
      raise TypeError(repr(o) + "" is not JSON serializable"") </p>
  
  <p>TypeError:     0   1   2   3 0  51  35  10  84 1  30  60  79  24 is not JSON
  serializable</p>
</blockquote>

<p>if I use</p>

<pre><code>return jsonify(number_elements=a * b,
                   my_table=df.to_json())
</code></pre>

<p>then there is no error but the table is still not displayed.</p>

<p>How would I do this correctly?</p>

<p>My <code>index.html</code> file looks like this:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang=""en""&gt;
  &lt;head&gt;
    &lt;script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js""&gt;&lt;/script&gt;
    &lt;link href=""https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css""
          rel=""stylesheet""&gt;
     &lt;link href=""https://cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css""
           rel=""stylesheet""&gt;
  &lt;script type=text/javascript&gt;
    $(function() {
      $('a#calculate').bind('click', function() {
        $.getJSON('/_get_table', {
          a: $('input[name=""a""]').val(),
          b: $('input[name=""b""]').val()
        }, function(data) {
          $(""#elements"").text(data.number_elements);
          $(""#a_nice_table"").DataTable(data.my_table);
        });
        return false;
      });
    });
  &lt;/script&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class=""container""&gt;
      &lt;div class=""header""&gt;
        &lt;h3 class=""text-muted""&gt;Create a pretty table&lt;/h3&gt;
      &lt;/div&gt;

      &lt;div&gt;
        &lt;p&gt;Number of rows&lt;/p&gt;
        &lt;input type=""text"" size=""5"" name=""a"" value=""2""&gt;
        &lt;p&gt;Number of columns&lt;/p&gt;
        &lt;input type=""text"" size=""5"" name=""b"" value=""4""&gt;

        &lt;p&gt;&lt;a href=""javascript:void();"" id=""calculate""&gt;get a pretty table&lt;/a&gt;&lt;/p&gt;
         &lt;p&gt;Result&lt;/p&gt;
        &lt;p&gt;Number of elements:&lt;/p&gt;
          &lt;span id=""elements""&gt;Hallo&lt;/span&gt;&lt;br&gt;
          &lt;span id=""a_nice_table""&gt;Here should be a table&lt;/span&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>And my file <code>app.py</code> looks like this:</p>

<pre><code>from flask import Flask, render_template, request, jsonify
import pandas as pd
import numpy as np

# Initialize the Flask application
app = Flask(__name__)


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/_get_table')
def get_table():
    a = request.args.get('a', type=int)
    b = request.args.get('b', type=int)

    df = pd.DataFrame(np.random.randint(0, 100, size=(a, b)))

    return jsonify(number_elements=a * b,
                   my_table=df)


if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
",1534017,25502,05-02-2018 12:21,05-02-2018 12:45,0,25522,157,21,119,92,"{'badge_counts': {'bronze': 157, 'silver': 119, 'gold': 21}, 'account_id': 1666782, 'is_employee': False, 'last_modified_date': 1689891001, 'last_access_date': 1711114689, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 40, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 25522, 'creation_date': 1342597863, 'user_type': 'registered', 'user_id': 1534017, 'accept_rate': 92, 'location': 'Netherlands', 'website_url': '', 'link': 'https://stackoverflow.com/users/1534017/cleb', 'profile_image': 'https://www.gravatar.com/avatar/aa667d0e455aa30f3410756d719de795?s=256&d=identicon&r=PG', 'display_name': 'Cleb'}","I want to display a table - which is a pandas dataframe - as a DataTable. In the simplified example below, I read two numbers provided by a user, that determine the row and column number of the table. The number of elements of this table is then displayed correctly, however, the table does not appear. The problem is, I think, that I pass the table in the wrong way. When I try I get the error anaconda2/lib/python2.7/json/encoder.py"", line 184, in default raise TypeError(repr(o) + "" is not JSON serializable"") TypeError: 0 1 2 3 0 51 35 10 84 1 30 60 79 24 is not JSON serializable if I use then there is no error but the table is still not displayed. How would I do this correctly? My file looks like this: And my file looks like this:","return jsonify(number_elements=a * b,
                   my_table=df)
 return jsonify(number_elements=a * b,
                   my_table=df.to_json())
 index.html &lt;!DOCTYPE html&gt;
&lt;html lang=""en""&gt;
  &lt;head&gt;
    &lt;script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js""&gt;&lt;/script&gt;
    &lt;link href=""https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css""
          rel=""stylesheet""&gt;
     &lt;link href=""https://cdn.datatables.net/1.10.16/css/jquery.dataTables.min.css""
           rel=""stylesheet""&gt;
  &lt;script type=text/javascript&gt;
    $(function() {
      $('a#calculate').bind('click', function() {
        $.getJSON('/_get_table', {
          a: $('input[name=""a""]').val(),
          b: $('input[name=""b""]').val()
        }, function(data) {
          $(""#elements"").text(data.number_elements);
          $(""#a_nice_table"").DataTable(data.my_table);
        });
        return false;
      });
    });
  &lt;/script&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class=""container""&gt;
      &lt;div class=""header""&gt;
        &lt;h3 class=""text-muted""&gt;Create a pretty table&lt;/h3&gt;
      &lt;/div&gt;

      &lt;div&gt;
        &lt;p&gt;Number of rows&lt;/p&gt;
        &lt;input type=""text"" size=""5"" name=""a"" value=""2""&gt;
        &lt;p&gt;Number of columns&lt;/p&gt;
        &lt;input type=""text"" size=""5"" name=""b"" value=""4""&gt;

        &lt;p&gt;&lt;a href=""javascript:void();"" id=""calculate""&gt;get a pretty table&lt;/a&gt;&lt;/p&gt;
         &lt;p&gt;Result&lt;/p&gt;
        &lt;p&gt;Number of elements:&lt;/p&gt;
          &lt;span id=""elements""&gt;Hallo&lt;/span&gt;&lt;br&gt;
          &lt;span id=""a_nice_table""&gt;Here should be a table&lt;/span&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
 app.py from flask import Flask, render_template, request, jsonify
import pandas as pd
import numpy as np

# Initialize the Flask application
app = Flask(__name__)


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/_get_table')
def get_table():
    a = request.args.get('a', type=int)
    b = request.args.get('b', type=int)

    df = pd.DataFrame(np.random.randint(0, 100, size=(a, b)))

    return jsonify(number_elements=a * b,
                   my_table=df)


if __name__ == '__main__':
    app.run(debug=True)
",68,105,0,1,
720,48722835,48722879,26906,Custom type hint annotation,1,<python><code-completion><type-hinting>,16,"<p>I just wrote a simple <a href=""https://github.com/allrod5/injectable"" rel=""noreferrer""><code>@autowired</code> decorator</a> for Python that instantiate classes based on type annotations.</p>
<p>To enable lazy initialization of the class, the package provides a <code>lazy(type_annotation: (Type, str))</code> function so that the caller can use it like this:</p>
<pre><code>@autowired
def foo(bla, *, dep: lazy(MyClass)):
   ...
</code></pre>
<p>This works very well, under the hood this <code>lazy</code> function just returns a function that returns the actual type and that has a <em>lazy_init</em> property set to <code>True</code>. Also this does <strong>not</strong> break IDEs' (e.g., PyCharm) code completion feature.</p>
<h2><a href=""https://github.com/allrod5/injectable/issues/3"" rel=""noreferrer"">But I want to enable the use of a subscriptable <code>Lazy</code> type use instead of the <code>lazy</code> function.</a></h2>
<p>Like this:</p>
<pre><code>@autowired
def foo(bla, *, dep: Lazy[MyClass]):
   ...
</code></pre>
<p>This would behave very much like <a href=""https://github.com/python/typing/blob/7d7ffcd51e1ed9bccfa0466b371e490b1555208b/python2/typing.py#L773"" rel=""noreferrer"">typing.Union</a>. And <strong>while I'm able to implement the subscriptable type, IDEs' code completion feature will be rendered useless</strong> as it will present suggestions for attributes in the <code>Lazy</code> class, not <code>MyClass</code>.</p>
<p>I've been working with this code:</p>
<pre><code>class LazyMetaclass(type):
    def __getitem__(lazy_type, type_annotation):
        return lazy_type(type_annotation)

class Lazy(metaclass=LazyMetaclass):
    def __init__(self, type_annotation):
        self.type_annotation = type_annotation
</code></pre>
<p>I tried redefining <code>Lazy.__dict__</code> as a property to forward to the subscripted type's <code>__dict__</code> but this seems to have no effect on the code completion feature of PyCharm.</p>
<p>I strongly believe that what I'm trying to achieve is possible as <a href=""https://github.com/python/typing/blob/7d7ffcd51e1ed9bccfa0466b371e490b1555208b/python2/typing.py#L773"" rel=""noreferrer"">typing.Union</a> works well with IDEs' code completion. I've been trying to decipher what in the source code of <a href=""https://github.com/python/typing/blob/7d7ffcd51e1ed9bccfa0466b371e490b1555208b/python2/typing.py#L773"" rel=""noreferrer"">typing.Union</a> makes it to behave well with code completion features but with no success so far.</p>
",2468458,1502,10-02-2018 16:16,10-02-2018 16:20,0,1502,37,4,20,85,"{'badge_counts': {'bronze': 37, 'silver': 20, 'gold': 4}, 'account_id': 519017, 'is_employee': False, 'last_modified_date': 1680472377, 'last_access_date': 1710952178, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1502, 'creation_date': 1370788333, 'user_type': 'registered', 'user_id': 2468458, 'accept_rate': 85, 'location': 'S&#227;o Paulo, SP, Brasil', 'website_url': 'https://github.com/allrod5', 'link': 'https://stackoverflow.com/users/2468458/rodrigo-oliveira', 'profile_image': 'https://i.stack.imgur.com/FTZQE.jpg?s=256&g=1', 'display_name': 'Rodrigo Oliveira'}","I just wrote a simple decorator for Python that instantiate classes based on type annotations. To enable lazy initialization of the class, the package provides a function so that the caller can use it like this: This works very well, under the hood this function just returns a function that returns the actual type and that has a lazy_init property set to . Also this does not break IDEs' (e.g., PyCharm) code completion feature. But I want to enable the use of a subscriptable type use instead of the function. Like this: This would behave very much like typing.Union. And while I'm able to implement the subscriptable type, IDEs' code completion feature will be rendered useless as it will present suggestions for attributes in the class, not . I've been working with this code: I tried redefining as a property to forward to the subscripted type's but this seems to have no effect on the code completion feature of PyCharm. I strongly believe that what I'm trying to achieve is possible as typing.Union works well with IDEs' code completion. I've been trying to decipher what in the source code of typing.Union makes it to behave well with code completion features but with no success so far.","@autowired lazy(type_annotation: (Type, str)) @autowired
def foo(bla, *, dep: lazy(MyClass)):
   ...
 lazy True Lazy lazy @autowired
def foo(bla, *, dep: Lazy[MyClass]):
   ...
 Lazy MyClass class LazyMetaclass(type):
    def __getitem__(lazy_type, type_annotation):
        return lazy_type(type_annotation)

class Lazy(metaclass=LazyMetaclass):
    def __init__(self, type_annotation):
        self.type_annotation = type_annotation
 Lazy.__dict__ __dict__",0,25,0,5,
721,49858774,49858921,2734,"Why don't Python 3.7 dataclasses support < > <= and >=, or do they?",1,<python><transcrypt><python-3.7><python-dataclasses>,16,"<p>For version 3.7.1 of the <a href=""http://www.transcrypt.org"" rel=""noreferrer"">Transcrypt Python to JavaScript compiler</a> I am currently using the new <code>@dataclass</code> decorator. I had expected that <code>==, !=, &lt;, &gt;, &gt;=, &lt;=</code> would be supported, as per <a href=""https://www.python.org/dev/peps/pep-0557/"" rel=""noreferrer"">the PEP's abstract</a>, but it doesn't seem to be the case:</p>

<pre><code>from dataclasses import dataclass

@dataclass
class C:
    x: int = 10
</code></pre>

<p>Some comparisons are not working:</p>

<pre><code>&gt;&gt;&gt; c1 = C(1)
&gt;&gt;&gt; c2 = C(2)
&gt;&gt;&gt; c1 == c2  # ok
False
&gt;&gt;&gt; c1 &lt; c2  # crash
TypeError: '&lt;' not supported between instances of 'C' and 'C'
</code></pre>

<p>Why are the comparison operators not supported, except for <code>==</code> and <code>!=</code>? Or did I overlook something?</p>
",1577341,6840,16-04-2018 13:41,16-04-2018 13:48,0,6840,46,2,30,50,"{'badge_counts': {'bronze': 46, 'silver': 30, 'gold': 2}, 'account_id': 1721668, 'is_employee': False, 'last_modified_date': 1680206714, 'last_access_date': 1708940585, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6840, 'creation_date': 1344169773, 'user_type': 'registered', 'user_id': 1577341, 'accept_rate': 50, 'location': 'Netherlands', 'website_url': 'http://www.transcrypt.org', 'link': 'https://stackoverflow.com/users/1577341/jacques-de-hooge', 'profile_image': 'https://i.stack.imgur.com/nmeAD.jpg?s=256&g=1', 'display_name': 'Jacques de Hooge'}","For version 3.7.1 of the Transcrypt Python to JavaScript compiler I am currently using the new decorator. I had expected that would be supported, as per the PEP's abstract, but it doesn't seem to be the case: Some comparisons are not working: Why are the comparison operators not supported, except for and ? Or did I overlook something?","@dataclass ==, !=, &lt;, &gt;, &gt;=, &lt;= from dataclasses import dataclass

@dataclass
class C:
    x: int = 10
 &gt;&gt;&gt; c1 = C(1)
&gt;&gt;&gt; c2 = C(2)
&gt;&gt;&gt; c1 == c2  # ok
False
&gt;&gt;&gt; c1 &lt; c2  # crash
TypeError: '&lt;' not supported between instances of 'C' and 'C'
 == !=",5,20,0,2,
722,50070979,50071082,3774,Wrong dashboard while adding flask-admin to project,2,<python><flask><flask-admin>,12,"<p><a href=""https://i.stack.imgur.com/gdXiE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gdXiE.png"" alt=""enter image description here""></a></p>

<p>I'm trying to extend the flask-base project <a href=""https://github.com/hack4impact/flask-base/tree/master/app"" rel=""nofollow noreferrer"">https://github.com/hack4impact/flask-base/tree/master/app</a>. This uses the the application factory pattern in app/init.py and blueprints.</p>

<p>In the app/init.py I have:</p>

<pre><code>import os
from flask import Flask
from flask_mail import Mail
from flask_sqlalchemy import SQLAlchemy
from flask_login import LoginManager
from flask_assets import Environment
from flask_wtf import CsrfProtect
from flask_compress import Compress
from flask_rq import RQ
from flask_admin import Admin, BaseView, expose
from flask_admin.contrib.sqla import ModelView
# from app.models import User


from config import config
from .assets import app_css, app_js, vendor_css, vendor_js




basedir = os.path.abspath(os.path.dirname(__file__))

mail = Mail()
db = SQLAlchemy()
csrf = CsrfProtect()
compress = Compress()


# Set up Flask-Login
login_manager = LoginManager()
login_manager.session_protection = 'strong'
login_manager.login_view = 'account.login'

from app.models import User


def create_app(config_name):
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
    # not using sqlalchemy event system, hence disabling it


    with app.app_context():
        m =app.url_map

    config[config_name].init_app(app)

    # Set up extensions
    mail.init_app(app)
    db.init_app(app)
    login_manager.init_app(app)
    csrf.init_app(app)
    compress.init_app(app)
    RQ(app)
    # adm = Admin(app, name='MyAPP')
    adm = Admin(endpoint='adminz', name='adz', url='/adminz')



   ......

    # Create app blueprints
    from .main import main as main_blueprint
    app.register_blueprint(main_blueprint)

    from .account import account as account_blueprint
    app.register_blueprint(account_blueprint, url_prefix='/account')

    from .admin import admin as admin_blueprint
    # from .admin import admin_blueprint
    app.register_blueprint(admin_blueprint, url_prefix='/admin')

    return app
</code></pre>

<p>templates/admin/db.html:</p>

<pre><code>&lt;p&gt;Hello world&lt;/p&gt;
</code></pre>

<p>To the admin views (<a href=""https://github.com/hack4impact/flask-base/blob/master/app/admin/views.py"" rel=""nofollow noreferrer"">https://github.com/hack4impact/flask-base/blob/master/app/admin/views.py</a>) I've added :</p>

<pre><code>from flask_admin import Admin, BaseView, expose
from flask_admin.contrib.sqla import ModelView
from app import adm as adm, db

class MyView(ModelView):
    @expose('/')
    def db(self):
        return self.render('admin/db.html')


# admin management setup
@main.route('/db')
def db():
    adm.add_view(MyView(User, db.session))
</code></pre>

<p>I'm getting the admin dashboard not the flask-admin basic view when I open:</p>

<pre><code> 127.0.0.1:5000/db
</code></pre>

<p><a href=""https://i.stack.imgur.com/2vY9u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2vY9u.png"" alt=""enter image description here""></a></p>

<p>What am I doing wrong?</p>

<p>EDIT:</p>

<p>following your directions I changed create_app to start with:</p>

<pre><code>def create_app(config_name):
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
    # not using sqlalchemy event system, hence disabling it
    # adm = Admin(name='admin2', endpoint='/db', url='/db', template_mode='bootstrap3',base_template='admin/db.html')


    config[config_name].init_app(app)

    # Set up extensions
    mail.init_app(app)
    db.init_app(app)
    login_manager.init_app(app)
    csrf.init_app(app)
    compress.init_app(app)
    RQ(app)
    adm = Admin(app, name='MyAPP')
    # adm = Admin(endpoint='adminz', name='adz', url='/adminz')
    adm.add_view(MyView(User, db.session, endpoint='db'))
</code></pre>

<p>This results in :</p>

<p>File ""....flask\app.py"", line 946, in register_blueprint
    (blueprint, self.blueprints[blueprint.name], blueprint.name)
AssertionError: A blueprint's name collision occurred between  and .  Both share the same name ""admin"".  Blueprints that are created on the fly need unique names.</p>

<p>EDIT2:</p>

<p>to the end of create_app I've added:</p>

<pre><code># Create app blueprints
from .main import main as main_blueprint
app.register_blueprint(main_blueprint)

from .account import account as account_blueprint
app.register_blueprint(account_blueprint, url_prefix='/account')

from .admin import admin as admin_blueprint
# from .admin import admin_blueprint
app.register_blueprint(admin_blueprint, url_prefix='/admin')
# app.register_blueprint(admin_blueprint, url_prefix='/ab')

with app.app_context():
    m =app.url_map

return app
</code></pre>

<p>I'm not sure what you want to see but m.rules gives:</p>

<pre><code> &lt;Rule '/account/manage/change-password' (HEAD, GET, OPTIONS, POST) -&gt; account.change_password&gt;,
 &lt;Rule '/account/manage/change-email' (HEAD, GET, OPTIONS, POST) -&gt; account.change_email_request&gt;,
 &lt;Rule '/account/manage/info' (HEAD, GET, OPTIONS, POST) -&gt; account.manage&gt;,
 &lt;Rule '/account/confirm-account' (HEAD, GET, OPTIONS) -&gt; account.confirm_request&gt;,
 &lt;Rule '/account/reset-password' (HEAD, GET, OPTIONS, POST) -&gt; account.reset_password_request&gt;,
 &lt;Rule '/account/unconfirmed' (HEAD, GET, OPTIONS) -&gt; account.unconfirmed&gt;,
 &lt;Rule '/account/register' (HEAD, GET, OPTIONS, POST) -&gt; account.register&gt;,
 &lt;Rule '/account/logout' (HEAD, GET, OPTIONS) -&gt; account.logout&gt;,
 &lt;Rule '/account/manage' (HEAD, GET, OPTIONS, POST) -&gt; account.manage&gt;,
 &lt;Rule '/account/login' (HEAD, GET, OPTIONS, POST) -&gt; account.login&gt;,
 &lt;Rule '/admin/_update_editor_contents' (OPTIONS, POST) -&gt; admin.update_editor_contents&gt;,
 &lt;Rule '/admin/invite-user' (HEAD, GET, OPTIONS, POST) -&gt; admin.invite_user&gt;,
 &lt;Rule '/admin/new-user' (HEAD, GET, OPTIONS, POST) -&gt; admin.new_user&gt;,
 &lt;Rule '/admin/users' (HEAD, GET, OPTIONS) -&gt; admin.registered_users&gt;,
 &lt;Rule '/get_session_job_value' (HEAD, GET, OPTIONS) -&gt; main.get_session_job_value&gt;,
 &lt;Rule '/cl_confirm_chrome' (HEAD, GET, OPTIONS, POST) -&gt; main.cl_confirm_chrome&gt;,
 &lt;Rule '/render_png' (HEAD, GET, OPTIONS) -&gt; main.render_png&gt;,
 &lt;Rule '/selected' (HEAD, GET, OPTIONS) -&gt; main.selected&gt;,
 &lt;Rule '/cl_dash' (HEAD, GET, OPTIONS, POST) -&gt; main.cl_dash&gt;,
 &lt;Rule '/about' (HEAD, GET, OPTIONS) -&gt; main.about&gt;,
 &lt;Rule '/admin/' (HEAD, GET, OPTIONS) -&gt; admin.index&gt;,
 &lt;Rule '/dash' (HEAD, GET, OPTIONS) -&gt; main.dash&gt;,
 &lt;Rule '/jobs' (HEAD, GET, OPTIONS) -&gt; main.get_jobs&gt;,
 &lt;Rule '/' (HEAD, GET, OPTIONS) -&gt; main.index&gt;,
 &lt;Rule '/account/manage/change-email/&lt;token&gt;' (HEAD, GET, OPTIONS, POST) -&gt; account.change_email&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/change-account-type' (HEAD, GET, OPTIONS, POST) -&gt; admin.change_account_type&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/change-email' (HEAD, GET, OPTIONS, POST) -&gt; admin.change_user_email&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/_delete' (HEAD, GET, OPTIONS) -&gt; admin.delete_user&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/delete' (HEAD, GET, OPTIONS) -&gt; admin.delete_user_request&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/info' (HEAD, GET, OPTIONS) -&gt; admin.user_info&gt;,
 &lt;Rule '/account/join-from-invite/&lt;user_id&gt;/&lt;token&gt;' (HEAD, GET, OPTIONS, POST) -&gt; account.join_from_invite&gt;,
 &lt;Rule '/account/confirm-account/&lt;token&gt;' (HEAD, GET, OPTIONS) -&gt; account.confirm&gt;,
 &lt;Rule '/account/reset-password/&lt;token&gt;' (HEAD, GET, OPTIONS, POST) -&gt; account.reset_password&gt;,
 &lt;Rule '/admin/static/&lt;filename&gt;' (HEAD, GET, OPTIONS) -&gt; admin.static&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;' (HEAD, GET, OPTIONS) -&gt; admin.user_info&gt;,
 &lt;Rule '/results/&lt;job_key&gt;' (HEAD, GET, OPTIONS) -&gt; main.get_results&gt;,
 &lt;Rule '/static/&lt;filename&gt;' (HEAD, GET, OPTIONS) -&gt; static&gt;
</code></pre>

<p>EDIT 3:</p>

<p>I have to say that is an incredible answer!  You have really taught me a lot. I have replaced the urls with the rules above following your directions. My original plan 10 days ago was just to use the basic flask-admin CRUD functionality. I'm not interested in the db.html template (its just something I tried).</p>

<p>anyway trying to change the name of the admin blueprint ( your number 1). I had tried this before changing app/admin/init.py to :</p>

<pre><code>from flask import Blueprint
admin = Blueprint('admin_blueprint', __name__)
from . import views  # noqa
</code></pre>

<p>Now when I open</p>

<pre><code>http://127.0.0.1:5000/adminz/
</code></pre>

<p>I get a 404 error</p>

<p>FINAL EDIT:</p>

<p>The problem was solved by <a href=""https://chat.stackoverflow.com/users/5819113/diego-quintana"">https://chat.stackoverflow.com/users/5819113/diego-quintana</a> who explained that there was a conflict between flask-admin which creates a blueprint and the flask-base admin blueprint. By changing both the name of the blueprint and the static file folder of the flask-base project. Flask-admin could work without being overridden. Please see <a href=""https://github.com/kc1/flask-base"" rel=""nofollow noreferrer"">https://github.com/kc1/flask-base</a></p>
",1592380,35093,27-04-2018 22:20,27-04-2018 22:33,0,35045,534,98,297,77,"{'badge_counts': {'bronze': 534, 'silver': 297, 'gold': 98}, 'account_id': 1740937, 'is_employee': False, 'last_modified_date': 1710430800, 'last_access_date': 1711070901, 'reputation_change_year': 178, 'reputation_change_quarter': 178, 'reputation_change_month': 48, 'reputation_change_week': -90, 'reputation_change_day': 0, 'reputation': 35045, 'creation_date': 1344696578, 'user_type': 'registered', 'user_id': 1592380, 'accept_rate': 77, 'website_url': '', 'link': 'https://stackoverflow.com/users/1592380/user1592380', 'profile_image': 'https://www.gravatar.com/avatar/1b375c795476432e64d4433a651d2804?s=256&d=identicon&r=PG', 'display_name': 'user1592380'}","I'm trying to extend the flask-base project https://github.com/hack4impact/flask-base/tree/master/app. This uses the the application factory pattern in app/init.py and blueprints. In the app/init.py I have: templates/admin/db.html: To the admin views (https://github.com/hack4impact/flask-base/blob/master/app/admin/views.py) I've added : I'm getting the admin dashboard not the flask-admin basic view when I open: What am I doing wrong? EDIT: following your directions I changed create_app to start with: This results in : File ""....flask\app.py"", line 946, in register_blueprint (blueprint, self.blueprints[blueprint.name], blueprint.name) AssertionError: A blueprint's name collision occurred between and . Both share the same name ""admin"". Blueprints that are created on the fly need unique names. EDIT2: to the end of create_app I've added: I'm not sure what you want to see but m.rules gives: EDIT 3: I have to say that is an incredible answer! You have really taught me a lot. I have replaced the urls with the rules above following your directions. My original plan 10 days ago was just to use the basic flask-admin CRUD functionality. I'm not interested in the db.html template (its just something I tried). anyway trying to change the name of the admin blueprint ( your number 1). I had tried this before changing app/admin/init.py to : Now when I open I get a 404 error FINAL EDIT: The problem was solved by https://chat.stackoverflow.com/users/5819113/diego-quintana who explained that there was a conflict between flask-admin which creates a blueprint and the flask-base admin blueprint. By changing both the name of the blueprint and the static file folder of the flask-base project. Flask-admin could work without being overridden. Please see https://github.com/kc1/flask-base","import os
from flask import Flask
from flask_mail import Mail
from flask_sqlalchemy import SQLAlchemy
from flask_login import LoginManager
from flask_assets import Environment
from flask_wtf import CsrfProtect
from flask_compress import Compress
from flask_rq import RQ
from flask_admin import Admin, BaseView, expose
from flask_admin.contrib.sqla import ModelView
# from app.models import User


from config import config
from .assets import app_css, app_js, vendor_css, vendor_js




basedir = os.path.abspath(os.path.dirname(__file__))

mail = Mail()
db = SQLAlchemy()
csrf = CsrfProtect()
compress = Compress()


# Set up Flask-Login
login_manager = LoginManager()
login_manager.session_protection = 'strong'
login_manager.login_view = 'account.login'

from app.models import User


def create_app(config_name):
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
    # not using sqlalchemy event system, hence disabling it


    with app.app_context():
        m =app.url_map

    config[config_name].init_app(app)

    # Set up extensions
    mail.init_app(app)
    db.init_app(app)
    login_manager.init_app(app)
    csrf.init_app(app)
    compress.init_app(app)
    RQ(app)
    # adm = Admin(app, name='MyAPP')
    adm = Admin(endpoint='adminz', name='adz', url='/adminz')



   ......

    # Create app blueprints
    from .main import main as main_blueprint
    app.register_blueprint(main_blueprint)

    from .account import account as account_blueprint
    app.register_blueprint(account_blueprint, url_prefix='/account')

    from .admin import admin as admin_blueprint
    # from .admin import admin_blueprint
    app.register_blueprint(admin_blueprint, url_prefix='/admin')

    return app
 &lt;p&gt;Hello world&lt;/p&gt;
 from flask_admin import Admin, BaseView, expose
from flask_admin.contrib.sqla import ModelView
from app import adm as adm, db

class MyView(ModelView):
    @expose('/')
    def db(self):
        return self.render('admin/db.html')


# admin management setup
@main.route('/db')
def db():
    adm.add_view(MyView(User, db.session))
  127.0.0.1:5000/db
 def create_app(config_name):
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
    # not using sqlalchemy event system, hence disabling it
    # adm = Admin(name='admin2', endpoint='/db', url='/db', template_mode='bootstrap3',base_template='admin/db.html')


    config[config_name].init_app(app)

    # Set up extensions
    mail.init_app(app)
    db.init_app(app)
    login_manager.init_app(app)
    csrf.init_app(app)
    compress.init_app(app)
    RQ(app)
    adm = Admin(app, name='MyAPP')
    # adm = Admin(endpoint='adminz', name='adz', url='/adminz')
    adm.add_view(MyView(User, db.session, endpoint='db'))
 # Create app blueprints
from .main import main as main_blueprint
app.register_blueprint(main_blueprint)

from .account import account as account_blueprint
app.register_blueprint(account_blueprint, url_prefix='/account')

from .admin import admin as admin_blueprint
# from .admin import admin_blueprint
app.register_blueprint(admin_blueprint, url_prefix='/admin')
# app.register_blueprint(admin_blueprint, url_prefix='/ab')

with app.app_context():
    m =app.url_map

return app
  &lt;Rule '/account/manage/change-password' (HEAD, GET, OPTIONS, POST) -&gt; account.change_password&gt;,
 &lt;Rule '/account/manage/change-email' (HEAD, GET, OPTIONS, POST) -&gt; account.change_email_request&gt;,
 &lt;Rule '/account/manage/info' (HEAD, GET, OPTIONS, POST) -&gt; account.manage&gt;,
 &lt;Rule '/account/confirm-account' (HEAD, GET, OPTIONS) -&gt; account.confirm_request&gt;,
 &lt;Rule '/account/reset-password' (HEAD, GET, OPTIONS, POST) -&gt; account.reset_password_request&gt;,
 &lt;Rule '/account/unconfirmed' (HEAD, GET, OPTIONS) -&gt; account.unconfirmed&gt;,
 &lt;Rule '/account/register' (HEAD, GET, OPTIONS, POST) -&gt; account.register&gt;,
 &lt;Rule '/account/logout' (HEAD, GET, OPTIONS) -&gt; account.logout&gt;,
 &lt;Rule '/account/manage' (HEAD, GET, OPTIONS, POST) -&gt; account.manage&gt;,
 &lt;Rule '/account/login' (HEAD, GET, OPTIONS, POST) -&gt; account.login&gt;,
 &lt;Rule '/admin/_update_editor_contents' (OPTIONS, POST) -&gt; admin.update_editor_contents&gt;,
 &lt;Rule '/admin/invite-user' (HEAD, GET, OPTIONS, POST) -&gt; admin.invite_user&gt;,
 &lt;Rule '/admin/new-user' (HEAD, GET, OPTIONS, POST) -&gt; admin.new_user&gt;,
 &lt;Rule '/admin/users' (HEAD, GET, OPTIONS) -&gt; admin.registered_users&gt;,
 &lt;Rule '/get_session_job_value' (HEAD, GET, OPTIONS) -&gt; main.get_session_job_value&gt;,
 &lt;Rule '/cl_confirm_chrome' (HEAD, GET, OPTIONS, POST) -&gt; main.cl_confirm_chrome&gt;,
 &lt;Rule '/render_png' (HEAD, GET, OPTIONS) -&gt; main.render_png&gt;,
 &lt;Rule '/selected' (HEAD, GET, OPTIONS) -&gt; main.selected&gt;,
 &lt;Rule '/cl_dash' (HEAD, GET, OPTIONS, POST) -&gt; main.cl_dash&gt;,
 &lt;Rule '/about' (HEAD, GET, OPTIONS) -&gt; main.about&gt;,
 &lt;Rule '/admin/' (HEAD, GET, OPTIONS) -&gt; admin.index&gt;,
 &lt;Rule '/dash' (HEAD, GET, OPTIONS) -&gt; main.dash&gt;,
 &lt;Rule '/jobs' (HEAD, GET, OPTIONS) -&gt; main.get_jobs&gt;,
 &lt;Rule '/' (HEAD, GET, OPTIONS) -&gt; main.index&gt;,
 &lt;Rule '/account/manage/change-email/&lt;token&gt;' (HEAD, GET, OPTIONS, POST) -&gt; account.change_email&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/change-account-type' (HEAD, GET, OPTIONS, POST) -&gt; admin.change_account_type&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/change-email' (HEAD, GET, OPTIONS, POST) -&gt; admin.change_user_email&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/_delete' (HEAD, GET, OPTIONS) -&gt; admin.delete_user&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/delete' (HEAD, GET, OPTIONS) -&gt; admin.delete_user_request&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;/info' (HEAD, GET, OPTIONS) -&gt; admin.user_info&gt;,
 &lt;Rule '/account/join-from-invite/&lt;user_id&gt;/&lt;token&gt;' (HEAD, GET, OPTIONS, POST) -&gt; account.join_from_invite&gt;,
 &lt;Rule '/account/confirm-account/&lt;token&gt;' (HEAD, GET, OPTIONS) -&gt; account.confirm&gt;,
 &lt;Rule '/account/reset-password/&lt;token&gt;' (HEAD, GET, OPTIONS, POST) -&gt; account.reset_password&gt;,
 &lt;Rule '/admin/static/&lt;filename&gt;' (HEAD, GET, OPTIONS) -&gt; admin.static&gt;,
 &lt;Rule '/admin/user/&lt;user_id&gt;' (HEAD, GET, OPTIONS) -&gt; admin.user_info&gt;,
 &lt;Rule '/results/&lt;job_key&gt;' (HEAD, GET, OPTIONS) -&gt; main.get_results&gt;,
 &lt;Rule '/static/&lt;filename&gt;' (HEAD, GET, OPTIONS) -&gt; static&gt;
 from flask import Blueprint
admin = Blueprint('admin_blueprint', __name__)
from . import views  # noqa
 http://127.0.0.1:5000/adminz/
",158,230,2,6,
723,48246345,48246530,37064,How to refresh Selenium Webdriver DOM data without reloading page?,2,<python><python-3.x><selenium><selenium-webdriver><webdriver>,15,"<p>I use Selenium with Python to parse search results from a database site. Search output is dynamic, so, when I type new request, page is not reloaded, but search results are new.</p>

<p>Problem is that Selenium doesn't update WebDriver DOM data, so next time I try something like <code>driver.find_elements_by_class_name('query_header')</code> I receive elements from previous search request and <code>StaleError</code>.</p>

<p>Using <code>WebDriverWait(driver, timeout).until(element_present)</code> doesn't help. Elements are there (all search result blocks have same classes, names, etc.)., but they're old :)</p>

<p>I fixed it by reloading page with <code>driver.refresh()</code> after each request, but it looks a bit unnatural + double requests.</p>

<p>Is there any way to refresh Selenium DOM data, so I'll get new elements with <code>find_elements</code> without page reload?</p>
",1598470,1557,14-01-2018 03:07,14-01-2018 03:49,0,1557,31,4,20,88,"{'badge_counts': {'bronze': 31, 'silver': 20, 'gold': 4}, 'account_id': 1748896, 'is_employee': False, 'last_modified_date': 1635524348, 'last_access_date': 1711179345, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1557, 'creation_date': 1344957916, 'user_type': 'registered', 'user_id': 1598470, 'accept_rate': 88, 'location': 'Pittsburgh, PA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1598470/sortas', 'profile_image': 'https://i.stack.imgur.com/W3sb2.jpg?s=256&g=1', 'display_name': 'sortas'}","I use Selenium with Python to parse search results from a database site. Search output is dynamic, so, when I type new request, page is not reloaded, but search results are new. Problem is that Selenium doesn't update WebDriver DOM data, so next time I try something like I receive elements from previous search request and . Using doesn't help. Elements are there (all search result blocks have same classes, names, etc.)., but they're old :) I fixed it by reloading page with after each request, but it looks a bit unnatural + double requests. Is there any way to refresh Selenium DOM data, so I'll get new elements with without page reload?","driver.find_elements_by_class_name('query_header') StaleError WebDriverWait(driver, timeout).until(element_present) driver.refresh() find_elements",-5,9,0,0,
724,48790378,48791644,30397,How to get ticks every hour,2,<python><pandas><matplotlib>,27,"<p>Consider this simple example</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
import matplotlib.dates as mdates

pd.__version__
Out[147]: u'0.22.0'

idx = pd.date_range('2017-01-01 05:03', '2017-01-01 18:03', freq = 'min')

df = pd.Series(np.random.randn(len(idx)),  index = idx)
df.head()
Out[145]: 
2017-01-01 05:03:00   0.4361
2017-01-01 05:04:00   0.9737
2017-01-01 05:05:00   0.8430
2017-01-01 05:06:00   0.4292
2017-01-01 05:07:00   0.5739
Freq: T, dtype: float64
</code></pre>

<p>I want to plot this, and have ticks every hour. I use:</p>

<pre><code>fig, ax = plt.subplots()
hours = mdates.HourLocator(interval = 1)  #
h_fmt = mdates.DateFormatter('%H:%M:%S')

df.plot(ax = ax, color = 'black', linewidth = 0.4)

ax.xaxis.set_major_locator(hours)
ax.xaxis.set_major_formatter(h_fmt)
</code></pre>

<p>which gives</p>

<p><a href=""https://i.stack.imgur.com/xZVkg.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xZVkg.png"" alt=""enter image description here""></a></p>

<p>why dont the ticks appear every hour here? Thanks for your help!</p>
",1609428,18969,14-02-2018 15:10,14-02-2018 16:14,0,18989,244,35,134,96,"{'badge_counts': {'bronze': 244, 'silver': 134, 'gold': 35}, 'account_id': 1762809, 'is_employee': False, 'last_modified_date': 1710510300, 'last_access_date': 1711110809, 'reputation_change_year': 158, 'reputation_change_quarter': 158, 'reputation_change_month': 42, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 18989, 'creation_date': 1345333693, 'user_type': 'registered', 'user_id': 1609428, 'accept_rate': 96, 'website_url': '', 'link': 'https://stackoverflow.com/users/1609428/%e2%84%95%ca%98%ca%98%e1%b8%86%e1%b8%bd%e1%b8%98', 'profile_image': 'https://i.stack.imgur.com/TnnKE.jpg?s=256&g=1', 'display_name': 'ℕʘʘḆḽḘ'}","Consider this simple example I want to plot this, and have ticks every hour. I use: which gives why dont the ticks appear every hour here? Thanks for your help!","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
import matplotlib.dates as mdates

pd.__version__
Out[147]: u'0.22.0'

idx = pd.date_range('2017-01-01 05:03', '2017-01-01 18:03', freq = 'min')

df = pd.Series(np.random.randn(len(idx)),  index = idx)
df.head()
Out[145]: 
2017-01-01 05:03:00   0.4361
2017-01-01 05:04:00   0.9737
2017-01-01 05:05:00   0.8430
2017-01-01 05:06:00   0.4292
2017-01-01 05:07:00   0.5739
Freq: T, dtype: float64
 fig, ax = plt.subplots()
hours = mdates.HourLocator(interval = 1)  #
h_fmt = mdates.DateFormatter('%H:%M:%S')

df.plot(ax = ax, color = 'black', linewidth = 0.4)

ax.xaxis.set_major_locator(hours)
ax.xaxis.set_major_formatter(h_fmt)
",26,41,1,1,
725,49841133,60860082,5100,typing: Define a type that can only be certain strings?,1,<python><python-3.x><typing>,14,"<p>How can I use the <em>typing</em> module, to create a type that can be certain strings?</p>

<p>E.g. let's say I need a type <code>CondOperator</code>, which can be any of these strings:</p>

<pre><code>['=', '&gt;', '&lt;', '&gt;=', '&lt;=', '&lt;&gt;', '!=']
</code></pre>

<p>I was hoping for <code>CondOperator = String['=', '&gt;', '&lt;', '&gt;=', '&lt;=', '&lt;&gt;', '!=']</code>, but there is no <code>String</code> in <code>typing</code>. So this doesn't work.</p>

<p>How can define such type?</p>
",1612318,51445,15-04-2018 10:55,26-03-2020 02:10,711,51575,114,23,114,80,"{'badge_counts': {'bronze': 114, 'silver': 114, 'gold': 23}, 'account_id': 1766552, 'is_employee': False, 'last_modified_date': 1701810302, 'last_access_date': 1711072179, 'reputation_change_year': 842, 'reputation_change_quarter': 842, 'reputation_change_month': 208, 'reputation_change_week': 70, 'reputation_change_day': 10, 'reputation': 51575, 'creation_date': 1345481288, 'user_type': 'registered', 'user_id': 1612318, 'accept_rate': 80, 'location': 'Planet Earth', 'website_url': 'http://www.mewo.dev', 'link': 'https://stackoverflow.com/users/1612318/rotareti', 'profile_image': 'https://i.stack.imgur.com/IqAVI.jpg?s=256&g=1', 'display_name': 'Rotareti'}","How can I use the typing module, to create a type that can be certain strings? E.g. let's say I need a type , which can be any of these strings: I was hoping for , but there is no in . So this doesn't work. How can define such type?","CondOperator ['=', '&gt;', '&lt;', '&gt;=', '&lt;=', '&lt;&gt;', '!=']
 CondOperator = String['=', '&gt;', '&lt;', '&gt;=', '&lt;=', '&lt;&gt;', '!='] String typing",-4,10,0,0,
726,48786693,48789494,5089,How to wrap a C++ object using pure Python Extension API (python3)?,1,<python><c++><python-3.x><word-wrap>,12,"<p>I want to know how to wrap a C++ object with <a href=""https://docs.python.org/3/extending/extending.html"" rel=""noreferrer"">Python Extension API</a> (and distutils) without external tools (like Cython, Boost, SWIG, ...). Just in pure Python way without creating a dll.</p>

<p>Note that my C++ object has memory allocations so destructor has to be called to avoid memory leaks.</p>

<pre><code>#include ""Voice.h""

namespace transformation
{ 
  Voice::Voice(int fftSize) { mem=new double[fftSize]; } 

  Voice::~Voice() { delete [] mem; } 

  int Voice::method1() { /*do stuff*/ return (1); } 
}
</code></pre>

<p>I just want to do somethings like that in Python :</p>

<pre><code>import voice

v=voice.Voice(512)
result=v.method1()
</code></pre>
",1619521,3484,14-02-2018 11:58,14-02-2018 14:24,0,3484,41,4,31,20,"{'badge_counts': {'bronze': 41, 'silver': 31, 'gold': 4}, 'account_id': 1775691, 'is_employee': False, 'last_modified_date': 1704506100, 'last_access_date': 1707841246, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3484, 'creation_date': 1345719418, 'user_type': 'registered', 'user_id': 1619521, 'accept_rate': 20, 'website_url': '', 'link': 'https://stackoverflow.com/users/1619521/doom', 'profile_image': 'https://www.gravatar.com/avatar/a54821d500ac089427ffe6bfec3d0e5c?s=256&d=identicon&r=PG', 'display_name': 'doom'}","I want to know how to wrap a C++ object with Python Extension API (and distutils) without external tools (like Cython, Boost, SWIG, ...). Just in pure Python way without creating a dll. Note that my C++ object has memory allocations so destructor has to be called to avoid memory leaks. I just want to do somethings like that in Python :","#include ""Voice.h""

namespace transformation
{ 
  Voice::Voice(int fftSize) { mem=new double[fftSize]; } 

  Voice::~Voice() { delete [] mem; } 

  int Voice::method1() { /*do stuff*/ return (1); } 
}
 import voice

v=voice.Voice(512)
result=v.method1()
",12,23,0,1,
727,48835609,55426662,246,How to handle the connection event of widget output in Orange3?,1,<python><orange>,13,"<p>I am developing an add-on widget for Orange3. Is there any way to handle the event of connection/disconnection of widget output?</p>

<p>I would like to postpone heavy calculations for one of the outputs until this output is connected with the input of another widget.</p>
",1636455,342,16-02-2018 22:15,29-03-2019 23:28,406,342,14,0,3,,"{'badge_counts': {'bronze': 14, 'silver': 3, 'gold': 0}, 'account_id': 1797304, 'is_employee': False, 'last_modified_date': 1700001254, 'last_access_date': 1710962956, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 342, 'creation_date': 1346340160, 'user_type': 'registered', 'user_id': 1636455, 'location': 'Fremont, CA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/1636455/dmitriy-kalugin-balashov', 'profile_image': 'https://www.gravatar.com/avatar/e846fa69b65d5a5ab3c1b1360db13675?s=256&d=identicon&r=PG', 'display_name': 'Dmitriy Kalugin-Balashov'}",I am developing an add-on widget for Orange3. Is there any way to handle the event of connection/disconnection of widget output? I would like to postpone heavy calculations for one of the outputs until this output is connected with the input of another widget.,,0,3,0,0,
728,49318826,49318922,17604,getting size of primitive data types in python,3,<python>,14,"<p>I am having a lot of confusion using the <code>sys.getsizeof</code> function in python. All I want to find out is that for say a floating point value, is the system using 4 or 8 bytes (i.e. single or double precision in C terms).</p>

<p>I do the following:</p>

<pre><code>import sys

x = 0.0
sys.getsizeof(x)  # Returns 24

type(x) # returns float

sys.getsizeof(float)  # Returns 400.
</code></pre>

<p>How can I simply find out the how many bytes are actually used for the floating point representation. I know it should be 8 bytes but how can I verify this (something like the <code>sizeof</code> operator in C++)</p>
",2713740,10700,16-03-2018 10:44,16-03-2018 10:49,0,10730,252,25,114,85,"{'badge_counts': {'bronze': 252, 'silver': 114, 'gold': 25}, 'account_id': 3214634, 'is_employee': False, 'last_modified_date': 1706925301, 'last_access_date': 1710793209, 'reputation_change_year': 152, 'reputation_change_quarter': 152, 'reputation_change_month': 60, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 10730, 'creation_date': 1377353333, 'user_type': 'registered', 'user_id': 2713740, 'accept_rate': 85, 'website_url': '', 'link': 'https://stackoverflow.com/users/2713740/luca', 'profile_image': 'https://www.gravatar.com/avatar/d07d4aaa7850d639f2115ecfd145b6f7?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Luca'}","I am having a lot of confusion using the function in python. All I want to find out is that for say a floating point value, is the system using 4 or 8 bytes (i.e. single or double precision in C terms). I do the following: How can I simply find out the how many bytes are actually used for the floating point representation. I know it should be 8 bytes but how can I verify this (something like the operator in C++)","sys.getsizeof import sys

x = 0.0
sys.getsizeof(x)  # Returns 24

type(x) # returns float

sys.getsizeof(float)  # Returns 400.
 sizeof",5,15,0,0,
729,48248567,48250808,8079,How to await a select.select call in python asyncio,1,<python><sockets><python-asyncio>,15,"<p>I have a python 3.6 program where I am using the asyncio package event loops.  One of my data sources comes from an api which was not build around asyncio.  My connection object contains a member called <code>_connection</code> which is just a python socket.  Right now I can use this in a select statement to tell when data is ready.</p>

<pre><code>async def run(self):
    while True:
        if select.select([self._q._connection], [], [])[0]:
            msg = self._q.receive()
            print(msg)
</code></pre>

<p>What I would really like is...</p>

<pre><code>async def run(self):
    while True:
        if await select.select([self._q._connection], [], [])[0]:
            msg = self._q.receive()
            print(msg)
</code></pre>

<p>I know there is a <code>sock_recv</code> function in the asyncio event loop however I need the api to do the actual reading and decoding.  I tried this but it would just fall through the await which I guess makes sense since I said 0 bytes.</p>

<pre><code>async def run(self):
    while True:
        print('A')
        await asyncio.get_event_loop().sock_recv(self._q._connection, 0)
        print('B')
        msg = self._q.receive()
        print(msg)
</code></pre>

<p>The only solution I can think of for now is to add a small timeout to the select and then call <code>asyncio.sleep</code> while there is no data but this seems like an inefficent approach.  I wish there was something like <code>asyncio.select</code>.  Do anyone want to recommend another approach?</p>

<p>EDIT:  Right now I have come up with this.  I don't like it because it adds an extra quarter second latency (probably doesn't matter much for my application but it still bugs me.)</p>

<pre><code>async def run(self):
    while True:
        if select.select([self._q._connection], [], [], 0)[0]:
           print(self._q.receive())
        else:
            await asyncio.sleep(0.25)
</code></pre>
",2876799,11925,14-01-2018 10:16,14-01-2018 14:52,0,11935,116,9,62,58,"{'badge_counts': {'bronze': 116, 'silver': 62, 'gold': 9}, 'account_id': 3432928, 'is_employee': False, 'last_modified_date': 1670848500, 'last_access_date': 1710963604, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 11935, 'creation_date': 1381690368, 'user_type': 'registered', 'user_id': 2876799, 'accept_rate': 58, 'website_url': '', 'link': 'https://stackoverflow.com/users/2876799/chasep255', 'profile_image': 'https://www.gravatar.com/avatar/0a498234b7c1de90570871958895f474?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'chasep255'}",I have a python 3.6 program where I am using the asyncio package event loops. One of my data sources comes from an api which was not build around asyncio. My connection object contains a member called which is just a python socket. Right now I can use this in a select statement to tell when data is ready. What I would really like is... I know there is a function in the asyncio event loop however I need the api to do the actual reading and decoding. I tried this but it would just fall through the await which I guess makes sense since I said 0 bytes. The only solution I can think of for now is to add a small timeout to the select and then call while there is no data but this seems like an inefficent approach. I wish there was something like . Do anyone want to recommend another approach? EDIT: Right now I have come up with this. I don't like it because it adds an extra quarter second latency (probably doesn't matter much for my application but it still bugs me.),"_connection async def run(self):
    while True:
        if select.select([self._q._connection], [], [])[0]:
            msg = self._q.receive()
            print(msg)
 async def run(self):
    while True:
        if await select.select([self._q._connection], [], [])[0]:
            msg = self._q.receive()
            print(msg)
 sock_recv async def run(self):
    while True:
        print('A')
        await asyncio.get_event_loop().sock_recv(self._q._connection, 0)
        print('B')
        msg = self._q.receive()
        print(msg)
 asyncio.sleep asyncio.select async def run(self):
    while True:
        if select.select([self._q._connection], [], [], 0)[0]:
           print(self._q.receive())
        else:
            await asyncio.sleep(0.25)
",15,40,0,0,
730,48543834,48543945,41666,How do I reduce a python (docker) image size using a multi-stage build?,4,<python><docker><dockerfile><docker-multi-stage-build>,53,"<p>I am looking for a way to create multistage builds with python and Dockerfile:</p>
<p>For example, using the following images:</p>
<p><strong>1st image</strong>: install all compile-time requirements, and install all needed python modules</p>
<p><strong>2nd image</strong>: copy all compiled/built packages from the first image to the second, without the compilers themselves (gcc, postgers-dev, python-dev, etc..)</p>
<p>The final objective is to have a smaller image, running python and the python packages that I need.</p>
<p>In short: <strong>how can I 'wrap' all the compiled modules</strong> (site-packages / external libs) that were created in the first image, and <strong>copy them in a 'clean' manner</strong>, to the 2nd image.</p>
",1668328,2909,31-01-2018 13:54,31-01-2018 13:58,0,2929,52,1,23,3,"{'badge_counts': {'bronze': 52, 'silver': 23, 'gold': 1}, 'account_id': 1838415, 'is_employee': False, 'last_modified_date': 1703901300, 'last_access_date': 1711009574, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 50, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 2929, 'creation_date': 1347532849, 'user_type': 'registered', 'user_id': 1668328, 'accept_rate': 3, 'location': 'Tel Aviv, Israel', 'website_url': 'https://www.blogfoobar.com/', 'link': 'https://stackoverflow.com/users/1668328/gcoh', 'profile_image': 'https://www.gravatar.com/avatar/5ed18ad13d8999f33099756cff554bef?s=256&d=identicon&r=PG', 'display_name': 'gCoh'}","I am looking for a way to create multistage builds with python and Dockerfile: For example, using the following images: 1st image: install all compile-time requirements, and install all needed python modules 2nd image: copy all compiled/built packages from the first image to the second, without the compilers themselves (gcc, postgers-dev, python-dev, etc..) The final objective is to have a smaller image, running python and the python packages that I need. In short: how can I 'wrap' all the compiled modules (site-packages / external libs) that were created in the first image, and copy them in a 'clean' manner, to the 2nd image.",,0,6,0,0,
731,49749706,49749970,9985,Randomly sample from multiple tf.data.Datasets in Tensorflow,3,<python><tensorflow><tensorflow-datasets>,12,"<p>suppose I have <em>N</em> tf.data.Datasets and a list of <em>N</em> probabilities (summing to 1), now I would like to create dataset such that the examples are sampled from the <em>N</em> datasets with the given probabilities.</p>

<p>I would like this to work for arbitrary probabilities -> simple zip/concat/flatmap with fixed number of examples from each dataset is probably not what I am looking for.</p>

<p>Is it possible to do this in TF? Thanks!</p>
",1705970,510,10-04-2018 09:10,10-04-2018 09:23,0,510,14,1,5,25,"{'badge_counts': {'bronze': 14, 'silver': 5, 'gold': 1}, 'account_id': 1886940, 'is_employee': False, 'last_modified_date': 1600707067, 'last_access_date': 1711138880, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 510, 'creation_date': 1348827574, 'user_type': 'registered', 'user_id': 1705970, 'accept_rate': 25, 'location': 'Czech Republic', 'website_url': '', 'link': 'https://stackoverflow.com/users/1705970/serycjon', 'profile_image': 'https://www.gravatar.com/avatar/fe433bc918ae85881cc89b6159985ca1?s=256&d=identicon&r=PG', 'display_name': 'serycjon'}","suppose I have N tf.data.Datasets and a list of N probabilities (summing to 1), now I would like to create dataset such that the examples are sampled from the N datasets with the given probabilities. I would like this to work for arbitrary probabilities -> simple zip/concat/flatmap with fixed number of examples from each dataset is probably not what I am looking for. Is it possible to do this in TF? Thanks!",,0,5,0,0,
732,48242324,48242325,105135,Undo last Alembic migration,3,<python><sqlalchemy><alembic>,110,"<p>I created a migration with <code>alembic revision --autogenerate</code>, applied it to my development database with <code>alembic upgrade head</code>, and then realised it wasn't quite what I wanted.</p>

<p>How can I revert the migration so that I can tweak it and try again?</p>
",1709587,147549,13-01-2018 17:17,13-01-2018 17:17,0,147845,463,86,412,71,"{'badge_counts': {'bronze': 463, 'silver': 412, 'gold': 86}, 'account_id': 1891677, 'is_employee': False, 'last_modified_date': 1711065300, 'last_access_date': 1711145248, 'reputation_change_year': 1960, 'reputation_change_quarter': 1960, 'reputation_change_month': 487, 'reputation_change_week': 203, 'reputation_change_day': 0, 'reputation': 147845, 'creation_date': 1348998629, 'user_type': 'registered', 'user_id': 1709587, 'accept_rate': 71, 'location': 'London', 'link': 'https://stackoverflow.com/users/1709587/mark-amery', 'profile_image': 'https://i.stack.imgur.com/cP8pZ.jpg?s=256&g=1', 'display_name': 'Mark Amery'}","I created a migration with , applied it to my development database with , and then realised it wasn't quite what I wanted. How can I revert the migration so that I can tweak it and try again?",alembic revision --autogenerate alembic upgrade head,-2,3,0,0,
733,49035200,49035538,53839,"Keras early stopping callback error, val_loss metric not available",13,<python><tensorflow><keras>,29,"<p>I am training a Keras (Tensorflow backend, Python, on MacBook) and am getting an error in the early stopping callback in fit_generator function.  The error is as follows:</p>

<pre><code>RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are:
  (self.monitor, ','.join(list(logs.keys()))),
RuntimeWarning: Can save best model only with val_acc available, skipping.

'skipping.' % (self.monitor), RuntimeWarning
[local-dir]/lib/python3.6/site-packages/keras/callbacks.py:497: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are:
  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning
[local-dir]/lib/python3.6/site-packages/keras/callbacks.py:406: RuntimeWarning: Can save best model only with val_acc available, skipping.
  'skipping.' % (self.monitor), RuntimeWarning)
Traceback (most recent call last):
  :
  [my-code]
  :
  File ""[local-dir]/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
return func(*args, **kwargs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/engine/training.py"", line 2213, in fit_generator
callbacks.on_epoch_end(epoch, epoch_logs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/callbacks.py"", line 76, in on_epoch_end
callback.on_epoch_end(epoch, logs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/callbacks.py"", line 310, in on_epoch_end
self.progbar.update(self.seen, self.log_values, force=True)
AttributeError: 'ProgbarLogger' object has no attribute 'log_values'
</code></pre>

<p>My code is as follows (which looks OK):</p>

<pre><code>:
ES = EarlyStopping(monitor=""val_loss"", min_delta=0.001, patience=3, mode=""min"", verbose=1)
:
self.model.fit_generator(
        generator        = train_batch,
        validation_data  = valid_batch,
        validation_steps = validation_steps,
        steps_per_epoch  = steps_per_epoch,
        epochs           = epochs,
        callbacks        = [ES],
        verbose          = 1,
        workers          = 3,
        max_queue_size   = 8)
</code></pre>

<p>The error message appears to relate to the early stopping callback but the callback looks OK.  Also the error states that the val_loss is not appropriate, but I am not sure why... one more unusual thing about this is that the error only occurs when I use smaller data sets.</p>

<p>Any help is appreciated.</p>
",3011570,6881,28-02-2018 17:19,28-02-2018 17:39,0,6921,78,6,52,94,"{'badge_counts': {'bronze': 78, 'silver': 52, 'gold': 6}, 'account_id': 3610515, 'is_employee': False, 'last_modified_date': 1654810800, 'last_access_date': 1711138518, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 60, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 6921, 'creation_date': 1384921554, 'user_type': 'registered', 'user_id': 3011570, 'accept_rate': 94, 'location': 'Ajax, Ontario, Canada', 'website_url': 'https://ca.linkedin.com/in/ericbroda', 'link': 'https://stackoverflow.com/users/3011570/eric-broda', 'profile_image': 'https://i.stack.imgur.com/m4Jeg.jpg?s=256&g=1', 'display_name': 'Eric Broda'}","I am training a Keras (Tensorflow backend, Python, on MacBook) and am getting an error in the early stopping callback in fit_generator function. The error is as follows: My code is as follows (which looks OK): The error message appears to relate to the early stopping callback but the callback looks OK. Also the error states that the val_loss is not appropriate, but I am not sure why... one more unusual thing about this is that the error only occurs when I use smaller data sets. Any help is appreciated.","RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are:
  (self.monitor, ','.join(list(logs.keys()))),
RuntimeWarning: Can save best model only with val_acc available, skipping.

'skipping.' % (self.monitor), RuntimeWarning
[local-dir]/lib/python3.6/site-packages/keras/callbacks.py:497: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are:
  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning
[local-dir]/lib/python3.6/site-packages/keras/callbacks.py:406: RuntimeWarning: Can save best model only with val_acc available, skipping.
  'skipping.' % (self.monitor), RuntimeWarning)
Traceback (most recent call last):
  :
  [my-code]
  :
  File ""[local-dir]/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
return func(*args, **kwargs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/engine/training.py"", line 2213, in fit_generator
callbacks.on_epoch_end(epoch, epoch_logs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/callbacks.py"", line 76, in on_epoch_end
callback.on_epoch_end(epoch, logs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/callbacks.py"", line 310, in on_epoch_end
self.progbar.update(self.seen, self.log_values, force=True)
AttributeError: 'ProgbarLogger' object has no attribute 'log_values'
 :
ES = EarlyStopping(monitor=""val_loss"", min_delta=0.001, patience=3, mode=""min"", verbose=1)
:
self.model.fit_generator(
        generator        = train_batch,
        validation_data  = valid_batch,
        validation_steps = validation_steps,
        steps_per_epoch  = steps_per_epoch,
        epochs           = epochs,
        callbacks        = [ES],
        verbose          = 1,
        workers          = 3,
        max_queue_size   = 8)
",33,46,0,0,
734,50373916,50375022,21431,pytest to insert caplog fixture in test method,1,<python><tdd><pytest>,12,"<p>I have the following test class for pytest:</p>

<pre><code>class TestConnection(AsyncTestCase):
      '''Integration test'''

      @gen_test
      def test_connecting_to_server(self):
          '''Connecting to the TCPserver'''
          client = server = None
          try:
              sock, port = bind_unused_port()
              with NullContext():
                  server = EchoServer()
                  server.add_socket(sock)
              client = IOStream(socket.socket())

              #### HERE I WANT TO HAVE THE caplog FIXTURE

              with ExpectLog(app_log, '.*decode.*'):
                  yield client.connect(('localhost', port))
                  yield client.write(b'hello\n')
                  # yield client.read_until(b'\n')
                  yield gen.moment
                  assert False
          finally:
              if server is not None:
                  server.stop()
              if client is not None:
                  client.close()
</code></pre>

<p>Within this class apparently ExpectLog is not working so after a day of digging around in pytest's documentation I found that there is this caplog fixture that you can have inserted in you methods in order to access the captured logs. It seems to work if I have a test function to which I add the caplog argument but how do I make the caplog fixture available within the methods of a test class like the one above?</p>
",1722634,1063,16-05-2018 14:44,16-05-2018 15:39,0,1073,33,2,12,58,"{'badge_counts': {'bronze': 33, 'silver': 12, 'gold': 2}, 'account_id': 1908200, 'is_employee': False, 'last_modified_date': 1684016100, 'last_access_date': 1679655898, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1073, 'creation_date': 1349431105, 'user_type': 'registered', 'user_id': 1722634, 'accept_rate': 58, 'location': 'Bucharest, Romania', 'website_url': '', 'link': 'https://stackoverflow.com/users/1722634/liviu', 'profile_image': 'https://www.gravatar.com/avatar/b3c76aec26f12e53aeb2c0f9a792ae27?s=256&d=identicon&r=PG', 'display_name': 'Liviu'}",I have the following test class for pytest: Within this class apparently ExpectLog is not working so after a day of digging around in pytest's documentation I found that there is this caplog fixture that you can have inserted in you methods in order to access the captured logs. It seems to work if I have a test function to which I add the caplog argument but how do I make the caplog fixture available within the methods of a test class like the one above?,"class TestConnection(AsyncTestCase):
      '''Integration test'''

      @gen_test
      def test_connecting_to_server(self):
          '''Connecting to the TCPserver'''
          client = server = None
          try:
              sock, port = bind_unused_port()
              with NullContext():
                  server = EchoServer()
                  server.add_socket(sock)
              client = IOStream(socket.socket())

              #### HERE I WANT TO HAVE THE caplog FIXTURE

              with ExpectLog(app_log, '.*decode.*'):
                  yield client.connect(('localhost', port))
                  yield client.write(b'hello\n')
                  # yield client.read_until(b'\n')
                  yield gen.moment
                  assert False
          finally:
              if server is not None:
                  server.stop()
              if client is not None:
                  client.close()
",26,32,0,0,
735,50368145,50372414,21899,Pandas Concat increases number of rows,4,<python><python-3.x><pandas><concatenation>,28,"<p>I'm concatenating two dataframes, so I want to one dataframe is located to another.
But first I did some transformation to initial dataframe:</p>

<pre><code>scaler = MinMaxScaler() 
real_data = pd.DataFrame(scaler.fit_transform(df[real_columns]), columns = real_columns)
</code></pre>

<p>And then concatenate:</p>

<pre><code>categorial_data  = pd.get_dummies(df[categor_columns], prefix_sep= '__')
train = pd.concat([real_data, categorial_data], axis=1, ignore_index=True)
</code></pre>

<p>I dont know why, but number of rows increased:</p>

<pre><code>print(df.shape, real_data.shape, categorial_data.shape, train.shape)
(1700645, 23) (1700645, 16) (1700645, 130) (1703915, 146)
</code></pre>

<p>What happened and how fix the problem?</p>

<p>As you can see number of columns for train equals to sum of columns real_data and categorial_data</p>
",1739325,5609,16-05-2018 10:14,16-05-2018 13:35,0,5609,128,23,79,98,"{'badge_counts': {'bronze': 128, 'silver': 79, 'gold': 23}, 'account_id': 1929634, 'is_employee': False, 'last_modified_date': 1691145300, 'last_access_date': 1711180050, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5609, 'creation_date': 1349985908, 'user_type': 'registered', 'user_id': 1739325, 'accept_rate': 98, 'location': 'Moscow, Russia', 'website_url': 'https://diyago.github.io/', 'link': 'https://stackoverflow.com/users/1739325/rocketq', 'profile_image': 'https://i.stack.imgur.com/HEDFv.jpg?s=256&g=1', 'display_name': 'Rocketq'}","I'm concatenating two dataframes, so I want to one dataframe is located to another. But first I did some transformation to initial dataframe: And then concatenate: I dont know why, but number of rows increased: What happened and how fix the problem? As you can see number of columns for train equals to sum of columns real_data and categorial_data","scaler = MinMaxScaler() 
real_data = pd.DataFrame(scaler.fit_transform(df[real_columns]), columns = real_columns)
 categorial_data  = pd.get_dummies(df[categor_columns], prefix_sep= '__')
train = pd.concat([real_data, categorial_data], axis=1, ignore_index=True)
 print(df.shape, real_data.shape, categorial_data.shape, train.shape)
(1700645, 23) (1700645, 16) (1700645, 130) (1703915, 146)
",3,22,0,0,
736,49268763,49268775,8605,What is the equivalent of Python Pandas value_counts in SQL?,2,<python><sql><pandas>,14,"<p>I learned python and pandas before SQL, so this question is a bit basic.</p>

<p>For example, I have a <code>type</code> column with values like 1, 2, 3.</p>

<p>Then when I do df['type'].value_counts, I can get the statistics of the <code>type</code>, maybe something like</p>

<pre><code>1: 1000 rows
2: 220 rows
3: 100 rows
</code></pre>

<p>I want to know What is the equivalent in SQL? I believe it should be something about group_by and count?</p>
",1794744,20573,14-03-2018 02:11,14-03-2018 02:13,0,20613,208,49,135,77,"{'badge_counts': {'bronze': 208, 'silver': 135, 'gold': 49}, 'account_id': 2004465, 'is_employee': False, 'last_modified_date': 1689473103, 'last_access_date': 1689547496, 'reputation_change_year': 290, 'reputation_change_quarter': 290, 'reputation_change_month': 50, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 20613, 'creation_date': 1351871295, 'user_type': 'registered', 'user_id': 1794744, 'accept_rate': 77, 'location': 'China', 'website_url': '', 'link': 'https://stackoverflow.com/users/1794744/zk-zhao', 'profile_image': 'https://www.gravatar.com/avatar/bed721a1153f97bdd0b31703673abee2?s=256&d=identicon&r=PG', 'display_name': 'ZK Zhao'}","I learned python and pandas before SQL, so this question is a bit basic. For example, I have a column with values like 1, 2, 3. Then when I do df['type'].value_counts, I can get the statistics of the , maybe something like I want to know What is the equivalent in SQL? I believe it should be something about group_by and count?","type type 1: 1000 rows
2: 220 rows
3: 100 rows
",0,12,0,0,
737,50161193,50162138,22102,matplotlib hatched and filled histograms,2,<python><python-2.7><matplotlib>,13,"<p>I would like to make histograms that are both hatched and filled (like these bar plots on the left in this <a href=""https://matplotlib.org/examples/pylab_examples/hatch_demo.html"" rel=""noreferrer"">matplotlib example</a>): </p>

<p><a href=""https://i.stack.imgur.com/rUdTs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rUdTs.png"" alt=""hatched_plot""></a></p>

<p>Here's the code I tried to use:</p>

<pre><code>import matplotlib.pyplot as plt
plt.hist(values, bins, histtype='step', linewidth=2, facecolor='c', hatch='/')
</code></pre>

<p>But no matter whether I specify ""facecolor"" or ""color"", only the lines of the hatching appear in colour and the histogram is still unfilled. How can I make the hatching show up on top of a filled histogram?</p>
",1803782,3404,03-05-2018 17:54,03-05-2018 18:57,0,3404,34,11,31,85,"{'badge_counts': {'bronze': 34, 'silver': 31, 'gold': 11}, 'account_id': 2016625, 'is_employee': False, 'last_modified_date': 1678907105, 'last_access_date': 1693554573, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3404, 'creation_date': 1352220169, 'user_type': 'registered', 'user_id': 1803782, 'accept_rate': 85, 'location': 'Houston, TX, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1803782/ylangylang', 'profile_image': 'https://www.gravatar.com/avatar/42e783449c131def6a4036f6ed745c81?s=256&d=identicon&r=PG', 'display_name': 'ylangylang'}","I would like to make histograms that are both hatched and filled (like these bar plots on the left in this matplotlib example): Here's the code I tried to use: But no matter whether I specify ""facecolor"" or ""color"", only the lines of the hatching appear in colour and the histogram is still unfilled. How can I make the hatching show up on top of a filled histogram?","import matplotlib.pyplot as plt
plt.hist(values, bins, histtype='step', linewidth=2, facecolor='c', hatch='/')
",1,11,1,2,
738,48428100,48428222,15751,Save pandas dataframe with numpy arrays column,2,<python><pandas><numpy>,17,"<p>Let us consider the following pandas dataframe:</p>

<pre><code>df = pd.DataFrame([[1,np.array([6,7])],[4,np.array([8,9])]], columns = {'A','B'})
</code></pre>

<p><a href=""https://i.stack.imgur.com/EzXPm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/EzXPm.png"" alt=""enter image description here""></a></p>

<p>where the <strong>B</strong> column is composed by two numpy arrays.</p>

<p>If we save the dataframe and the load it again, the numpy array is converted into a string.</p>

<pre><code>df.to_csv('test.csv', index = False)
df.read_csv('test.csv')
</code></pre>

<p>Is there any simple way of solve this problem? Here is the output of the loaded dataframe.</p>

<p><a href=""https://i.stack.imgur.com/N4c45.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/N4c45.png"" alt=""enter image description here""></a></p>
",3152047,2685,24-01-2018 17:11,24-01-2018 17:18,0,2705,50,4,28,97,"{'badge_counts': {'bronze': 50, 'silver': 28, 'gold': 4}, 'account_id': 3797494, 'is_employee': False, 'last_modified_date': 1668726264, 'last_access_date': 1710430814, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 2705, 'creation_date': 1388615511, 'user_type': 'registered', 'user_id': 3152047, 'accept_rate': 97, 'location': 'Porto, Portugal', 'website_url': '', 'link': 'https://stackoverflow.com/users/3152047/nunodsousa', 'profile_image': 'https://www.gravatar.com/avatar/7e42ff413d2f4929b98d1f6c42e36bdb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'nunodsousa'}","Let us consider the following pandas dataframe: where the B column is composed by two numpy arrays. If we save the dataframe and the load it again, the numpy array is converted into a string. Is there any simple way of solve this problem? Here is the output of the loaded dataframe.","df = pd.DataFrame([[1,np.array([6,7])],[4,np.array([8,9])]], columns = {'A','B'})
 df.to_csv('test.csv', index = False)
df.read_csv('test.csv')
",1,18,2,2,
739,49595809,49616676,6143,docker-compose integration not working in pycharm,2,<python><docker><pycharm><docker-compose>,13,"<p>I've set up docker-compose as a remote interpreter in pycharm using the instructions found <a href=""https://www.jetbrains.com/help/pycharm/using-docker-compose-as-a-remote-interpreter-1.html"" rel=""noreferrer"">here</a>.</p>

<p>The docker-compose build works fine, but when trying to run a test using this interpreter I get the following error:</p>

<pre><code>Testing started at 12:13 AM ...
docker-compose://[/home/melchoir55/gitrepos/python_rest/docker-compose.yml]:python_rest/python -u /opt/.pycharm_helpers/pycharm/_jb_unittest_runner.py --path /opt/project/tests
Traceback (most recent call last):
  File ""bin/docker-compose"", line 6, in &lt;module&gt;
  File ""compose/cli/main.py"", line 71, in main
  File ""compose/cli/main.py"", line 124, in perform_command
  File ""compose/cli/command.py"", line 38, in project_from_options
  File ""compose/cli/docker_client.py"", line 84, in tls_config_from_options
  File ""site-packages/docker/tls.py"", line 81, in __init__
docker.errors.TLSParameterError: Path to a certificate and key files must be provided through the client_config param. TLS configurations should map the Docker CLI client configurations. See https://docs.docker.com/engine/articles/https/ for API details.
[8964] Failed to execute script docker-compose

Process finished with exit code 255
</code></pre>

<p>I haven't done much to the system other than installing pycharm, docker, docker-compose, and adding my user to the docker group for permissions. Does anyone know what docker-compose is complaining about here via pycharm?</p>

<p>I did note that there seem to be some env vars which I don't have set related to these certificates, but it isn't clear to me what they should be set to, if I were to set them.</p>

<p><strong>Update:</strong></p>

<p>For reference I am running pycharm in Ubuntu within a virtualbox machine.</p>
",1812993,7032,01-04-2018 07:28,02-04-2018 18:28,1,7032,107,8,65,80,"{'badge_counts': {'bronze': 107, 'silver': 65, 'gold': 8}, 'account_id': 2028664, 'is_employee': False, 'last_modified_date': 1675669927, 'last_access_date': 1709947917, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7032, 'creation_date': 1352483535, 'user_type': 'registered', 'user_id': 1812993, 'accept_rate': 80, 'location': 'San Francisco, CA', 'website_url': '', 'link': 'https://stackoverflow.com/users/1812993/melchoir55', 'profile_image': 'https://www.gravatar.com/avatar/9cf8b9493978bce37aa55602c7d9f660?s=256&d=identicon&r=PG', 'display_name': 'melchoir55'}","I've set up docker-compose as a remote interpreter in pycharm using the instructions found here. The docker-compose build works fine, but when trying to run a test using this interpreter I get the following error: I haven't done much to the system other than installing pycharm, docker, docker-compose, and adding my user to the docker group for permissions. Does anyone know what docker-compose is complaining about here via pycharm? I did note that there seem to be some env vars which I don't have set related to these certificates, but it isn't clear to me what they should be set to, if I were to set them. Update: For reference I am running pycharm in Ubuntu within a virtualbox machine.","Testing started at 12:13 AM ...
docker-compose://[/home/melchoir55/gitrepos/python_rest/docker-compose.yml]:python_rest/python -u /opt/.pycharm_helpers/pycharm/_jb_unittest_runner.py --path /opt/project/tests
Traceback (most recent call last):
  File ""bin/docker-compose"", line 6, in &lt;module&gt;
  File ""compose/cli/main.py"", line 71, in main
  File ""compose/cli/main.py"", line 124, in perform_command
  File ""compose/cli/command.py"", line 38, in project_from_options
  File ""compose/cli/docker_client.py"", line 84, in tls_config_from_options
  File ""site-packages/docker/tls.py"", line 81, in __init__
docker.errors.TLSParameterError: Path to a certificate and key files must be provided through the client_config param. TLS configurations should map the Docker CLI client configurations. See https://docs.docker.com/engine/articles/https/ for API details.
[8964] Failed to execute script docker-compose

Process finished with exit code 255
",12,26,0,1,
740,48291546,48413576,15473,virtualenv activation doesn't work,4,<python><pip><virtualenv>,11,"<p>I've created a virtual environment with:</p>

<pre><code>$ virtualenv my_ven_test
</code></pre>

<p>then let's activate the environment with:</p>

<pre><code>$ source my_ven_test/bin/activate
</code></pre>

<p>now let's install a package:</p>

<pre><code>(my_ven_test) $ pip install mysql-connector==2.1.3
</code></pre>

<p>This last line does not take effect. In fact if I check:</p>

<pre><code>(my_ven_test) $ pip freeze
</code></pre>

<p>I see no package installed (as well as the <code>my_ven_test/lib/python/site-package</code> directory doesn't contain the mysql-connector package)</p>

<p>Could you guide me in solving this issue?</p>

<p>Some notes:</p>

<ul>
<li>python version: 2.7</li>
<li>virtualenv version: 15.1.0</li>
</ul>
",3211950,1029,16-01-2018 23:39,24-01-2018 01:40,8,1039,34,3,16,83,"{'badge_counts': {'bronze': 34, 'silver': 16, 'gold': 3}, 'account_id': 3878748, 'is_employee': False, 'last_modified_date': 1692899400, 'last_access_date': 1710322722, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1039, 'creation_date': 1390129629, 'user_type': 'registered', 'user_id': 3211950, 'accept_rate': 83, 'location': 'Italy', 'website_url': '', 'link': 'https://stackoverflow.com/users/3211950/enneppi', 'profile_image': 'https://i.stack.imgur.com/Lw6mr.jpg?s=256&g=1', 'display_name': 'enneppi'}",I've created a virtual environment with: then let's activate the environment with: now let's install a package: This last line does not take effect. In fact if I check: I see no package installed (as well as the directory doesn't contain the mysql-connector package) Could you guide me in solving this issue? Some notes: python version: 2.7 virtualenv version: 15.1.0,"$ virtualenv my_ven_test
 $ source my_ven_test/bin/activate
 (my_ven_test) $ pip install mysql-connector==2.1.3
 (my_ven_test) $ pip freeze
 my_ven_test/lib/python/site-package",-1,30,0,0,
741,48277599,48278291,25293,Pip inside dockerfile under proxy,4,<python><docker><proxy><pip>,18,"<p>I am trying to build a Docker image for elasticsearch-curator, </p>

<p>Here is the dockerfile:</p>

<pre><code>FROM alpine:3.7

RUN adduser -S curator

RUN apk add --update \
    python \
    python-dev \
    py-pip \
    build-base \
  &amp;&amp; pip install virtualenv \
  &amp;&amp; pip install elasticsearch-curator \
  &amp;&amp; rm -rf /var/cache/apk/*

USER curator

ENTRYPOINT [ ""/usr/bin/curator""]
</code></pre>

<p>Thing is I am under a proxy, so I must build my image with:</p>

<pre><code>docker build  --no-cache --build-arg HTTP_PROXY=http://xx.xx.xx.xx:xx -t elasticsearch-curator:5.4 .
</code></pre>

<p>But when it wants to get virtualenv, I get:</p>

<pre><code>Collecting virtualenv
  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'ConnectTimeoutError(&lt;pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8259ed350&gt;, 'Connection to pypi.python.org timed out. (connect timeout=15)')': /simple/virtualenv/
  Retrying (Retry(total=3, connect=None, read=None, redirect=None)) after connection broken by 'ConnectTimeoutError(&lt;pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8259ed210&gt;, 'Connection to pypi.python.org timed out. (connect timeout=15)')': /simple/virtualenv/
</code></pre>

<p>I found people solving issue inserting </p>

<pre><code>ENV http_proxy http://proxy-chain.xxx.com:911/
ENV https_proxy http://proxy-chain.xxx.com:912/
</code></pre>

<p>in the Dockerfile, but it is not possible for me, because my proxy is only valid on my building, so if another person from another place want to build the image, he will need to remove http_proxy env var from Dockerfile.</p>

<p>Is there any other way to achieve it? It seems like a very common use case...</p>
",1956558,19003,16-01-2018 09:10,16-01-2018 09:45,0,19033,339,46,176,63,"{'badge_counts': {'bronze': 339, 'silver': 176, 'gold': 46}, 'account_id': 2215969, 'is_employee': False, 'last_modified_date': 1709344800, 'last_access_date': 1710925552, 'reputation_change_year': 220, 'reputation_change_quarter': 220, 'reputation_change_month': 50, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 19033, 'creation_date': 1357602416, 'user_type': 'registered', 'user_id': 1956558, 'accept_rate': 63, 'location': 'Perpignan, France', 'website_url': 'https://julien.cappiello.fr', 'link': 'https://stackoverflow.com/users/1956558/juliatzin', 'profile_image': 'https://i.stack.imgur.com/8D66D.jpg?s=256&g=1', 'display_name': 'Juliatzin'}","I am trying to build a Docker image for elasticsearch-curator, Here is the dockerfile: Thing is I am under a proxy, so I must build my image with: But when it wants to get virtualenv, I get: I found people solving issue inserting in the Dockerfile, but it is not possible for me, because my proxy is only valid on my building, so if another person from another place want to build the image, he will need to remove http_proxy env var from Dockerfile. Is there any other way to achieve it? It seems like a very common use case...","FROM alpine:3.7

RUN adduser -S curator

RUN apk add --update \
    python \
    python-dev \
    py-pip \
    build-base \
  &amp;&amp; pip install virtualenv \
  &amp;&amp; pip install elasticsearch-curator \
  &amp;&amp; rm -rf /var/cache/apk/*

USER curator

ENTRYPOINT [ ""/usr/bin/curator""]
 docker build  --no-cache --build-arg HTTP_PROXY=http://xx.xx.xx.xx:xx -t elasticsearch-curator:5.4 .
 Collecting virtualenv
  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'ConnectTimeoutError(&lt;pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8259ed350&gt;, 'Connection to pypi.python.org timed out. (connect timeout=15)')': /simple/virtualenv/
  Retrying (Retry(total=3, connect=None, read=None, redirect=None)) after connection broken by 'ConnectTimeoutError(&lt;pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fb8259ed210&gt;, 'Connection to pypi.python.org timed out. (connect timeout=15)')': /simple/virtualenv/
 ENV http_proxy http://proxy-chain.xxx.com:911/
ENV https_proxy http://proxy-chain.xxx.com:912/
",18,43,0,0,
742,49263247,49263265,20296,How can I make a pandas dataframe out of multiple numpy arrays,3,<python><pandas><numpy><dataframe>,12,"<p>I have 2 arrays as below that I want to be converted to dataframe columns: </p>

<pre><code>arr1 = np.array([2, 4, 6, 8])
arr2 = np.array([3, 6, 9, 12])
df_from_arr = pd.DataFrame(data=[arr1, arr2])
print(df_from_arr)
</code></pre>

<p>Actual Output: </p>

<pre><code>   0  1  2   3
0  2  4  6   8
1  3  6  9  12
</code></pre>

<p>Expected output: </p>

<pre><code>   0  1 
0  2  4 
1  4  6
2  6  9
3  8  12 
</code></pre>

<p>How can I get the expected output?</p>
",2005490,4983,13-03-2018 18:20,13-03-2018 18:21,0,4993,103,10,63,59,"{'badge_counts': {'bronze': 103, 'silver': 63, 'gold': 10}, 'account_id': 212445, 'is_employee': False, 'last_modified_date': 1697248200, 'last_access_date': 1710881798, 'reputation_change_year': 72, 'reputation_change_quarter': 72, 'reputation_change_month': 34, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4993, 'creation_date': 1358976877, 'user_type': 'registered', 'user_id': 2005490, 'accept_rate': 59, 'website_url': '', 'link': 'https://stackoverflow.com/users/2005490/atihska', 'profile_image': 'https://www.gravatar.com/avatar/3419bb2799672ef53b1535fdd48b30e7?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Atihska'}",I have 2 arrays as below that I want to be converted to dataframe columns: Actual Output: Expected output: How can I get the expected output?,"arr1 = np.array([2, 4, 6, 8])
arr2 = np.array([3, 6, 9, 12])
df_from_arr = pd.DataFrame(data=[arr1, arr2])
print(df_from_arr)
    0  1  2   3
0  2  4  6   8
1  3  6  9  12
    0  1 
0  2  4 
1  4  6
2  6  9
3  8  12 
",9,25,0,0,
743,49647160,49647897,26918,Visualizing your code's architecture,3,<python><architecture><pycharm><production-environment><productivity-power-tools>,40,"<p>Each weekend I'm coding on a personal project which over time has reached a certain complexity by now, where I have sequences of different functions that take my input, that I save in some class objects, chop it up, process it and then finally output it.<br>
Since I make large breaks between coding sessions I usually forget the precise structure of my code. Therefore, each time I have a bug, I have to re-familiarize myself with how the input data flows through my code, how inside some module that provides functionality things are organized etc.</p>

<p>I'm not sure if this is due to bad code structure of my software, or simply inherent complexity.</p>

<p>Is there a tool that, given the source code, visually shows me how the ""architecture"" of my code, i.e. how the classes methods and functions all work together?</p>

<p>Ideally this would also help me understand code other people wrote faster, to get quickly an overview how the individual code pieces interact.</p>

<p>(I'm coding in Python with Pycharm, if that helps you.)</p>
",2051959,1319,04-04-2018 09:16,04-04-2018 09:49,0,1329,20,1,10,30,"{'badge_counts': {'bronze': 20, 'silver': 10, 'gold': 1}, 'account_id': 511673, 'is_employee': False, 'last_modified_date': 1577524516, 'last_access_date': 1642595943, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1329, 'creation_date': 1360261616, 'user_type': 'registered', 'user_id': 2051959, 'accept_rate': 30, 'link': 'https://stackoverflow.com/users/2051959/l7ll7', 'profile_image': 'https://i.stack.imgur.com/002uq.jpg?s=256&g=1', 'display_name': 'l7ll7'}","Each weekend I'm coding on a personal project which over time has reached a certain complexity by now, where I have sequences of different functions that take my input, that I save in some class objects, chop it up, process it and then finally output it. Since I make large breaks between coding sessions I usually forget the precise structure of my code. Therefore, each time I have a bug, I have to re-familiarize myself with how the input data flows through my code, how inside some module that provides functionality things are organized etc. I'm not sure if this is due to bad code structure of my software, or simply inherent complexity. Is there a tool that, given the source code, visually shows me how the ""architecture"" of my code, i.e. how the classes methods and functions all work together? Ideally this would also help me understand code other people wrote faster, to get quickly an overview how the individual code pieces interact. (I'm coding in Python with Pycharm, if that helps you.)",,0,10,0,0,
744,49560974,49561056,6394,Inspect params and return types,3,<python><python-3.x>,14,"<p>Is it possible using Python 3 syntax for declaring input parameters and return value types determine those types? Similarly to determining the number of parameters of a function?</p>

<pre><code>def foo(name: str) -&gt; int:
    ....
</code></pre>

<p>I would like to get <code>str</code> and <code>int</code> respectively.</p>
",2146414,900,29-03-2018 16:29,29-03-2018 16:33,0,920,31,1,13,45,"{'badge_counts': {'bronze': 31, 'silver': 13, 'gold': 1}, 'account_id': 2462579, 'is_employee': False, 'last_modified_date': 1700718300, 'last_access_date': 1711138860, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 920, 'creation_date': 1362696972, 'user_type': 'registered', 'user_id': 2146414, 'accept_rate': 45, 'link': 'https://stackoverflow.com/users/2146414/user2146414', 'profile_image': 'https://www.gravatar.com/avatar/6aa31bda323666c4a65f4f03a07420d9?s=256&d=identicon&r=PG', 'display_name': 'user2146414'}",Is it possible using Python 3 syntax for declaring input parameters and return value types determine those types? Similarly to determining the number of parameters of a function? I would like to get and respectively.,"def foo(name: str) -&gt; int:
    ....
 str int",-1,7,0,0,
745,48709104,48709142,58246,How do I specify multiple types for a parameter using type-hints?,1,<python><python-3.x><arguments><type-hinting><python-typing>,83,"<p>I have a Python function which accepts XML data as an <code>str</code>.</p>

<p>For convenience, the function also checks for <code>xml.etree.ElementTree.Element</code> and will automatically convert to <code>str</code> if necessary.</p>

<pre><code>import xml.etree.ElementTree as ET

def post_xml(data: str):
    if type(data) is ET.Element:
        data = ET.tostring(data).decode()
    # ...
</code></pre>

<p>Is it possible to specify with type-hints that a parameter can be given as one of two types?</p>

<pre><code>def post_xml(data: str or ET.Element):
    # ...
</code></pre>
",3357935,25006,09-02-2018 15:29,09-02-2018 15:32,0,25082,230,29,128,82,"{'badge_counts': {'bronze': 230, 'silver': 128, 'gold': 29}, 'account_id': 4030991, 'is_employee': False, 'last_modified_date': 1703299500, 'last_access_date': 1711164785, 'reputation_change_year': 482, 'reputation_change_quarter': 482, 'reputation_change_month': 114, 'reputation_change_week': 26, 'reputation_change_day': 0, 'reputation': 25082, 'creation_date': 1393451089, 'user_type': 'registered', 'user_id': 3357935, 'accept_rate': 82, 'location': 'NY, USA', 'link': 'https://stackoverflow.com/users/3357935/stevoisiak', 'profile_image': 'https://i.stack.imgur.com/jaomO.png?s=256&g=1', 'display_name': 'Stevoisiak'}","I have a Python function which accepts XML data as an . For convenience, the function also checks for and will automatically convert to if necessary. Is it possible to specify with type-hints that a parameter can be given as one of two types?","str xml.etree.ElementTree.Element str import xml.etree.ElementTree as ET

def post_xml(data: str):
    if type(data) is ET.Element:
        data = ET.tostring(data).decode()
    # ...
 def post_xml(data: str or ET.Element):
    # ...
",3,17,0,0,
746,49262379,49262528,59727,Does 'finally' always execute in Python?,6,<python><exception><try-catch-finally><finally>,188,"<p>For any possible try-finally block in Python, is it guaranteed that the <code>finally</code> block will always be executed?</p>

<p>For example, let’s say I return while in an <code>except</code> block:</p>

<pre><code>try:
    1/0
except ZeroDivisionError:
    return
finally:
    print(""Does this code run?"")
</code></pre>

<p>Or maybe I re-raise an <code>Exception</code>:</p>

<pre><code>try:
    1/0
except ZeroDivisionError:
    raise
finally:
    print(""What about this code?"")
</code></pre>

<p>Testing shows that <code>finally</code> does get executed for the above examples, but I imagine there are other scenarios I haven't thought of.</p>

<p><strong>Are there any scenarios in which a <code>finally</code> block can fail to execute in Python?</strong></p>
",3357935,25006,13-03-2018 17:30,13-03-2018 17:38,0,25082,230,29,128,82,"{'badge_counts': {'bronze': 230, 'silver': 128, 'gold': 29}, 'account_id': 4030991, 'is_employee': False, 'last_modified_date': 1703299500, 'last_access_date': 1711164785, 'reputation_change_year': 482, 'reputation_change_quarter': 482, 'reputation_change_month': 114, 'reputation_change_week': 26, 'reputation_change_day': 0, 'reputation': 25082, 'creation_date': 1393451089, 'user_type': 'registered', 'user_id': 3357935, 'accept_rate': 82, 'location': 'NY, USA', 'link': 'https://stackoverflow.com/users/3357935/stevoisiak', 'profile_image': 'https://i.stack.imgur.com/jaomO.png?s=256&g=1', 'display_name': 'Stevoisiak'}","For any possible try-finally block in Python, is it guaranteed that the block will always be executed? For example, let’s say I return while in an block: Or maybe I re-raise an : Testing shows that does get executed for the above examples, but I imagine there are other scenarios I haven't thought of. Are there any scenarios in which a block can fail to execute in Python?","finally except try:
    1/0
except ZeroDivisionError:
    return
finally:
    print(""Does this code run?"")
 Exception try:
    1/0
except ZeroDivisionError:
    raise
finally:
    print(""What about this code?"")
 finally finally",5,25,0,0,
747,50382040,50382096,10425,How can I use enumerate to count backwards?,8,<python><python-3.x>,15,"<pre><code>letters = ['a', 'b', 'c']
</code></pre>

<p>Assume this is my list. Where <code>for i, letter in enumerate(letters)</code> would be:</p>

<pre><code>0, a
1, b
2, c
</code></pre>

<p>How can I instead make it enumerate backwards, as:</p>

<pre><code>2, a
1, b
0, c
</code></pre>
",2148127,583,17-05-2018 01:23,17-05-2018 01:33,0,583,18,2,6,14,"{'badge_counts': {'bronze': 18, 'silver': 6, 'gold': 2}, 'account_id': 503310, 'is_employee': False, 'last_modified_date': 1600702200, 'last_access_date': 1703048477, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 583, 'creation_date': 1362740667, 'user_type': 'registered', 'user_id': 2148127, 'accept_rate': 14, 'link': 'https://stackoverflow.com/users/2148127/craig', 'profile_image': 'https://www.gravatar.com/avatar/3e8f40ba42a5cc58364629782c9ad99e?s=256&d=identicon&r=PG', 'display_name': 'Craig'}","Assume this is my list. Where would be: How can I instead make it enumerate backwards, as:","letters = ['a', 'b', 'c']
 for i, letter in enumerate(letters) 0, a
1, b
2, c
 2, a
1, b
0, c
",3,16,0,0,
748,49237522,49237841,23839,How to annotate end of lines using python and matplotlib?,3,<python><matplotlib>,29,"<p>With a dataframe and basic plot such as this:</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(123456)
rows = 75
df = pd.DataFrame(np.random.randint(-4,5,size=(rows, 3)), columns=['A', 'B', 'C'])
datelist = pd.date_range(pd.datetime(2017, 1, 1).strftime('%Y-%m-%d'), periods=rows).tolist()
df['dates'] = datelist 
df = df.set_index(['dates'])
df.index = pd.to_datetime(df.index)
df = df.cumsum()

df.plot()
</code></pre>

<p><a href=""https://i.stack.imgur.com/TGjsi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TGjsi.png"" alt=""enter image description here""></a></p>

<p>What is the best way of annotating the last points on the lines so that you get the result below?</p>

<p><a href=""https://i.stack.imgur.com/Tkx1z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Tkx1z.png"" alt=""enter image description here""></a></p>
",3437787,58000,12-03-2018 14:19,12-03-2018 14:35,0,58150,321,39,198,100,"{'badge_counts': {'bronze': 321, 'silver': 198, 'gold': 39}, 'collectives': [{'collective': {'tags': ['lubridate', 'data.table', 'tibble', 'rlang', 'rvest', 'r-caret', 'dplyr', 'dtplyr', 'shinydashboard', 'zoo', 'readr', 'tidyr', 'r-raster', 'purrr', 'quantmod', 'stringr', 'rstudio', 'r-package', 'r', 'forcats', 'plyr', 'knitr', 'tidyverse', 'shinyapps', 'shiny-server', 'shiny', 'ggplot2'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where data scientists and AI researchers gather to find, share, and learn about R and other subtags like knitr and dplyr.', 'link': '/collectives/r-language', 'name': 'R Language', 'slug': 'r-language'}, 'role': 'member'}, {'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 4196502, 'is_employee': False, 'last_modified_date': 1703311200, 'last_access_date': 1711133929, 'reputation_change_year': 1406, 'reputation_change_quarter': 1406, 'reputation_change_month': 380, 'reputation_change_week': 60, 'reputation_change_day': 0, 'reputation': 58150, 'creation_date': 1395235213, 'user_type': 'registered', 'user_id': 3437787, 'accept_rate': 100, 'location': 'Norway', 'website_url': '', 'link': 'https://stackoverflow.com/users/3437787/vestland', 'profile_image': 'https://i.stack.imgur.com/W8tmU.jpg?s=256&g=1', 'display_name': 'vestland'}",With a dataframe and basic plot such as this: What is the best way of annotating the last points on the lines so that you get the result below?,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(123456)
rows = 75
df = pd.DataFrame(np.random.randint(-4,5,size=(rows, 3)), columns=['A', 'B', 'C'])
datelist = pd.date_range(pd.datetime(2017, 1, 1).strftime('%Y-%m-%d'), periods=rows).tolist()
df['dates'] = datelist 
df = df.set_index(['dates'])
df.index = pd.to_datetime(df.index)
df = df.cumsum()

df.plot()
",13,23,2,2,
749,49138457,49138893,3403,What is the opposite of cv2.VideoWriter_fourcc?,1,<python><opencv>,11,"<p>The function <code>cv2.VideoWriter_fourcc</code> converts from a string (four chars) to an int.
For example, <code>cv2.VideoWriter_fourcc(*'MJPG')</code> gives an <code>int</code> for codec MJPG (whatever that is).</p>
<p>Does OpenCV provide the opposite function? I'd like to display the value as a string. I'd like to get a string from a fourcc <code>int</code> value.</p>
<p>I could write the conversion myself, but I'd use something from OpenCV if it exists.</p>
",2166177,1099,06-03-2018 19:22,06-03-2018 19:51,0,1099,33,2,15,67,"{'badge_counts': {'bronze': 33, 'silver': 15, 'gold': 2}, 'account_id': 2488135, 'is_employee': False, 'last_modified_date': 1706319600, 'last_access_date': 1711063201, 'reputation_change_year': 48, 'reputation_change_quarter': 48, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1099, 'creation_date': 1363189189, 'user_type': 'registered', 'user_id': 2166177, 'accept_rate': 67, 'location': 'Minnesota', 'website_url': '', 'link': 'https://stackoverflow.com/users/2166177/steve', 'profile_image': 'https://i.stack.imgur.com/z51hk.jpg?s=256&g=1', 'display_name': 'steve'}","The function converts from a string (four chars) to an int. For example, gives an for codec MJPG (whatever that is). Does OpenCV provide the opposite function? I'd like to display the value as a string. I'd like to get a string from a fourcc value. I could write the conversion myself, but I'd use something from OpenCV if it exists.",cv2.VideoWriter_fourcc cv2.VideoWriter_fourcc(*'MJPG') int int,-4,4,0,0,
750,49398594,49398623,5899,Sort a list of lists by length and value in Python,2,<python><python-2.7><list>,18,"<p>How can I sort a Python list (with sublists)? For example, I have the following list:</p>

<pre><code>list1 = [[0, 4, 1, 5], [3, 1, 5], [4, 0, 1, 5]]
</code></pre>

<p>After sorting I am expecting:</p>

<pre><code>list1 = [[3, 1, 5], [0, 4, 1, 5], [4, 0, 1, 5]]
</code></pre>

<p>Another example. I have the following list:</p>

<pre><code>list2 = [[4, 5, 2], [2, 5, 4], [2, 4, 5]]
</code></pre>

<p>After sorting I am expecting:</p>

<pre><code>list2 = [[2, 4, 5], [2, 5, 4], [4, 5, 2]]
</code></pre>

<p>At first I want to sort by length, and then by itemwise in each sublist. <strong>I do not want to sort any sublist.</strong></p>

<p>I have tried the following code, which helped me to sort by length only:</p>

<pre><code>list1.sort(key=len)
</code></pre>
",2203826,453,21-03-2018 05:09,21-03-2018 05:12,0,453,14,1,5,67,"{'badge_counts': {'bronze': 14, 'silver': 5, 'gold': 1}, 'account_id': 2537003, 'is_employee': False, 'last_modified_date': 1636965300, 'last_access_date': 1709036251, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 453, 'creation_date': 1364099113, 'user_type': 'registered', 'user_id': 2203826, 'accept_rate': 67, 'website_url': '', 'link': 'https://stackoverflow.com/users/2203826/utij2004', 'profile_image': 'https://www.gravatar.com/avatar/2186ee491904be6192cb87767806a72f?s=256&d=identicon&r=PG', 'display_name': 'utij2004'}","How can I sort a Python list (with sublists)? For example, I have the following list: After sorting I am expecting: Another example. I have the following list: After sorting I am expecting: At first I want to sort by length, and then by itemwise in each sublist. I do not want to sort any sublist. I have tried the following code, which helped me to sort by length only:","list1 = [[0, 4, 1, 5], [3, 1, 5], [4, 0, 1, 5]]
 list1 = [[3, 1, 5], [0, 4, 1, 5], [4, 0, 1, 5]]
 list2 = [[4, 5, 2], [2, 5, 4], [2, 4, 5]]
 list2 = [[2, 4, 5], [2, 5, 4], [4, 5, 2]]
 list1.sort(key=len)
",0,26,0,0,
751,49497391,49497469,26256,GoogleTrans API Error - Expecting value: line 1 column 1 (char 0),11,<python><google-translate><googletrans>,25,"<p>I am having this error when translating thousands of text data in an iteration:</p>

<pre><code>Expecting value: line 1 column 1 (char 0)
</code></pre>

<p>My code for translating big amounts of text:</p>

<pre><code>translatedList = []
for index, row in df.iterrows():
    newrow = copy.deepcopy(row)
    try:
        # translate the 'text' column
        translated = translator.translate(row['text'], dest='en')
        newrow['translated'] = translated.text
    except Exception as e:
        print(str(e))
        continue
    translatedList.append(newrow)
</code></pre>

<p>I receive this error after translating about 2-3k rows. </p>
",2231702,1513,26-03-2018 17:37,26-03-2018 17:42,0,1523,28,2,18,71,"{'badge_counts': {'bronze': 28, 'silver': 18, 'gold': 2}, 'account_id': 2573493, 'is_employee': False, 'last_modified_date': 1686124500, 'last_access_date': 1708697724, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1523, 'creation_date': 1364811671, 'user_type': 'registered', 'user_id': 2231702, 'accept_rate': 71, 'location': 'United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/2231702/kerem', 'profile_image': 'https://i.stack.imgur.com/H1ncb.jpg?s=256&g=1', 'display_name': 'Kerem'}",I am having this error when translating thousands of text data in an iteration: My code for translating big amounts of text: I receive this error after translating about 2-3k rows.,"Expecting value: line 1 column 1 (char 0)
 translatedList = []
for index, row in df.iterrows():
    newrow = copy.deepcopy(row)
    try:
        # translate the 'text' column
        translated = translator.translate(row['text'], dest='en')
        newrow['translated'] = translated.text
    except Exception as e:
        print(str(e))
        continue
    translatedList.append(newrow)
",10,21,0,0,
752,50418449,50418970,53021,How do I use google.oauth2 python library?,3,<python><google-oauth><google-api-python-client>,18,"<p>I'm trying to just make a simple rest call to a secure predict endpoint for a google machine learning project but it can't find the google.oauth2 module. This is my code:</p>

<pre><code>import urllib2
from google.oauth2 import service_account

# Constants
ENDPOINT_URL = 'ml.googleapis.com/v1/projects/{project}/models/{model}:predict?access_token='
SCOPES = ['https://www.googleapis.com/auth/sqlservice.admin']
SERVICE_ACCOUNT_FILE = 'service.json'

credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
access_token=credentials.get_access_token()

opener = urllib2.build_opener(urllib2.HTTPHandler)
request = urllib2.Request(ENDPOINT_URL)
request.get_method = lambda: 'POST'
result = opener.open(request).read()
print(str(result))
</code></pre>

<p>When I run this I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""wakeUpMLServer.py"", line 30, in &lt;module&gt;
    from google.oauth2 import service_account
ImportError: No module named oauth2
</code></pre>

<p>I installed the google-api-python-client library using pip (from instructions here: <a href=""https://developers.google.com/api-client-library/python/apis/oauth2/v1"" rel=""noreferrer"">https://developers.google.com/api-client-library/python/apis/oauth2/v1</a>). The install said it was successful.
Python version: 2.7.6
Pip version: 1.5.4</p>

<p>In case it is somehow conflicting you should know I also do protobuf file processing on the same server, so I have the protobufs library installed as well. When I do pip list it shows both libraries:</p>

<pre><code>google-api-python-client (1.6.7)
protobuf (3.1.0.post1)
</code></pre>

<p>Is there some special way I should be installing the module? How do I verify that the module was installed? Is there any simpler way to just make a simple rest call to a secured ml endpoint that just uses the Rest API and no python?</p>
",2284202,2004,18-05-2018 19:56,18-05-2018 20:40,0,2014,47,3,25,74,"{'badge_counts': {'bronze': 47, 'silver': 25, 'gold': 3}, 'account_id': 2640420, 'is_employee': False, 'last_modified_date': 1636160400, 'last_access_date': 1710795117, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2014, 'creation_date': 1366060111, 'user_type': 'registered', 'user_id': 2284202, 'accept_rate': 74, 'website_url': '', 'link': 'https://stackoverflow.com/users/2284202/alex-egli', 'profile_image': 'https://i.stack.imgur.com/T1TRx.jpg?s=256&g=1', 'display_name': 'Alex Egli'}","I'm trying to just make a simple rest call to a secure predict endpoint for a google machine learning project but it can't find the google.oauth2 module. This is my code: When I run this I get this error: I installed the google-api-python-client library using pip (from instructions here: https://developers.google.com/api-client-library/python/apis/oauth2/v1). The install said it was successful. Python version: 2.7.6 Pip version: 1.5.4 In case it is somehow conflicting you should know I also do protobuf file processing on the same server, so I have the protobufs library installed as well. When I do pip list it shows both libraries: Is there some special way I should be installing the module? How do I verify that the module was installed? Is there any simpler way to just make a simple rest call to a secured ml endpoint that just uses the Rest API and no python?","import urllib2
from google.oauth2 import service_account

# Constants
ENDPOINT_URL = 'ml.googleapis.com/v1/projects/{project}/models/{model}:predict?access_token='
SCOPES = ['https://www.googleapis.com/auth/sqlservice.admin']
SERVICE_ACCOUNT_FILE = 'service.json'

credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
access_token=credentials.get_access_token()

opener = urllib2.build_opener(urllib2.HTTPHandler)
request = urllib2.Request(ENDPOINT_URL)
request.get_method = lambda: 'POST'
result = opener.open(request).read()
print(str(result))
 Traceback (most recent call last):
  File ""wakeUpMLServer.py"", line 30, in &lt;module&gt;
    from google.oauth2 import service_account
ImportError: No module named oauth2
 google-api-python-client (1.6.7)
protobuf (3.1.0.post1)
",19,39,0,1,
753,50164001,50166485,23168,multiple column/row facet wrap in altair,6,<python><plot><facet><facet-wrap><altair>,26,"<p>In <code>ggplot2</code>, it's easy to create a faceted plot with facets that span both rows and columns. Is there a ""slick"" way to do this in <code>altair</code>? <a href=""https://altair-viz.github.io/user_guide/API.html?highlight=facet#altair.Chart.facet"" rel=""noreferrer""><code>facet</code> documentation</a></p>

<p>It's possible to have facets plot in a single column,</p>

<pre><code>import altair as alt
from vega_datasets import data
iris = data.iris

chart = alt.Chart(iris).mark_point().encode(
    x='petalLength:Q',
    y='petalWidth:Q',
    color='species:N'
).properties(
    width=180,
    height=180
).facet(
    row='species:N'
)
</code></pre>

<p>and in a single row,</p>

<pre><code>chart = alt.Chart(iris).mark_point().encode(
    x='petalLength:Q',
    y='petalWidth:Q',
    color='species:N'
).properties(
    width=180,
    height=180
).facet(
    column='species:N'
)
</code></pre>

<p>but often, I just want to plot them in a grid using more than one column/row, i.e. those that line up in a single column/row don't mean anything in particular.</p>

<p>For example, see <code>facet_wrap</code> from <code>ggplot2</code>: <a href=""http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/#facetwrap"" rel=""noreferrer"">http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/#facetwrap</a></p>
",2320823,3193,03-05-2018 21:18,04-05-2018 02:57,1,3193,65,6,37,69,"{'badge_counts': {'bronze': 65, 'silver': 37, 'gold': 6}, 'account_id': 2686526, 'is_employee': False, 'last_modified_date': 1685406975, 'last_access_date': 1703181682, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3193, 'creation_date': 1366909452, 'user_type': 'registered', 'user_id': 2320823, 'accept_rate': 69, 'location': 'Los Angeles, CA', 'website_url': 'https://shyam.saladi.org', 'link': 'https://stackoverflow.com/users/2320823/saladi', 'profile_image': 'https://www.gravatar.com/avatar/49aa7c65c89ac4d7eab47391e3fb4dd4?s=256&d=identicon&r=PG', 'display_name': 'saladi'}","In , it's easy to create a faceted plot with facets that span both rows and columns. Is there a ""slick"" way to do this in ? documentation It's possible to have facets plot in a single column, and in a single row, but often, I just want to plot them in a grid using more than one column/row, i.e. those that line up in a single column/row don't mean anything in particular. For example, see from : http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/#facetwrap","ggplot2 altair facet import altair as alt
from vega_datasets import data
iris = data.iris

chart = alt.Chart(iris).mark_point().encode(
    x='petalLength:Q',
    y='petalWidth:Q',
    color='species:N'
).properties(
    width=180,
    height=180
).facet(
    row='species:N'
)
 chart = alt.Chart(iris).mark_point().encode(
    x='petalLength:Q',
    y='petalWidth:Q',
    color='species:N'
).properties(
    width=180,
    height=180
).facet(
    column='species:N'
)
 facet_wrap ggplot2",17,37,0,2,
754,49809027,49809114,19969,matplotlib subplots - too many indices for array,1,<python><matplotlib>,17,"<p>I'm quite confused with the way plt.subplots work</p>

<p>This snippet works - displays a 2 by 2 Layout</p>

<pre><code>fig, axs = plt.subplots(2,2, figsize=(20, 10))
axs[0,0].set_title('Sobel')
axs[0,0].imshow(sobelx)
axs[0,1].set_title('S Channel')
axs[0,1].imshow(s_channel)
axs[1,0].set_title('Combined Binary')
axs[1,0].imshow(combined_binary)
axs[1,1].set_title('Color Stack')
axs[1,1].imshow(color_stack)
</code></pre>

<p>This snippet doesn't work - 1 by 2 Layout</p>

<pre><code>fig, axs = plt.subplots(1,2, figsize=(20, 10))
axs[0,0].set_title('Undistorted Image')
axs[0,0].imshow(undistort_img)
axs[0,1].set_title('Warped Image')
axs[0,1].imshow(warped_img)
</code></pre>

<p>This errors out with <code>IndexError: too many indices for array</code></p>

<p>When I print axs shape, it is <code>(2, 2)</code> in the first case where as <code>(2,)</code> in the second case. What is this axs ? And how do i make the 2nd piece of code work?</p>
",2350784,581,13-04-2018 03:49,13-04-2018 04:00,0,581,19,3,7,33,"{'badge_counts': {'bronze': 19, 'silver': 7, 'gold': 3}, 'account_id': 2725623, 'is_employee': False, 'last_modified_date': 1636446900, 'last_access_date': 1696620593, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 581, 'creation_date': 1367703137, 'user_type': 'registered', 'user_id': 2350784, 'accept_rate': 33, 'location': 'Kalamazoo, MI, United States', 'link': 'https://stackoverflow.com/users/2350784/ravi', 'profile_image': 'https://www.gravatar.com/avatar/bb261c24834b81253b1cf506b54464c1?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ravi'}","I'm quite confused with the way plt.subplots work This snippet works - displays a 2 by 2 Layout This snippet doesn't work - 1 by 2 Layout This errors out with When I print axs shape, it is in the first case where as in the second case. What is this axs ? And how do i make the 2nd piece of code work?","fig, axs = plt.subplots(2,2, figsize=(20, 10))
axs[0,0].set_title('Sobel')
axs[0,0].imshow(sobelx)
axs[0,1].set_title('S Channel')
axs[0,1].imshow(s_channel)
axs[1,0].set_title('Combined Binary')
axs[1,0].imshow(combined_binary)
axs[1,1].set_title('Color Stack')
axs[1,1].imshow(color_stack)
 fig, axs = plt.subplots(1,2, figsize=(20, 10))
axs[0,0].set_title('Undistorted Image')
axs[0,0].imshow(undistort_img)
axs[0,1].set_title('Warped Image')
axs[0,1].imshow(warped_img)
 IndexError: too many indices for array (2, 2) (2,)",9,27,0,0,
755,48179297,48179385,14692,Reindexing a specific level of a MultiIndex dataframe,1,<python><pandas><dataframe><multi-index><reindex>,18,"<p>I have a DataFrame with two indices and would like to reindex it by one of the indices.</p>

<pre><code>from pandas_datareader import data
import matplotlib.pyplot as plt
import pandas as pd

# Instruments to download
tickers = ['AAPL']

# Online source one should use
data_source = 'yahoo'

# Data range
start_date = '2000-01-01'
end_date = '2018-01-09'

# Load the desired data
panel_data = data.DataReader(tickers, data_source, start_date, end_date).to_frame()
panel_data.head()
</code></pre>

<p><a href=""https://i.stack.imgur.com/l1QKH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/l1QKH.png"" alt=""Screenshot""></a></p>

<p>The reindexing goes as follows:</p>

<pre><code># Get just the adjusted closing prices
adj_close = panel_data['Adj Close']

# Gett all weekdays between start and end dates
all_weekdays = pd.date_range(start=start_date, end=end_date, freq='B')

# Align the existing prices in adj_close with our new set of dates
adj_close = adj_close.reindex(all_weekdays, method=""ffill"")
</code></pre>

<p>The last line gives the following error:</p>

<pre><code>TypeError: '&lt;' not supported between instances of 'tuple' and 'int'
</code></pre>

<p>This is because the DataFrame index is a list of tuples:</p>

<pre><code>panel_data.index[0]
</code></pre>

<pre>(Timestamp('2018-01-09 00:00:00'), 'AAPL')</pre>

<p>Is it possible to reindex <code>adj_close</code>? By the way, if I don't convert the Panel object to a DataFrame using <code>to_frame()</code>, the reindexing works as it is. But it seems that Panel objects are deprecated...</p>
",2359430,1349,10-01-2018 02:08,10-01-2018 02:22,0,1349,36,2,17,76,"{'badge_counts': {'bronze': 36, 'silver': 17, 'gold': 2}, 'account_id': 2736960, 'is_employee': False, 'last_modified_date': 1662779401, 'last_access_date': 1709395600, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1349, 'creation_date': 1367949175, 'user_type': 'registered', 'user_id': 2359430, 'accept_rate': 76, 'website_url': 'http://bacalfa.com', 'link': 'https://stackoverflow.com/users/2359430/bruno', 'profile_image': 'https://i.stack.imgur.com/gOtyH.png?s=256&g=1', 'display_name': 'Bruno'}","I have a DataFrame with two indices and would like to reindex it by one of the indices. The reindexing goes as follows: The last line gives the following error: This is because the DataFrame index is a list of tuples: (Timestamp('2018-01-09 00:00:00'), 'AAPL') Is it possible to reindex ? By the way, if I don't convert the Panel object to a DataFrame using , the reindexing works as it is. But it seems that Panel objects are deprecated...","from pandas_datareader import data
import matplotlib.pyplot as plt
import pandas as pd

# Instruments to download
tickers = ['AAPL']

# Online source one should use
data_source = 'yahoo'

# Data range
start_date = '2000-01-01'
end_date = '2018-01-09'

# Load the desired data
panel_data = data.DataReader(tickers, data_source, start_date, end_date).to_frame()
panel_data.head()
 # Get just the adjusted closing prices
adj_close = panel_data['Adj Close']

# Gett all weekdays between start and end dates
all_weekdays = pd.date_range(start=start_date, end=end_date, freq='B')

# Align the existing prices in adj_close with our new set of dates
adj_close = adj_close.reindex(all_weekdays, method=""ffill"")
 TypeError: '&lt;' not supported between instances of 'tuple' and 'int'
 panel_data.index[0]
 adj_close to_frame()",21,48,1,1,
756,49854628,49854726,13291,How do I pip install the latest patch number of a package?,4,<python><pip><versioning>,19,"<p>How do I pip install the latest patch number version of a package within a major-minor release.
So let's say I want the latest patch release of 1.10 so  if there's 1.10.8, 1.10.9, 1.11.3 available, I want to get 1.10.9.</p>

<p><strong>clarification: I don't want to install a specific package, I want to install the latest package within a range. Above I want the latest package within the 1.10.0 &lt;= x &lt; 1.11.0 range</strong></p>
",2390362,2835,16-04-2018 10:05,16-04-2018 10:10,0,2835,49,6,29,67,"{'badge_counts': {'bronze': 49, 'silver': 29, 'gold': 6}, 'account_id': 2776970, 'is_employee': False, 'last_modified_date': 1703298900, 'last_access_date': 1696849770, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2835, 'creation_date': 1368713765, 'user_type': 'registered', 'user_id': 2390362, 'accept_rate': 67, 'location': 'Montreal, QC, Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/2390362/jad-s', 'profile_image': 'https://www.gravatar.com/avatar/9fd8789c0e2d4f0143d7574ee0cadc59?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jad S'}","How do I pip install the latest patch number version of a package within a major-minor release. So let's say I want the latest patch release of 1.10 so if there's 1.10.8, 1.10.9, 1.11.3 available, I want to get 1.10.9. clarification: I don't want to install a specific package, I want to install the latest package within a range. Above I want the latest package within the 1.10.0 &lt;= x &lt; 1.11.0 range",,0,4,0,0,
757,49182502,49182947,38016,How to show line numbers in Google Colaboratory?,3,<python><ipython><jupyter-notebook><google-colaboratory>,50,"<p>Normally it is possible in jupyter or iPython notebooks to show lines number for a cell, however I don't see where in Google Colaboratory (Colab). </p>
",2476920,10128,08-03-2018 21:09,08-03-2018 21:40,0,10148,81,9,51,90,"{'badge_counts': {'bronze': 81, 'silver': 51, 'gold': 9}, 'account_id': 2507405, 'is_employee': False, 'last_modified_date': 1697245500, 'last_access_date': 1709792151, 'reputation_change_year': 250, 'reputation_change_quarter': 250, 'reputation_change_month': 50, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 10148, 'creation_date': 1371012115, 'user_type': 'registered', 'user_id': 2476920, 'accept_rate': 90, 'location': 'Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/2476920/guillaume-chevalier', 'profile_image': 'https://i.stack.imgur.com/hJXI1.jpg?s=256&g=1', 'display_name': 'Guillaume Chevalier'}","Normally it is possible in jupyter or iPython notebooks to show lines number for a cell, however I don't see where in Google Colaboratory (Colab).",,0,1,0,0,
758,50204613,50214790,36982,"Download pretrained ImageNet model of ResNet, VGG, etc. (.PB file)",2,<python><tensorflow><pre-trained-model>,14,"<p>I have downloaded a pre-trained model on ImageNet of Inception v3 from <a href=""http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz"" rel=""noreferrer"">http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz</a> (found this link while following one of the tutorials on codelabs).</p>

<p>This zip file contains .pb file that I can directly import as GraphDef in TensorFlow.</p>

<p>I would like to know if there are similar .pb files for other architectures as well, such as ResNet, VGG16, MobileNet, etc. If yes, could you provide the link of those?</p>

<p>Thanks in advance.</p>

<p>Kind Regards,</p>

<p>Ajay</p>
",2493867,750,06-05-2018 21:19,07-05-2018 12:56,1,750,17,2,9,,"{'badge_counts': {'bronze': 17, 'silver': 9, 'gold': 2}, 'account_id': 2909220, 'is_employee': False, 'last_modified_date': 1629743100, 'last_access_date': 1710685398, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 750, 'creation_date': 1371480727, 'user_type': 'registered', 'user_id': 2493867, 'location': 'Dublin, Ireland', 'website_url': '', 'link': 'https://stackoverflow.com/users/2493867/ajay-maity', 'profile_image': 'https://i.stack.imgur.com/JzyXB.jpg?s=256&g=1', 'display_name': 'Ajay Maity'}","I have downloaded a pre-trained model on ImageNet of Inception v3 from http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz (found this link while following one of the tutorials on codelabs). This zip file contains .pb file that I can directly import as GraphDef in TensorFlow. I would like to know if there are similar .pb files for other architectures as well, such as ResNet, VGG16, MobileNet, etc. If yes, could you provide the link of those? Thanks in advance. Kind Regards, Ajay",,0,11,0,1,
759,48845989,48845999,12176,Understanding class type '__main__.ClassName',2,<python><python-2.7><oop>,11,"<p>Code:</p>

<pre><code>class Fraction(object):
    def __init__(self, num, denom):
        self.numerator = num
        self.denominator = denom

def main():
    f = Fraction(1, 3)
    print type(f)

if __name__ == ""__main__"":
    main()
</code></pre>

<p>Output:</p>

<pre><code>&lt;class '__main__.Fraction'&gt;
</code></pre>

<p>Question: </p>

<ol>
<li>Why is the type <code>__main__.Fraction</code> instead of just <code>Fraction</code>? </li>
<li><p>Why is there ""."" between <code>__main__</code> and <code>Fraction</code>? ""."" implies that <code>Fraction</code> is a sub-class of <code>__main__</code>. But why? Even if I remove <code>If __name__ == ""__main__""</code> from the code, I still get the same output:</p>

<pre><code>class Fraction(object):
def __init__(self, num, denom):
    self.numerator = num
    self.denominator = denom

f = Fraction(1,3)
print type(f)

output: &lt;class '__main__.Fraction'&gt;
</code></pre></li>
</ol>
",3742823,3141,17-02-2018 21:13,17-02-2018 21:15,0,3141,54,7,29,67,"{'badge_counts': {'bronze': 54, 'silver': 29, 'gold': 7}, 'account_id': 4616483, 'is_employee': False, 'last_modified_date': 1668821700, 'last_access_date': 1710865109, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3141, 'creation_date': 1402856836, 'user_type': 'registered', 'user_id': 3742823, 'accept_rate': 67, 'website_url': '', 'link': 'https://stackoverflow.com/users/3742823/the-wanderer', 'profile_image': 'https://i.stack.imgur.com/qSDYy.jpg?s=256&g=1', 'display_name': 'The Wanderer'}","Code: Output: Question: Why is the type instead of just ? Why is there ""."" between and ? ""."" implies that is a sub-class of . But why? Even if I remove from the code, I still get the same output:","class Fraction(object):
    def __init__(self, num, denom):
        self.numerator = num
        self.denominator = denom

def main():
    f = Fraction(1, 3)
    print type(f)

if __name__ == ""__main__"":
    main()
 &lt;class '__main__.Fraction'&gt;
 __main__.Fraction Fraction __main__ Fraction Fraction __main__ If __name__ == ""__main__"" class Fraction(object):
def __init__(self, num, denom):
    self.numerator = num
    self.denominator = denom

f = Fraction(1,3)
print type(f)

output: &lt;class '__main__.Fraction'&gt;
",11,37,0,0,
760,48862235,48862333,22958,Python - ValueError: Cannot index with vector containing NA / NaN values,2,<python><python-3.x><pandas><dataframe><valueerror>,16,"<p>I'm trying to get the average price of products containing any substrings from a wordlist from a dataframe. I've been able to do so with the following code on multiple spreadsheets - </p>

<pre><code>dframe['Product'].fillna('', inplace=True)
dframe['Price'].fillna(0, inplace=True)
total_count = 0
total_price = 0
for word in ransomware_wordlist:
    mask = dframe.Product.str.contains(word, case=False)
    total_count += mask.sum()
    total_price += dframe.loc[mask, 'Price'].sum()
average_price = total_price / total_count
print(average_price)
</code></pre>

<p>However, one of the spreadsheets throws an error at line -</p>

<pre><code>dframe['Product'].fillna('', inplace=True)
</code></pre>

<p>with</p>

<pre><code>ValueError: cannot index with vector containing NA / NaN values
</code></pre>

<p>I fail to understand why <code>dframe['Product'].fillna('', inplace=True)</code> isn't handling this problem.</p>

<p>In desperate need of some help! Thanks!</p>
",2552610,900,19-02-2018 08:41,19-02-2018 08:48,0,900,21,2,10,56,"{'badge_counts': {'bronze': 21, 'silver': 10, 'gold': 2}, 'account_id': 3008673, 'is_employee': False, 'last_modified_date': 1683338400, 'last_access_date': 1705639974, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 900, 'creation_date': 1373005824, 'user_type': 'registered', 'user_id': 2552610, 'accept_rate': 56, 'website_url': '', 'link': 'https://stackoverflow.com/users/2552610/harry04', 'profile_image': 'https://i.stack.imgur.com/jD7Hz.png?s=256&g=1', 'display_name': 'harry04'}","I'm trying to get the average price of products containing any substrings from a wordlist from a dataframe. I've been able to do so with the following code on multiple spreadsheets - However, one of the spreadsheets throws an error at line - with I fail to understand why isn't handling this problem. In desperate need of some help! Thanks!","dframe['Product'].fillna('', inplace=True)
dframe['Price'].fillna(0, inplace=True)
total_count = 0
total_price = 0
for word in ransomware_wordlist:
    mask = dframe.Product.str.contains(word, case=False)
    total_count += mask.sum()
    total_price += dframe.loc[mask, 'Price'].sum()
average_price = total_price / total_count
print(average_price)
 dframe['Product'].fillna('', inplace=True)
 ValueError: cannot index with vector containing NA / NaN values
 dframe['Product'].fillna('', inplace=True)",8,27,0,0,
761,49290266,49290555,56325,python matplotlib histogram specify different colours for different bars,1,<python><matplotlib><histogram>,17,"<p>I want to colour different bars in a histogram based on which bin they belong to.  e.g. in the below example, I want the first 3 bars to be blue, the next 2 to be red, and the rest black (the actual bars and colour is determined by other parts of the code).</p>

<p>I can change the colour of all the bars using the color option, but I would like to be able to give a list of colours that are used.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

data = np.random.rand(1000)
plt.hist(data,color = 'r')
</code></pre>
",2613271,1420,15-03-2018 01:41,15-03-2018 02:20,0,1430,31,4,19,79,"{'badge_counts': {'bronze': 31, 'silver': 19, 'gold': 4}, 'account_id': 514235, 'is_employee': False, 'last_modified_date': 1708134900, 'last_access_date': 1683856845, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1430, 'creation_date': 1374646450, 'user_type': 'registered', 'user_id': 2613271, 'accept_rate': 79, 'website_url': '', 'link': 'https://stackoverflow.com/users/2613271/esme', 'profile_image': 'https://www.gravatar.com/avatar/20c85a9a9d4abc98d0bb6a52f4a2c025?s=256&d=identicon&r=PG', 'display_name': 'Esme_'}","I want to colour different bars in a histogram based on which bin they belong to. e.g. in the below example, I want the first 3 bars to be blue, the next 2 to be red, and the rest black (the actual bars and colour is determined by other parts of the code). I can change the colour of all the bars using the color option, but I would like to be able to give a list of colours that are used.","import numpy as np
import matplotlib.pyplot as plt

data = np.random.rand(1000)
plt.hist(data,color = 'r')
",4,10,0,0,
762,48373577,48373785,45651,Recovering command history in Jupyter Notebook?,3,<python><python-3.x><ipython><jupyter-notebook><jupyter>,25,"<p>I have been editing a Jupyter Notebook for the past week, and tried saving it today. While attempting to save it, I got an error, so I refreshed the page and successfully saved it.</p>

<p>However, to my dismay almost all of my command history was lost! I still have access to the variables (the kernel never died), but I don't have access to any of the code.</p>

<p>Is there any way to recover the code? The kernel is still running, but I do not see any checkpoints in my notebook.</p>
",2636317,14926,22-01-2018 01:28,22-01-2018 02:03,0,14926,106,15,81,82,"{'badge_counts': {'bronze': 106, 'silver': 81, 'gold': 15}, 'account_id': 3075056, 'is_employee': False, 'last_modified_date': 1703300400, 'last_access_date': 1694011242, 'reputation_change_year': 158, 'reputation_change_quarter': 158, 'reputation_change_month': 40, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 14926, 'creation_date': 1375235611, 'user_type': 'registered', 'user_id': 2636317, 'accept_rate': 82, 'website_url': '', 'link': 'https://stackoverflow.com/users/2636317/mgoldwasser', 'profile_image': 'https://www.gravatar.com/avatar/d665de9176035b991a9c84b82a23337d?s=256&d=identicon&r=PG', 'display_name': 'mgoldwasser'}","I have been editing a Jupyter Notebook for the past week, and tried saving it today. While attempting to save it, I got an error, so I refreshed the page and successfully saved it. However, to my dismay almost all of my command history was lost! I still have access to the variables (the kernel never died), but I don't have access to any of the code. Is there any way to recover the code? The kernel is still running, but I do not see any checkpoints in my notebook.",,0,5,0,0,
763,50110044,50137686,15036,How to force parquet dtypes when saving pd.DataFrame?,1,<python><pandas><parquet><dask><pyarrow>,18,"<p>Is there a way to force a parquet file to encode a <code>pd.DataFrame</code> column as a given type, even though all values for the column are null? The fact that parquet automatically assigns ""null"" in its schema is preventing me from loading many files into a single <code>dask.dataframe</code>.</p>

<p>Trying to cast the pandas column using <code>df.column_name = df.column_name.astype(sometype)</code> didn't work.</p>

<p><strong>Why I'm asking this</strong></p>

<p>I want to load many parquet files into a single <code>dask.dataframe</code>. All files were generated from as many instances of <code>pd.DataFrame</code>, using <code>df.to_parquet(filename)</code>. All dataframes have the same columns, but for some a given column might contain only null values. When trying to load all files into the <code>dask.dataframe</code> (using <code>df = dd.read_parquet('*.parquet')</code> , I get the following error:</p>

<pre><code>Schema in filename.parquet was different.
id: int64
text: string
[...]
some_column: double

vs

id: int64
text: string
[...]
some_column: null
</code></pre>

<p><strong>Steps to reproduce my problem</strong></p>

<pre><code>import pandas as pd
import dask.dataframe as dd
a = pd.DataFrame(['1', '1'], columns=('value',))
b = pd.DataFrame([None, None], columns=('value',))
a.to_parquet('a.parquet')
b.to_parquet('b.parquet')
df = dd.read_parquet('*.parquet')  # Reads a and b
</code></pre>

<p>This gives me the following:</p>

<pre><code>ValueError: Schema in path/to/b.parquet was different. 
value: null
__index_level_0__: int64
metadata
--------
{b'pandas': b'{""index_columns"": [""__index_level_0__""], ""column_indexes"": [{""na'
            b'me"": null, ""field_name"": null, ""pandas_type"": ""unicode"", ""numpy_'
            b'type"": ""object"", ""metadata"": {""encoding"": ""UTF-8""}}], ""columns"":'
            b' [{""name"": ""value"", ""field_name"": ""value"", ""pandas_type"": ""empty'
            b'"", ""numpy_type"": ""object"", ""metadata"": null}, {""name"": null, ""fi'
            b'eld_name"": ""__index_level_0__"", ""pandas_type"": ""int64"", ""numpy_t'
            b'ype"": ""int64"", ""metadata"": null}], ""pandas_version"": ""0.22.0""}'}

vs

value: string
__index_level_0__: int64
metadata
--------
{b'pandas': b'{""index_columns"": [""__index_level_0__""], ""column_indexes"": [{""na'
            b'me"": null, ""field_name"": null, ""pandas_type"": ""unicode"", ""numpy_'
            b'type"": ""object"", ""metadata"": {""encoding"": ""UTF-8""}}], ""columns"":'
            b' [{""name"": ""value"", ""field_name"": ""value"", ""pandas_type"": ""unico'
            b'de"", ""numpy_type"": ""object"", ""metadata"": null}, {""name"": null, ""'
            b'field_name"": ""__index_level_0__"", ""pandas_type"": ""int64"", ""numpy'
            b'_type"": ""int64"", ""metadata"": null}], ""pandas_version"": ""0.22.0""}'}
</code></pre>

<p>Notice how in one case we have <code>""pandas_type"": ""unicode""</code> and in the other we have <code>""pandas_type"": ""empty""</code>.</p>

<p><strong>Related questions that didn't provide me with a solution</strong>  </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/49172428/how-to-specify-logical-types-when-writing-parquet-files-from-pyarrow"">How to specify logical types when writing Parquet files from PyArrow?</a></li>
</ul>
",2651194,1275,01-05-2018 00:45,02-05-2018 14:57,1,1275,19,1,10,,"{'badge_counts': {'bronze': 19, 'silver': 10, 'gold': 1}, 'account_id': 3134350, 'is_employee': False, 'last_modified_date': 1708140048, 'last_access_date': 1700236458, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1275, 'creation_date': 1375649244, 'user_type': 'registered', 'user_id': 2651194, 'location': 'California', 'link': 'https://stackoverflow.com/users/2651194/hugom', 'profile_image': 'https://www.gravatar.com/avatar/41c1561f89847070e5b467ab4290c09b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'hugom'}","Is there a way to force a parquet file to encode a column as a given type, even though all values for the column are null? The fact that parquet automatically assigns ""null"" in its schema is preventing me from loading many files into a single . Trying to cast the pandas column using didn't work. Why I'm asking this I want to load many parquet files into a single . All files were generated from as many instances of , using . All dataframes have the same columns, but for some a given column might contain only null values. When trying to load all files into the (using , I get the following error: Steps to reproduce my problem This gives me the following: Notice how in one case we have and in the other we have . Related questions that didn't provide me with a solution How to specify logical types when writing Parquet files from PyArrow?","pd.DataFrame dask.dataframe df.column_name = df.column_name.astype(sometype) dask.dataframe pd.DataFrame df.to_parquet(filename) dask.dataframe df = dd.read_parquet('*.parquet') Schema in filename.parquet was different.
id: int64
text: string
[...]
some_column: double

vs

id: int64
text: string
[...]
some_column: null
 import pandas as pd
import dask.dataframe as dd
a = pd.DataFrame(['1', '1'], columns=('value',))
b = pd.DataFrame([None, None], columns=('value',))
a.to_parquet('a.parquet')
b.to_parquet('b.parquet')
df = dd.read_parquet('*.parquet')  # Reads a and b
 ValueError: Schema in path/to/b.parquet was different. 
value: null
__index_level_0__: int64
metadata
--------
{b'pandas': b'{""index_columns"": [""__index_level_0__""], ""column_indexes"": [{""na'
            b'me"": null, ""field_name"": null, ""pandas_type"": ""unicode"", ""numpy_'
            b'type"": ""object"", ""metadata"": {""encoding"": ""UTF-8""}}], ""columns"":'
            b' [{""name"": ""value"", ""field_name"": ""value"", ""pandas_type"": ""empty'
            b'"", ""numpy_type"": ""object"", ""metadata"": null}, {""name"": null, ""fi'
            b'eld_name"": ""__index_level_0__"", ""pandas_type"": ""int64"", ""numpy_t'
            b'ype"": ""int64"", ""metadata"": null}], ""pandas_version"": ""0.22.0""}'}

vs

value: string
__index_level_0__: int64
metadata
--------
{b'pandas': b'{""index_columns"": [""__index_level_0__""], ""column_indexes"": [{""na'
            b'me"": null, ""field_name"": null, ""pandas_type"": ""unicode"", ""numpy_'
            b'type"": ""object"", ""metadata"": {""encoding"": ""UTF-8""}}], ""columns"":'
            b' [{""name"": ""value"", ""field_name"": ""value"", ""pandas_type"": ""unico'
            b'de"", ""numpy_type"": ""object"", ""metadata"": null}, {""name"": null, ""'
            b'field_name"": ""__index_level_0__"", ""pandas_type"": ""int64"", ""numpy'
            b'_type"": ""int64"", ""metadata"": null}], ""pandas_version"": ""0.22.0""}'}
 ""pandas_type"": ""unicode"" ""pandas_type"": ""empty""",32,70,0,1,
764,49433383,49642742,536,conda-build of official AnacondaRecipes/opencv-feedstock fails looking for libpng.h,1,<python><opencv><conda><conda-build>,14,"<p>I have downloaded the official conda recipe of <a href=""https://github.com/AnacondaRecipes/opencv-feedstock"" rel=""noreferrer"">opencv in AnacondaRecipes</a>.</p>

<p>I have tried to build this package executing:</p>

<pre><code>conda-build recipe -c conda-forge
</code></pre>

<p>I am getting the following error when the recipe compiles opencv, when doing <code>[ 72%] Built target opencv_dnn</code> . The error is the following:</p>

<pre><code>[ 67%] Building CXX object modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/src/grfmt_png.cpp.o
/opt/conda/conda-bld/opencv_1521187259162/work/modules/imgcodecs/src/grfmt_png.cpp:62:10: fatal error: libpng/png.h: No such file or directory
 #include &lt;libpng/png.h&gt;
          ^~~~~~~~~~~~~~
compilation terminated.
modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/build.make:326: recipe for target 'modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/src/grfmt_png.cpp.o' failed
make[2]: *** [modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/src/grfmt_png.cpp.o] Error 1
CMakeFiles/Makefile2:4645: recipe for target 'modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/all' failed
make[1]: *** [modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
</code></pre>

<p>Lookin in the <code>$PREFIX</code> directory, there is not libpng folder, only a libpng16 folder:</p>

<pre><code>/opt/conda/conda-bld/opencv_1521187259162/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh/include/
</code></pre>

<p>I suspect it must be something related to how conda-build manages the path environment, but I do not have any clue of how to solve it.</p>

<p>Environment: conda 4.4.11, OS: Ubuntu 16.04.</p>

<p><strong>UPDATE 23/03/2018</strong></p>

<p>I have also tried:</p>

<ul>
<li>add <code>libpng/png.h</code> to the location pointed by the $PREFIX directory.</li>
<li>add <code>/usr/include</code> to the <code>$PATH</code></li>
</ul>

<p>No success in either case.</p>

<p><strong>UPDATE 04/03/2018</strong></p>

<ul>
<li><code>libpng-dev</code> is installed in the environment.</li>
</ul>

<p><strong>UPDATE 09/04/2018</strong></p>

<ul>
<li><a href=""https://gist.github.com/jruizaranguren/deead8fe2b1865b53f32310397d3e3eb"" rel=""noreferrer"">Docker recipe to reproduce environment</a></li>
</ul>

<p><strong>UPDATE 12/04/2018</strong></p>

<ul>
<li><a href=""https://gist.github.com/jruizaranguren/deead8fe2b1865b53f32310397d3e3eb#file-dockerfile-usr-local"" rel=""noreferrer"">Docker recipe using miniconda3 and installing conda in /usr/local/conda instead of /opt/conda</a></li>
</ul>
",2660176,13109,22-03-2018 16:07,04-04-2018 04:23,13,13159,74,7,57,90,"{'badge_counts': {'bronze': 74, 'silver': 57, 'gold': 7}, 'account_id': 1486450, 'is_employee': False, 'last_modified_date': 1702179300, 'last_access_date': 1710939017, 'reputation_change_year': 168, 'reputation_change_quarter': 168, 'reputation_change_month': 58, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 13159, 'creation_date': 1375868812, 'user_type': 'registered', 'user_id': 2660176, 'accept_rate': 90, 'location': 'Sarriguren, Spain', 'website_url': 'http://es.linkedin.com/in/jruiza/', 'link': 'https://stackoverflow.com/users/2660176/jruizaranguren', 'profile_image': 'https://i.stack.imgur.com/orbgS.png?s=256&g=1', 'display_name': 'jruizaranguren'}","I have downloaded the official conda recipe of opencv in AnacondaRecipes. I have tried to build this package executing: I am getting the following error when the recipe compiles opencv, when doing . The error is the following: Lookin in the directory, there is not libpng folder, only a libpng16 folder: I suspect it must be something related to how conda-build manages the path environment, but I do not have any clue of how to solve it. Environment: conda 4.4.11, OS: Ubuntu 16.04. UPDATE 23/03/2018 I have also tried: add to the location pointed by the $PREFIX directory. add to the No success in either case. UPDATE 04/03/2018 is installed in the environment. UPDATE 09/04/2018 Docker recipe to reproduce environment UPDATE 12/04/2018 Docker recipe using miniconda3 and installing conda in /usr/local/conda instead of /opt/conda","conda-build recipe -c conda-forge
 [ 72%] Built target opencv_dnn [ 67%] Building CXX object modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/src/grfmt_png.cpp.o
/opt/conda/conda-bld/opencv_1521187259162/work/modules/imgcodecs/src/grfmt_png.cpp:62:10: fatal error: libpng/png.h: No such file or directory
 #include &lt;libpng/png.h&gt;
          ^~~~~~~~~~~~~~
compilation terminated.
modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/build.make:326: recipe for target 'modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/src/grfmt_png.cpp.o' failed
make[2]: *** [modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/src/grfmt_png.cpp.o] Error 1
CMakeFiles/Makefile2:4645: recipe for target 'modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/all' failed
make[1]: *** [modules/imgcodecs/CMakeFiles/opencv_imgcodecs.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
 $PREFIX /opt/conda/conda-bld/opencv_1521187259162/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh/include/
 libpng/png.h /usr/include $PATH libpng-dev",3,58,0,3,
765,48435774,48435859,16858,`SyntaxError: invalid syntax` when starting Python script in VS Code on macOS,3,<python><json><visual-studio-code>,36,"<p>I'm trying to run a Python script from Visual Studio code, but the script fails to run and crashes with a <code>SyntaxError</code> pointing to the comment at the beginning of <code>launch.json</code>.</p>
<p><code>launch.json</code>:</p>
<pre><code>{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    &quot;version&quot;: &quot;0.2.0&quot;,
    &quot;configurations&quot;: [
        {
            &quot;name&quot;: &quot;Python | Default&quot;,
            &quot;type&quot;: &quot;python&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;stopOnEntry&quot;: false,
            &quot;pythonPath&quot;: &quot;${config:python.pythonPath}&quot;,
            &quot;program&quot;: &quot;${file}&quot;,
            &quot;cwd&quot;: &quot;${workspaceFolder}&quot;,
            &quot;env&quot;: {},
            &quot;envFile&quot;: &quot;${workspaceFolder}/.env&quot;,
            &quot;debugOptions&quot;: [
                &quot;RedirectOutput&quot;
            ]
        }
    ]
}
</code></pre>
<hr />
<p>Terminal Output:</p>
<pre><code>File &quot;.../.vscode/launch.json&quot;, line 2
    // Use IntelliSense to learn about possible attributes.
     ^
SyntaxError: invalid syntax
</code></pre>
<hr />
<p><code>settings.json</code>:</p>
<pre><code>{
    &quot;python.pythonPath&quot;: &quot;${workspaceFolder}/venv/bin/python&quot;
}
</code></pre>
<hr />
<p>I was working on my Windows machine earlier and all of this worked perfectly fine. For some reason, VSCode is trying to run the <code>launch.json</code> file through Python and <code>//</code> is an invalid comment syntax in Python. If I remove the comments, I get this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;.../.vscode/launch.json&quot;, line 8, in &lt;module&gt;
    &quot;stopOnEntry&quot;: false,
NameError: name 'false' is not defined
</code></pre>
<p>If I use Python's <code>False</code>, I don't crash but nothing happens and my script does not run. It seems very much like <code>launch.json</code> is being parsed by Python erroneously. Any fix for this?</p>
",2888865,1583,25-01-2018 04:37,25-01-2018 04:47,0,1583,26,1,14,37,"{'badge_counts': {'bronze': 26, 'silver': 14, 'gold': 1}, 'account_id': 3448744, 'is_employee': False, 'last_modified_date': 1611485100, 'last_access_date': 1633242405, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1583, 'creation_date': 1381981162, 'user_type': 'registered', 'user_id': 2888865, 'accept_rate': 37, 'link': 'https://stackoverflow.com/users/2888865/nick-alexander', 'profile_image': 'https://graph.facebook.com/762339961/picture?type=large', 'display_name': 'Nick Alexander'}","I'm trying to run a Python script from Visual Studio code, but the script fails to run and crashes with a pointing to the comment at the beginning of . : Terminal Output: : I was working on my Windows machine earlier and all of this worked perfectly fine. For some reason, VSCode is trying to run the file through Python and is an invalid comment syntax in Python. If I remove the comments, I get this error: If I use Python's , I don't crash but nothing happens and my script does not run. It seems very much like is being parsed by Python erroneously. Any fix for this?","SyntaxError launch.json launch.json {
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    &quot;version&quot;: &quot;0.2.0&quot;,
    &quot;configurations&quot;: [
        {
            &quot;name&quot;: &quot;Python | Default&quot;,
            &quot;type&quot;: &quot;python&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;stopOnEntry&quot;: false,
            &quot;pythonPath&quot;: &quot;${config:python.pythonPath}&quot;,
            &quot;program&quot;: &quot;${file}&quot;,
            &quot;cwd&quot;: &quot;${workspaceFolder}&quot;,
            &quot;env&quot;: {},
            &quot;envFile&quot;: &quot;${workspaceFolder}/.env&quot;,
            &quot;debugOptions&quot;: [
                &quot;RedirectOutput&quot;
            ]
        }
    ]
}
 File &quot;.../.vscode/launch.json&quot;, line 2
    // Use IntelliSense to learn about possible attributes.
     ^
SyntaxError: invalid syntax
 settings.json {
    &quot;python.pythonPath&quot;: &quot;${workspaceFolder}/venv/bin/python&quot;
}
 launch.json // Traceback (most recent call last):
  File &quot;.../.vscode/launch.json&quot;, line 8, in &lt;module&gt;
    &quot;stopOnEntry&quot;: false,
NameError: name 'false' is not defined
 False launch.json",21,46,0,0,
766,48324152,48324389,103686,How to change the learning rate of an optimizer at any given moment (no LR schedule)?,2,<python><optimization><neural-network><deep-learning><pytorch>,91,"<p>Is it possible in PyTorch to change the learning rate of the optimizer in the middle of training dynamically (I don't want to define a learning rate schedule beforehand)?</p>

<p>So let's say I have an optimizer:</p>

<pre><code>optim = torch.optim.SGD(model.parameters(), lr=0.01)
</code></pre>

<p>Now due to some tests which I perform during training, I realize my learning rate is too high so I want to change it to say <code>0.001</code>. There doesn't seem to be a method <code>optim.set_lr(0.001)</code> but is there some way to do this? </p>
",3990607,18103,18-01-2018 14:55,18-01-2018 15:07,0,18143,135,13,94,77,"{'badge_counts': {'bronze': 135, 'silver': 94, 'gold': 13}, 'account_id': 4958744, 'is_employee': False, 'last_modified_date': 1652904600, 'last_access_date': 1710448393, 'reputation_change_year': 190, 'reputation_change_quarter': 190, 'reputation_change_month': 50, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 18143, 'creation_date': 1409324095, 'user_type': 'registered', 'user_id': 3990607, 'accept_rate': 77, 'location': 'Laniakea Supercluster : Milky Way : Orion Arm : Sun System : 3rd planet', 'website_url': '', 'link': 'https://stackoverflow.com/users/3990607/patapouf-ai', 'profile_image': 'https://i.stack.imgur.com/0ZRtB.png?s=256&g=1', 'display_name': 'patapouf_ai'}","Is it possible in PyTorch to change the learning rate of the optimizer in the middle of training dynamically (I don't want to define a learning rate schedule beforehand)? So let's say I have an optimizer: Now due to some tests which I perform during training, I realize my learning rate is too high so I want to change it to say . There doesn't seem to be a method but is there some way to do this?","optim = torch.optim.SGD(model.parameters(), lr=0.01)
 0.001 optim.set_lr(0.001)",-2,8,0,0,
767,48338847,48339768,67175,How to copy a Python class instance if deepcopy() does not work?,3,<python><class><deep-copy>,37,"<p>I would like to make a copy of a class instance in python. I tried <code>copy.deepcopy</code> but I get the error message:</p>

<blockquote>
  <p>RuntimeError: Only Variables created explicitly by the user (graph leaves) support the deepcopy protocol at the moment</p>
</blockquote>

<p>So suppose I have something like:</p>

<pre><code>class C(object):
    def __init__(self,a,b, **kwargs):
        self.a=a
        self.b=b
        for x, v in kwargs.items():
            setattr(self, x, v)

c = C(4,5,'r'=2)
c.a = 11
del c.b
</code></pre>

<p>And now I want to make an identical deep copy of <code>c</code>, is there an easy way?</p>
",3990607,18103,19-01-2018 10:23,19-01-2018 11:09,0,18143,135,13,94,77,"{'badge_counts': {'bronze': 135, 'silver': 94, 'gold': 13}, 'account_id': 4958744, 'is_employee': False, 'last_modified_date': 1652904600, 'last_access_date': 1710448393, 'reputation_change_year': 190, 'reputation_change_quarter': 190, 'reputation_change_month': 50, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 18143, 'creation_date': 1409324095, 'user_type': 'registered', 'user_id': 3990607, 'accept_rate': 77, 'location': 'Laniakea Supercluster : Milky Way : Orion Arm : Sun System : 3rd planet', 'website_url': '', 'link': 'https://stackoverflow.com/users/3990607/patapouf-ai', 'profile_image': 'https://i.stack.imgur.com/0ZRtB.png?s=256&g=1', 'display_name': 'patapouf_ai'}","I would like to make a copy of a class instance in python. I tried but I get the error message: RuntimeError: Only Variables created explicitly by the user (graph leaves) support the deepcopy protocol at the moment So suppose I have something like: And now I want to make an identical deep copy of , is there an easy way?","copy.deepcopy class C(object):
    def __init__(self,a,b, **kwargs):
        self.a=a
        self.b=b
        for x, v in kwargs.items():
            setattr(self, x, v)

c = C(4,5,'r'=2)
c.a = 11
del c.b
 c",7,21,0,0,
768,49355010,49435131,49512,How do i watch python source code files and restart when i save?,6,<python><nodemon>,87,"<p>When I save a python source code file, I want to re-run the script. Is there a command that works like this (sort of like nodemon for node)?</p>
",2953703,2789,19-03-2018 02:32,22-03-2018 17:37,3,2799,20,3,17,100,"{'badge_counts': {'bronze': 20, 'silver': 17, 'gold': 3}, 'account_id': 3535210, 'is_employee': False, 'last_modified_date': 1698239400, 'last_access_date': 1710892824, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2799, 'creation_date': 1383590405, 'user_type': 'registered', 'user_id': 2953703, 'accept_rate': 100, 'location': 'Honolulu, HI', 'website_url': 'https://www.linkedin.com/in/bkinsey/', 'link': 'https://stackoverflow.com/users/2953703/bkinsey808', 'profile_image': 'https://www.gravatar.com/avatar/cca6532d95976f00c65be674101a91af?s=256&d=identicon&r=PG', 'display_name': 'bkinsey808'}","When I save a python source code file, I want to re-run the script. Is there a command that works like this (sort of like nodemon for node)?",,0,1,0,0,
769,48157735,48158449,42789,Plot multiple bars for categorical data,1,<python><matplotlib>,13,"<p>I'm looking for a way to plot multiple bars per value in matplotlib. For numerical data, this can be achieved be adding an offset to the X data, as described for example <a href=""https://stackoverflow.com/q/14270391/2969841"">here</a>:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

X = np.array([1,3,5])
Y = [1,2,3]
Z = [2,3,4]

plt.bar(X - 0.4, Y) # offset of -0.4
plt.bar(X + 0.4, Z) # offset of  0.4
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/WoTrc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WoTrc.png"" alt=""Multiple bars for numerical data""></a></p>

<p><code>plt.bar()</code> (and <code>ax.bar()</code>) also handle categorical data automatically:</p>

<pre><code>X = ['A','B','C']
Y = [1,2,3]

plt.bar(X, Y)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/huKwR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/huKwR.png"" alt=""Category handling""></a></p>

<p>Here, it is obviously not possible to add an offset, as the categories are not directly associated with a value on the axis. I can manually assign numerical values to the categories and set labels on the x axis with <code>plt.xticks()</code>:, </p>

<pre><code>X = ['A','B','C']
Y = [1,2,3]
Z = [2,3,4]
_X = np.arange(len(X))

plt.bar(_X - 0.2, Y, 0.4)
plt.bar(_X + 0.2, Z, 0.4)
plt.xticks(_X, X) # set labels manually
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/jV9pe.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jV9pe.png"" alt=""Manually setting category labels""></a></p>

<p>However, I'm wondering if there is a more elegant way that makes use of the automatic category handling of <code>bar()</code>, especially if the number of categories and bars per category is not known in before (this causes some fiddling with the bar widths to avoid overlaps).</p>
",2969841,2161,08-01-2018 20:39,08-01-2018 21:40,0,2181,30,1,21,100,"{'badge_counts': {'bronze': 30, 'silver': 21, 'gold': 1}, 'account_id': 3556099, 'is_employee': False, 'last_modified_date': 1688720149, 'last_access_date': 1711099368, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 2181, 'creation_date': 1383930306, 'user_type': 'registered', 'user_id': 2969841, 'accept_rate': 100, 'location': 'Germany', 'website_url': 'http://www.sabsch.com', 'link': 'https://stackoverflow.com/users/2969841/tsabsch', 'profile_image': 'https://i.stack.imgur.com/wPVC6.jpg?s=256&g=1', 'display_name': 'tsabsch'}","I'm looking for a way to plot multiple bars per value in matplotlib. For numerical data, this can be achieved be adding an offset to the X data, as described for example here: (and ) also handle categorical data automatically: Here, it is obviously not possible to add an offset, as the categories are not directly associated with a value on the axis. I can manually assign numerical values to the categories and set labels on the x axis with :, However, I'm wondering if there is a more elegant way that makes use of the automatic category handling of , especially if the number of categories and bars per category is not known in before (this causes some fiddling with the bar widths to avoid overlaps).","import numpy as np
import matplotlib.pyplot as plt

X = np.array([1,3,5])
Y = [1,2,3]
Z = [2,3,4]

plt.bar(X - 0.4, Y) # offset of -0.4
plt.bar(X + 0.4, Z) # offset of  0.4
plt.show()
 plt.bar() ax.bar() X = ['A','B','C']
Y = [1,2,3]

plt.bar(X, Y)
plt.show()
 plt.xticks() X = ['A','B','C']
Y = [1,2,3]
Z = [2,3,4]
_X = np.arange(len(X))

plt.bar(_X - 0.2, Y, 0.4)
plt.bar(_X + 0.2, Z, 0.4)
plt.xticks(_X, X) # set labels manually
plt.show()
 bar()",17,43,3,4,
770,49683653,49732825,1984,How to pass additional parameters to numba cfunc passed as LowLevelCallable to scipy.integrate.quad,2,<python><scipy><numba>,12,"<p>The documentation <a href=""http://numba.pydata.org/numba-doc/0.34.0/user/cfunc.html#example"" rel=""noreferrer"">discusses</a> using numba's <code>cfunc</code>s as <code>LowLevelCallable</code> argument of <code>scipy.integrate.quad</code>. I need the same thing with additional parameter.</p>

<p>I'm basically trying to do something like this:</p>

<pre><code>import numpy as np
from numba import cfunc
import numba.types
voidp = numba.types.voidptr
def integrand(t, params):
    a = params[0] # this is additional parameter
    return np.exp(-t/a) / t**2
nb_integrand = cfunc(numba.float32(numba.float32, voidp))(integrand)
</code></pre>

<p>However, it does not work, because <code>params</code> are supposed to be <code>voidptr</code>/<code>void*</code> and they cannot be transformed to <code>double</code>. I have the following error message:</p>

<pre><code>TypingError: Failed at nopython (nopython frontend)
Invalid usage of getitem with parameters (void*, int64)
 * parameterized
</code></pre>

<p>I didn't find any information on how to extract values from <code>void*</code> in Numba. In C, it should be something like <code>a = *((double*) params)</code> — is it possible to do the same thing in Numba?</p>
",3025981,7777,06-04-2018 00:52,09-04-2018 12:18,3,7787,81,4,44,80,"{'badge_counts': {'bronze': 81, 'silver': 44, 'gold': 4}, 'account_id': 1376552, 'is_employee': False, 'last_modified_date': 1698182100, 'last_access_date': 1711135281, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7787, 'creation_date': 1385242865, 'user_type': 'registered', 'user_id': 3025981, 'accept_rate': 80, 'website_url': 'https://ilya.schurov.com/', 'link': 'https://stackoverflow.com/users/3025981/ilya-v-schurov', 'profile_image': 'https://www.gravatar.com/avatar/8b0f3e2f5e5cff8ac5e1307d81c8df1e?s=256&d=identicon&r=PG', 'display_name': 'Ilya V. Schurov'}","The documentation discusses using numba's s as argument of . I need the same thing with additional parameter. I'm basically trying to do something like this: However, it does not work, because are supposed to be / and they cannot be transformed to . I have the following error message: I didn't find any information on how to extract values from in Numba. In C, it should be something like — is it possible to do the same thing in Numba?","cfunc LowLevelCallable scipy.integrate.quad import numpy as np
from numba import cfunc
import numba.types
voidp = numba.types.voidptr
def integrand(t, params):
    a = params[0] # this is additional parameter
    return np.exp(-t/a) / t**2
nb_integrand = cfunc(numba.float32(numba.float32, voidp))(integrand)
 params voidptr void* double TypingError: Failed at nopython (nopython frontend)
Invalid usage of getitem with parameters (void*, int64)
 * parameterized
 void* a = *((double*) params)",0,22,0,1,
771,49908399,49908595,30266,Replace attributes in Data Class objects,5,<python><python-3.7><python-dataclasses>,34,"<p>I'd like to replace the attributes of a <a href=""https://www.python.org/dev/peps/pep-0557/"" rel=""noreferrer"">dataclass</a> instance, analogous to <code>namedtuple._replace()</code>, i.e. making an altered copy of the original object:</p>

<pre><code>from dataclasses import dataclass
from collections import namedtuple

U = namedtuple(""U"", ""x"")

@dataclass
class V:
    x: int

u = U(x=1)
u_ = u._replace(x=-1)
v = V(x=1)

print(u)
print(u_)
print(v)
</code></pre>

<p>This returns:</p>

<pre><code>U(x=1)
U(x=-1)
V(x=1)
</code></pre>

<p>How can I mimic this functionality in dataclass objects?</p>
",3072050,1121,18-04-2018 20:21,18-04-2018 20:34,0,1121,24,2,12,75,"{'badge_counts': {'bronze': 24, 'silver': 12, 'gold': 2}, 'account_id': 3688960, 'is_employee': False, 'last_modified_date': 1642109100, 'last_access_date': 1710498260, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1121, 'creation_date': 1386277194, 'user_type': 'registered', 'user_id': 3072050, 'accept_rate': 75, 'location': 'Konstanz, Deutschland', 'website_url': '', 'link': 'https://stackoverflow.com/users/3072050/bayerse', 'profile_image': 'https://www.gravatar.com/avatar/9c2773cdc6cccbed838627b62cfc4a62?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'BayerSe'}","I'd like to replace the attributes of a dataclass instance, analogous to , i.e. making an altered copy of the original object: This returns: How can I mimic this functionality in dataclass objects?","namedtuple._replace() from dataclasses import dataclass
from collections import namedtuple

U = namedtuple(""U"", ""x"")

@dataclass
class V:
    x: int

u = U(x=1)
u_ = u._replace(x=-1)
v = V(x=1)

print(u)
print(u_)
print(v)
 U(x=1)
U(x=-1)
V(x=1)
",16,28,0,1,
772,50368143,50368265,30634,Create unique id based on date in Python,4,<python><python-3.x>,15,"<p>In MongoDb the automatically assigned id (ObjectId) for new documents is unique and contains within itself the timestamp of its creation. </p>

<p>Without using any library (other than built-in libs of Python), how can I create this type of concise unique ids that also somehow holds its timestamp of creation?</p>
",3100523,6929,16-05-2018 10:14,16-05-2018 10:20,0,6959,44,4,33,,"{'badge_counts': {'bronze': 44, 'silver': 33, 'gold': 4}, 'account_id': 3726852, 'is_employee': False, 'last_modified_date': 1695432300, 'last_access_date': 1700673856, 'reputation_change_year': 320, 'reputation_change_quarter': 320, 'reputation_change_month': 60, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 6959, 'creation_date': 1386960519, 'user_type': 'registered', 'user_id': 3100523, 'link': 'https://stackoverflow.com/users/3100523/jundiaius', 'profile_image': 'https://i.stack.imgur.com/bpNa9.png?s=256&g=1', 'display_name': 'Jundiaius'}","In MongoDb the automatically assigned id (ObjectId) for new documents is unique and contains within itself the timestamp of its creation. Without using any library (other than built-in libs of Python), how can I create this type of concise unique ids that also somehow holds its timestamp of creation?",,0,3,0,0,
773,49966770,49966799,9259,How to cancel the effect of numpy seed()?,1,<python><numpy><random-seed>,19,"<p>I would like to use np.random.seed() in the first part of my program and cancel it in the second part. 
Again, </p>

<ul>
<li>in the first part of my python file, I want the same random numbers to be generated at each execution</li>
<li>in the second part , I want different random numbers to be generated at each execution</li>
</ul>
",3131604,7091,22-04-2018 14:00,22-04-2018 14:04,0,7121,77,8,52,61,"{'badge_counts': {'bronze': 77, 'silver': 52, 'gold': 8}, 'account_id': 3768888, 'is_employee': False, 'last_modified_date': 1709947500, 'last_access_date': 1711078137, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 40, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7121, 'creation_date': 1387865865, 'user_type': 'registered', 'user_id': 3131604, 'accept_rate': 61, 'location': 'Paris', 'website_url': '', 'link': 'https://stackoverflow.com/users/3131604/u2gilles', 'profile_image': 'https://i.stack.imgur.com/DQKBd.jpg?s=256&g=1', 'display_name': 'u2gilles'}","I would like to use np.random.seed() in the first part of my program and cancel it in the second part. Again, in the first part of my python file, I want the same random numbers to be generated at each execution in the second part , I want different random numbers to be generated at each execution",,0,7,0,0,
774,49861405,49861569,33630,Extracting groups in a regex match,3,<python><regex>,13,"<p>I have a set of inputs.
I am trying to write a regex to match the following pattern in the input:</p>

<blockquote>
  <p><strong>Day</strong> at <strong>Time</strong> on <strong>location</strong></p>
</blockquote>

<p>Example input: </p>

<blockquote>
  <p><strong>Today</strong> at <strong>12:30 PM</strong> on <strong>Sam's living room</strong></p>
</blockquote>

<p>The bolded part of the text varies in each input. </p>

<p>I wrote the following regex:</p>

<pre><code>import regex as re

input_example = ""Today at 12:30 PM on Rakesh's Echo""
regexp_1 = re.compile(r'(\w+) at (\d+):(\d+) (\w+) on (\w+)')
re_match = regexp_1.match(input_example)
</code></pre>

<p>Which works, I am matching the correct patterns. I am now trying to extract groups from within the pattern. </p>

<p>My desired output is: </p>

<pre><code>re_match.group(1)
&gt;&gt; ""Today""
re_match.group(2)
&gt;&gt; ""12:30 PM""
re_match.group(3)
&gt;&gt; ""Sam's living room""
</code></pre>

<p>However, my current regular expression match does not give me this output. What is the correct regex that will give me the above outputs? </p>
",4168397,12296,16-04-2018 15:49,16-04-2018 15:58,0,12316,77,18,53,92,"{'badge_counts': {'bronze': 77, 'silver': 53, 'gold': 18}, 'account_id': 2076775, 'is_employee': False, 'last_modified_date': 1682732700, 'last_access_date': 1680809286, 'reputation_change_year': 190, 'reputation_change_quarter': 190, 'reputation_change_month': 40, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 12316, 'creation_date': 1370599635, 'user_type': 'registered', 'user_id': 4168397, 'accept_rate': 92, 'location': 'Chicago, IL, United States', 'website_url': 'http://www.adhikesavan.com/', 'link': 'https://stackoverflow.com/users/4168397/rakesh-adhikesavan', 'profile_image': 'https://www.gravatar.com/avatar/8e31d5748edf285187f67c329542ef96?s=256&d=identicon&r=PG', 'display_name': 'Rakesh Adhikesavan'}","I have a set of inputs. I am trying to write a regex to match the following pattern in the input: Day at Time on location Example input: Today at 12:30 PM on Sam's living room The bolded part of the text varies in each input. I wrote the following regex: Which works, I am matching the correct patterns. I am now trying to extract groups from within the pattern. My desired output is: However, my current regular expression match does not give me this output. What is the correct regex that will give me the above outputs?","import regex as re

input_example = ""Today at 12:30 PM on Rakesh's Echo""
regexp_1 = re.compile(r'(\w+) at (\d+):(\d+) (\w+) on (\w+)')
re_match = regexp_1.match(input_example)
 re_match.group(1)
&gt;&gt; ""Today""
re_match.group(2)
&gt;&gt; ""12:30 PM""
re_match.group(3)
&gt;&gt; ""Sam's living room""
",9,37,0,0,
775,50151417,50151487,3122,Numpy find indices of groups with same value,1,<python><arrays><numpy>,11,"<p>I have a numpy array of zeros and ones:</p>

<p><code>y=[1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1]</code></p>

<p>I want to calculate the indices of groups of ones (or zeros). So for the above example the result for groups of ones should be something similar to:</p>

<p><code>result=[(0,2), (8,9), (16,19)]</code></p>

<p>(How) Can I do that with numpy? I found nothing like a group-by function.</p>

<p>I experimented around with <a href=""https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ediff1d.html"" rel=""noreferrer"">np.ediff1d</a>, but couldn't figure out a good solution. Not that the array may or may not begin/end with a group of ones:</p>

<pre><code>import numpy as np

y = [1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1]
mask = np.ediff1d(y)
starts = np.where(mask &gt; 0)
ends = np.where(mask &lt; 0)
</code></pre>

<p>I also found a partial solution here:
<a href=""https://stackoverflow.com/questions/19125661/find-index-where-elements-change-value-numpy"">Find index where elements change value numpy</a></p>

<p>But that one only gives me the indices where the values change.</p>
",1842240,769,03-05-2018 09:22,03-05-2018 09:26,0,769,21,0,7,50,"{'badge_counts': {'bronze': 21, 'silver': 7, 'gold': 0}, 'account_id': 2066878, 'is_employee': False, 'last_modified_date': 1709566200, 'last_access_date': 1711018232, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 769, 'creation_date': 1353507013, 'user_type': 'registered', 'user_id': 1842240, 'accept_rate': 50, 'location': 'Switzerland', 'website_url': 'http://tiefenauer.info', 'link': 'https://stackoverflow.com/users/1842240/tiefenauer', 'profile_image': 'https://www.gravatar.com/avatar/701f29b6a18f95399687a1537ff600a1?s=256&d=identicon&r=PG', 'display_name': 'tiefenauer'}","I have a numpy array of zeros and ones: I want to calculate the indices of groups of ones (or zeros). So for the above example the result for groups of ones should be something similar to: (How) Can I do that with numpy? I found nothing like a group-by function. I experimented around with np.ediff1d, but couldn't figure out a good solution. Not that the array may or may not begin/end with a group of ones: I also found a partial solution here: Find index where elements change value numpy But that one only gives me the indices where the values change.","y=[1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1] result=[(0,2), (8,9), (16,19)] import numpy as np

y = [1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1]
mask = np.ediff1d(y)
starts = np.where(mask &gt; 0)
ends = np.where(mask &lt; 0)
",3,24,0,2,
776,48893528,49073626,9580,keras vs. tensorflow.python.keras - which one to use?,2,<python><tensorflow><pip><deep-learning><keras>,36,"<p>Which one is the recommended (or more future-proof) way to use Keras?</p>

<p>What are the advantages/disadvantages of each?</p>

<p>I guess there are more differences than simply saving one <code>pip install</code> step and writing <code>tensorflow.python.keras</code> instead of <code>keras</code>.</p>
",1866775,10290,20-02-2018 20:16,02-03-2018 16:47,10,10322,142,7,69,86,"{'badge_counts': {'bronze': 142, 'silver': 69, 'gold': 7}, 'account_id': 2098538, 'is_employee': False, 'last_modified_date': 1703300400, 'last_access_date': 1711129533, 'reputation_change_year': 242, 'reputation_change_quarter': 242, 'reputation_change_month': 82, 'reputation_change_week': 12, 'reputation_change_day': 0, 'reputation': 10322, 'creation_date': 1354289806, 'user_type': 'registered', 'user_id': 1866775, 'accept_rate': 86, 'location': 'Germany', 'website_url': 'https://github.com/dobiasd', 'link': 'https://stackoverflow.com/users/1866775/tobias-hermann', 'profile_image': 'https://i.stack.imgur.com/4K99R.jpg?s=256&g=1', 'display_name': 'Tobias Hermann'}",Which one is the recommended (or more future-proof) way to use Keras? What are the advantages/disadvantages of each? I guess there are more differences than simply saving one step and writing instead of .,pip install tensorflow.python.keras keras,-3,5,0,0,
777,49413504,49415242,16303,brew-installed Python not overriding system python,3,<python><macos><path><homebrew>,20,"<p>I just used brew to install Python 3 on OS X.  The <code>python3</code> command now starts the interpreter using brew Python 3.6, but <code>python</code> still opens the interpreter with the default system Python 2.7.  </p>

<p>My understanding was that, by default, brew Python should now override system Python.  (I.e., see <a href=""https://stackoverflow.com/questions/34984870/order-of-usr-bin-and-usr-local-bin-and-more-in-path"">Order of /usr/bin and /usr/local/bin and more in $PATH</a>). In my PATH, /usr/local/bin comes before /usr/bin, so it shouldn't be a PATH issue.  I have tried restarting Terminal, with no effect.</p>

<p>Here is my full PATH in case that is relevant.</p>

<pre><code>/Users/**/.rvm/gems/ruby-1.9.3-p362/bin:/Users/**/.rvm/gems/ruby-1.9.3-p362@global/bin:/Users/**/.rvm/rubies/ruby-1.9.3-p362/bin:/Users/**/.rvm/bin:/Users/**/.rvm/bin:/Users/**/Python/PmagPy/programs/conversion_scripts2/:/Users/**/Python/PmagPy/programs/conversion_scripts/:/Users/**/Python/PmagPy/programs:/usr/local/heroku/bin:./bin:/usr/local/sbin:/usr/local/bin:/usr/local/share/npm/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/opt/X11/bin
</code></pre>

<p>Why isn't brew Python taking precedence?  And how can I fix (or troubleshoot) this?  If I can't find another option, I can create an alias, but I prefer to understand what's happening and get to the root of the problem.</p>

<p>Update:</p>

<p>I checked out the ""possible duplicate"" question, but my issue doesn't appear to be a linking problem:</p>

<pre><code> ~ brew link --overwrite --dry-run python
Warning: Already linked: /usr/local/Cellar/python/3.6.4_4
To relink: brew unlink python &amp;&amp; brew link python
 ~ 
</code></pre>
",1945087,3120,21-03-2018 17:56,21-03-2018 19:41,0,3120,46,4,27,83,"{'badge_counts': {'bronze': 46, 'silver': 27, 'gold': 4}, 'account_id': 1586118, 'is_employee': False, 'last_modified_date': 1607614797, 'last_access_date': 1633297074, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3120, 'creation_date': 1357203423, 'user_type': 'registered', 'user_id': 1945087, 'accept_rate': 83, 'website_url': '', 'link': 'https://stackoverflow.com/users/1945087/j-jones', 'profile_image': 'https://www.gravatar.com/avatar/a821bc5d486ccd8228e6cf740e12d51f?s=256&d=identicon&r=PG', 'display_name': 'J Jones'}","I just used brew to install Python 3 on OS X. The command now starts the interpreter using brew Python 3.6, but still opens the interpreter with the default system Python 2.7. My understanding was that, by default, brew Python should now override system Python. (I.e., see Order of /usr/bin and /usr/local/bin and more in $PATH). In my PATH, /usr/local/bin comes before /usr/bin, so it shouldn't be a PATH issue. I have tried restarting Terminal, with no effect. Here is my full PATH in case that is relevant. Why isn't brew Python taking precedence? And how can I fix (or troubleshoot) this? If I can't find another option, I can create an alias, but I prefer to understand what's happening and get to the root of the problem. Update: I checked out the ""possible duplicate"" question, but my issue doesn't appear to be a linking problem:","python3 python /Users/**/.rvm/gems/ruby-1.9.3-p362/bin:/Users/**/.rvm/gems/ruby-1.9.3-p362@global/bin:/Users/**/.rvm/rubies/ruby-1.9.3-p362/bin:/Users/**/.rvm/bin:/Users/**/.rvm/bin:/Users/**/Python/PmagPy/programs/conversion_scripts2/:/Users/**/Python/PmagPy/programs/conversion_scripts/:/Users/**/Python/PmagPy/programs:/usr/local/heroku/bin:./bin:/usr/local/sbin:/usr/local/bin:/usr/local/share/npm/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/opt/X11/bin
  ~ brew link --overwrite --dry-run python
Warning: Already linked: /usr/local/Cellar/python/3.6.4_4
To relink: brew unlink python &amp;&amp; brew link python
 ~ 
",1,20,0,1,
778,48929124,48929642,23802,scikit-learn: How to compose LabelEncoder and OneHotEncoder with a pipeline?,4,<python><scikit-learn><one-hot-encoding>,13,"<p>While preprocessing the labels for a machine learning classifying task, I need to one hot encode the labels which take string values. It happens that <code>OneHotEncoder</code> from <code>sklearn.preprocessing</code> or <code>to_categorical</code> from <code>kera.np_utils</code> require <code>int</code> inputs. This means that I need to precede the one hot encoder with a <code>LabelEncoder</code>. I have done it by hand with a custom class:</p>

<pre><code>class LabelOneHotEncoder():
    def __init__(self):
        self.ohe = OneHotEncoder()
        self.le = LabelEncoder()
    def fit_transform(self, x):
        features = self.le.fit_transform( x)
        return self.ohe.fit_transform( features.reshape(-1,1))
    def transform( self, x):
        return self.ohe.transform( self.la.transform( x.reshape(-1,1)))
    def inverse_tranform( self, x):
        return self.le.inverse_transform( self.ohe.inverse_tranform( x))
    def inverse_labels( self, x):
        return self.le.inverse_transform( x)
</code></pre>

<p>I am confident there must a way of doing it within the sklearn API using a <code>sklearn.pipeline</code>, but when using:</p>

<pre><code>LabelOneHotEncoder = Pipeline( [ (""le"",LabelEncoder), (""ohe"", OneHotEncoder)])
</code></pre>

<p>I get the error <code>ValueError: bad input shape ()</code> from the <code>OneHotEncoder</code>. My guess is that the output of the <code>LabelEncoder</code> needs to be reshaped, by adding a trivial second axis. I am not sure how to add this feature though.</p>
",3275464,8058,22-02-2018 13:51,22-02-2018 14:17,0,8068,76,7,39,93,"{'badge_counts': {'bronze': 76, 'silver': 39, 'gold': 7}, 'account_id': 2146603, 'is_employee': False, 'last_modified_date': 1708134600, 'last_access_date': 1711143302, 'reputation_change_year': 6, 'reputation_change_quarter': 6, 'reputation_change_month': 8, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 8068, 'creation_date': 1391608205, 'user_type': 'registered', 'user_id': 3275464, 'accept_rate': 93, 'location': 'Between Good and Bad', 'website_url': '', 'link': 'https://stackoverflow.com/users/3275464/learning-is-a-mess', 'profile_image': 'https://i.stack.imgur.com/g6Pnl.png?s=256&g=1', 'display_name': 'Learning is a mess'}","While preprocessing the labels for a machine learning classifying task, I need to one hot encode the labels which take string values. It happens that from or from require inputs. This means that I need to precede the one hot encoder with a . I have done it by hand with a custom class: I am confident there must a way of doing it within the sklearn API using a , but when using: I get the error from the . My guess is that the output of the needs to be reshaped, by adding a trivial second axis. I am not sure how to add this feature though.","OneHotEncoder sklearn.preprocessing to_categorical kera.np_utils int LabelEncoder class LabelOneHotEncoder():
    def __init__(self):
        self.ohe = OneHotEncoder()
        self.le = LabelEncoder()
    def fit_transform(self, x):
        features = self.le.fit_transform( x)
        return self.ohe.fit_transform( features.reshape(-1,1))
    def transform( self, x):
        return self.ohe.transform( self.la.transform( x.reshape(-1,1)))
    def inverse_tranform( self, x):
        return self.le.inverse_transform( self.ohe.inverse_tranform( x))
    def inverse_labels( self, x):
        return self.le.inverse_transform( x)
 sklearn.pipeline LabelOneHotEncoder = Pipeline( [ (""le"",LabelEncoder), (""ohe"", OneHotEncoder)])
 ValueError: bad input shape () OneHotEncoder LabelEncoder",2,23,0,0,
779,49302298,49302503,23468,Docker crontab: not found,1,<python><django><docker>,18,"<p>I use <a href=""https://github.com/kraiz/django-crontab"" rel=""noreferrer"">django-crontab</a> in my project. Locally in my project work fine. But I want to use Docker. When I run Docker, I have the following error:</p>
<pre><code>/bin/sh: 1: /usr/bin/crontab: not found
</code></pre>
<p>My <code>docker-compose.yml</code></p>
<pre><code>version: '2.0'
services:
  web:
    build: .
    container_name: test_api
    volumes:
      - .:/usr/django/app/
    expose:
      - &quot;8000&quot;
    env_file: main.env
    command: bash django_run.sh

  nginx:
    build: ./nginx
    container_name: test_ng
    ports:
      - &quot;8000:8000&quot;
    volumes:
      - ./nginx/api.conf:/etc/nginx/conf.d/api.conf
      - .:/usr/django/app/
    depends_on:
      - web
    links:
      - web:web
</code></pre>
<p>django_run.sh</p>
<pre><code>#!/usr/bin/env bash
set -e

if [ &quot;$ADD_CRON&quot; == &quot;true&quot; ]; then
    python manage.py crontab show
fi

if [ &quot;$ADD_CRON&quot; == &quot;true&quot; ]; then
    python manage.py crontab add
fi

if [ &quot;$ADD_CRON&quot; == &quot;true&quot; ]; then
    python manage.py crontab show
fi

if [ &quot;$ADD_CRON&quot; == &quot;true&quot; ]; then
    python m/usr/local/bin/gunicorn ${DJANGO_APP}.wsgi:application --timeout ${GUNICORN_TIMEOUT} --keep-alive ${GUNICORN_KKEP_ALIVE} -k gevent -w ${GUNICORN_WORKERS} --threads ${GUNICORN_THREADS} -b :${GUNICORN_PORT}
</code></pre>
<p>My logs:</p>
<pre><code>test_api | /bin/sh: 1: /usr/bin/crontab: not found
test_api | Currently active jobs in crontab:
test_api | /bin/sh: 1: /usr/bin/crontab: not found
test_api | sh: 1: /usr/bin/crontab: not found
test_api |   adding cronjob: (649feb1a8431f09891b644aa4ba2075b) -&gt; ('*/1 * * * *', 'cron.cron_jubs.clear_pdf_files_scheduled_job', '&gt;&gt; /tmp/scheduled_job.log')
test_api | /bin/sh: 1: /usr/bin/crontab: not found
test_api | Currently active jobs in crontab:
test_api | [2018-03-15 14:23:41 +0000] [35] [INFO] Starting gunicorn 19.7.1
</code></pre>
",3316855,878,15-03-2018 14:37,15-03-2018 14:46,0,878,30,2,9,95,"{'badge_counts': {'bronze': 30, 'silver': 9, 'gold': 2}, 'account_id': 4030575, 'is_employee': False, 'last_modified_date': 1610791500, 'last_access_date': 1590331940, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 878, 'creation_date': 1392584011, 'user_type': 'registered', 'user_id': 3316855, 'accept_rate': 95, 'website_url': '', 'link': 'https://stackoverflow.com/users/3316855/nesalexy', 'profile_image': 'https://i.stack.imgur.com/0FNDT.jpg?s=256&g=1', 'display_name': 'nesalexy'}","I use django-crontab in my project. Locally in my project work fine. But I want to use Docker. When I run Docker, I have the following error: My django_run.sh My logs:","/bin/sh: 1: /usr/bin/crontab: not found
 docker-compose.yml version: '2.0'
services:
  web:
    build: .
    container_name: test_api
    volumes:
      - .:/usr/django/app/
    expose:
      - &quot;8000&quot;
    env_file: main.env
    command: bash django_run.sh

  nginx:
    build: ./nginx
    container_name: test_ng
    ports:
      - &quot;8000:8000&quot;
    volumes:
      - ./nginx/api.conf:/etc/nginx/conf.d/api.conf
      - .:/usr/django/app/
    depends_on:
      - web
    links:
      - web:web
 #!/usr/bin/env bash
set -e

if [ &quot;$ADD_CRON&quot; == &quot;true&quot; ]; then
    python manage.py crontab show
fi

if [ &quot;$ADD_CRON&quot; == &quot;true&quot; ]; then
    python manage.py crontab add
fi

if [ &quot;$ADD_CRON&quot; == &quot;true&quot; ]; then
    python manage.py crontab show
fi

if [ &quot;$ADD_CRON&quot; == &quot;true&quot; ]; then
    python m/usr/local/bin/gunicorn ${DJANGO_APP}.wsgi:application --timeout ${GUNICORN_TIMEOUT} --keep-alive ${GUNICORN_KKEP_ALIVE} -k gevent -w ${GUNICORN_WORKERS} --threads ${GUNICORN_THREADS} -b :${GUNICORN_PORT}
 test_api | /bin/sh: 1: /usr/bin/crontab: not found
test_api | Currently active jobs in crontab:
test_api | /bin/sh: 1: /usr/bin/crontab: not found
test_api | sh: 1: /usr/bin/crontab: not found
test_api |   adding cronjob: (649feb1a8431f09891b644aa4ba2075b) -&gt; ('*/1 * * * *', 'cron.cron_jubs.clear_pdf_files_scheduled_job', '&gt;&gt; /tmp/scheduled_job.log')
test_api | /bin/sh: 1: /usr/bin/crontab: not found
test_api | Currently active jobs in crontab:
test_api | [2018-03-15 14:23:41 +0000] [35] [INFO] Starting gunicorn 19.7.1
",45,58,0,1,
780,48190959,48191073,105435,How do I append a string to a Path?,2,<python><python-3.x><pathlib>,87,"<p>The following code:</p>

<pre><code>from pathlib import Path
Desktop = Path('Desktop')
SubDeskTop = Desktop + ""/subdir""
</code></pre>

<p>gets the following error: </p>

<pre><code>    ---------------------------------------------------------------------------
 TypeError                                 Traceback (most recent call last)
    &lt;ipython-input-4-eb31bbeb869b&gt; in &lt;module&gt;()
             1 from pathlib import Path
             2 Desktop = Path('Desktop')
       ----&gt; 3 SubDeskTop = Desktop+""/subdir""

     TypeError: unsupported operand type(s) for +: 'PosixPath' and 'str'
</code></pre>

<p>I'm clearly doing something shady here, but it raises the question:  How do I access a subdirectory of a <code>Path</code> object?</p>
",4360746,5575,10-01-2018 15:37,10-01-2018 15:41,0,5605,69,4,35,53,"{'badge_counts': {'bronze': 69, 'silver': 35, 'gold': 4}, 'account_id': 5486002, 'is_employee': False, 'last_modified_date': 1668700805, 'last_access_date': 1695806972, 'reputation_change_year': 140, 'reputation_change_quarter': 140, 'reputation_change_month': 80, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 5605, 'creation_date': 1418606642, 'user_type': 'registered', 'user_id': 4360746, 'accept_rate': 53, 'location': 'Boston, MA', 'website_url': 'http://www.raydanielmystery.com', 'link': 'https://stackoverflow.com/users/4360746/ray-salemi', 'profile_image': 'https://www.gravatar.com/avatar/1daf5b4cd599a49ba6464490ee2a7dd1?s=256&d=identicon&r=PG', 'display_name': 'Ray Salemi'}","The following code: gets the following error: I'm clearly doing something shady here, but it raises the question: How do I access a subdirectory of a object?","from pathlib import Path
Desktop = Path('Desktop')
SubDeskTop = Desktop + ""/subdir""
     ---------------------------------------------------------------------------
 TypeError                                 Traceback (most recent call last)
    &lt;ipython-input-4-eb31bbeb869b&gt; in &lt;module&gt;()
             1 from pathlib import Path
             2 Desktop = Path('Desktop')
       ----&gt; 3 SubDeskTop = Desktop+""/subdir""

     TypeError: unsupported operand type(s) for +: 'PosixPath' and 'str'
 Path",8,20,0,0,
781,48090119,48484697,27953,Jupyter notebook: TypeError: __init__() got an unexpected keyword argument 'io_loop',4,<python><jupyter-notebook>,27,"<p>I recently installed jupyter notebooks on my macbook pro.
When I create a new notebook, I see the following exception coming continuously on the terminal where I started the notebook.</p>

<pre><code>Monideeps-MacBook-Pro:PythonNotebooks monideepde$ jupyter-notebook 
[I 12:18:43.675 NotebookApp] Serving notebooks from local directory: /Users/monideepde/Documents/PythonNotebooks
[I 12:18:43.675 NotebookApp] 0 active kernels
[I 12:18:43.676 NotebookApp] The Jupyter Notebook is running at:
[I 12:18:43.676 NotebookApp] http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa
[I 12:18:43.676 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 12:18:43.677 NotebookApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa
[I 12:18:43.896 NotebookApp] Accepting one-time-token-authenticated connection from ::1
[W 12:18:44.778 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 21.10ms referer=http://localhost:8888/tree
[I 12:18:54.840 NotebookApp] Creating new notebook in 
[W 12:18:55.716 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 3.06ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
[I 12:18:55.920 NotebookApp] Kernel started: 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
[W 12:18:55.941 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20180104121843 (::1) 5.57ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
[I 12:18:56.998 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
[E 12:18:57.001 NotebookApp] Uncaught exception in /api/kernels/5e16fa4b-3e35-4265-89b0-ab36bb0573f5/channels
    Traceback (most recent call last):
      File ""/Library/Python/2.7/site-packages/tornado-5.0a1-py2.7-macosx-10.13-intel.egg/tornado/websocket.py"", line 494, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/services/kernels/handlers.py"", line 258, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/base/zmqhandlers.py"", line 168, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
[I 12:18:58.021 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
</code></pre>

<p>Python version is 2.7.</p>

<p>Any pointers to how I can resolve this?</p>
",4441239,889,04-01-2018 06:53,28-01-2018 08:57,24,889,23,4,9,40,"{'badge_counts': {'bronze': 23, 'silver': 9, 'gold': 4}, 'account_id': 5607073, 'is_employee': False, 'last_modified_date': 1680863588, 'last_access_date': 1709559143, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 889, 'creation_date': 1420937532, 'user_type': 'registered', 'user_id': 4441239, 'accept_rate': 40, 'location': 'London, UK', 'website_url': 'https://www.linkedin.com/in/monideepde/', 'link': 'https://stackoverflow.com/users/4441239/moni', 'profile_image': 'https://www.gravatar.com/avatar/5c486c57ec71ca8b4030aeaf3dcf4bf7?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Moni'}","I recently installed jupyter notebooks on my macbook pro. When I create a new notebook, I see the following exception coming continuously on the terminal where I started the notebook. Python version is 2.7. Any pointers to how I can resolve this?","Monideeps-MacBook-Pro:PythonNotebooks monideepde$ jupyter-notebook 
[I 12:18:43.675 NotebookApp] Serving notebooks from local directory: /Users/monideepde/Documents/PythonNotebooks
[I 12:18:43.675 NotebookApp] 0 active kernels
[I 12:18:43.676 NotebookApp] The Jupyter Notebook is running at:
[I 12:18:43.676 NotebookApp] http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa
[I 12:18:43.676 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 12:18:43.677 NotebookApp] 

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa
[I 12:18:43.896 NotebookApp] Accepting one-time-token-authenticated connection from ::1
[W 12:18:44.778 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 21.10ms referer=http://localhost:8888/tree
[I 12:18:54.840 NotebookApp] Creating new notebook in 
[W 12:18:55.716 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 3.06ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
[I 12:18:55.920 NotebookApp] Kernel started: 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
[W 12:18:55.941 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20180104121843 (::1) 5.57ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2
[I 12:18:56.998 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
[E 12:18:57.001 NotebookApp] Uncaught exception in /api/kernels/5e16fa4b-3e35-4265-89b0-ab36bb0573f5/channels
    Traceback (most recent call last):
      File ""/Library/Python/2.7/site-packages/tornado-5.0a1-py2.7-macosx-10.13-intel.egg/tornado/websocket.py"", line 494, in _run_callback
        result = callback(*args, **kwargs)
      File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/services/kernels/handlers.py"", line 258, in open
        super(ZMQChannelsHandler, self).open()
      File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/base/zmqhandlers.py"", line 168, in open
        self.send_ping, self.ping_interval, io_loop=loop,
    TypeError: __init__() got an unexpected keyword argument 'io_loop'
[I 12:18:58.021 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5
",27,36,0,0,
782,48929553,48929832,110982,Get hard disk size in Python,6,<python><macos><python-2.7><hard-drive>,70,"<p>I am trying to get the hard drive size and free space using Python (I am using Python 2.7 with macOS).</p>

<p>I am trying with <code>os.statvfs('/')</code>, especially with the following code.
Is it correct what I am doing? Which definition of the variable <code>giga</code> shall I use?</p>

<pre><code>import os

def get_machine_storage():
    result=os.statvfs('/')
    block_size=result.f_frsize
    total_blocks=result.f_blocks
    free_blocks=result.f_bfree
    # giga=1024*1024*1024
    giga=1000*1000*1000
    total_size=total_blocks*block_size/giga
    free_size=free_blocks*block_size/giga
    print('total_size = %s' % total_size)
    print('free_size = %s' % free_size)

get_machine_storage()
</code></pre>

<p>EDIT:
<code>statvfs</code> is deprecated in Python 3, do you know any alternative?</p>
",3343783,3268,22-02-2018 14:12,22-02-2018 14:24,0,3288,48,2,31,69,"{'badge_counts': {'bronze': 48, 'silver': 31, 'gold': 2}, 'account_id': 4070493, 'is_employee': False, 'last_modified_date': 1666222503, 'last_access_date': 1711139682, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 20, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 3288, 'creation_date': 1393173748, 'user_type': 'registered', 'user_id': 3343783, 'accept_rate': 69, 'link': 'https://stackoverflow.com/users/3343783/nisba', 'profile_image': 'https://www.gravatar.com/avatar/e58b52d86a90f4164cf151a4d7c9e8b5?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nisba'}","I am trying to get the hard drive size and free space using Python (I am using Python 2.7 with macOS). I am trying with , especially with the following code. Is it correct what I am doing? Which definition of the variable shall I use? EDIT: is deprecated in Python 3, do you know any alternative?","os.statvfs('/') giga import os

def get_machine_storage():
    result=os.statvfs('/')
    block_size=result.f_frsize
    total_blocks=result.f_blocks
    free_blocks=result.f_bfree
    # giga=1024*1024*1024
    giga=1000*1000*1000
    total_size=total_blocks*block_size/giga
    free_size=free_blocks*block_size/giga
    print('total_size = %s' % total_size)
    print('free_size = %s' % free_size)

get_machine_storage()
 statvfs",11,24,0,0,
783,49048520,49070994,5691,How to prevent PyCharm from overriding default backend as set in matplotlib?,1,<python><matplotlib><pycharm>,13,"<p>I've set my default backend to <code>Qt5Agg</code> in <code>.config/matplotlib/matplotlibrc</code>. This works if I use a regular ssh prompt and open <code>ipython</code> and run <code>import matplotlib as mpl</code></p>

<p>I correctly get: 
<code>mpl.get_backend() =&gt; ""Qt5Agg""</code></p>

<p>When I connect through pyCharm remote console, the default backend is set to <code>'module://backend_interagg'</code> which seems to be a purpose built helper extension by pycharm.</p>

<p>Using <code>mpl.use(""Qt5Agg"")</code> works as expected (i.e. correctly sets the backend and allows me to use it). </p>

<p>I'm just trying to get the default working and the pycharm remote console to properly use my rc file parameters.</p>

<p>Fwiw, I've tried actually setting my master rc file (in the site-packages directory) to have Qt5Agg and I still get this problem.</p>

<p>Also, <code>mpl.get_configdir()</code> correctly returns <code>~/.config/matplotlib</code></p>

<p>Any ideas?</p>
",4514378,1350,01-03-2018 11:16,02-03-2018 14:17,1,1350,19,0,12,90,"{'badge_counts': {'bronze': 19, 'silver': 12, 'gold': 0}, 'account_id': 5147924, 'is_employee': False, 'last_modified_date': 1666402800, 'last_access_date': 1710525752, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1350, 'creation_date': 1422702024, 'user_type': 'registered', 'user_id': 4514378, 'accept_rate': 90, 'link': 'https://stackoverflow.com/users/4514378/mb', 'profile_image': 'https://www.gravatar.com/avatar/c15246e9ad442bba56bb03094eafe3fd?s=256&d=identicon&r=PG', 'display_name': 'MB.'}","I've set my default backend to in . This works if I use a regular ssh prompt and open and run I correctly get: When I connect through pyCharm remote console, the default backend is set to which seems to be a purpose built helper extension by pycharm. Using works as expected (i.e. correctly sets the backend and allows me to use it). I'm just trying to get the default working and the pycharm remote console to properly use my rc file parameters. Fwiw, I've tried actually setting my master rc file (in the site-packages directory) to have Qt5Agg and I still get this problem. Also, correctly returns Any ideas?","Qt5Agg .config/matplotlib/matplotlibrc ipython import matplotlib as mpl mpl.get_backend() =&gt; ""Qt5Agg"" 'module://backend_interagg' mpl.use(""Qt5Agg"") mpl.get_configdir() ~/.config/matplotlib",-9,16,0,0,
784,49175961,49176557,10255,How to use additional features along with word embeddings in Keras ?,3,<python><tensorflow><machine-learning><keras><lstm>,18,"<p>I am training a LSTM model with Keras on the dataset which looks like following. The variable ""Description"" is a text field and ""Age"" and ""Gender"" are categorical and continuous fields. </p>

<pre><code>Age, Gender, Description
22, M, ""purchased a phone""
35, F, ""shopping for kids""
</code></pre>

<p>I am using word-embedding to convert the text fields to word vectors and then input it in the keras model. The code is given below: </p>

<pre><code>model = Sequential()
model.add(Embedding(word_index, 300, weights=[embedding_matrix], input_length=70, trainable=False))

model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))
model.add(Dropout(0.6))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics['accuracy'])
</code></pre>

<p>This model is running successfully but I want to input ""age"" and ""gender"" variables as features as well. What changes are required in the code to use these features as well ? </p>
",2104088,796,08-03-2018 14:50,08-03-2018 15:19,0,796,18,1,10,57,"{'badge_counts': {'bronze': 18, 'silver': 10, 'gold': 1}, 'account_id': 2408265, 'is_employee': False, 'last_modified_date': 1573681593, 'last_access_date': 1564667867, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 796, 'creation_date': 1361695887, 'user_type': 'registered', 'user_id': 2104088, 'accept_rate': 57, 'website_url': '', 'link': 'https://stackoverflow.com/users/2104088/userxxx', 'profile_image': 'https://www.gravatar.com/avatar/82de471609b8489633f6f7308954d65a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'userxxx'}","I am training a LSTM model with Keras on the dataset which looks like following. The variable ""Description"" is a text field and ""Age"" and ""Gender"" are categorical and continuous fields. I am using word-embedding to convert the text fields to word vectors and then input it in the keras model. The code is given below: This model is running successfully but I want to input ""age"" and ""gender"" variables as features as well. What changes are required in the code to use these features as well ?","Age, Gender, Description
22, M, ""purchased a phone""
35, F, ""shopping for kids""
 model = Sequential()
model.add(Embedding(word_index, 300, weights=[embedding_matrix], input_length=70, trainable=False))

model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))
model.add(Dropout(0.6))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics['accuracy'])
",9,20,0,0,
785,48706548,48706601,152929,How to free disk space taken up by (ana)conda?,2,<python><anaconda><conda>,149,"<p>I am using the conda package manager - a lot. By now I have quite a few environments and a lot of downloaded packages taking a lot of space on my SSD. An obvious path to free some of that space is to use the command</p>

<pre><code>conda env export &gt; environment.yml
</code></pre>

<p>from <a href=""https://conda.io/docs/user-guide/tasks/manage-environments.html#exporting-the-environment-file"" rel=""noreferrer"">https://conda.io/docs/user-guide/tasks/manage-environments.html#exporting-the-environment-file</a> to export which packages my old, inactive projects use(d) and then delete these environments. As far as I understand, this should free some of the space in <code>anaconda2/envs/</code>, but not in <code>anaconda2/pkgs/</code>. How do I get rid of these packages? Also, I suspect that there might be quite a few packages still sitting around, to which no environment is linking to - could that happen?</p>

<p>Questions:</p>

<ol>
<li>In general: What is the best way to reduce the space taken up by conda?</li>
<li>How do I get rid of packages that no environment is using anymore? How do I prune my packages? I am searching for something like <code>sudo apt-get autoremove</code> from Ubuntu/Debian.</li>
</ol>
",4533188,12586,09-02-2018 13:04,09-02-2018 13:07,0,12596,156,25,84,52,"{'badge_counts': {'bronze': 156, 'silver': 84, 'gold': 25}, 'account_id': 4279202, 'is_employee': False, 'last_modified_date': 1701517500, 'last_access_date': 1709026739, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 12596, 'creation_date': 1423144219, 'user_type': 'registered', 'user_id': 4533188, 'accept_rate': 52, 'website_url': '', 'link': 'https://stackoverflow.com/users/4533188/make42', 'profile_image': 'https://www.gravatar.com/avatar/d8c5bd85dc73f5783c00babbec5cef63?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Make42'}","I am using the conda package manager - a lot. By now I have quite a few environments and a lot of downloaded packages taking a lot of space on my SSD. An obvious path to free some of that space is to use the command from https://conda.io/docs/user-guide/tasks/manage-environments.html#exporting-the-environment-file to export which packages my old, inactive projects use(d) and then delete these environments. As far as I understand, this should free some of the space in , but not in . How do I get rid of these packages? Also, I suspect that there might be quite a few packages still sitting around, to which no environment is linking to - could that happen? Questions: In general: What is the best way to reduce the space taken up by conda? How do I get rid of packages that no environment is using anymore? How do I prune my packages? I am searching for something like from Ubuntu/Debian.","conda env export &gt; environment.yml
 anaconda2/envs/ anaconda2/pkgs/ sudo apt-get autoremove",-3,13,0,1,
786,49201915,49202159,14317,Debugging Scrapy Project in Visual Studio Code,8,<python><python-3.x><visual-studio><scrapy><visual-studio-code>,25,"<p>I have Visual Studio Code on a Windows Machine, on which I am making a new Scrapy Crawler. The crawler is working fine but I want to debug the code, for which I am adding this in my <code>launch.json</code> file:</p>

<pre><code>{
    ""name"": ""Scrapy with Integrated Terminal/Console"",
    ""type"": ""python"",
    ""request"": ""launch"",
    ""stopOnEntry"": true,
    ""pythonPath"": ""${config:python.pythonPath}"",
    ""program"": ""C:/Users/neo/.virtualenvs/Gers-Crawler-77pVkqzP/Scripts/scrapy.exe"",
    ""cwd"": ""${workspaceRoot}"",
    ""args"": [
        ""crawl"",
        ""amazon"",
        ""-o"",
        ""amazon.json""
    ],
    ""console"": ""integratedTerminal"",
    ""env"": {},
    ""envFile"": ""${workspaceRoot}/.env"",
    ""debugOptions"": [
        ""RedirectOutput""
    ]
}
</code></pre>

<p>But I am unable to hit any breakpoints.
PS: I took the JSON script from here: <a href=""http://www.stevetrefethen.com/blog/debugging-a-python-scrapy-project-in-vscode"" rel=""noreferrer"">http://www.stevetrefethen.com/blog/debugging-a-python-scrapy-project-in-vscode</a></p>
",3365919,774,09-03-2018 20:47,09-03-2018 21:07,0,774,24,1,10,78,"{'badge_counts': {'bronze': 24, 'silver': 10, 'gold': 1}, 'account_id': 4100158, 'is_employee': False, 'last_modified_date': 1587184637, 'last_access_date': 1697171678, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 774, 'creation_date': 1393608560, 'user_type': 'registered', 'user_id': 3365919, 'accept_rate': 78, 'location': 'Noida, Uttar Pradesh, India', 'website_url': 'http://naqushab.github.io/', 'link': 'https://stackoverflow.com/users/3365919/naqushab', 'profile_image': 'https://i.stack.imgur.com/fPKCA.jpg?s=256&g=1', 'display_name': 'naqushab'}","I have Visual Studio Code on a Windows Machine, on which I am making a new Scrapy Crawler. The crawler is working fine but I want to debug the code, for which I am adding this in my file: But I am unable to hit any breakpoints. PS: I took the JSON script from here: http://www.stevetrefethen.com/blog/debugging-a-python-scrapy-project-in-vscode","launch.json {
    ""name"": ""Scrapy with Integrated Terminal/Console"",
    ""type"": ""python"",
    ""request"": ""launch"",
    ""stopOnEntry"": true,
    ""pythonPath"": ""${config:python.pythonPath}"",
    ""program"": ""C:/Users/neo/.virtualenvs/Gers-Crawler-77pVkqzP/Scripts/scrapy.exe"",
    ""cwd"": ""${workspaceRoot}"",
    ""args"": [
        ""crawl"",
        ""amazon"",
        ""-o"",
        ""amazon.json""
    ],
    ""console"": ""integratedTerminal"",
    ""env"": {},
    ""envFile"": ""${workspaceRoot}/.env"",
    ""debugOptions"": [
        ""RedirectOutput""
    ]
}
",19,27,0,1,
787,49076378,49117044,7877,Pytest Generate Tests Based on Arguments,1,<python><pytest>,13,"<p>New to pytest...</p>

<p>I have the following in conftest.py to collect a team argument from the command line, and read in a yaml config file:</p>

<pre><code>import pytest
import yaml


def pytest_addoption(parser):
    parser.addoption(
        '--team',
        action='store',
        )


@pytest.fixture
def team(request):
    return request.config.getoption('--team')


@pytest.fixture
def conf(request):
    with open('config.yml', 'r') as f:
        conf = yaml.load(f.read())
    return conf
</code></pre>

<p>I want to run a test on each player inside conf[team]['players'] (a list). I can do so as follows in test_players.py:</p>

<pre><code>def test_players(team, conf):
    players = conf[team]['players']
    for p in players:
        assert p == something
</code></pre>

<p>This sort of works, in that it iterates through the players, but the whole thing gets treated as a single test. If anything fails the whole test is treated as failed. I'd like each player to be tested separately.</p>

<p>If I put in the players manually I can get this to work:</p>

<pre><code>import pytest

class Test_Player():
    @pytest.mark.parametrize(
        'player', [
            'player1',
            'player2',
            'player3',
        ],
    )
    def test_player(self, player):
        assert player == something
</code></pre>

<p>So my problem is that I don't know how to get conf[team] passed into pytest.mark.parametrize. I've tried these, but in both cases it complains that conf isn't defined.</p>

<pre><code>import pytest

class Test_Player():
    @pytest.mark.parametrize(
        'player', conf[team]['players'],
    )
    def test_player(self, player):
        assert player == something
</code></pre>

<p>and</p>

<pre><code>import pytest

class Test_Player(team, conf):
    @pytest.mark.parametrize(
        'player', conf[team]['players'],
    )
    def test_player(self, player):
        assert player == something
</code></pre>

<p>What am I missing here?</p>
",3449833,789,02-03-2018 19:52,05-03-2018 18:22,3,789,28,2,10,55,"{'badge_counts': {'bronze': 28, 'silver': 10, 'gold': 2}, 'account_id': 4212585, 'is_employee': False, 'last_modified_date': 1604729400, 'last_access_date': 1660844283, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 789, 'creation_date': 1395497065, 'user_type': 'registered', 'user_id': 3449833, 'accept_rate': 55, 'link': 'https://stackoverflow.com/users/3449833/user3449833', 'profile_image': 'https://www.gravatar.com/avatar/13cd20f96b6c615ac524ff3bf2c50b4d?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user3449833'}","New to pytest... I have the following in conftest.py to collect a team argument from the command line, and read in a yaml config file: I want to run a test on each player inside conf[team]['players'] (a list). I can do so as follows in test_players.py: This sort of works, in that it iterates through the players, but the whole thing gets treated as a single test. If anything fails the whole test is treated as failed. I'd like each player to be tested separately. If I put in the players manually I can get this to work: So my problem is that I don't know how to get conf[team] passed into pytest.mark.parametrize. I've tried these, but in both cases it complains that conf isn't defined. and What am I missing here?","import pytest
import yaml


def pytest_addoption(parser):
    parser.addoption(
        '--team',
        action='store',
        )


@pytest.fixture
def team(request):
    return request.config.getoption('--team')


@pytest.fixture
def conf(request):
    with open('config.yml', 'r') as f:
        conf = yaml.load(f.read())
    return conf
 def test_players(team, conf):
    players = conf[team]['players']
    for p in players:
        assert p == something
 import pytest

class Test_Player():
    @pytest.mark.parametrize(
        'player', [
            'player1',
            'player2',
            'player3',
        ],
    )
    def test_player(self, player):
        assert player == something
 import pytest

class Test_Player():
    @pytest.mark.parametrize(
        'player', conf[team]['players'],
    )
    def test_player(self, player):
        assert player == something
 import pytest

class Test_Player(team, conf):
    @pytest.mark.parametrize(
        'player', conf[team]['players'],
    )
    def test_player(self, player):
        assert player == something
",48,78,0,0,
788,49371629,49375038,7325,Converting a series of ints to strings - Why is apply much faster than astype?,2,<python><string><performance><pandas><python-internals>,30,"<p>I have a <code>pandas.Series</code> containing integers, but I need to convert these to strings for some downstream tools. So suppose I had a <code>Series</code> object:</p>

<pre><code>import numpy as np
import pandas as pd

x = pd.Series(np.random.randint(0, 100, 1000000))
</code></pre>

<p>On StackOverflow and other websites, I've seen most people argue that the best way to do this is:</p>

<pre><code>%% timeit
x = x.astype(str)
</code></pre>

<p>This takes about 2 seconds. </p>

<p>When I use <code>x = x.apply(str)</code>, it only takes 0.2 seconds.</p>

<p>Why is <code>x.astype(str)</code> so slow? Should the recommended way be <code>x.apply(str)</code>? </p>

<p>I'm mainly interested in python 3's behavior for this. </p>
",3502161,1237,19-03-2018 20:13,20-03-2018 01:33,1,1237,17,2,14,27,"{'badge_counts': {'bronze': 17, 'silver': 14, 'gold': 2}, 'account_id': 4283585, 'is_employee': False, 'last_modified_date': 1578115698, 'last_access_date': 1628079561, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1237, 'creation_date': 1396730292, 'user_type': 'registered', 'user_id': 3502161, 'accept_rate': 27, 'website_url': '', 'link': 'https://stackoverflow.com/users/3502161/none', 'profile_image': 'https://www.gravatar.com/avatar/895c4f77d531f12ff5e297c6fe6a9f24?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'none'}","I have a containing integers, but I need to convert these to strings for some downstream tools. So suppose I had a object: On StackOverflow and other websites, I've seen most people argue that the best way to do this is: This takes about 2 seconds. When I use , it only takes 0.2 seconds. Why is so slow? Should the recommended way be ? I'm mainly interested in python 3's behavior for this.","pandas.Series Series import numpy as np
import pandas as pd

x = pd.Series(np.random.randint(0, 100, 1000000))
 %% timeit
x = x.astype(str)
 x = x.apply(str) x.astype(str) x.apply(str)",-1,21,0,0,
789,48823400,48823420,24048,Pandas series to 2d array,1,<python><pandas>,12,"<p>So, I used the answer from <a href=""https://stackoverflow.com/questions/38840319/put-a-2d-array-into-a-pandas-series"">Put a 2d Array into a Pandas Series</a> to put 2D numpy array to pandas series.
In short, it is</p>

<pre><code>a = np.zeros((5,2))
s = pd.Series(list(a))
</code></pre>

<p>Now, what is the cheapest way to convert that pandas Series back to 2D array?
If I try <code>s.values</code>, I get array of arrays with <code>object</code> dtype.</p>

<p>So far I tried <code>np.vstack(s.values)</code> but it copies the data, of course.</p>
",3526116,2385,16-02-2018 09:13,16-02-2018 09:15,0,2385,18,2,12,100,"{'badge_counts': {'bronze': 18, 'silver': 12, 'gold': 2}, 'account_id': 4316423, 'is_employee': False, 'last_modified_date': 1699185300, 'last_access_date': 1709883417, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2385, 'creation_date': 1397284525, 'user_type': 'registered', 'user_id': 3526116, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/3526116/crayxt', 'profile_image': 'https://www.gravatar.com/avatar/98979e367aa641c0e874a927c665830c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'crayxt'}","So, I used the answer from Put a 2d Array into a Pandas Series to put 2D numpy array to pandas series. In short, it is Now, what is the cheapest way to convert that pandas Series back to 2D array? If I try , I get array of arrays with dtype. So far I tried but it copies the data, of course.","a = np.zeros((5,2))
s = pd.Series(list(a))
 s.values object np.vstack(s.values)",-2,11,0,1,
790,49181373,49206054,3895,"Create a text file, and email it without saving it locally",2,<python><email><text>,12,"<p>I would like to create a text file in Python, write something to it, and then e-mail it out (put <code>test.txt</code> as an email attachment). </p>

<p>However, I cannot save this file locally. Does anyone know how to go about doing this?</p>

<p>As soon as I open the text file to write in, it is saved locally on my computer.</p>

<pre><code>f = open(""test.txt"",""w+"")
</code></pre>

<p>I am using <code>smtplib</code> and <code>MIMEMultipart</code> to send the mail.</p>
",3558939,155,08-03-2018 19:51,10-03-2018 06:25,2,155,8,0,1,75,"{'badge_counts': {'bronze': 8, 'silver': 1, 'gold': 0}, 'account_id': 4362253, 'is_employee': False, 'last_modified_date': 1703304900, 'last_access_date': 1542052266, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 155, 'creation_date': 1398142236, 'user_type': 'registered', 'user_id': 3558939, 'accept_rate': 75, 'link': 'https://stackoverflow.com/users/3558939/user3558939', 'profile_image': 'https://graph.facebook.com/510379556/picture?type=large', 'display_name': 'user3558939'}","I would like to create a text file in Python, write something to it, and then e-mail it out (put as an email attachment). However, I cannot save this file locally. Does anyone know how to go about doing this? As soon as I open the text file to write in, it is saved locally on my computer. I am using and to send the mail.","test.txt f = open(""test.txt"",""w+"")
 smtplib MIMEMultipart",-3,10,0,0,
791,49749981,50335132,40235,"Cannot find declaration to go to, error in PyCharm",6,<python><pycharm>,32,"<p>I changed my project code from Python 2.7 to 3.x.</p>
<p>After these changes when I hover over any method and press <kbd>Ctrl</kbd> I get a message:</p>
<blockquote>
<p>cannot find declaration to go to</p>
</blockquote>
<p>I'm trying to update PyCharm from 2017.3 to 18.1, I removed the <code>.idea</code> directory but my issue still persists.</p>
",3559620,434,10-04-2018 09:23,14-05-2018 16:37,34,434,12,1,5,,"{'badge_counts': {'bronze': 12, 'silver': 5, 'gold': 1}, 'account_id': 4363119, 'is_employee': False, 'last_modified_date': 1575637500, 'last_access_date': 1699622838, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 434, 'creation_date': 1398156308, 'user_type': 'registered', 'user_id': 3559620, 'website_url': '', 'link': 'https://stackoverflow.com/users/3559620/qchar90', 'profile_image': 'https://www.gravatar.com/avatar/9e154df12402898fac7afbef5c42521c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'qchar90'}","I changed my project code from Python 2.7 to 3.x. After these changes when I hover over any method and press Ctrl I get a message: cannot find declaration to go to I'm trying to update PyCharm from 2017.3 to 18.1, I removed the directory but my issue still persists.",.idea,-1,6,0,0,
792,48829993,48830780,137419,GroupBy column and filter rows with maximum value in Pyspark,5,<python><apache-spark><pyspark><apache-spark-sql>,63,"<p>I am almost certain this has been asked before, but <a href=""https://www.google.de/search?q=pyspark%20filter%20by%20max%20column%20value%20site:stackoverflow.com&amp;rlz=1C1CHBF_deDE735DE735&amp;sa=X&amp;ved=0ahUKEwiFofPX3qrZAhUByqQKHUvvAN8QrQIINCgEMAA&amp;biw=1536&amp;bih=760&amp;dpr=1.25"" rel=""noreferrer"">a search through stackoverflow</a> did not answer my question. Not a duplicate of <a href=""https://stackoverflow.com/questions/35218882/find-maximum-row-per-group-in-spark-dataframe"">[2]</a> since I want the maximum value, not the most frequent item. I am new to pyspark and trying to do something really simple: I want to groupBy column ""A"" and then only keep the row of each group that has the maximum value in column ""B"". Like this:</p>

<pre><code>df_cleaned = df.groupBy(""A"").agg(F.max(""B""))
</code></pre>

<p>Unfortunately, this throws away all other columns - df_cleaned only contains the columns ""A"" and the max value of B. How do I instead keep the rows? (""A"", ""B"", ""C""...)</p>
",4629950,4876,16-02-2018 15:31,16-02-2018 16:17,0,4896,73,5,40,74,"{'badge_counts': {'bronze': 73, 'silver': 40, 'gold': 5}, 'account_id': 5880344, 'is_employee': False, 'last_modified_date': 1685000701, 'last_access_date': 1709215900, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4896, 'creation_date': 1425425047, 'user_type': 'registered', 'user_id': 4629950, 'accept_rate': 74, 'link': 'https://stackoverflow.com/users/4629950/thomas', 'profile_image': 'https://www.gravatar.com/avatar/9ae623322cb5b2ff9e5d21ddfab31733?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Thomas'}","I am almost certain this has been asked before, but a search through stackoverflow did not answer my question. Not a duplicate of [2] since I want the maximum value, not the most frequent item. I am new to pyspark and trying to do something really simple: I want to groupBy column ""A"" and then only keep the row of each group that has the maximum value in column ""B"". Like this: Unfortunately, this throws away all other columns - df_cleaned only contains the columns ""A"" and the max value of B. How do I instead keep the rows? (""A"", ""B"", ""C""...)","df_cleaned = df.groupBy(""A"").agg(F.max(""B""))
",0,6,0,2,
793,50353544,50353545,4617,Celery + Redis tasks in different files,1,<python><celery>,14,"<p>When I run Celery from the command line I can see only the tasks that are in the same file as the Celery object but not those in other files.</p>

<p>The structure of the project is the following:</p>

<pre><code>celery_test
    celery_tasks
        __init__.py
        celery_app.py
        async
            __init__.py
            tasks.py
        marker
            __init__.py
            tasks.py
</code></pre>

<p>The content of the files is as follows</p>

<pre><code>celery_app.py

from __future__ import absolute_import
from celery import Celery

celery_application = Celery('celery_test', backend='redis://localhost', broker='redis://localhost')

@celery_application.task
def test_celery():
    print 4
</code></pre>

<p>And any of the <code>tasks.py</code> files has something like this</p>

<pre><code>async/tasks.py

from __future__ import absolute_import
import time

from celery_tasks.celery_app import celery_application


@celery_application.task
def async_test():
    print 'Start async_test'
    time.sleep(3)
    print 'Finish async_test'
</code></pre>

<p>When I run Celery as follows</p>

<pre><code>celery --app=celery_tasks.celery_app:celery_application worker -l debug
</code></pre>

<p>I get the following</p>

<pre><code> -------------- celery@LAPTOP-HCR4G00Q v3.1.25 (Cipater)
---- **** -----
--- * ***  * -- Windows-10-10.0.16299
-- * - **** ---
- ** ---------- [config]
- ** ---------- .&gt; app:         celery_test:0x6ff3f28
- ** ---------- .&gt; transport:   redis://localhost:6379//
- ** ---------- .&gt; results:     redis://localhost/
- *** --- * --- .&gt; concurrency: 4 (prefork)
-- ******* ----
--- ***** ----- [queues]
 -------------- .&gt; celery           exchange=celery(direct) key=celery


[tasks]
  . celery.backend_cleanup
  . celery.chain
  . celery.chord
  . celery.chord_unlock
  . celery.chunks
  . celery.group
  . celery.map
  . celery.starmap
  . celery_tasks.celery_app.test_celery
</code></pre>

<p>that is just the task that is in the same file as the application.</p>

<p>Any suggestions on how to solve it? I really need to separate the tasks by topics because they are many so that they are in a single file.</p>
",3591470,898,15-05-2018 15:04,15-05-2018 15:04,0,898,30,2,12,64,"{'badge_counts': {'bronze': 30, 'silver': 12, 'gold': 2}, 'account_id': 4407537, 'is_employee': False, 'last_modified_date': 1696035900, 'last_access_date': 1710914309, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 898, 'creation_date': 1398910450, 'user_type': 'registered', 'user_id': 3591470, 'accept_rate': 64, 'location': 'Barcelona, Spain', 'website_url': '', 'link': 'https://stackoverflow.com/users/3591470/fferrin', 'profile_image': 'https://lh4.googleusercontent.com/-0ehuqmWbVAc/AAAAAAAAAAI/AAAAAAAAAEY/bMXqcomDArM/photo.jpg?sz=256', 'display_name': 'fferrin'}",When I run Celery from the command line I can see only the tasks that are in the same file as the Celery object but not those in other files. The structure of the project is the following: The content of the files is as follows And any of the files has something like this When I run Celery as follows I get the following that is just the task that is in the same file as the application. Any suggestions on how to solve it? I really need to separate the tasks by topics because they are many so that they are in a single file.,"celery_test
    celery_tasks
        __init__.py
        celery_app.py
        async
            __init__.py
            tasks.py
        marker
            __init__.py
            tasks.py
 celery_app.py

from __future__ import absolute_import
from celery import Celery

celery_application = Celery('celery_test', backend='redis://localhost', broker='redis://localhost')

@celery_application.task
def test_celery():
    print 4
 tasks.py async/tasks.py

from __future__ import absolute_import
import time

from celery_tasks.celery_app import celery_application


@celery_application.task
def async_test():
    print 'Start async_test'
    time.sleep(3)
    print 'Finish async_test'
 celery --app=celery_tasks.celery_app:celery_application worker -l debug
  -------------- celery@LAPTOP-HCR4G00Q v3.1.25 (Cipater)
---- **** -----
--- * ***  * -- Windows-10-10.0.16299
-- * - **** ---
- ** ---------- [config]
- ** ---------- .&gt; app:         celery_test:0x6ff3f28
- ** ---------- .&gt; transport:   redis://localhost:6379//
- ** ---------- .&gt; results:     redis://localhost/
- *** --- * --- .&gt; concurrency: 4 (prefork)
-- ******* ----
--- ***** ----- [queues]
 -------------- .&gt; celery           exchange=celery(direct) key=celery


[tasks]
  . celery.backend_cleanup
  . celery.chain
  . celery.chord
  . celery.chord_unlock
  . celery.chunks
  . celery.group
  . celery.map
  . celery.starmap
  . celery_tasks.celery_app.test_celery
",52,83,0,0,
794,49606482,49607525,39745,How to resolve runtime error due to size mismatch in PyTorch?,3,<python><pytorch><autoencoder>,13,"<p>I am trying to implement a simple autoencoder using <code>PyTorch</code>. My dataset consists of 256 x 256 x 3 images. I have built a <code>torch.utils.data.dataloader.DataLoader</code> object which has the image stored as tensor. When I run the autoencoder, I get a runtime error:</p>

<blockquote>
  <p>size mismatch, m1: [76800 x 256], m2: [784 x 128] at
  /Users/soumith/minicondabuild3/conda-bld/pytorch_1518371252923/work/torch/lib/TH/generic/THTensorMath.c:1434</p>
</blockquote>

<p>These are my hyperparameters:</p>

<pre><code>batch_size=100,
learning_rate = 1e-3,
num_epochs = 100
</code></pre>

<p>Following is the architecture of my auto-encoder:</p>

<pre><code>class autoencoder(nn.Module):
    def __init__(self):
        super(autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(3*256*256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(True),
            nn.Linear(64, 12),
            nn.ReLU(True),
            nn.Linear(12, 3))

        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.ReLU(True),
            nn.Linear(12, 64),
            nn.ReLU(True),
            nn.Linear(64, 128),
            nn.Linear(128, 3*256*256),
            nn.ReLU())

def forward(self, x):
    x = self.encoder(x)
    #x = self.decoder(x)
    return x
</code></pre>

<p>This is the code I used to run the model:</p>

<pre><code>for epoch in range(num_epochs):
for data in dataloader:
    img = data['image']
    img = Variable(img)
    # ===================forward=====================
    output = model(img)
    loss = criterion(output, img)
    # ===================backward====================
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
# ===================log========================
print('epoch [{}/{}], loss:{:.4f}'
      .format(epoch+1, num_epochs, loss.data[0]))
if epoch % 10 == 0:
    pic = show_img(output.cpu().data)
    save_image(pic, './dc_img/image_{}.jpg'.format(epoch))
</code></pre>
",3619334,419,02-04-2018 06:39,02-04-2018 08:01,0,419,14,1,5,71,"{'badge_counts': {'bronze': 14, 'silver': 5, 'gold': 1}, 'account_id': 4446108, 'is_employee': False, 'last_modified_date': 1580888588, 'last_access_date': 1694518214, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 419, 'creation_date': 1399618661, 'user_type': 'registered', 'user_id': 3619334, 'accept_rate': 71, 'location': 'Mumbai, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/3619334/shreyas', 'profile_image': 'https://i.stack.imgur.com/lFvky.jpg?s=256&g=1', 'display_name': 'Shreyas'}","I am trying to implement a simple autoencoder using . My dataset consists of 256 x 256 x 3 images. I have built a object which has the image stored as tensor. When I run the autoencoder, I get a runtime error: size mismatch, m1: [76800 x 256], m2: [784 x 128] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518371252923/work/torch/lib/TH/generic/THTensorMath.c:1434 These are my hyperparameters: Following is the architecture of my auto-encoder: This is the code I used to run the model:","PyTorch torch.utils.data.dataloader.DataLoader batch_size=100,
learning_rate = 1e-3,
num_epochs = 100
 class autoencoder(nn.Module):
    def __init__(self):
        super(autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(3*256*256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(True),
            nn.Linear(64, 12),
            nn.ReLU(True),
            nn.Linear(12, 3))

        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.ReLU(True),
            nn.Linear(12, 64),
            nn.ReLU(True),
            nn.Linear(64, 128),
            nn.Linear(128, 3*256*256),
            nn.ReLU())

def forward(self, x):
    x = self.encoder(x)
    #x = self.decoder(x)
    return x
 for epoch in range(num_epochs):
for data in dataloader:
    img = data['image']
    img = Variable(img)
    # ===================forward=====================
    output = model(img)
    loss = criterion(output, img)
    # ===================backward====================
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
# ===================log========================
print('epoch [{}/{}], loss:{:.4f}'
      .format(epoch+1, num_epochs, loss.data[0]))
if epoch % 10 == 0:
    pic = show_img(output.cpu().data)
    save_image(pic, './dc_img/image_{}.jpg'.format(epoch))
",40,63,0,0,
795,49389068,49393485,2101,Large Performance difference when summing ints vs floats in Cython vs NumPy,1,<python><numpy><cython>,15,"<p>I am summing each element in a 1D array using either Cython or NumPy. When summing <strong>integers</strong> Cython is ~20% faster. When summing <strong>floats</strong>, Cython is ~2.5x <strong>slower</strong>. Below are the two simple functions used.</p>

<pre><code>#cython: boundscheck=False
#cython: wraparound=False

def sum_int(ndarray[np.int64_t] a):
    cdef:
        Py_ssize_t i, n = len(a)
        np.int64_t total = 0

    for i in range(n):
        total += a[i]
    return total 

def sum_float(ndarray[np.float64_t] a):
    cdef:
        Py_ssize_t i, n = len(a)
        np.float64_t total = 0

    for i in range(n):
        total += a[i]
    return total
</code></pre>

<h3>Timings</h3>

<p>Create two arrays of 1 million elements each:</p>

<pre><code>a_int = np.random.randint(0, 100, 10**6)
a_float = np.random.rand(10**6)

%timeit sum_int(a_int)
394 µs ± 30 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

%timeit a_int.sum()
490 µs ± 34.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

%timeit sum_float(a_float)
982 µs ± 10.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

%timeit a_float.sum()
383 µs ± 4.42 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre>

<h3>Additional points</h3>

<ul>
<li>NumPy is outperforming (by quite a large margin) with floats and even beats its own integer sum.</li>
<li>The performance difference for <code>sum_float</code> is the same with the <code>boundscheck</code> and <code>wraparound</code> directives missing. Why?</li>
<li>Converting the integer numpy array in <code>sum_int</code> to a C pointer (<code>np.int64_t *arr = &lt;np.int64_t *&gt; a.data</code>) improves performance by an additional 25%. Doing so for the floats does nothing</li>
</ul>

<h3>Main Question</h3>

<p>How can I get the same performance in Cython with floats that I do with integers?</p>

<h3>EDIT - Just Counting is Slow?!?</h3>

<p>I wrote an even simpler function that just counts the number of iterations. The first stores the count as an int, the latter as a double.</p>

<pre><code>def count_int():
    cdef:
        Py_ssize_t i, n = 1000000
        int ct=0

    for i in range(n):
        ct += 1
    return ct

def count_double():
    cdef:
        Py_ssize_t i, n = 1000000
        double ct=0

    for i in range(n):
        ct += 1
    return ct
</code></pre>

<h3>Timings of counting</h3>

<p>I ran these just once (afraid of caching). No idea if the loop is actually being executed for the integer, but <code>count_double</code> has the <strong>same</strong> performance as the <code>sum_float</code> from above. This is crazy...</p>

<pre><code>%timeit -n 1 -r 1 count_int()
1.1 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)

%timeit -n 1 -r 1 count_double()
971 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
</code></pre>
",3707607,60340,20-03-2018 16:08,20-03-2018 20:22,0,60460,138,19,133,71,"{'badge_counts': {'bronze': 138, 'silver': 133, 'gold': 19}, 'account_id': 4566798, 'is_employee': False, 'last_modified_date': 1688505007, 'last_access_date': 1711059084, 'reputation_change_year': 570, 'reputation_change_quarter': 570, 'reputation_change_month': 180, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 60460, 'creation_date': 1401893779, 'user_type': 'registered', 'user_id': 3707607, 'accept_rate': 71, 'location': 'Houston, TX, USA', 'website_url': 'http://www.dunderdata.com', 'link': 'https://stackoverflow.com/users/3707607/ted-petrou', 'profile_image': 'https://www.gravatar.com/avatar/6af07819191e479e05e35e37774fdbe0?s=256&d=identicon&r=PG', 'display_name': 'Ted Petrou'}","I am summing each element in a 1D array using either Cython or NumPy. When summing integers Cython is ~20% faster. When summing floats, Cython is ~2.5x slower. Below are the two simple functions used. Timings Create two arrays of 1 million elements each: Additional points NumPy is outperforming (by quite a large margin) with floats and even beats its own integer sum. The performance difference for is the same with the and directives missing. Why? Converting the integer numpy array in to a C pointer () improves performance by an additional 25%. Doing so for the floats does nothing Main Question How can I get the same performance in Cython with floats that I do with integers? EDIT - Just Counting is Slow?!? I wrote an even simpler function that just counts the number of iterations. The first stores the count as an int, the latter as a double. Timings of counting I ran these just once (afraid of caching). No idea if the loop is actually being executed for the integer, but has the same performance as the from above. This is crazy...","#cython: boundscheck=False
#cython: wraparound=False

def sum_int(ndarray[np.int64_t] a):
    cdef:
        Py_ssize_t i, n = len(a)
        np.int64_t total = 0

    for i in range(n):
        total += a[i]
    return total 

def sum_float(ndarray[np.float64_t] a):
    cdef:
        Py_ssize_t i, n = len(a)
        np.float64_t total = 0

    for i in range(n):
        total += a[i]
    return total
 a_int = np.random.randint(0, 100, 10**6)
a_float = np.random.rand(10**6)

%timeit sum_int(a_int)
394 µs ± 30 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

%timeit a_int.sum()
490 µs ± 34.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

%timeit sum_float(a_float)
982 µs ± 10.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

%timeit a_float.sum()
383 µs ± 4.42 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
 sum_float boundscheck wraparound sum_int np.int64_t *arr = &lt;np.int64_t *&gt; a.data def count_int():
    cdef:
        Py_ssize_t i, n = 1000000
        int ct=0

    for i in range(n):
        ct += 1
    return ct

def count_double():
    cdef:
        Py_ssize_t i, n = 1000000
        double ct=0

    for i in range(n):
        ct += 1
    return ct
 count_double sum_float %timeit -n 1 -r 1 count_int()
1.1 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)

%timeit -n 1 -r 1 count_double()
971 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
",45,89,0,0,
796,50278029,55019719,69761,How to import a .pyd file as a python module?,4,<python><windows><pycharm><pyd>,18,"<p>I am using PyCharm. I have a python script in the following location:</p>

<pre><code>C:\Users\XYZ\PycharmProjects\Project1\playground.py
</code></pre>

<p><code>playground.py</code> only has a line of code as shown below:</p>

<pre><code>import PyTbl
</code></pre>

<p>In the <code>Project1</code> folder there's another file:</p>

<pre><code>C:\Users\XYZ\PycharmProjects\Project1\PyTbl.pyd
</code></pre>

<p>When I run the Python script <code>playground.py</code> I get the following error:</p>

<pre><code>ImportError: numpy.core.multiarray failed to import
Traceback (most recent call last):
  File ""C:/Users/XYZ/PycharmProjects/Project1/playground.py"", line 1, in &lt;module&gt;
    import PyTbl
SystemError: initialization of PyTbl raised unreported exception
</code></pre>

<p>If I hover my mouse over the line of Python code in <code>playground.py</code> in the PyCharm editor I get the following error message:</p>

<pre><code>""No module named PyTbl""
</code></pre>

<p>Any idea how should I import a <code>.pyd</code> file into a Python script?</p>
",3719399,908,10-05-2018 17:04,06-03-2019 09:32,300,908,22,2,8,50,"{'badge_counts': {'bronze': 22, 'silver': 8, 'gold': 2}, 'account_id': 3530579, 'is_employee': False, 'last_modified_date': 1703299200, 'last_access_date': 1711027833, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 908, 'creation_date': 1402220338, 'user_type': 'registered', 'user_id': 3719399, 'accept_rate': 50, 'location': 'London, United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/3719399/chengcj', 'profile_image': 'https://www.gravatar.com/avatar/25ecaf9b44e9adffb693dd12bae16e50?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'chengcj'}",I am using PyCharm. I have a python script in the following location: only has a line of code as shown below: In the folder there's another file: When I run the Python script I get the following error: If I hover my mouse over the line of Python code in in the PyCharm editor I get the following error message: Any idea how should I import a file into a Python script?,"C:\Users\XYZ\PycharmProjects\Project1\playground.py
 playground.py import PyTbl
 Project1 C:\Users\XYZ\PycharmProjects\Project1\PyTbl.pyd
 playground.py ImportError: numpy.core.multiarray failed to import
Traceback (most recent call last):
  File ""C:/Users/XYZ/PycharmProjects/Project1/playground.py"", line 1, in &lt;module&gt;
    import PyTbl
SystemError: initialization of PyTbl raised unreported exception
 playground.py ""No module named PyTbl""
 .pyd",-1,30,0,0,
797,48735671,48736702,42694,Use Vim Retab to solve TabError: inconsistent use of tabs and spaces in indentation?,3,<python><vim>,25,"<p>Apologize for the newbie question, but I have read the manual, <a href=""https://stackoverflow.com/questions/1024435/how-to-fix-python-indentation"">this</a> question, and tried several times without results I expected. </p>

<p>So I was using vim to edit a file (attached). But when running, I got the TabError: inconsistent use of tabs and spaces in indentation error.</p>

<p>Here is what I have tried:</p>

<ul>
<li>Open the file with Vim. type <code>:retab</code>, and <code>:x</code>. Run the file again. Still got the TabError message.</li>
<li>Open the file again and type <code>:retab!</code> and <code>:x</code>. Run the file again. Still got the TabError message.</li>
<li>Open the file again and type <code>:retab! 4</code> and <code>:x</code>. Run the file again. This time it works but I have no idea why? Plus, in the files indentation seems excessively long. (I read <a href=""https://stackoverflow.com/questions/5685406/inconsistent-use-of-tabs-and-spaces-in-indentation"">here</a> that the editor might display 8 spaces for a tab)</li>
</ul>

<p><strong>My questions are:</strong></p>

<ul>
<li><p>What does <code>:retab</code>, <code>:retab!</code>, and <code>:retab! 4</code> mean?</p></li>
<li><p>Why doesn't <code>:retab</code> work on my file?  </p>

<pre><code>#!/usr/bin/env python
#Reduce function for computing matrix multiply A*B    
#Input arguments:
#variable n should be set to the inner dimension of the matrix product (i.e., the number of columns of A/rows of B) 
import sys
import string
import numpy

#number of columns of A/rows of B
n = int(sys.argv[1]) 

#Create data structures to hold the current row/column values (if needed; your code goes here)

currentkey = None
alist = [] # list for elelents in  A
blist = [] # list for elements in B
# input comes from STDIN (stream data that goes to the program)
for line in sys.stdin:
    #Remove leading and trailing whitespace
    line = line.strip()
    #Get key/value 
    key, value = line.split('\t',1)
    print(key, value)
    #Parse key/value input (your code goes here)
    key = (key.split(',', 1)[0], key.split(',',1)[1])   
    value = (value.split(',', 1)[0], value.split(',',1)[1], value.split(',',1)[2])  
    #If we are still on the same key...
    if key==currentkey:
        #Process key/value pair (your code goes here)
        # store all values in a lisl
        if value[0]=='A':
        alist.append([value[1], value[2]])
        else:
        blist.append([value[1], value[2]])
        #Otherwise, if this is a new key...
    else:
    #If this is a new key and not the first key we've seen, i.e. currentkey!=None
        if currentkey:
    #compute/output result to STDOUT (your code goes here)
        alist = sorted(alist)
        blist = sorted(blist)
        newlist = [a[1]*b[1] for a,b in zip(alist, blist)]
        res = newlist.sum() 
        print(currentkey, res)
        currentkey = key
        #Process input for new key (your code goes here)
</code></pre></li>
</ul>
",3736306,4731,11-02-2018 19:41,11-02-2018 21:33,0,4741,50,8,30,100,"{'badge_counts': {'bronze': 50, 'silver': 30, 'gold': 8}, 'account_id': 4607169, 'is_employee': False, 'last_modified_date': 1607614630, 'last_access_date': 1711139978, 'reputation_change_year': 130, 'reputation_change_quarter': 130, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4741, 'creation_date': 1402631331, 'user_type': 'registered', 'user_id': 3736306, 'accept_rate': 100, 'location': 'Bay Area, CA, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/3736306/yuqli', 'profile_image': 'https://www.gravatar.com/avatar/b8c51590728edce8b9218094a16c1079?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'yuqli'}","Apologize for the newbie question, but I have read the manual, this question, and tried several times without results I expected. So I was using vim to edit a file (attached). But when running, I got the TabError: inconsistent use of tabs and spaces in indentation error. Here is what I have tried: Open the file with Vim. type , and . Run the file again. Still got the TabError message. Open the file again and type and . Run the file again. Still got the TabError message. Open the file again and type and . Run the file again. This time it works but I have no idea why? Plus, in the files indentation seems excessively long. (I read here that the editor might display 8 spaces for a tab) My questions are: What does , , and mean? Why doesn't work on my file?",":retab :x :retab! :x :retab! 4 :x :retab :retab! :retab! 4 :retab #!/usr/bin/env python
#Reduce function for computing matrix multiply A*B    
#Input arguments:
#variable n should be set to the inner dimension of the matrix product (i.e., the number of columns of A/rows of B) 
import sys
import string
import numpy

#number of columns of A/rows of B
n = int(sys.argv[1]) 

#Create data structures to hold the current row/column values (if needed; your code goes here)

currentkey = None
alist = [] # list for elelents in  A
blist = [] # list for elements in B
# input comes from STDIN (stream data that goes to the program)
for line in sys.stdin:
    #Remove leading and trailing whitespace
    line = line.strip()
    #Get key/value 
    key, value = line.split('\t',1)
    print(key, value)
    #Parse key/value input (your code goes here)
    key = (key.split(',', 1)[0], key.split(',',1)[1])   
    value = (value.split(',', 1)[0], value.split(',',1)[1], value.split(',',1)[2])  
    #If we are still on the same key...
    if key==currentkey:
        #Process key/value pair (your code goes here)
        # store all values in a lisl
        if value[0]=='A':
        alist.append([value[1], value[2]])
        else:
        blist.append([value[1], value[2]])
        #Otherwise, if this is a new key...
    else:
    #If this is a new key and not the first key we've seen, i.e. currentkey!=None
        if currentkey:
    #compute/output result to STDOUT (your code goes here)
        alist = sorted(alist)
        blist = sorted(blist)
        newlist = [a[1]*b[1] for a,b in zip(alist, blist)]
        res = newlist.sum() 
        print(currentkey, res)
        currentkey = key
        #Process input for new key (your code goes here)
",35,66,0,2,
798,50362802,50368006,4449,How to type generator function in Cython?,1,<python><cython>,17,"<p>If I have a generator function in Python, say:</p>

<pre><code>def gen(x):
    for i in range(x):
        yield(i ** 2)
</code></pre>

<p>How do I declare that the output data type is <code>int</code> in Cython? Is it even worth while?</p>

<p>Thanks.</p>

<p><strong>Edit:</strong> I read mentions of (async) generators being implemented in the changelog: <a href=""http://cython.readthedocs.io/en/latest/src/changes.html?highlight=generators#id23"" rel=""noreferrer"">http://cython.readthedocs.io/en/latest/src/changes.html?highlight=generators#id23</a> </p>

<p>However there is no documentation about how to use them. Is it because they are supported but there is no particular advantage in using them with Cython or no optimization possible?</p>
",3758232,810,16-05-2018 05:07,16-05-2018 10:07,0,810,22,0,6,100,"{'badge_counts': {'bronze': 22, 'silver': 6, 'gold': 0}, 'account_id': 4637360, 'is_employee': False, 'last_modified_date': 1653046800, 'last_access_date': 1711027209, 'reputation_change_year': 2, 'reputation_change_quarter': 2, 'reputation_change_month': 2, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 810, 'creation_date': 1403215155, 'user_type': 'registered', 'user_id': 3758232, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/3758232/user3758232', 'profile_image': 'https://www.gravatar.com/avatar/e4331ac7e59c1fb06be859ac44adee33?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user3758232'}","If I have a generator function in Python, say: How do I declare that the output data type is in Cython? Is it even worth while? Thanks. Edit: I read mentions of (async) generators being implemented in the changelog: http://cython.readthedocs.io/en/latest/src/changes.html?highlight=generators#id23 However there is no documentation about how to use them. Is it because they are supported but there is no particular advantage in using them with Cython or no optimization possible?","def gen(x):
    for i in range(x):
        yield(i ** 2)
 int",1,14,0,1,
799,48586738,48587137,27644,Seaborn heatmap - colorbar label font size,2,<python><matplotlib><seaborn>,12,"<p>How do I set the font size of the colorbar label?</p>

<pre><code>ax=sns.heatmap(table, vmin=60, vmax=100, xticklabels=[4,8,16,32,64,128],yticklabels=[2,4,6,8], cmap=""PuBu"",linewidths=.0, 
        annot=True,cbar_kws={'label': 'Accuracy %'}
</code></pre>

<p><a href=""https://i.stack.imgur.com/q774U.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/q774U.png"" alt=""enter image description here""></a></p>
",4907108,738,02-02-2018 16:17,02-02-2018 16:38,0,738,29,3,13,54,"{'badge_counts': {'bronze': 29, 'silver': 13, 'gold': 3}, 'account_id': 6317822, 'is_employee': False, 'last_modified_date': 1626481800, 'last_access_date': 1710689069, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 738, 'creation_date': 1431789535, 'user_type': 'registered', 'user_id': 4907108, 'accept_rate': 54, 'website_url': '', 'link': 'https://stackoverflow.com/users/4907108/alessandro-gaballo', 'profile_image': 'https://www.gravatar.com/avatar/39c24657d3eccf0224e33cba463330f1?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Alessandro Gaballo'}",How do I set the font size of the colorbar label?,"ax=sns.heatmap(table, vmin=60, vmax=100, xticklabels=[4,8,16,32,64,128],yticklabels=[2,4,6,8], cmap=""PuBu"",linewidths=.0, 
        annot=True,cbar_kws={'label': 'Accuracy %'}
",1,7,1,1,
800,48161445,48161670,1461,Changing the fill_values in a SparseDataFrame - replace throws TypeError,2,<python><pandas><sparse-matrix><sparse-dataframe>,14,"<p>Current pandas version: <code>0.22</code></p>

<hr>

<p>I have a SparseDataFrame.</p>

<pre><code>A = pd.SparseDataFrame(
    [['a',0,0,'b'],
     [0,0,0,'c'],
     [0,0,0,0],
     [0,0,0,'a']])
</code></pre>

<p></p>

<pre><code>A

   0  1  2  3
0  a  0  0  b
1  0  0  0  c
2  0  0  0  0
3  0  0  0  a
</code></pre>

<p>Right now, the fill values are <code>0</code>. However, I'd like to change the fill_values to <code>np.nan</code>. My first instinct was to call <code>replace</code>:</p>

<pre><code>A.replace(0, np.nan)
</code></pre>

<p>But this gives </p>

<pre><code>TypeError: cannot convert int to an sparseblock
</code></pre>

<p>Which doesn't really help me understand what I'm doing wrong.</p>

<p>I know I can do </p>

<pre><code>A.to_dense().replace(0, np.nan).to_sparse()
</code></pre>

<p>But is there a better way? Or is my fundamental understanding of Sparse dataframes flawed?</p>
",4909087,390235,09-01-2018 04:28,09-01-2018 04:59,0,391021,775,102,720,97,"{'badge_counts': {'bronze': 775, 'silver': 720, 'gold': 102}, 'collectives': [{'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 6321039, 'is_employee': False, 'last_modified_date': 1709572500, 'last_access_date': 1710735667, 'reputation_change_year': 4488, 'reputation_change_quarter': 4488, 'reputation_change_month': 1116, 'reputation_change_week': 416, 'reputation_change_day': 30, 'reputation': 391021, 'creation_date': 1431873091, 'user_type': 'registered', 'user_id': 4909087, 'accept_rate': 97, 'location': 'At an ipython shell near you', 'website_url': '', 'link': 'https://stackoverflow.com/users/4909087/this-be-shiva', 'profile_image': 'https://i.stack.imgur.com/Zcszo.png?s=256&g=1', 'display_name': 'this be Shiva'}","Current pandas version: I have a SparseDataFrame. Right now, the fill values are . However, I'd like to change the fill_values to . My first instinct was to call : But this gives Which doesn't really help me understand what I'm doing wrong. I know I can do But is there a better way? Or is my fundamental understanding of Sparse dataframes flawed?","0.22 A = pd.SparseDataFrame(
    [['a',0,0,'b'],
     [0,0,0,'c'],
     [0,0,0,0],
     [0,0,0,'a']])
 A

   0  1  2  3
0  a  0  0  b
1  0  0  0  c
2  0  0  0  0
3  0  0  0  a
 0 np.nan replace A.replace(0, np.nan)
 TypeError: cannot convert int to an sparseblock
 A.to_dense().replace(0, np.nan).to_sparse()
",6,42,0,0,
801,48816457,48816657,9036,pandas ffill based on condition in another column,1,<python><pandas><dataframe>,11,"<p>I have a pandas DataFrame as shown below. </p>

<pre><code>df = pd.DataFrame({
    'date': ['2011-01-01', '2011-01-01', '2011-02-01', '2011-02-01', '2011-03-01', '2011-03-01', '2011-04-01', '2011-04-01'],
    'category': [1, 2, 1, 2, 1, 2, 1, 2],
    'rate': [0.5, 0.75, np.nan, np.nan, 1, 1.25, np.nan, np.nan]
})
</code></pre>

<p>I want to use <code>ffill</code> to forward fill the values of <code>rate</code>, except that I want each value to correspond also to the appropriate <code>category</code>. How can I get <code>df</code> to look like this?: </p>

<pre><code>df
    category    date    rate
    1     2011-01-01    0.50
    2     2011-01-01    0.75
    1     2011-02-01    0.50
    2     2011-02-01    0.75
    1     2011-03-01    1.00
    2     2011-03-01    1.25
    1     2011-04-01    1.00
    2     2011-04-01    1.25
</code></pre>
",4913108,5384,15-02-2018 21:19,15-02-2018 21:33,0,5394,95,16,50,82,"{'badge_counts': {'bronze': 95, 'silver': 50, 'gold': 16}, 'account_id': 6326933, 'is_employee': False, 'last_modified_date': 1698105900, 'last_access_date': 1711057300, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5394, 'creation_date': 1431970105, 'user_type': 'registered', 'user_id': 4913108, 'accept_rate': 82, 'link': 'https://stackoverflow.com/users/4913108/gaurav-bansal', 'profile_image': 'https://www.gravatar.com/avatar/5e3a6f7f2a1b879ae4b56a425d7b5e1e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Gaurav Bansal'}","I have a pandas DataFrame as shown below. I want to use to forward fill the values of , except that I want each value to correspond also to the appropriate . How can I get to look like this?:","df = pd.DataFrame({
    'date': ['2011-01-01', '2011-01-01', '2011-02-01', '2011-02-01', '2011-03-01', '2011-03-01', '2011-04-01', '2011-04-01'],
    'category': [1, 2, 1, 2, 1, 2, 1, 2],
    'rate': [0.5, 0.75, np.nan, np.nan, 1, 1.25, np.nan, np.nan]
})
 ffill rate category df df
    category    date    rate
    1     2011-01-01    0.50
    2     2011-01-01    0.75
    1     2011-02-01    0.50
    2     2011-02-01    0.75
    1     2011-03-01    1.00
    2     2011-03-01    1.25
    1     2011-04-01    1.00
    2     2011-04-01    1.25
",9,22,0,0,
802,48211590,48212674,5590,scikit learn: custom classifier compatible with GridSearchCV,2,<python><machine-learning><scikit-learn>,11,"<p>I have implemented my own classifier and now I want to run a grid search over it, but I'm getting the following error: <code>estimator.fit(X_train, y_train, **fit_params)
TypeError: fit() takes 2 positional arguments but 3 were given</code></p>

<p>I followed <a href=""http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/"" rel=""noreferrer"">this tutorial</a> and used <a href=""https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/template.py"" rel=""noreferrer"">this template</a> provided by <a href=""http://scikit-learn.org/stable/developers/contributing.html"" rel=""noreferrer"">scikit's official documentation</a>. My class is defined as follows:</p>

<pre><code>class MyClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, lr=0.1):
        self.lr=lr

    def fit(self, X, y):
        # Some code
        return self
    def predict(self, X):
        # Some code
        return y_pred
    def get_params(self, deep=True)
        return {'lr'=self.lr}
    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self
</code></pre>

<p>And I'm trying to grid search throw it as follows:</p>

<pre><code>params = {
    'lr': [0.1, 0.5, 0.7]
}
gs = GridSearchCV(MyClassifier(), param_grid=params, cv=4)
</code></pre>

<p><strong>EDIT I</strong></p>

<p>This is how I'm calling it:
    gs.fit(['hello world', 'trying','hello world', 'trying', 'hello world', 'trying', 'hello world', 'trying'],
           ['I', 'Z', 'I', 'Z', 'I', 'Z', 'I', 'Z'])</p>

<p><strong>END EDIT I</strong></p>

<p>The error is produced by <code>_fit_and_score</code> method in file <code>python3.5/site-packages/sklearn/model_selection/_validation.py</code></p>

<p>It is calling <code>estimator.fit(X_train, y_train, **fit_params)</code> with 3 arguments, but my estimator only have two, so the error makes sense for me, but I don't know how to solve it... I also tried adding some dummy arguments to <code>fit</code> method but it didn't work.</p>

<p><strong>EDIT II</strong></p>

<p>Complete error output:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/rodrigo/no_version/text_classifier/MyClassifier.py"", line 355, in &lt;module&gt;
    ['I', 'Z', 'I', 'Z', 'I', 'Z', 'I', 'Z'])
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/model_selection/_search.py"", line 639, in fit
    cv.split(X, y, groups)))
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 332, in __init__
    self.results = batch()
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in &lt;listcomp&gt;
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 458, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
TypeError: fit() takes 2 positional arguments but 3 were given
</code></pre>

<p><strong>END EDIT II</strong></p>

<p><strong>SOLVED</strong>
Thanks you all, I had a stupid mistake: there was two different functions with same name (fit), (I implemented the other for custom purposes with different parameters, as soon as I renamed my 'custom fit', it worked correctly.)</p>

<p>Thank you and sorry </p>
",4913143,1834,11-01-2018 16:19,11-01-2018 17:20,0,1844,46,1,28,47,"{'badge_counts': {'bronze': 46, 'silver': 28, 'gold': 1}, 'account_id': 6326973, 'is_employee': False, 'last_modified_date': 1702378814, 'last_access_date': 1711119898, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1844, 'creation_date': 1431970667, 'user_type': 'registered', 'user_id': 4913143, 'accept_rate': 47, 'website_url': '', 'link': 'https://stackoverflow.com/users/4913143/rodrigo-laguna', 'profile_image': 'https://graph.facebook.com/10206877151395113/picture?type=large', 'display_name': 'Rodrigo Laguna'}","I have implemented my own classifier and now I want to run a grid search over it, but I'm getting the following error: I followed this tutorial and used this template provided by scikit's official documentation. My class is defined as follows: And I'm trying to grid search throw it as follows: EDIT I This is how I'm calling it: gs.fit(['hello world', 'trying','hello world', 'trying', 'hello world', 'trying', 'hello world', 'trying'], ['I', 'Z', 'I', 'Z', 'I', 'Z', 'I', 'Z']) END EDIT I The error is produced by method in file It is calling with 3 arguments, but my estimator only have two, so the error makes sense for me, but I don't know how to solve it... I also tried adding some dummy arguments to method but it didn't work. EDIT II Complete error output: END EDIT II SOLVED Thanks you all, I had a stupid mistake: there was two different functions with same name (fit), (I implemented the other for custom purposes with different parameters, as soon as I renamed my 'custom fit', it worked correctly.) Thank you and sorry","estimator.fit(X_train, y_train, **fit_params)
TypeError: fit() takes 2 positional arguments but 3 were given class MyClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, lr=0.1):
        self.lr=lr

    def fit(self, X, y):
        # Some code
        return self
    def predict(self, X):
        # Some code
        return y_pred
    def get_params(self, deep=True)
        return {'lr'=self.lr}
    def set_params(self, **parameters):
        for parameter, value in parameters.items():
            setattr(self, parameter, value)
        return self
 params = {
    'lr': [0.1, 0.5, 0.7]
}
gs = GridSearchCV(MyClassifier(), param_grid=params, cv=4)
 _fit_and_score python3.5/site-packages/sklearn/model_selection/_validation.py estimator.fit(X_train, y_train, **fit_params) fit Traceback (most recent call last):
  File ""/home/rodrigo/no_version/text_classifier/MyClassifier.py"", line 355, in &lt;module&gt;
    ['I', 'Z', 'I', 'Z', 'I', 'Z', 'I', 'Z'])
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/model_selection/_search.py"", line 639, in fit
    cv.split(X, y, groups)))
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 332, in __init__
    self.results = batch()
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in &lt;listcomp&gt;
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/rodrigo/no_version/text_classifier/.env/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 458, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
TypeError: fit() takes 2 positional arguments but 3 were given
",35,77,0,3,
803,48267017,48316142,13905,"DRF: Validate nested serializer data when creating, but not when updating",3,<python><django><django-rest-framework>,13,"<p>When using writable nested serializers in DRF there is the known problem with validating eventual unique fields and preventing the updating of the parent serializer. This issue has been asked many times in questions like these:</p>

<ol>
<li><a href=""https://stackoverflow.com/questions/38438167/unique-validation-on-nested-serializer-on-django-rest-framework"">Unique validation on nested serializer on Django Rest Framework</a></li>
<li><a href=""https://stackoverflow.com/questions/43618662/django-rest-framework-not-creating-object-with-fk-to-a-model-with-unique-true-fi/43689912?noredirect=1#comment83511542_43689912"">Django rest framework not creating object with FK to a model with unique=True field</a></li>
</ol>

<p>For simplicity let's take the example from the first question:</p>

<pre><code>class GenreSerializer(serializers.ModelSerializer):
    class Meta:
        fields = ('name',) #This field is unique
        model = Genre
        extra_kwargs = {
            'name': {'validators': []},
        }

class BookSerializer(serializers.ModelSerializer):
    genre = GenreSerializer()

    class Meta:
        model = Book
        fields = ('name', 'genre')

    def create(self, validated_data):
        # implement creating

    def update(self, instance, validated_data):
        # implement updating
</code></pre>

<p>Now the problem is that the uniqueness validation is dropped out also for creating. This could be intercepted in the view, for example:</p>

<pre><code>class BookViewSet(viewsets.ModelViewSet):
    queryset = Book.objects.all()
    serializer = BookSerializer

    def perform_create(self):
        # implement logic and raise ValidationError
</code></pre>

<p>However this doesn't feel really right because we're validating the uniqueness of <code>Genre</code> in <code>BookViewSet</code>.</p>

<p>The other option is to implement the validation in the <code>create()</code> method of <code>BookSerializer</code> as explained in the second question (see list above).</p>

<p>What I really miss in both solutions is that the validation error is not attached to the field <code>name</code> of the model <code>Genre</code> and the user input in the form is lost.</p>

<p>What I'd like is to add the validation error for <code>Genre.name</code> to the existing validation errors, keep the user input and do this only for creating, not for updating.</p>

<p>My ideas were something like this:</p>

<pre><code>class GenreSerializer(serializers.ModelSerializer):
    # ...
    def validate_name(self, value):
        # is it possible to check here if it is create or update?
        if create: # this is a placeholder for the logic
             if self.Meta.model.objects.filter(name=value).exists():
                 raise ValidationError('A genre with this name already exists.')
        return value

    # or to override the __init__ method

    def __init__(self, *args, **kwargs):
        super(GenreSerializer, self).__init__(*args, **kwargs)
        # check if create or update
        if create:
            self.fields['name'].validators.append('validation logic')
</code></pre>

<p>Is this possible or is there any other way to achieve the before mentioned goal - keep user input and add validation error attached to the field <code>name</code> to the list of existing validation errors when creating new instance?      </p>
",3848833,11766,15-01-2018 16:28,18-01-2018 07:46,3,11776,86,6,48,85,"{'badge_counts': {'bronze': 86, 'silver': 48, 'gold': 6}, 'account_id': 4761572, 'is_employee': False, 'last_modified_date': 1704465900, 'last_access_date': 1711115550, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 11776, 'creation_date': 1405594950, 'user_type': 'registered', 'user_id': 3848833, 'accept_rate': 85, 'location': 'Edinburgh, United Kingdom', 'website_url': '', 'link': 'https://stackoverflow.com/users/3848833/cezar', 'profile_image': 'https://www.gravatar.com/avatar/003a7e62cfa0ff8097aba525dc5727a8?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'cezar'}","When using writable nested serializers in DRF there is the known problem with validating eventual unique fields and preventing the updating of the parent serializer. This issue has been asked many times in questions like these: Unique validation on nested serializer on Django Rest Framework Django rest framework not creating object with FK to a model with unique=True field For simplicity let's take the example from the first question: Now the problem is that the uniqueness validation is dropped out also for creating. This could be intercepted in the view, for example: However this doesn't feel really right because we're validating the uniqueness of in . The other option is to implement the validation in the method of as explained in the second question (see list above). What I really miss in both solutions is that the validation error is not attached to the field of the model and the user input in the form is lost. What I'd like is to add the validation error for to the existing validation errors, keep the user input and do this only for creating, not for updating. My ideas were something like this: Is this possible or is there any other way to achieve the before mentioned goal - keep user input and add validation error attached to the field to the list of existing validation errors when creating new instance?","class GenreSerializer(serializers.ModelSerializer):
    class Meta:
        fields = ('name',) #This field is unique
        model = Genre
        extra_kwargs = {
            'name': {'validators': []},
        }

class BookSerializer(serializers.ModelSerializer):
    genre = GenreSerializer()

    class Meta:
        model = Book
        fields = ('name', 'genre')

    def create(self, validated_data):
        # implement creating

    def update(self, instance, validated_data):
        # implement updating
 class BookViewSet(viewsets.ModelViewSet):
    queryset = Book.objects.all()
    serializer = BookSerializer

    def perform_create(self):
        # implement logic and raise ValidationError
 Genre BookViewSet create() BookSerializer name Genre Genre.name class GenreSerializer(serializers.ModelSerializer):
    # ...
    def validate_name(self, value):
        # is it possible to check here if it is create or update?
        if create: # this is a placeholder for the logic
             if self.Meta.model.objects.filter(name=value).exists():
                 raise ValidationError('A genre with this name already exists.')
        return value

    # or to override the __init__ method

    def __init__(self, *args, **kwargs):
        super(GenreSerializer, self).__init__(*args, **kwargs)
        # check if create or update
        if create:
            self.fields['name'].validators.append('validation logic')
 name",31,70,0,2,
804,49615715,54729246,18939,Coursera jupyterNotebook: revert to the beginning,7,<python><jupyter-notebook>,13,"<p>I'm using jupyterNotebook from Coursera but see no way to revert everything to the beginning.</p>

<p><a href=""https://i.stack.imgur.com/prnmh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/prnmh.png"" alt=""enter image description here""></a></p>

<p>The only option relevant seems to be ""Revert to Checkpoint"" -- but I didn't save a checkpoint at the beginning.</p>

<p>Does it mean that I am unable to revert to it?</p>
",3853711,5174,02-04-2018 17:22,17-02-2019 01:12,321,5172,65,4,33,63,"{'badge_counts': {'bronze': 65, 'silver': 33, 'gold': 4}, 'account_id': 4768114, 'is_employee': False, 'last_modified_date': 1708345200, 'last_access_date': 1711077718, 'reputation_change_year': 153, 'reputation_change_quarter': 153, 'reputation_change_month': 7, 'reputation_change_week': -2, 'reputation_change_day': -2, 'reputation': 5172, 'creation_date': 1405699583, 'user_type': 'registered', 'user_id': 3853711, 'accept_rate': 63, 'website_url': '', 'link': 'https://stackoverflow.com/users/3853711/rahn', 'profile_image': 'https://i.stack.imgur.com/SwXly.png?s=256&g=1', 'display_name': 'Rahn'}","I'm using jupyterNotebook from Coursera but see no way to revert everything to the beginning. The only option relevant seems to be ""Revert to Checkpoint"" -- but I didn't save a checkpoint at the beginning. Does it mean that I am unable to revert to it?",,0,7,1,1,
805,49904516,49904589,9107,Stop a python script without losing data,2,<python>,21,"<p>We have been running a script on partner's computer for 18 hours.  We underestimated how long it would take, and now need to turn in the results.  Is it possible to stop the script from running, but still have access to all the lists we are building?</p>

<p>We need to add additional code to the one we are currently running that will use the lists being populated right now.  Is there a way to stop the process, but still use (what has been generated of) the lists in the next portion of code?</p>

<p>My partner was using python interactively.</p>

<hr>

<p><strong>update</strong></p>

<p>We were able to successfully print the results and copy and paste after interrupting the program with control-C.</p>
",3865115,375,18-04-2018 16:19,18-04-2018 16:23,0,375,10,1,4,,"{'badge_counts': {'bronze': 10, 'silver': 4, 'gold': 1}, 'account_id': 2857037, 'is_employee': False, 'last_modified_date': 1573680181, 'last_access_date': 1652892929, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 375, 'creation_date': 1406039872, 'user_type': 'registered', 'user_id': 3865115, 'link': 'https://stackoverflow.com/users/3865115/curiousgeorge119', 'profile_image': 'https://www.gravatar.com/avatar/f9af550300285b2e81cfed533164b117?s=256&d=identicon&r=PG', 'display_name': 'CuriousGeorge119'}","We have been running a script on partner's computer for 18 hours. We underestimated how long it would take, and now need to turn in the results. Is it possible to stop the script from running, but still have access to all the lists we are building? We need to add additional code to the one we are currently running that will use the lists being populated right now. Is there a way to stop the process, but still use (what has been generated of) the lists in the next portion of code? My partner was using python interactively. update We were able to successfully print the results and copy and paste after interrupting the program with control-C.",,0,11,0,0,
806,48508036,48512157,84519,"Sklearn StratifiedKFold: ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead",6,<python><machine-learning><keras><scikit-learn><cross-validation>,44,"<p>Working with Sklearn stratified kfold split, and when I attempt to split using multi-class, I received on error (see below).  When I tried and split using binary, it works no problem.</p>



<pre class=""lang-python prettyprint-override""><code>num_classes = len(np.unique(y_train))
y_train_categorical = keras.utils.to_categorical(y_train, num_classes)
kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=999)

# splitting data into different folds
for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train_categorical)):
    x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]
    y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]

ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.
</code></pre>
",3866549,2407,29-01-2018 18:48,30-01-2018 00:29,1,2407,48,6,35,45,"{'badge_counts': {'bronze': 48, 'silver': 35, 'gold': 6}, 'account_id': 4786093, 'is_employee': False, 'last_modified_date': 1703294700, 'last_access_date': 1710039437, 'reputation_change_year': 32, 'reputation_change_quarter': 32, 'reputation_change_month': 2, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2407, 'creation_date': 1406068130, 'user_type': 'registered', 'user_id': 3866549, 'accept_rate': 45, 'link': 'https://stackoverflow.com/users/3866549/jkraut', 'profile_image': 'https://www.gravatar.com/avatar/0a0d2a4338aa80278b2f8c83dc8132ef?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jKraut'}","Working with Sklearn stratified kfold split, and when I attempt to split using multi-class, I received on error (see below). When I tried and split using binary, it works no problem.","num_classes = len(np.unique(y_train))
y_train_categorical = keras.utils.to_categorical(y_train, num_classes)
kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=999)

# splitting data into different folds
for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train_categorical)):
    x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]
    y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]

ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.
",9,15,0,0,
807,48264536,49350976,18459,ImportError: Failed to import the Cloud Firestore library for Python,4,<python><firebase><google-app-engine><google-cloud-firestore><firebase-admin>,12,"<p>Trying to integrate Google firestore API at python server</p>

<pre><code>...
  File ""/home/daffolap-355/repos/subscriptions/appvendor/firebase_admin/firestore.py"", line 28, in &lt;module&gt;
    raise ImportError('Failed to import the Cloud Firestore library for Python. Make sure '
ImportError: Failed to import the Cloud Firestore library for Python. Make sure to install the ""google-cloud-firestore"" module.
</code></pre>

<p>I get this error here:</p>

<p><code>from firebase_admin import credentials, auth, firestore</code></p>

<p>I installed the <code>firebase-admin</code> module:</p>

<p><code>pip install --upgrade -t libs firebase-admin</code></p>

<p>And run the app</p>

<p><code>dev_appserver app.yaml</code></p>
",5035469,1457,15-01-2018 14:00,18-03-2018 17:39,62,1457,37,6,19,91,"{'badge_counts': {'bronze': 37, 'silver': 19, 'gold': 6}, 'account_id': 6506852, 'is_employee': False, 'last_modified_date': 1689420618, 'last_access_date': 1710979999, 'reputation_change_year': 8, 'reputation_change_quarter': 8, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1457, 'creation_date': 1434963532, 'user_type': 'registered', 'user_id': 5035469, 'accept_rate': 91, 'location': 'Delhi, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/5035469/anubhav-dhawan', 'profile_image': 'https://graph.facebook.com/917777174930670/picture?type=large', 'display_name': 'Anubhav Dhawan'}",Trying to integrate Google firestore API at python server I get this error here: I installed the module: And run the app,"...
  File ""/home/daffolap-355/repos/subscriptions/appvendor/firebase_admin/firestore.py"", line 28, in &lt;module&gt;
    raise ImportError('Failed to import the Cloud Firestore library for Python. Make sure '
ImportError: Failed to import the Cloud Firestore library for Python. Make sure to install the ""google-cloud-firestore"" module.
 from firebase_admin import credentials, auth, firestore firebase-admin pip install --upgrade -t libs firebase-admin dev_appserver app.yaml",-1,19,0,0,
808,49204453,49204479,42390,How can I get year - month - day from a numpy datetime64?,3,<python><numpy><datetime>,13,"<p>I have a numpy datetime.</p>

<pre><code>numpy.datetime64('2010-06-01T00:00:00.000000000')
</code></pre>

<p>How can I get something like:</p>

<pre><code>numpy.datetime64('2010-06-01')
</code></pre>

<p>or</p>

<pre><code>'2010-06-01'
</code></pre>

<p>Basically, I want to remove the hour and beyond timestamp.</p>
",3942806,5402,10-03-2018 01:31,10-03-2018 01:35,0,5402,101,11,55,72,"{'badge_counts': {'bronze': 101, 'silver': 55, 'gold': 11}, 'account_id': 4891905, 'is_employee': False, 'last_modified_date': 1639184100, 'last_access_date': 1692882345, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5402, 'creation_date': 1408048417, 'user_type': 'registered', 'user_id': 3942806, 'accept_rate': 72, 'link': 'https://stackoverflow.com/users/3942806/maximusdooku', 'profile_image': 'https://www.gravatar.com/avatar/b398d0b92434b4c65cb081294892589f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'maximusdooku'}","I have a numpy datetime. How can I get something like: or Basically, I want to remove the hour and beyond timestamp.","numpy.datetime64('2010-06-01T00:00:00.000000000')
 numpy.datetime64('2010-06-01')
 '2010-06-01'
",0,16,0,0,
809,48223558,48223590,3345,typehints -> None or leave blank,1,<python><python-3.x><type-hinting><python-typing>,22,"<p>Using python 3, one has the option to use typehints.</p>
<p>If a function returns None, should one add this, or leave it blank?</p>
<p>i.e.</p>
<pre><code>def hint(p: str) -&gt; None:
    ...

def no_hint(p: str):
    ...
</code></pre>
<p>And which PEP addresses this?</p>
",4025874,7469,12-01-2018 09:49,12-01-2018 09:51,0,7509,45,2,29,67,"{'badge_counts': {'bronze': 45, 'silver': 29, 'gold': 2}, 'collectives': [{'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 5009025, 'is_employee': False, 'last_modified_date': 1701693000, 'last_access_date': 1711096084, 'reputation_change_year': 170, 'reputation_change_quarter': 170, 'reputation_change_month': 80, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 7509, 'creation_date': 1410335306, 'user_type': 'registered', 'user_id': 4025874, 'accept_rate': 67, 'location': 'London, UK', 'website_url': 'http://puntdoctor.co.za', 'link': 'https://stackoverflow.com/users/4025874/daniel-lee', 'profile_image': 'https://i.stack.imgur.com/JcYk0.jpg?s=256&g=1', 'display_name': 'Daniel Lee'}","Using python 3, one has the option to use typehints. If a function returns None, should one add this, or leave it blank? i.e. And which PEP addresses this?","def hint(p: str) -&gt; None:
    ...

def no_hint(p: str):
    ...
",4,10,0,0,
810,49865751,49865883,68659,VSCode Extension to fix inconsistent tab issue of Python,6,<python><visual-studio-code>,42,"<p>First of all, I wonder who was the brainless genius that decided to have indentation based language, and why couldn't he simply used semi-colons. It is so frustrating that the tabs in python files are always go out of sync, specially when you change either an editor and/or OS.</p>
<p>Just wondering if there is an extension in VSCode that could fix this tab inconsistency?</p>
",4054527,3872,16-04-2018 20:30,16-04-2018 20:40,0,3882,67,7,43,86,"{'badge_counts': {'bronze': 67, 'silver': 43, 'gold': 7}, 'account_id': 5049530, 'is_employee': False, 'last_modified_date': 1706544904, 'last_access_date': 1711116867, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 3882, 'creation_date': 1411044900, 'user_type': 'registered', 'user_id': 4054527, 'accept_rate': 86, 'location': 'Edmonton, AB, Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/4054527/hafiz-temuri', 'profile_image': 'https://i.stack.imgur.com/pZzgC.jpg?s=256&g=1', 'display_name': 'Hafiz Temuri'}","First of all, I wonder who was the brainless genius that decided to have indentation based language, and why couldn't he simply used semi-colons. It is so frustrating that the tabs in python files are always go out of sync, specially when you change either an editor and/or OS. Just wondering if there is an extension in VSCode that could fix this tab inconsistency?",,0,2,0,0,
811,48429265,49931524,11521,Pip install without progress bars,2,<python><pip><progress-bar><circleci>,18,"<p>In my Django application I have a circle.yml file that runs 'pip install -r requirements/base.txt'. When I push up code, and check the CircleCI logs when there is an error, its hard to get to because there are so many dependencies and as of pip6 they started showing progress bars for the installations. Because of that it get busy pretty quick. I read on pip's github page that a few people were requesting a flag to the install command to remove the progress bars, but continue to show everything else like exceptions. something like </p>

<p><code>pip install --no-progress-bar foo</code> </p>

<p><a href=""https://github.com/pypa/pip/pull/4194"" rel=""noreferrer"">https://github.com/pypa/pip/pull/4194</a>. It doesn't look like this has been released yet though. Is there any way to currently do this without using --<strong>no-cache-dir</strong> ?</p>
",5159043,3746,24-01-2018 18:27,19-04-2018 23:05,85,3746,107,10,52,98,"{'badge_counts': {'bronze': 107, 'silver': 52, 'gold': 10}, 'account_id': 6688964, 'is_employee': False, 'last_modified_date': 1661877300, 'last_access_date': 1710474697, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3746, 'creation_date': 1437971661, 'user_type': 'registered', 'user_id': 5159043, 'accept_rate': 98, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/5159043/tjb', 'profile_image': 'https://i.stack.imgur.com/FgQVc.png?s=256&g=1', 'display_name': 'TJB'}","In my Django application I have a circle.yml file that runs 'pip install -r requirements/base.txt'. When I push up code, and check the CircleCI logs when there is an error, its hard to get to because there are so many dependencies and as of pip6 they started showing progress bars for the installations. Because of that it get busy pretty quick. I read on pip's github page that a few people were requesting a flag to the install command to remove the progress bars, but continue to show everything else like exceptions. something like https://github.com/pypa/pip/pull/4194. It doesn't look like this has been released yet though. Is there any way to currently do this without using --no-cache-dir ?",pip install --no-progress-bar foo,-1,5,0,1,
812,49844189,60326285,17330,How to get interactive plot of pyplot when using pycharm,1,<python><matplotlib><pycharm>,22,"<p>I am using PyCharm as the IDE for python, and when you make a plot (<strong>with the same code</strong> like pyplot.plot(...), pyplot.show()) pycharm displays it within its IDE. However, this looks like a static image. When you zoom in, the plot starts to blur. </p>

<p>In other IDE, pyplot creates an interactive plot. When you zoom in, it basically re-plots the curve. And you can also drag the plot. Is there anyway in PyCharm I can have the interactive plot from pyplot?</p>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/xVR0N.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xVR0N.png"" alt=""enter image description here""></a>
  <a href=""https://i.stack.imgur.com/1G4Da.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1G4Da.png"" alt=""enter image description here""></a></p>
</blockquote>
",4089301,1315,15-04-2018 16:24,20-02-2020 18:19,676,1335,26,3,12,71,"{'badge_counts': {'bronze': 26, 'silver': 12, 'gold': 3}, 'account_id': 5035927, 'is_employee': False, 'last_modified_date': 1674398401, 'last_access_date': 1676168457, 'reputation_change_year': 90, 'reputation_change_quarter': 90, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1335, 'creation_date': 1411941774, 'user_type': 'registered', 'user_id': 4089301, 'accept_rate': 71, 'link': 'https://stackoverflow.com/users/4089301/tony', 'profile_image': 'https://www.gravatar.com/avatar/2c4d740feee093d805efb2bf86514a6a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Tony'}","I am using PyCharm as the IDE for python, and when you make a plot (with the same code like pyplot.plot(...), pyplot.show()) pycharm displays it within its IDE. However, this looks like a static image. When you zoom in, the plot starts to blur. In other IDE, pyplot creates an interactive plot. When you zoom in, it basically re-plots the curve. And you can also drag the plot. Is there anyway in PyCharm I can have the interactive plot from pyplot?",,0,8,2,2,
813,49625148,49625165,7399,Pandas group by on groupby to list of lists,1,<python><pandas><dataframe>,17,"<p>Given a dataframe structured like:</p>

<pre><code>rule_id | ordering | sequence_id
   1    |    0     |     12     
   1    |    1     |     13
   1    |    1     |     14
   2    |    0     |     1
   2    |    1     |     2
   2    |    2     |     12 
</code></pre>

<p>I need to transform it into:</p>

<pre><code>rule_id |  sequences
   1    |  [[12],[13,14]]
   2    |  [[1],[2],[12]]
</code></pre>

<p>that seems like easy groupby into groupby to list operation - I can not however make it work in pandas.</p>

<pre><code>df.groupby(['rule_id', 'ordering'])['sequence_id'].apply(list)
</code></pre>

<p>leaves me with</p>

<pre><code>rule_id  ordering
1        0               [12]
         1            [13,14]
2        0                [1]
         1                [2]
         2               [12]
</code></pre>

<p>How does one apply another <code>groupBy</code> operation to furtherly concat results into one list?</p>
",4119226,1231,03-04-2018 08:05,03-04-2018 08:06,0,1231,33,3,16,53,"{'badge_counts': {'bronze': 33, 'silver': 16, 'gold': 3}, 'account_id': 5142223, 'is_employee': False, 'last_modified_date': 1673660100, 'last_access_date': 1710337607, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1231, 'creation_date': 1412725166, 'user_type': 'registered', 'user_id': 4119226, 'accept_rate': 53, 'website_url': '', 'link': 'https://stackoverflow.com/users/4119226/blahblah', 'profile_image': 'https://www.gravatar.com/avatar/22299b171eaca25be8e9102806994ee3?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'blahblah'}",Given a dataframe structured like: I need to transform it into: that seems like easy groupby into groupby to list operation - I can not however make it work in pandas. leaves me with How does one apply another operation to furtherly concat results into one list?,"rule_id | ordering | sequence_id
   1    |    0     |     12     
   1    |    1     |     13
   1    |    1     |     14
   2    |    0     |     1
   2    |    1     |     2
   2    |    2     |     12 
 rule_id |  sequences
   1    |  [[12],[13,14]]
   2    |  [[1],[2],[12]]
 df.groupby(['rule_id', 'ordering'])['sequence_id'].apply(list)
 rule_id  ordering
1        0               [12]
         1            [13,14]
2        0                [1]
         1                [2]
         2               [12]
 groupBy",12,34,0,0,
814,48825785,48826574,16667,How can I filter tf.data.Dataset by specific values?,4,<python><tensorflow><tensorflow-datasets>,19,"<p>I create a dataset by reading the TFRecords, I map the values and I want to filter the dataset for specific values, but since the result is a dict with tensors, I am not able to get the actual value of a tensor or to check it with <code>tf.cond()</code> / <code>tf.equal</code>. How can I do that?</p>

<pre><code>def mapping_func(serialized_example):
    feature = { 'label': tf.FixedLenFeature([1], tf.string) }
    features = tf.parse_single_example(serialized_example, features=feature)
    return features

def filter_func(features):
    # this doesn't work
    #result = features['label'] == 'some_label_value'
    # neither this
    result = tf.reshape(tf.equal(features['label'], 'some_label_value'), [])
    return result

def main():
    file_names = [""/var/data/file1.tfrecord"", ""/var/data/file2.tfrecord""]
    dataset = tf.contrib.data.TFRecordDataset(file_names)
    dataset = dataset.map(mapping_func)
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.filter(filter_func)
    dataset = dataset.repeat()
    iterator = dataset.make_one_shot_iterator()
    sample = iterator.get_next()
</code></pre>
",4137497,7378,16-02-2018 11:27,16-02-2018 12:16,0,7388,41,5,49,,"{'badge_counts': {'bronze': 41, 'silver': 49, 'gold': 5}, 'account_id': 5168102, 'is_employee': False, 'last_modified_date': 1662744600, 'last_access_date': 1709216152, 'reputation_change_year': 228, 'reputation_change_quarter': 228, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7388, 'creation_date': 1413204408, 'user_type': 'registered', 'user_id': 4137497, 'location': 'Amsterdam, Netherlands', 'link': 'https://stackoverflow.com/users/4137497/tsveti-iko', 'profile_image': 'https://i.stack.imgur.com/Wl10v.jpg?s=256&g=1', 'display_name': 'tsveti_iko'}","I create a dataset by reading the TFRecords, I map the values and I want to filter the dataset for specific values, but since the result is a dict with tensors, I am not able to get the actual value of a tensor or to check it with / . How can I do that?","tf.cond() tf.equal def mapping_func(serialized_example):
    feature = { 'label': tf.FixedLenFeature([1], tf.string) }
    features = tf.parse_single_example(serialized_example, features=feature)
    return features

def filter_func(features):
    # this doesn't work
    #result = features['label'] == 'some_label_value'
    # neither this
    result = tf.reshape(tf.equal(features['label'], 'some_label_value'), [])
    return result

def main():
    file_names = [""/var/data/file1.tfrecord"", ""/var/data/file2.tfrecord""]
    dataset = tf.contrib.data.TFRecordDataset(file_names)
    dataset = dataset.map(mapping_func)
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.filter(filter_func)
    dataset = dataset.repeat()
    iterator = dataset.make_one_shot_iterator()
    sample = iterator.get_next()
",18,24,0,0,
815,48064965,48065041,12460,drop unused categories using groupby on categorical variable in pandas,4,<python><pandas>,12,"<p>As per <a href=""https://pandas.pydata.org/pandas-docs/stable/categorical.html#operations"" rel=""noreferrer"">Categorical Data - Operations</a>, by default <code>groupby</code> will show “unused” categories:</p>

<pre><code>In [118]: cats = pd.Categorical([""a"",""b"",""b"",""b"",""c"",""c"",""c""], categories=[""a"",""b"",""c"",""d""])

In [119]: df = pd.DataFrame({""cats"":cats,""values"":[1,2,2,2,3,4,5]})

In [120]: df.groupby(""cats"").mean()
Out[120]: 
      values
cats        
a        1.0
b        2.0
c        4.0
d        NaN
</code></pre>

<p>How to obtain the result with the “unused” categories dropped? e.g.</p>

<pre><code>  values
cats        
a        1.0
b        2.0
c        4.0
</code></pre>
",3160671,603,02-01-2018 17:01,02-01-2018 17:07,0,603,12,2,6,80,"{'badge_counts': {'bronze': 12, 'silver': 6, 'gold': 2}, 'account_id': 3784962, 'is_employee': False, 'last_modified_date': 1585568113, 'last_access_date': 1710932828, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 603, 'creation_date': 1388851104, 'user_type': 'registered', 'user_id': 3160671, 'accept_rate': 80, 'link': 'https://stackoverflow.com/users/3160671/tales', 'profile_image': 'https://graph.facebook.com/100000928053424/picture?type=large', 'display_name': 'tales'}","As per Categorical Data - Operations, by default will show “unused” categories: How to obtain the result with the “unused” categories dropped? e.g.","groupby In [118]: cats = pd.Categorical([""a"",""b"",""b"",""b"",""c"",""c"",""c""], categories=[""a"",""b"",""c"",""d""])

In [119]: df = pd.DataFrame({""cats"":cats,""values"":[1,2,2,2,3,4,5]})

In [120]: df.groupby(""cats"").mean()
Out[120]: 
      values
cats        
a        1.0
b        2.0
c        4.0
d        NaN
   values
cats        
a        1.0
b        2.0
c        4.0
",14,24,0,1,
816,48629486,48643004,54295,How can I create the minimum size executable with pyinstaller?,3,<python><pandas><anaconda><virtualenv><pyinstaller>,44,"<p>I am on Windows 10, I have anaconda installed but I want to create an executable independently in a new, clean minimal environment using python 3.5. So I did some tests: </p>

<p>TEST1: 
I created a python script test1.py in the folder testenv with only:</p>

<pre><code>print('Hello World')
</code></pre>

<p>Then I created the environment, installed pyinstaller and created the executable</p>

<pre><code>D:\testenv&gt; python -m venv venv_test
...
D:\testenv\venv_test\Scripts&gt;activate.bat
...
(venv_test) D:\testenv&gt;pip install pyinstaller
(venv_test) D:\testenv&gt;pyinstaller --clean -F test1.py
</code></pre>

<p>And it creates my test1.exe of about 6 Mb</p>

<p>TEST 2: I modified test1.py as follows:</p>

<pre><code>import pandas as pd
print('Hello World')  
</code></pre>

<p>I installed pandas in the environment and created the new executable:</p>

<pre><code>(venv_test) D:\testenv&gt;pip install pandas
(venv_test) D:\testenv&gt;pyinstaller --clean -F test1.py
</code></pre>

<p>Ant it creates my test1.exe which is now of <strong>230 Mb</strong>!!! </p>

<p>if I run the command </p>

<pre><code>(venv_test) D:\testenv&gt;python -V
Python 3.5.2 :: Anaconda custom (64-bit)
</code></pre>

<p>when I am running pyinstaller I get some messages I do not understand, for example: </p>

<pre><code>INFO: site: retargeting to fake-dir 'c:\\users\\username\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\PyInstaller\\fake-modules'
</code></pre>

<p>Also I am getting messages about matplotlib and other modules that have nothing to do with my code, for example: </p>

<pre><code>INFO:   Matplotlib backend ""pdf"": added
INFO:   Matplotlib backend ""pgf"": added
INFO:   Matplotlib backend ""ps"": added
INFO:   Matplotlib backend ""svg"": added
</code></pre>

<p>I know there are some related questions: 
<a href=""https://stackoverflow.com/questions/47692213/reducing-size-of-pyinstaller-exe"">Reducing size of pyinstaller exe</a>, <a href=""https://stackoverflow.com/questions/47769904/size-of-executable-using-pyinstaller-and-numpy"">size of executable using pyinstaller and numpy</a>
but I could not solve the problem and I am afraid I am doing something wrong with respect to anaconda.  </p>

<p>So my questions are:
what am I doing wrong? can I reduce the size of my executable?</p>
",3256651,1872,05-02-2018 18:49,06-02-2018 12:23,1,1872,21,2,15,75,"{'badge_counts': {'bronze': 21, 'silver': 15, 'gold': 2}, 'account_id': 3770265, 'is_employee': False, 'last_modified_date': 1701780900, 'last_access_date': 1711096811, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1872, 'creation_date': 1391159666, 'user_type': 'registered', 'user_id': 3256651, 'accept_rate': 75, 'link': 'https://stackoverflow.com/users/3256651/esperluette', 'profile_image': 'https://www.gravatar.com/avatar/63d6795d3de2e1fc35ada887a19b87f0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'esperluette'}","I am on Windows 10, I have anaconda installed but I want to create an executable independently in a new, clean minimal environment using python 3.5. So I did some tests: TEST1: I created a python script test1.py in the folder testenv with only: Then I created the environment, installed pyinstaller and created the executable And it creates my test1.exe of about 6 Mb TEST 2: I modified test1.py as follows: I installed pandas in the environment and created the new executable: Ant it creates my test1.exe which is now of 230 Mb!!! if I run the command when I am running pyinstaller I get some messages I do not understand, for example: Also I am getting messages about matplotlib and other modules that have nothing to do with my code, for example: I know there are some related questions: Reducing size of pyinstaller exe, size of executable using pyinstaller and numpy but I could not solve the problem and I am afraid I am doing something wrong with respect to anaconda. So my questions are: what am I doing wrong? can I reduce the size of my executable?","print('Hello World')
 D:\testenv&gt; python -m venv venv_test
...
D:\testenv\venv_test\Scripts&gt;activate.bat
...
(venv_test) D:\testenv&gt;pip install pyinstaller
(venv_test) D:\testenv&gt;pyinstaller --clean -F test1.py
 import pandas as pd
print('Hello World')  
 (venv_test) D:\testenv&gt;pip install pandas
(venv_test) D:\testenv&gt;pyinstaller --clean -F test1.py
 (venv_test) D:\testenv&gt;python -V
Python 3.5.2 :: Anaconda custom (64-bit)
 INFO: site: retargeting to fake-dir 'c:\\users\\username\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\PyInstaller\\fake-modules'
 INFO:   Matplotlib backend ""pdf"": added
INFO:   Matplotlib backend ""pgf"": added
INFO:   Matplotlib backend ""ps"": added
INFO:   Matplotlib backend ""svg"": added
",11,59,0,2,
817,48174935,48178776,254156,Conda: Creating a virtual environment,4,<python><macos><virtual><environment><conda>,56,"<p>I'm trying to create a virtual environment. I've followed steps from both <a href=""https://conda.io/docs/user-guide/tasks/manage-environments.html#"" rel=""noreferrer"" title=""Conda"">Conda</a> and <a href=""https://medium.com/@tk2bit/how-to-set-up-and-maintain-conda-virtual-environment-for-your-python-projects-643c8f0c81a1"" rel=""noreferrer"" title=""Medium"">Medium</a>.</p>
<p>Everything works fine until I need to <em><a href=""https://en.wikipedia.org/wiki/Dot_(command)#Source"" rel=""noreferrer"">source</a></em> the new environment:</p>
<pre class=""lang-none prettyprint-override""><code>conda info -e

# conda environments:
#
base                  *  /Users/fwrenn/anaconda3
test_env                 /Users/fwrenn/anaconda3/envs/test_env

source ~/anaconda3/bin/activate test_env
</code></pre>
<blockquote>
<p>_CONDA_ROOT=/Users/fwrenn/anaconda3: Command not found.
Badly placed ()'s.</p>
</blockquote>
<p>I can't figure out the problem. Searching on here has solutions that say adding lines to your <em>bash_profile</em> file, but I don't work in Bash, only <a href=""https://en.wikipedia.org/wiki/C_shell"" rel=""noreferrer"">C shell</a> (csh). It <em>looks</em> like it's unable to build the directory path in <code>activate</code>.</p>
<p>My particulars:</p>
<ul>
<li><p>OS X</p>
</li>
<li><p>Output of <code>python --version</code>:</p>
<pre class=""lang-none prettyprint-override""><code>Python 3.6.3 :: Anaconda custom (64-bit)
</code></pre>
</li>
<li><p>Output of <code>conda --version</code>:</p>
<pre class=""lang-none prettyprint-override""><code>conda 4.4.7
</code></pre>
</li>
</ul>
",4276403,638,09-01-2018 18:55,10-01-2018 00:46,1,658,10,1,6,,"{'badge_counts': {'bronze': 10, 'silver': 6, 'gold': 1}, 'account_id': 5366646, 'is_employee': False, 'last_modified_date': 1573679952, 'last_access_date': 1709666122, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 658, 'creation_date': 1416521734, 'user_type': 'registered', 'user_id': 4276403, 'website_url': '', 'link': 'https://stackoverflow.com/users/4276403/forrest', 'profile_image': 'https://i.stack.imgur.com/oHEhM.jpg?s=256&g=1', 'display_name': 'Forrest'}","I'm trying to create a virtual environment. I've followed steps from both Conda and Medium. Everything works fine until I need to source the new environment: _CONDA_ROOT=/Users/fwrenn/anaconda3: Command not found. Badly placed ()'s. I can't figure out the problem. Searching on here has solutions that say adding lines to your bash_profile file, but I don't work in Bash, only C shell (csh). It looks like it's unable to build the directory path in . My particulars: OS X Output of : Output of :","conda info -e

# conda environments:
#
base                  *  /Users/fwrenn/anaconda3
test_env                 /Users/fwrenn/anaconda3/envs/test_env

source ~/anaconda3/bin/activate test_env
 activate python --version Python 3.6.3 :: Anaconda custom (64-bit)
 conda --version conda 4.4.7
",4,29,0,4,
818,49224770,49224771,25744,Default message in custom exception - Python,5,<python><exception>,43,"<p>I want to create a custom exception in Python, that when raised without any arguments, it will print a default message.</p>
<p>Code Example:</p>
<pre class=""lang-py prettyprint-override""><code>class CustomException(Exception):
    pass # some code

raise CustomException()
</code></pre>
<p>and get the below output:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
__main__.CustomException: This is a default message!
</code></pre>
",4501084,5563,11-03-2018 20:31,11-03-2018 20:31,0,5593,53,5,37,53,"{'badge_counts': {'bronze': 53, 'silver': 37, 'gold': 5}, 'account_id': 5694021, 'is_employee': False, 'last_modified_date': 1707167877, 'last_access_date': 1707701910, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 50, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 5593, 'creation_date': 1422410825, 'user_type': 'registered', 'user_id': 4501084, 'accept_rate': 53, 'location': 'Αθήνα, Ελλάδα', 'link': 'https://stackoverflow.com/users/4501084/pgmank', 'profile_image': 'https://i.stack.imgur.com/JQ1ra.jpg?s=256&g=1', 'display_name': 'pgmank'}","I want to create a custom exception in Python, that when raised without any arguments, it will print a default message. Code Example: and get the below output:","class CustomException(Exception):
    pass # some code

raise CustomException()
 Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
__main__.CustomException: This is a default message!
",5,12,0,0,
819,48939847,48945440,6415,Mutliprocessing Queue vs. Pool,1,<python><python-3.x><multiprocessing>,13,"<p>I'm having the hardest time trying to figure out the difference in usage between <code>multiprocessing.Pool</code> and <code>multiprocessing.Queue</code>.</p>

<p>To help, this is bit of code is a barebones example of what I'm trying to do.</p>

<pre><code>def update():
    def _hold(url):
        soup = BeautifulSoup(url)
        return soup
    def _queue(url):
        soup = BeautifulSoup(url)
        li = [l for l in soup.find('li')]
        return True if li else False

    url = 'www.ur_url_here.org'
    _hold(url)
    _queue(url)
</code></pre>

<p>I'm trying to run <code>_hold()</code> and <code>_queue()</code> at the same time. I'm not trying to have them communicate with each other so there is no need for a <code>Pipe</code>. <code>update()</code> is called every 5 seconds.</p>

<p>I can't really rap my head around the difference between creating a pool of workers, or creating a queue of functions. Can anyone assist me?</p>

<p>The real <code>_hold()</code> and <code>_queue()</code> functions are much more elaborate than the example so concurrent execution actually is necessary, I just thought this example would suffice for asking the question.  </p>
",4531286,344,23-02-2018 01:32,23-02-2018 10:10,0,344,12,1,3,36,"{'badge_counts': {'bronze': 12, 'silver': 3, 'gold': 1}, 'account_id': 5737732, 'is_employee': False, 'last_modified_date': 1631320200, 'last_access_date': 1618188571, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 344, 'creation_date': 1423108616, 'user_type': 'registered', 'user_id': 4531286, 'accept_rate': 36, 'link': 'https://stackoverflow.com/users/4531286/aseylys', 'profile_image': 'https://www.gravatar.com/avatar/8b0b89aadab669cec58083d69f245c33?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'aseylys'}","I'm having the hardest time trying to figure out the difference in usage between and . To help, this is bit of code is a barebones example of what I'm trying to do. I'm trying to run and at the same time. I'm not trying to have them communicate with each other so there is no need for a . is called every 5 seconds. I can't really rap my head around the difference between creating a pool of workers, or creating a queue of functions. Can anyone assist me? The real and functions are much more elaborate than the example so concurrent execution actually is necessary, I just thought this example would suffice for asking the question.","multiprocessing.Pool multiprocessing.Queue def update():
    def _hold(url):
        soup = BeautifulSoup(url)
        return soup
    def _queue(url):
        soup = BeautifulSoup(url)
        li = [l for l in soup.find('li')]
        return True if li else False

    url = 'www.ur_url_here.org'
    _hold(url)
    _queue(url)
 _hold() _queue() Pipe update() _hold() _queue()",3,23,0,0,
820,49335263,49335777,5410,How to properly annotate a ContextManager in PyCharm?,2,<python><pycharm><python-typing><contextmanager>,17,"<p>How can I annotate the yield type of a <code>contextmanager</code> in PyCharm so that it properly guesses the type of the value used in the <code>with</code> clauses - just as it guesses that the <code>f</code> created in <code>with open(...) as f</code> is a file?</p>
<p>For example, I have a context manager like this:</p>
<pre class=""lang-py prettyprint-override""><code>@contextlib.contextmanager
def temp_borders_file(geometry: GEOSGeometry, name='borders.json'):
    with TemporaryDirectory() as temp_dir:
        borders_file = Path(dir) / name
        with borders_file.open('w+') as f:
            f.write(geometry.json)
        yield borders_file

with temp_borders_file(my_geom) as borders_f:
    do_some_code_with(borders_f...)
</code></pre>
<p>How do I let PyCharm know that every <code>borders_f</code> created like this is a <code>pathlib.Path</code> (and thus enable the autocompletion for the <code>Path</code> methods on <code>border_f</code>)? Of course, I can make a comment like <code># type: Path</code> after every <code>with</code> statement, but it seems that this can be done by properly annotating <code>temp_border_file</code>.</p>
<p>I tried <code>Path</code>, <code>typing.Iterator[Path]</code> and <code>typing.Generator[Path, None, None]</code> as the return type of <code>temp_border_file</code>, as well as adding <code># type: Path</code> on <code>borders_file</code> within the context manager's code, but it seems like it doesn't help.</p>
",4534687,5345,17-03-2018 11:06,17-03-2018 12:04,0,5345,28,2,14,54,"{'badge_counts': {'bronze': 28, 'silver': 14, 'gold': 2}, 'account_id': 5742417, 'is_employee': False, 'last_modified_date': 1650387000, 'last_access_date': 1710746577, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5345, 'creation_date': 1423168789, 'user_type': 'registered', 'user_id': 4534687, 'accept_rate': 54, 'website_url': 'https://www.pealim.com', 'link': 'https://stackoverflow.com/users/4534687/aleph-aleph', 'profile_image': 'https://www.gravatar.com/avatar/203295ff528e77799ca17a9e9d1e26b2?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Aleph Aleph'}","How can I annotate the yield type of a in PyCharm so that it properly guesses the type of the value used in the clauses - just as it guesses that the created in is a file? For example, I have a context manager like this: How do I let PyCharm know that every created like this is a (and thus enable the autocompletion for the methods on )? Of course, I can make a comment like after every statement, but it seems that this can be done by properly annotating . I tried , and as the return type of , as well as adding on within the context manager's code, but it seems like it doesn't help.","contextmanager with f with open(...) as f @contextlib.contextmanager
def temp_borders_file(geometry: GEOSGeometry, name='borders.json'):
    with TemporaryDirectory() as temp_dir:
        borders_file = Path(dir) / name
        with borders_file.open('w+') as f:
            f.write(geometry.json)
        yield borders_file

with temp_borders_file(my_geom) as borders_f:
    do_some_code_with(borders_f...)
 borders_f pathlib.Path Path border_f # type: Path with temp_border_file Path typing.Iterator[Path] typing.Generator[Path, None, None] temp_border_file # type: Path borders_file",-8,15,0,0,
821,50193538,50193539,115788,How to kill process on GPUs with PID in nvidia-smi using keyword?,6,<python><gpu><nvidia><keyword><pid>,47,"<p>How to kill running processes on GPUs for a specific program (e.g. python) in terminal?
For example two processes are running with python in the top picture and kill them to see the bottom picture in nvidia-smi</p>

<p><a href=""https://i.stack.imgur.com/SMzOK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/SMzOK.png"" alt=""For example two processes are running with python in the top picture and kill them to see the bottom picture in nvidia-smi""></a></p>
",4538031,7328,05-05-2018 20:01,05-05-2018 20:01,0,7328,26,3,19,33,"{'badge_counts': {'bronze': 26, 'silver': 19, 'gold': 3}, 'account_id': 5747109, 'is_employee': False, 'last_modified_date': 1703340002, 'last_access_date': 1710361965, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 7328, 'creation_date': 1423239942, 'user_type': 'registered', 'user_id': 4538031, 'accept_rate': 33, 'location': 'University of Toronto', 'website_url': '', 'link': 'https://stackoverflow.com/users/4538031/salehinejad', 'profile_image': 'https://www.gravatar.com/avatar/ab36223e6268c6da580c7a7924553213?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'salehinejad'}",How to kill running processes on GPUs for a specific program (e.g. python) in terminal? For example two processes are running with python in the top picture and kill them to see the bottom picture in nvidia-smi,,0,4,1,1,
822,50186904,50187147,92491,PathLib recursively remove directory?,7,<python><directory><pathlib>,159,"<p>Is there any way to remove a directory and its contents in the PathLib module? With <code>path.unlink()</code> it only removes a file, with <code>path.rmdir()</code> the directory has to be empty. Is there no way to do it in one function call?</p>
",4556675,5218,05-05-2018 07:29,05-05-2018 08:01,0,5228,45,6,28,23,"{'badge_counts': {'bronze': 45, 'silver': 28, 'gold': 6}, 'account_id': 5774063, 'is_employee': False, 'last_modified_date': 1703294100, 'last_access_date': 1710818391, 'reputation_change_year': 120, 'reputation_change_quarter': 120, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5228, 'creation_date': 1423688796, 'user_type': 'registered', 'user_id': 4556675, 'accept_rate': 23, 'location': 'Fairfax, VA', 'website_url': '', 'link': 'https://stackoverflow.com/users/4556675/captaindriftwood', 'profile_image': 'https://i.stack.imgur.com/UQtz4.jpg?s=256&g=1', 'display_name': 'CaptainDriftwood'}","Is there any way to remove a directory and its contents in the PathLib module? With it only removes a file, with the directory has to be empty. Is there no way to do it in one function call?",path.unlink() path.rmdir(),-2,1,0,0,
823,50305112,53168096,105397,Pip Install Timeout Issue,5,<python><pandas><pip>,48,"<p>I am trying to install pandas in my company computer.
I tried to do</p>
<pre><code>pip install pandas
</code></pre>
<p>but operation retries and then timesout.</p>
<p>then I downloaded the package:</p>
<p>pandas-0.22.0-cp27-cp27m-win_amd64.whl</p>
<p>and install:</p>
<pre><code>pip install pandas-0.22.0-cp27-cp27m-win_amd64
</code></pre>
<p>But I get the following error:</p>
<blockquote>
<pre><code>Retrying (Retry(total=4, connect=None, read=None, redirect=None,
status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16320&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16C50&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16C18&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16780&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16898&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Could not find a version that satisfies the requirement pytz&gt;=2011k (from pandas==0.22.0) (from versions: )
    No matching distribution found for pytz&gt;=2011k (from pandas==0.22.0)
</code></pre>
</blockquote>
<p>I did the same with package: <code>pandas-0.22.0-cp27-cp27m-win_amd64.whl</code></p>
<p>I also tried to use proxies:</p>
<pre><code>pip --proxy=IND\namit.kewat:xl123456@192.168.180.150:8880 install numpy
</code></pre>
<p>But I am unable to get pandas.</p>
<p>when I tried to access the site : <a href=""https://pypi.org/project/pandas/#files"" rel=""noreferrer"">https://pypi.org/project/pandas/#files</a> I can access it without any problem on explorer</p>
",4570833,1788,12-05-2018 10:13,06-11-2018 08:17,178,1788,32,3,20,47,"{'badge_counts': {'bronze': 32, 'silver': 20, 'gold': 3}, 'account_id': 5794680, 'is_employee': False, 'last_modified_date': 1695434400, 'last_access_date': 1705297861, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1788, 'creation_date': 1424070112, 'user_type': 'registered', 'user_id': 4570833, 'accept_rate': 47, 'link': 'https://stackoverflow.com/users/4570833/adhil', 'profile_image': 'https://www.gravatar.com/avatar/9ab3fb2bb9685a9888a6a86371596a41?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Adhil'}",I am trying to install pandas in my company computer. I tried to do but operation retries and then timesout. then I downloaded the package: pandas-0.22.0-cp27-cp27m-win_amd64.whl and install: But I get the following error: I did the same with package: I also tried to use proxies: But I am unable to get pandas. when I tried to access the site : https://pypi.org/project/pandas/#files I can access it without any problem on explorer,"pip install pandas
 pip install pandas-0.22.0-cp27-cp27m-win_amd64
 Retrying (Retry(total=4, connect=None, read=None, redirect=None,
status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16320&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16C50&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16C18&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16780&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by
'ConnectTimeoutError(&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection
object at 0x0000000003F16898&gt;, 'Connection to pypi.python.org timed
out. (connect timeout=15)')': /simple/pytz/
      Could not find a version that satisfies the requirement pytz&gt;=2011k (from pandas==0.22.0) (from versions: )
    No matching distribution found for pytz&gt;=2011k (from pandas==0.22.0)
 pandas-0.22.0-cp27-cp27m-win_amd64.whl pip --proxy=IND\namit.kewat:xl123456@192.168.180.150:8880 install numpy
",21,43,0,1,
824,48996494,48997550,19790,Send http request through specific network interface,4,<python><python-requests>,24,"<p>I have two network interfaces (wifi and ethernet) both with internet access. Let's say my interfaces are <code>eth</code> (ethernet) and <code>wlp2</code> (wifi). I need specific requests to go through <code>eth</code> interface and others through <code>wpl2</code>.</p>

<p>Something like:</p>

<pre><code>// Through ""eth""
request.post(url=""http://myapi.com/store_ip"", iface=""eth"")
// Through ""wlp2"" 
request.post(url=""http://myapi.com/log"", iface=""wlp2"")
</code></pre>

<p>I'm using <code>requests</code>, but I can use <code>pycurl</code> or <code>urllib</code> if there isn't any way to do it with <code>requests</code>.</p>

<p><a href=""https://stackoverflow.com/questions/26335544/how-to-specify-source-interface-in-python-requests-module"">How to specify source interface in python requests module?</a> refers to <a href=""https://stackoverflow.com/questions/12585317/requests-bind-to-an-ip"">Requests, bind to an ip</a> and it doesn't work.</p>
",4585081,1455,26-02-2018 20:22,26-02-2018 21:39,0,1455,28,1,13,,"{'badge_counts': {'bronze': 28, 'silver': 13, 'gold': 1}, 'account_id': 5815204, 'is_employee': False, 'last_modified_date': 1676682900, 'last_access_date': 1710535605, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1455, 'creation_date': 1424367400, 'user_type': 'registered', 'user_id': 4585081, 'location': 'Miami, FL, United States', 'website_url': 'http://www.go4ai.com', 'link': 'https://stackoverflow.com/users/4585081/epinal', 'profile_image': 'https://i.stack.imgur.com/vnGRq.jpg?s=256&g=1', 'display_name': 'epinal'}","I have two network interfaces (wifi and ethernet) both with internet access. Let's say my interfaces are (ethernet) and (wifi). I need specific requests to go through interface and others through . Something like: I'm using , but I can use or if there isn't any way to do it with . How to specify source interface in python requests module? refers to Requests, bind to an ip and it doesn't work.","eth wlp2 eth wpl2 // Through ""eth""
request.post(url=""http://myapi.com/store_ip"", iface=""eth"")
// Through ""wlp2"" 
request.post(url=""http://myapi.com/log"", iface=""wlp2"")
 requests pycurl urllib requests",-5,13,0,2,
825,48864923,48864996,59799,Select certain rows by index of another DataFrame,2,<python><pandas><dataframe>,47,"<p>I have a DataFrame and I would select only rows that contain index value into df1.index.</p>
<p>for Example:</p>
<pre><code>In [96]: df
Out[96]:
   A  B  C  D
1  1  4  9  1
2  4  5  0  2
3  5  5  1  0
22 1  3  9  6
</code></pre>
<p>and these indexes</p>
<pre><code>In[96]:df1.index
Out[96]:
Int64Index([  1,   3,   4,   5,   6,   7,  22,  28,  29,  32,], dtype='int64', length=253)
</code></pre>
<p>I would like this output:</p>
<pre><code>In [96]: df
Out[96]:
   A  B  C  D
1  1  4  9  1
3  5  5  1  0
22 1  3  9  6
</code></pre>
",4624106,821,19-02-2018 11:13,19-02-2018 11:16,0,821,13,1,6,67,"{'badge_counts': {'bronze': 13, 'silver': 6, 'gold': 1}, 'account_id': 5872076, 'is_employee': False, 'last_modified_date': 1648864800, 'last_access_date': 1639779605, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 821, 'creation_date': 1425311681, 'user_type': 'registered', 'user_id': 4624106, 'accept_rate': 67, 'link': 'https://stackoverflow.com/users/4624106/giupardeb', 'profile_image': 'https://www.gravatar.com/avatar/4353cea69512a06300b51f434c647eeb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'giupardeb'}",I have a DataFrame and I would select only rows that contain index value into df1.index. for Example: and these indexes I would like this output:,"In [96]: df
Out[96]:
   A  B  C  D
1  1  4  9  1
2  4  5  0  2
3  5  5  1  0
22 1  3  9  6
 In[96]:df1.index
Out[96]:
Int64Index([  1,   3,   4,   5,   6,   7,  22,  28,  29,  32,], dtype='int64', length=253)
 In [96]: df
Out[96]:
   A  B  C  D
1  1  4  9  1
3  5  5  1  0
22 1  3  9  6
",13,23,0,0,
826,49814077,49814125,23687,Replacing -inf values to np.nan in a feature pandas.series,2,<python><pandas><numpy><replace><series>,11,"<p>I want to replace the -inf values in a pandas.series feature (column of my dataframe) to np.nan, but I could not make it. </p>

<p>I have tried: </p>

<pre><code>    df[feature] = df[feature].replace(-np.infty, np.nan)
    df[feature] = df[feature].replace(-np.inf, np.nan)
    df[feature] = df[feature].replace('-inf', np.nan)
    df[feature] = df[feature].replace(float('-inf'), np.nan)
</code></pre>

<p>But it does not work. Any ideas how to replace these values?</p>

<p>Edit: </p>

<pre><code>df[feature] =  df[feature].replace(-np.inf, np.nan)
</code></pre>

<p>works</p>

<p>BUT:</p>

<pre><code>df =  df.replace(-np.inf, np.nan)
</code></pre>

<p>does not work. </p>
",4685779,785,13-04-2018 09:50,13-04-2018 09:52,0,785,25,3,10,64,"{'badge_counts': {'bronze': 25, 'silver': 10, 'gold': 3}, 'collectives': [{'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 5751982, 'is_employee': False, 'last_modified_date': 1709946900, 'last_access_date': 1700612433, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 785, 'creation_date': 1426689257, 'user_type': 'registered', 'user_id': 4685779, 'accept_rate': 64, 'link': 'https://stackoverflow.com/users/4685779/javiss', 'profile_image': 'https://graph.facebook.com/10206144474355537/picture?type=large', 'display_name': 'Javiss'}","I want to replace the -inf values in a pandas.series feature (column of my dataframe) to np.nan, but I could not make it. I have tried: But it does not work. Any ideas how to replace these values? Edit: works BUT: does not work.","    df[feature] = df[feature].replace(-np.infty, np.nan)
    df[feature] = df[feature].replace(-np.inf, np.nan)
    df[feature] = df[feature].replace('-inf', np.nan)
    df[feature] = df[feature].replace(float('-inf'), np.nan)
 df[feature] =  df[feature].replace(-np.inf, np.nan)
 df =  df.replace(-np.inf, np.nan)
",3,25,0,0,
827,48377296,48377388,38858,Get feature importance from GridSearchCV,3,<python><scikit-learn><grid-search>,18,"<p>Is there a way to get feature importance from a sklearn's GridSearchCV? </p>

<p>For example : </p>

<pre><code>from sklearn.model_selection import GridSearchCV
print(""starting grid search ......"")
optimized_GBM = GridSearchCV(LGBMRegressor(),
                             params,
                             cv=3,
                             n_jobs=-1)
# 
optimized_GBM.fit(tr, yvar)
preds2 = optimized_GBM.predict(te)
</code></pre>

<p>Is there a way I can access feature importance ? </p>

<p>Maybe something like </p>

<pre><code>optimized_GBM.feature_importances_
</code></pre>
",4733010,832,22-01-2018 08:23,22-01-2018 08:30,0,842,20,1,7,80,"{'badge_counts': {'bronze': 20, 'silver': 7, 'gold': 1}, 'account_id': 6061386, 'is_employee': False, 'last_modified_date': 1607614567, 'last_access_date': 1686566534, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 842, 'creation_date': 1427790187, 'user_type': 'registered', 'user_id': 4733010, 'accept_rate': 80, 'location': 'Pune, Maharashtra, India', 'website_url': 'https://www.kaggle.com/nmud19', 'link': 'https://stackoverflow.com/users/4733010/nick-m', 'profile_image': 'https://www.gravatar.com/avatar/a1df00c15fcf9d8967b3db036876af9e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nick M'}",Is there a way to get feature importance from a sklearn's GridSearchCV? For example : Is there a way I can access feature importance ? Maybe something like,"from sklearn.model_selection import GridSearchCV
print(""starting grid search ......"")
optimized_GBM = GridSearchCV(LGBMRegressor(),
                             params,
                             cv=3,
                             n_jobs=-1)
# 
optimized_GBM.fit(tr, yvar)
preds2 = optimized_GBM.predict(te)
 optimized_GBM.feature_importances_
",8,21,0,0,
828,50065731,50065808,65578,Pandas Dataframe or similar in C#.NET,4,<c#><python><.net><pandas><dataframe>,40,"<p>I am currently working on implement the C# version of a Gurobi linear program model that was earlier built in Python. I have a number of CSV files from which I was importing the data and creating pandas dataframes, and I was fetching columns from those dataframes to create variables that I was using in my Linear Program. The python code for creating the variables using dataframes is as follows:</p>

<pre><code>dataPath = ""C:/Users/XYZ/Desktop/LinearProgramming/TestData""
routeData = pd.DataFrame.from_csv(os.path.join(dataPath, ""DirectLink.csv""), index_col=None)
#Creating 3 Python-dictionaries from Python Multi-Dict using column names and keeping RouteID as the key
routeID, transportCost, routeType = multidict({x[0]:[x[1],x[2]] for x in routeData[['RouteID', 'TransportCost','RouteType']].values}) 
</code></pre>

<p>Example: If the csv structure is as follows:</p>

<pre><code>RouteID  RouteEfficiency  TransportCost  RouteType
  1           0.8              2.00          F
  2           0.9              5.00          D
  3           0.7              6.00          R
  4           0.6              3.00          T     
</code></pre>

<p>The 3 variables should be:
RouteID: 1 2 3 4</p>

<p>TransportCost:</p>

<pre><code>1:2.00
2:5.00
3:6.00
4:3.00
</code></pre>

<p>RouteType:</p>

<pre><code>1:F
2:D
3:R
4:T
</code></pre>

<p>Now, I want to create a C# version of the above code that does the same task, but I learnt that C# doesn't support dataframes. I tried looking for a few alternatives, but am unable to find anything. Please help me with this.</p>
",4758347,1033,27-04-2018 15:31,27-04-2018 15:35,0,1033,23,2,12,64,"{'badge_counts': {'bronze': 23, 'silver': 12, 'gold': 2}, 'account_id': 6098273, 'is_employee': False, 'last_modified_date': 1580885102, 'last_access_date': 1615150603, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1033, 'creation_date': 1428403080, 'user_type': 'registered', 'user_id': 4758347, 'accept_rate': 64, 'location': 'Delhi', 'website_url': '', 'link': 'https://stackoverflow.com/users/4758347/amit-madan', 'profile_image': 'https://www.gravatar.com/avatar/5e8b74ffe47956599e08b528fa09c752?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Amit Madan'}","I am currently working on implement the C# version of a Gurobi linear program model that was earlier built in Python. I have a number of CSV files from which I was importing the data and creating pandas dataframes, and I was fetching columns from those dataframes to create variables that I was using in my Linear Program. The python code for creating the variables using dataframes is as follows: Example: If the csv structure is as follows: The 3 variables should be: RouteID: 1 2 3 4 TransportCost: RouteType: Now, I want to create a C# version of the above code that does the same task, but I learnt that C# doesn't support dataframes. I tried looking for a few alternatives, but am unable to find anything. Please help me with this.","dataPath = ""C:/Users/XYZ/Desktop/LinearProgramming/TestData""
routeData = pd.DataFrame.from_csv(os.path.join(dataPath, ""DirectLink.csv""), index_col=None)
#Creating 3 Python-dictionaries from Python Multi-Dict using column names and keeping RouteID as the key
routeID, transportCost, routeType = multidict({x[0]:[x[1],x[2]] for x in routeData[['RouteID', 'TransportCost','RouteType']].values}) 
 RouteID  RouteEfficiency  TransportCost  RouteType
  1           0.8              2.00          F
  2           0.9              5.00          D
  3           0.7              6.00          R
  4           0.6              3.00          T     
 1:2.00
2:5.00
3:6.00
4:3.00
 1:F
2:D
3:R
4:T
",13,37,0,0,
829,49643225,49644300,140515,What's the difference between reshape and view in pytorch?,5,<python><pytorch>,241,"<p>In numpy, we use <code>ndarray.reshape()</code> for reshaping an array.</p>

<p>I noticed that in pytorch, people use <code>torch.view(...)</code> for the same purpose, but at the same time, there is also a <code>torch.reshape(...)</code> existing.</p>

<p>So I am wondering what the differences are between them and when I should use either of them?</p>
",4794308,12288,04-04-2018 05:09,04-04-2018 06:35,0,12328,77,15,56,81,"{'badge_counts': {'bronze': 77, 'silver': 56, 'gold': 15}, 'account_id': 6149437, 'is_employee': False, 'last_modified_date': 1706212507, 'last_access_date': 1710103083, 'reputation_change_year': 198, 'reputation_change_quarter': 198, 'reputation_change_month': 40, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 12328, 'creation_date': 1429144654, 'user_type': 'registered', 'user_id': 4794308, 'accept_rate': 81, 'location': 'Bellevue, WA, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/4794308/lifu-huang', 'profile_image': 'https://www.gravatar.com/avatar/3bee362180a8edf2f044f3ef6ff2f9ef?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Lifu Huang'}","In numpy, we use for reshaping an array. I noticed that in pytorch, people use for the same purpose, but at the same time, there is also a existing. So I am wondering what the differences are between them and when I should use either of them?",ndarray.reshape() torch.view(...) torch.reshape(...),-3,5,0,0,
830,48198031,48206009,6643,How to add variables to progress bar in Keras?,3,<python><keras><tensorboard><keras-2>,16,"<p>I'd like to monitor eg. the learning rate during training in Keras both in the progress bar and in Tensorboard. I figure there must be a way to specify which variables are logged, but there's no immediate clarification on this issue on the Keras <a href=""https://keras.io"" rel=""noreferrer"">website</a>.</p>

<p>I guess it's got something to do with creating a custom <a href=""https://keras.io/callbacks"" rel=""noreferrer"">Callback</a> function, however, it should be possible to modify the already existing progress bar callback, no?</p>
",4838303,454,11-01-2018 00:06,11-01-2018 11:29,0,454,16,0,4,100,"{'badge_counts': {'bronze': 16, 'silver': 4, 'gold': 0}, 'account_id': 6212387, 'is_employee': False, 'last_modified_date': 1613570700, 'last_access_date': 1583247128, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 454, 'creation_date': 1430147290, 'user_type': 'registered', 'user_id': 4838303, 'accept_rate': 100, 'location': 'Palo Alto, CA, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/4838303/neergaard', 'profile_image': 'https://www.gravatar.com/avatar/bfc37fe64855a1da773dba6bb9b03df1?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Neergaard'}","I'd like to monitor eg. the learning rate during training in Keras both in the progress bar and in Tensorboard. I figure there must be a way to specify which variables are logged, but there's no immediate clarification on this issue on the Keras website. I guess it's got something to do with creating a custom Callback function, however, it should be possible to modify the already existing progress bar callback, no?",,0,3,0,2,
831,48073380,54976550,23925,HashSets and HashTables in Python,3,<python><data-structures>,11,"<p>Is there any <strong>HashSet</strong> implementation in Python? I know <strong>HashTable</strong> can be represented using dictionaries, but how do we represent <strong>HashSet</strong> implementation.</p>

<p>I am NOT looking for a data structure with the same methods as <strong>HashSets</strong> but rather someone with  a CONSTANT lookup time, or the order of O(1);</p>

<p>Also, I want to know if the lookup time in a Python <code>Dictionary</code> is constant aka O(1)</p>
",6021597,8976,03-01-2018 07:59,04-03-2019 04:17,425,8996,93,10,60,77,"{'badge_counts': {'bronze': 93, 'silver': 60, 'gold': 10}, 'account_id': 7979803, 'is_employee': False, 'last_modified_date': 1701246900, 'last_access_date': 1711115178, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 8996, 'creation_date': 1457165168, 'user_type': 'registered', 'user_id': 6021597, 'accept_rate': 77, 'location': 'Delhi, India', 'website_url': 'http://ayushgupta.me', 'link': 'https://stackoverflow.com/users/6021597/ayush-gupta', 'profile_image': 'https://i.stack.imgur.com/iyl6H.jpg?s=256&g=1', 'display_name': 'Ayush Gupta'}","Is there any HashSet implementation in Python? I know HashTable can be represented using dictionaries, but how do we represent HashSet implementation. I am NOT looking for a data structure with the same methods as HashSets but rather someone with a CONSTANT lookup time, or the order of O(1); Also, I want to know if the lookup time in a Python is constant aka O(1)",Dictionary,-1,5,0,0,
832,48849047,48849088,21077,Replacing PyString_FromString method in python 3,1,<python><c++>,21,"<p>TLDR; <code>PyString_FromString</code> doesn't work in Python3.5 so I need an alternative.</p>

<p>I am following an example for including python 3 within a C++ project from this python documentation:
<a href=""https://docs.python.org/3/extending/embedding.html"" rel=""noreferrer"">https://docs.python.org/3/extending/embedding.html</a></p>

<p>Everything is working fine but now I want to change the line that reads:</p>

<p><code>pValue = PyLong_FromLong(atoi(argv[i + 3]));</code></p>

<p>to the following (I also no longer use i, that was from a loop):</p>

<p><code>pValue = PyString_FromString(""A string instead of a number"");</code></p>

<p>It seems the <code>PyString_FromString</code> function is no longer an option Python3.5 and I get the following error when I compile the code with g++:</p>

<pre><code>main.cpp:559:60: error: ‘PyString_FromString’ was not declared in this scope
         pValue = PyString_FromString(""A string instead of a number"");`
</code></pre>

<p>Any idea on how I get around this? No matter what I look for I can't seem to find a solution that doesn't throw an error. I need to pass a string to my Python file and I have a string to start with already.</p>
",4903907,213,18-02-2018 06:23,18-02-2018 06:29,0,213,6,1,2,,"{'badge_counts': {'bronze': 6, 'silver': 2, 'gold': 1}, 'account_id': 6312929, 'is_employee': False, 'last_modified_date': 1573679624, 'last_access_date': 1520822268, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 213, 'creation_date': 1431694423, 'user_type': 'registered', 'user_id': 4903907, 'link': 'https://stackoverflow.com/users/4903907/mr-c', 'profile_image': 'https://www.gravatar.com/avatar/28379b4b6683c919f8a48d930166ef47?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Mr. C'}","TLDR; doesn't work in Python3.5 so I need an alternative. I am following an example for including python 3 within a C++ project from this python documentation: https://docs.python.org/3/extending/embedding.html Everything is working fine but now I want to change the line that reads: to the following (I also no longer use i, that was from a loop): It seems the function is no longer an option Python3.5 and I get the following error when I compile the code with g++: Any idea on how I get around this? No matter what I look for I can't seem to find a solution that doesn't throw an error. I need to pass a string to my Python file and I have a string to start with already.","PyString_FromString pValue = PyLong_FromLong(atoi(argv[i + 3])); pValue = PyString_FromString(""A string instead of a number""); PyString_FromString main.cpp:559:60: error: ‘PyString_FromString’ was not declared in this scope
         pValue = PyString_FromString(""A string instead of a number"");`
",-3,20,0,1,
833,48054521,48054538,18625,Indicating multiple value in a Dict[] for type hints,1,<python><dictionary>,26,"<p>How do I express the type of a <code>Dict</code> which has two keys that take two different types of values? For example:</p>

<pre><code>a = {'1': [], '2': {})
</code></pre>

<p>The following is just to give you an idea of what I am looking for.</p>

<blockquote>
  <p>Dict[(str, List), (str, Set)]</p>
</blockquote>
",4961390,608,02-01-2018 01:05,02-01-2018 01:10,0,608,14,1,7,33,"{'badge_counts': {'bronze': 14, 'silver': 7, 'gold': 1}, 'account_id': 4367581, 'is_employee': False, 'last_modified_date': 1607614554, 'last_access_date': 1606669403, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 608, 'creation_date': 1433162172, 'user_type': 'registered', 'user_id': 4961390, 'accept_rate': 33, 'link': 'https://stackoverflow.com/users/4961390/nijan', 'profile_image': 'https://www.gravatar.com/avatar/00ee8742b84cd0fdc9541058157c0c35?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Nijan'}","How do I express the type of a which has two keys that take two different types of values? For example: The following is just to give you an idea of what I am looking for. Dict[(str, List), (str, Set)]","Dict a = {'1': [], '2': {})
",-1,10,0,0,
834,49134831,49134952,8296,Django: Make user email required,3,<python><django>,23,"<p>For my application the email field of a User should be required. That's not the case for the default User model. So I thought it would make sense to create a custom user model as described in the <a href=""https://docs.djangoproject.com/en/2.0/topics/auth/customizing/#using-a-custom-user-model-when-starting-a-project"" rel=""noreferrer"">docs</a>:</p>

<blockquote>
  <p>If you’re starting a new project, it’s highly recommended to set up a
  custom user model, even if the default User model is sufficient for
  you. This model behaves identically to the default user model, but
  you’ll be able to customize it in the future if the need arises:</p>
</blockquote>

<pre><code>from django.contrib.auth.models import AbstractUser

class User(AbstractUser):
    email = models.EmailField(_('email address'), blank=False)
</code></pre>

<p>However, I am not sure if that's the correct way to achieve this.</p>

<p>The reason is that <a href=""https://docs.djangoproject.com/en/2.0/topics/db/models/#abstract-base-classes"" rel=""noreferrer"">here</a> the Django docs say:</p>

<blockquote>
  <p>Abstract base classes are useful when you want to put some common
  information into a number of other models. You write your base class
  and put abstract=True in the Meta class. This model will then not be
  used to create any database table. Instead, when it is used as a base
  class for other models, its fields will be added to those of the child
  class. <strong>It is an error to have fields in the abstract base class with
  the same name as those in the child (and Django will raise an
  exception).</strong></p>
</blockquote>

<p>What should I do now?</p>
",5005715,2151,06-03-2018 15:47,06-03-2018 15:52,0,2151,40,4,21,44,"{'badge_counts': {'bronze': 40, 'silver': 21, 'gold': 4}, 'account_id': 6462405, 'is_employee': False, 'last_modified_date': 1676319600, 'last_access_date': 1576579298, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2151, 'creation_date': 1434182735, 'user_type': 'registered', 'user_id': 5005715, 'accept_rate': 44, 'link': 'https://stackoverflow.com/users/5005715/aliquis', 'profile_image': 'https://www.gravatar.com/avatar/5e42356804cea53698bbf4317b5602cb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Aliquis'}","For my application the email field of a User should be required. That's not the case for the default User model. So I thought it would make sense to create a custom user model as described in the docs: If you’re starting a new project, it’s highly recommended to set up a custom user model, even if the default User model is sufficient for you. This model behaves identically to the default user model, but you’ll be able to customize it in the future if the need arises: However, I am not sure if that's the correct way to achieve this. The reason is that here the Django docs say: Abstract base classes are useful when you want to put some common information into a number of other models. You write your base class and put abstract=True in the Meta class. This model will then not be used to create any database table. Instead, when it is used as a base class for other models, its fields will be added to those of the child class. It is an error to have fields in the abstract base class with the same name as those in the child (and Django will raise an exception). What should I do now?","from django.contrib.auth.models import AbstractUser

class User(AbstractUser):
    email = models.EmailField(_('email address'), blank=False)
",3,31,0,2,
835,50199917,50199964,29437,Python - JSON array to DataFrame,2,<python><json><pandas><dataframe>,14,"<p>I have this following <code>JSON</code> array.</p>

<pre><code>[
    {
        ""foo""=1
    },
    {
        ""foo""=2
    },
    ...
]
</code></pre>

<p>I would like to convert it to <code>DataFrame</code> object using <code>pd.read_json()</code> command like below.</p>

<pre><code>df = pd.read_json(my_json) #my_json is JSON array above
</code></pre>

<p>However, I got the error, since <code>my_json</code> is a <code>list</code>/<code>array</code> of <code>json</code>. The error is <code>ValueError: Invalid file path or buffer object type: &lt;class 'list'&gt;</code>.</p>

<p>Besides iterating through the <code>list</code>, is there any efficient way to extract/convert the <code>JSON</code> to <code>DataFrame</code> object?</p>
",5023889,4361,06-05-2018 12:52,06-05-2018 12:57,0,4381,37,4,21,77,"{'badge_counts': {'bronze': 37, 'silver': 21, 'gold': 4}, 'account_id': 6489344, 'is_employee': False, 'last_modified_date': 1700271600, 'last_access_date': 1708644635, 'reputation_change_year': 240, 'reputation_change_quarter': 240, 'reputation_change_month': 70, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4381, 'creation_date': 1434628695, 'user_type': 'registered', 'user_id': 5023889, 'accept_rate': 77, 'location': 'Brisbane QLD, Australia', 'website_url': '', 'link': 'https://stackoverflow.com/users/5023889/darren-christopher', 'profile_image': 'https://www.gravatar.com/avatar/e1e62d1a2e782e56c9ecd382fb680c47?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Darren Christopher'}","I have this following array. I would like to convert it to object using command like below. However, I got the error, since is a / of . The error is . Besides iterating through the , is there any efficient way to extract/convert the to object?","JSON [
    {
        ""foo""=1
    },
    {
        ""foo""=2
    },
    ...
]
 DataFrame pd.read_json() df = pd.read_json(my_json) #my_json is JSON array above
 my_json list array json ValueError: Invalid file path or buffer object type: &lt;class 'list'&gt; list JSON DataFrame",-3,21,0,0,
836,48391777,48394004,19502,nargs=* equivalent for options in Click,4,<python><python-3.x><python-click>,37,"<p>Is there an equivalent to <code>argparse</code>'s <code>nargs='*'</code> functionality for optional arguments in Click?</p>
<p>I am writing a command line script, and one of the options needs to be able to take an unlimited number of arguments, like:</p>
<pre><code>foo --users alice bob charlie --bar baz
</code></pre>
<p>So <code>users</code> would be <code>['alice', 'bob', 'charlie']</code> and <code>bar</code> would be <code>'baz'</code>.</p>
<p>In <a href=""https://docs.python.org/3/library/argparse.html#nargs"" rel=""noreferrer""><code>argparse</code></a>, I can specify multiple optional arguments to collect all of the arguments that follow them by setting <code>nargs='*'</code>.</p>
<pre><code>&gt;&gt;&gt; parser = argparse.ArgumentParser()
&gt;&gt;&gt; parser.add_argument('--users', nargs='*')
&gt;&gt;&gt; parser.add_argument('--bar')
&gt;&gt;&gt; parser.parse_args('--users alice bob charlie --bar baz'.split())
Namespace(bar='baz', users=['alice', 'bob', 'charlie'])
</code></pre>
<p>I know Click allows you to specify an argument to <a href=""http://click.pocoo.org/6/arguments/#variadic-arguments"" rel=""noreferrer"">accept unlimited inputs</a> by setting <code>nargs=-1</code>, but when I try to set an optional argument's <code>nargs</code> to -1, I get:</p>
<blockquote>
<p>TypeError: Options cannot have nargs &lt; 0</p>
</blockquote>
<p>Is there a way to make Click accept an unspecified number of arguments for an option?</p>
<h3>Update:</h3>
<p>I need to be able to specify options after the option that takes unlimited arguments.</p>
<h3>Update:</h3>
<p>@Stephen Rauch's answer answers this question.  However, I don't recommend using the approach I ask for here.  My feature request is <a href=""https://github.com/pallets/click/issues/484"" rel=""noreferrer"">intentionally not implemented in Click</a>, since it can result in unexpected behaviors.  <a href=""https://click.palletsprojects.com/en/7.x/options/#multiple-options"" rel=""noreferrer"">Click's recommended approach is to use <code>multiple=True</code></a>:</p>
<pre><code>@click.option('-u', '--user', 'users', multiple=True)
</code></pre>
<p>And in the command line, it will look like:</p>
<pre><code>foo -u alice -u bob -u charlie --bar baz
</code></pre>
",5031373,4130,22-01-2018 23:16,23-01-2018 04:01,1,4150,69,9,42,69,"{'badge_counts': {'bronze': 69, 'silver': 42, 'gold': 9}, 'account_id': 6500265, 'is_employee': False, 'last_modified_date': 1672249074, 'last_access_date': 1709661672, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 4150, 'creation_date': 1434811060, 'user_type': 'registered', 'user_id': 5031373, 'accept_rate': 69, 'location': 'Texas', 'website_url': 'https://thunderrock.org', 'link': 'https://stackoverflow.com/users/5031373/jpyams', 'profile_image': 'https://www.gravatar.com/avatar/d81e978e1224d0daa1eee8d105ba3ff8?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jpyams'}","Is there an equivalent to 's functionality for optional arguments in Click? I am writing a command line script, and one of the options needs to be able to take an unlimited number of arguments, like: So would be and would be . In , I can specify multiple optional arguments to collect all of the arguments that follow them by setting . I know Click allows you to specify an argument to accept unlimited inputs by setting , but when I try to set an optional argument's to -1, I get: TypeError: Options cannot have nargs &lt; 0 Is there a way to make Click accept an unspecified number of arguments for an option? Update: I need to be able to specify options after the option that takes unlimited arguments. Update: @Stephen Rauch's answer answers this question. However, I don't recommend using the approach I ask for here. My feature request is intentionally not implemented in Click, since it can result in unexpected behaviors. Click's recommended approach is to use : And in the command line, it will look like:","argparse nargs='*' foo --users alice bob charlie --bar baz
 users ['alice', 'bob', 'charlie'] bar 'baz' argparse nargs='*' &gt;&gt;&gt; parser = argparse.ArgumentParser()
&gt;&gt;&gt; parser.add_argument('--users', nargs='*')
&gt;&gt;&gt; parser.add_argument('--bar')
&gt;&gt;&gt; parser.parse_args('--users alice bob charlie --bar baz'.split())
Namespace(bar='baz', users=['alice', 'bob', 'charlie'])
 nargs=-1 nargs multiple=True @click.option('-u', '--user', 'users', multiple=True)
 foo -u alice -u bob -u charlie --bar baz
",-7,26,0,4,
837,48743032,48743122,13924,Get intermediate data state in scikit-learn Pipeline,7,<python><scikit-learn>,30,"<p>Given the  following example:</p>

<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
from sklearn.pipeline import Pipeline
import pandas as pd

pipe = Pipeline([
    (""tf_idf"", TfidfVectorizer()),
    (""nmf"", NMF())
])

data = pd.DataFrame([[""Salut comment tu vas"", ""Hey how are you today"", ""I am okay and you ?""]]).T
data.columns = [""test""]

pipe.fit_transform(data.test)
</code></pre>

<p>I would like to get intermediate data state in scikit learn pipeline corresponding to tf_idf output (after fit_transform on tf_idf but not NMF) or NMF input.
Or to say things in another way, it would be the same than to apply</p>

<pre><code>TfidfVectorizer().fit_transform(data.test)
</code></pre>

<p>I know pipe.named_steps[""tf_idf""] ti get intermediate transformer, but I can't get data, only parameters of the transformer with this method.</p>
",5074448,934,12-02-2018 09:22,12-02-2018 09:28,0,934,20,1,10,,"{'badge_counts': {'bronze': 20, 'silver': 10, 'gold': 1}, 'account_id': 6564317, 'is_employee': False, 'last_modified_date': 1677291000, 'last_access_date': 1678999120, 'reputation_change_year': 8, 'reputation_change_quarter': 8, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 934, 'creation_date': 1435849870, 'user_type': 'registered', 'user_id': 5074448, 'location': 'Paris, France', 'website_url': 'https://thibaultbl.github.io/', 'link': 'https://stackoverflow.com/users/5074448/thibaultbl', 'profile_image': 'https://www.gravatar.com/avatar/5c8ea2f83366e8699b607454da88caeb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'thibaultbl'}","Given the following example: I would like to get intermediate data state in scikit learn pipeline corresponding to tf_idf output (after fit_transform on tf_idf but not NMF) or NMF input. Or to say things in another way, it would be the same than to apply I know pipe.named_steps[""tf_idf""] ti get intermediate transformer, but I can't get data, only parameters of the transformer with this method.","from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
from sklearn.pipeline import Pipeline
import pandas as pd

pipe = Pipeline([
    (""tf_idf"", TfidfVectorizer()),
    (""nmf"", NMF())
])

data = pd.DataFrame([[""Salut comment tu vas"", ""Hey how are you today"", ""I am okay and you ?""]]).T
data.columns = [""test""]

pipe.fit_transform(data.test)
 TfidfVectorizer().fit_transform(data.test)
",13,25,0,0,
838,48234072,48234099,5759,How to sort a list and handle None values properly?,3,<python>,15,"<p>I am trying to sort a list of objects using the date attribute with</p>
<pre class=""lang-py prettyprint-override""><code>list_of_objects.sort(key=lambda x: x.date, reverse=True)
</code></pre>
<p>but some dates are just <code>None</code>, which means that I get the error</p>
<pre class=""lang-none prettyprint-override""><code>TypeError: can't compare datetime.datetime to NoneType
</code></pre>
<p>is there a way to account for this? e.g. Have objects with <code>date == None</code> at the top or bottom of the sorted list—or do I need to do this manually?</p>
",5121448,4316,12-01-2018 21:13,12-01-2018 21:15,0,4326,106,10,57,53,"{'badge_counts': {'bronze': 106, 'silver': 57, 'gold': 10}, 'account_id': 6633576, 'is_employee': False, 'last_modified_date': 1693681201, 'last_access_date': 1699531640, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4326, 'creation_date': 1437002279, 'user_type': 'registered', 'user_id': 5121448, 'accept_rate': 53, 'link': 'https://stackoverflow.com/users/5121448/carl', 'profile_image': 'https://www.gravatar.com/avatar/dfa05fc1b915aaa82096aceae1cf6903?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'carl'}","I am trying to sort a list of objects using the date attribute with but some dates are just , which means that I get the error is there a way to account for this? e.g. Have objects with at the top or bottom of the sorted list—or do I need to do this manually?","list_of_objects.sort(key=lambda x: x.date, reverse=True)
 None TypeError: can't compare datetime.datetime to NoneType
 date == None",-2,7,0,0,
839,50052295,50052355,47372,How do you load MNIST images into Pytorch DataLoader?,2,<python><pytorch>,23,"<p>The pytorch tutorial for data loading and processing is quite specific to one example, could someone help me with what the function should look like for a more generic simple loading of images?</p>

<p>Tutorial: <a href=""http://pytorch.org/tutorials/beginner/data_loading_tutorial.html"" rel=""noreferrer"">http://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a></p>

<p>My Data:</p>

<p>I have the MINST dataset as jpg's in the following folder structure. (I know I can just use the dataset class, but this is purely to see how to load simple images into pytorch without csv's or complex features).</p>

<p>The folder name is the label and the images are 28x28 png's in greyscale, no transformations required.</p>

<pre><code>data
    train
        0
            3.png
            5.png
            13.png
            23.png
            ...
        1
            3.png
            10.png
            11.png
            ...
        2
            4.png
            13.png
            ...
        3
            8.png
            ...
        4
            ...
        5
            ...
        6
            ...
        7
            ...
        8
            ...
        9
            ...
</code></pre>
",5149399,1641,26-04-2018 21:46,26-04-2018 21:51,0,1651,45,4,25,85,"{'badge_counts': {'bronze': 45, 'silver': 25, 'gold': 4}, 'account_id': 6674135, 'is_employee': False, 'last_modified_date': 1679924109, 'last_access_date': 1605203160, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1651, 'creation_date': 1437674967, 'user_type': 'registered', 'user_id': 5149399, 'accept_rate': 85, 'link': 'https://stackoverflow.com/users/5149399/terry', 'profile_image': 'https://www.gravatar.com/avatar/64fc9a4cfe89435665c32ebcdad465b9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Terry'}","The pytorch tutorial for data loading and processing is quite specific to one example, could someone help me with what the function should look like for a more generic simple loading of images? Tutorial: http://pytorch.org/tutorials/beginner/data_loading_tutorial.html My Data: I have the MINST dataset as jpg's in the following folder structure. (I know I can just use the dataset class, but this is purely to see how to load simple images into pytorch without csv's or complex features). The folder name is the label and the images are 28x28 png's in greyscale, no transformations required.","data
    train
        0
            3.png
            5.png
            13.png
            23.png
            ...
        1
            3.png
            10.png
            11.png
            ...
        2
            4.png
            13.png
            ...
        3
            8.png
            ...
        4
            ...
        5
            ...
        6
            ...
        7
            ...
        8
            ...
        9
            ...
",31,43,0,1,
840,49782598,49783226,31121,Progress bar with tqdm while iterating over the items in a python dictionary,2,<python><dictionary><iteration><items><tqdm>,16,"<p>If I'm trying to get a progress bar while iterating over a dict, how can I do this with tqdm? I'm using Python 2.7.</p>

<p>This works great with lists:</p>

<pre><code>for i in tdqm(l, len(l):
    &lt;do stuff&gt;   
</code></pre>

<p>But fails over dicts:</p>

<pre><code>for k, v in tqdm(d.items(), len(d)):
   &lt;do stuff&gt;
</code></pre>

<p>What's the proper way to do this with dicts?</p>

<p>Here's a real example:</p>

<pre><code>d = {'k1':1, 'k2':2}
for k, v in tqdm(d.items(), len(d)):
    print 'foo'
    a = 1 + 100
    print 'bar'
</code></pre>

<p>I get:</p>

<pre><code>-------------------------------------------------------------------
TypeError                         Traceback (most recent call last)
&lt;ipython-input-30-7e4ce2b85414&gt; in &lt;module&gt;()
      1 d = {'k1':1, 'k2':2}
----&gt; 2 for k, v in tqdm(d.items(), len(d)):
      3     print 'oasdlkfj'
      4     a = 1 + 100
      5     print 'y'

/home/monica/anaconda2/envs/pytorch_p27/lib/python2.7/site-packages/tqdm/_tqdm.pyc in __init__(self, iterable, desc, total, leave, file, ncols, mininterval, maxinterval, miniters, ascii, disable, unit, unit_scale, dynamic_ncols, smoothing, bar_format, initial, position, postfix, unit_divisor, gui, **kwargs)
    810                 if self.pos:
    811                     self.moveto(self.pos)
--&gt; 812                 self.sp(self.__repr__(elapsed=0))
    813                 if self.pos:
    814                     self.moveto(-self.pos)

/home/monica/anaconda2/envs/pytorch_p27/lib/python2.7/site-packages/tqdm/_tqdm.pyc in __repr__(self, elapsed)
    842             self.desc, self.ascii, self.unit,
    843             self.unit_scale, 1 / self.avg_time if self.avg_time else None,
--&gt; 844             self.bar_format, self.postfix, self.unit_divisor)
    845 
    846     def __lt__(self, other):

/home/monica/anaconda2/envs/pytorch_p27/lib/python2.7/site-packages/tqdm/_tqdm.pyc in format_meter(n, total, elapsed, ncols, prefix, ascii, unit, unit_scale, rate, bar_format, postfix, unit_divisor)
    288             if prefix:
    289                 # old prefix setup work around
--&gt; 290                 bool_prefix_colon_already = (prefix[-2:] == "": "")
    291                 l_bar = prefix if bool_prefix_colon_already else prefix + "": ""
    292             else:

TypeError: 'int' object has no attribute '__getitem__'
</code></pre>
",5225453,3033,11-04-2018 18:58,11-04-2018 19:39,0,3043,89,10,56,50,"{'badge_counts': {'bronze': 89, 'silver': 56, 'gold': 10}, 'account_id': 6787088, 'is_employee': False, 'last_modified_date': 1710555300, 'last_access_date': 1711086446, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3043, 'creation_date': 1439507750, 'user_type': 'registered', 'user_id': 5225453, 'accept_rate': 50, 'link': 'https://stackoverflow.com/users/5225453/monica-heddneck', 'profile_image': 'https://www.gravatar.com/avatar/b5cb9fe975f6d5a7417f9ab0efd87ba0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Monica Heddneck'}","If I'm trying to get a progress bar while iterating over a dict, how can I do this with tqdm? I'm using Python 2.7. This works great with lists: But fails over dicts: What's the proper way to do this with dicts? Here's a real example: I get:","for i in tdqm(l, len(l):
    &lt;do stuff&gt;   
 for k, v in tqdm(d.items(), len(d)):
   &lt;do stuff&gt;
 d = {'k1':1, 'k2':2}
for k, v in tqdm(d.items(), len(d)):
    print 'foo'
    a = 1 + 100
    print 'bar'
 -------------------------------------------------------------------
TypeError                         Traceback (most recent call last)
&lt;ipython-input-30-7e4ce2b85414&gt; in &lt;module&gt;()
      1 d = {'k1':1, 'k2':2}
----&gt; 2 for k, v in tqdm(d.items(), len(d)):
      3     print 'oasdlkfj'
      4     a = 1 + 100
      5     print 'y'

/home/monica/anaconda2/envs/pytorch_p27/lib/python2.7/site-packages/tqdm/_tqdm.pyc in __init__(self, iterable, desc, total, leave, file, ncols, mininterval, maxinterval, miniters, ascii, disable, unit, unit_scale, dynamic_ncols, smoothing, bar_format, initial, position, postfix, unit_divisor, gui, **kwargs)
    810                 if self.pos:
    811                     self.moveto(self.pos)
--&gt; 812                 self.sp(self.__repr__(elapsed=0))
    813                 if self.pos:
    814                     self.moveto(-self.pos)

/home/monica/anaconda2/envs/pytorch_p27/lib/python2.7/site-packages/tqdm/_tqdm.pyc in __repr__(self, elapsed)
    842             self.desc, self.ascii, self.unit,
    843             self.unit_scale, 1 / self.avg_time if self.avg_time else None,
--&gt; 844             self.bar_format, self.postfix, self.unit_divisor)
    845 
    846     def __lt__(self, other):

/home/monica/anaconda2/envs/pytorch_p27/lib/python2.7/site-packages/tqdm/_tqdm.pyc in format_meter(n, total, elapsed, ncols, prefix, ascii, unit, unit_scale, rate, bar_format, postfix, unit_divisor)
    288             if prefix:
    289                 # old prefix setup work around
--&gt; 290                 bool_prefix_colon_already = (prefix[-2:] == "": "")
    291                 l_bar = prefix if bool_prefix_colon_already else prefix + "": ""
    292             else:

TypeError: 'int' object has no attribute '__getitem__'
",36,59,0,0,
841,50361218,50495306,6430,"Remove the word ""module"" from Sphinx documentation",2,<python><python-sphinx>,13,"<p>Using <a href=""http://www.sphinx-doc.org/en/master/"" rel=""noreferrer"">Sphinx</a> for documenting my Python project. I want to <strong>remove the word ""module""</strong> which follows the name of each python file (in the navbar, TOC, the page title, etc).</p>

<p><strong>e.g. Details:</strong></p>

<p>The project is composed of 2 files <code>utils.py</code> and <code>main.py</code>.</p>

<p>In my <code>index.rst</code> file, I use:</p>

<pre><code>.. toctree::
   :maxdepth: 2

   utils
   main
</code></pre>

<p>to import both files as ""modules"". From the <code>docs/</code> folder, I then call:</p>

<pre><code>sphinx-apidoc -f -o ./source/ .. 
make html
</code></pre>

<p>to generate the static site. In the site, the word ""module"" follows every file name, and I would like to remove it.</p>
",5228288,763,16-05-2018 01:27,23-05-2018 18:39,7,763,21,0,7,,"{'badge_counts': {'bronze': 21, 'silver': 7, 'gold': 0}, 'account_id': 6791216, 'is_employee': False, 'last_modified_date': 1707864924, 'last_access_date': 1711096451, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 763, 'creation_date': 1439572873, 'user_type': 'registered', 'user_id': 5228288, 'location': 'Toronto, ON, Canada', 'website_url': '', 'link': 'https://stackoverflow.com/users/5228288/jessexknight', 'profile_image': 'https://lh5.googleusercontent.com/-K890rK5vFtQ/AAAAAAAAAAI/AAAAAAAAABM/RKRbvVHdL0w/photo.jpg?sz=256', 'display_name': 'jessexknight'}","Using Sphinx for documenting my Python project. I want to remove the word ""module"" which follows the name of each python file (in the navbar, TOC, the page title, etc). e.g. Details: The project is composed of 2 files and . In my file, I use: to import both files as ""modules"". From the folder, I then call: to generate the static site. In the site, the word ""module"" follows every file name, and I would like to remove it.","utils.py main.py index.rst .. toctree::
   :maxdepth: 2

   utils
   main
 docs/ sphinx-apidoc -f -o ./source/ .. 
make html
",1,22,0,1,
842,48170405,48170725,17950,Is pd.get_dummies one-hot encoding?,2,<python><pandas><scikit-learn>,16,"<p><a href=""https://stats.stackexchange.com/q/224051/99338"">Given</a> the difference between one-hot encoding and dummy coding, is the <code>pandas.get_dummies</code> method one-hot encoding when using default parameters (i.e. <code>drop_first=False</code>)?</p>

<p>If so, does it make sense that I remove the intercept from the logistic regression model? Here is an example:</p>

<pre><code># I assume I have already my dataset in a DataFrame X and the true labels in y
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X = pd.get_dummies(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .80)

clf = LogisticRegression(fit_intercept=False)
clf.fit(X_train, y_train)
</code></pre>
",4146521,1316,09-01-2018 14:31,09-01-2018 14:48,0,1316,32,4,15,87,"{'badge_counts': {'bronze': 32, 'silver': 15, 'gold': 4}, 'account_id': 5180719, 'is_employee': False, 'last_modified_date': 1573680028, 'last_access_date': 1710499504, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1316, 'creation_date': 1413391296, 'user_type': 'registered', 'user_id': 4146521, 'accept_rate': 87, 'location': 'Copenhagen, Denmark', 'website_url': 'http://soundcloud.com/mattiapaterna', 'link': 'https://stackoverflow.com/users/4146521/mattia-paterna', 'profile_image': 'https://i.stack.imgur.com/jx6Ei.jpg?s=256&g=1', 'display_name': 'Mattia Paterna'}","Given the difference between one-hot encoding and dummy coding, is the method one-hot encoding when using default parameters (i.e. )? If so, does it make sense that I remove the intercept from the logistic regression model? Here is an example:","pandas.get_dummies drop_first=False # I assume I have already my dataset in a DataFrame X and the true labels in y
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X = pd.get_dummies(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .80)

clf = LogisticRegression(fit_intercept=False)
clf.fit(X_train, y_train)
",7,15,0,1,
843,50093718,50095130,35490,Airflow Python Script with execution_date in op_kwargs,1,<python><airflow>,19,"<p>With assistance from this answer <a href=""https://stackoverflow.com/a/41730510/4200352"">https://stackoverflow.com/a/41730510/4200352</a> I am executing a python file.</p>

<p>I use PythonOperator and am trying to include the execution date as an argument passed to the script.</p>

<p>I believe I can access it somehow through kwargs['execution_date'].</p>

<p>The below fails</p>

<p><strong>DAG.py</strong></p>

<pre><code>from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

import sys
import os
sys.path.append(os.path.abspath(""/home/glsam/OmegaAPI/airflow/scripts/PyPer_ogi_simple""))
from update_benchmarks import *


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2018, 4, 23),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG('run_pyPer', default_args=default_args)

update_BM_G027 = PythonOperator(
    task_id='update_BM_G027',
    python_callable=update_bmk,
    dag=dag,
    op_kwargs={
        'bmk_code': 'G027',
        'is_hedged': False,
        'from_date': kwargs['execution_date'],
    })
</code></pre>

<p>Do maybe i need to use this answer to get the date then XCOM it to the task? <a href=""https://stackoverflow.com/a/36754930/4200352"">https://stackoverflow.com/a/36754930/4200352</a></p>
",4200352,1218,30-04-2018 03:54,30-04-2018 06:34,0,1218,31,3,13,95,"{'badge_counts': {'bronze': 31, 'silver': 13, 'gold': 3}, 'account_id': 5258035, 'is_employee': False, 'last_modified_date': 1707527700, 'last_access_date': 1710813773, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1218, 'creation_date': 1414705473, 'user_type': 'registered', 'user_id': 4200352, 'accept_rate': 95, 'link': 'https://stackoverflow.com/users/4200352/glenn-sampson', 'profile_image': 'https://www.gravatar.com/avatar/31842bc17e796c78ebaf71797193ef16?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Glenn Sampson'}",With assistance from this answer https://stackoverflow.com/a/41730510/4200352 I am executing a python file. I use PythonOperator and am trying to include the execution date as an argument passed to the script. I believe I can access it somehow through kwargs['execution_date']. The below fails DAG.py Do maybe i need to use this answer to get the date then XCOM it to the task? https://stackoverflow.com/a/36754930/4200352,"from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

import sys
import os
sys.path.append(os.path.abspath(""/home/glsam/OmegaAPI/airflow/scripts/PyPer_ogi_simple""))
from update_benchmarks import *


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2018, 4, 23),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG('run_pyPer', default_args=default_args)

update_BM_G027 = PythonOperator(
    task_id='update_BM_G027',
    python_callable=update_bmk,
    dag=dag,
    op_kwargs={
        'bmk_code': 'G027',
        'is_hedged': False,
        'from_date': kwargs['execution_date'],
    })
",31,45,0,2,
844,48682982,48683152,18688,django Cannot find static/css files error 404,4,<python><css><django><static>,14,"<p>This problem seems to be widely known, however, most are old Q&amp;A's and they do not solve my problem.</p>

<p>I am using Django 2.0.2 and when my simple web page is not rendering my bootstrap.css file because it simply cannot find it.</p>

<p>I am following this tutorial: <a href=""https://simpleisbetterthancomplex.com/series/2017/09/11/a-complete-beginners-guide-to-django-part-2.html#static-files-setup"" rel=""noreferrer"">https://simpleisbetterthancomplex.com/series/2017/09/11/a-complete-beginners-guide-to-django-part-2.html#static-files-setup</a></p>

<p>This is my file structure:</p>

<pre><code>myproject/
 |-- myproject/
 |    |-- boards/
 |    |-- myproject/
 |    |-- templates/
 |    |-- static/
 |    |    +-- css/
 |    |         +-- bootstrap.min.css    &lt;-- here
 |    +-- manage.py
</code></pre>

<p>This is my static variables defined in the <code>settings.py</code> file.</p>

<pre><code>STATIC_URL = '/static/'

STATICFILES_DIR = [
    os.path.join(BASE_DIR, 'static'),
]
</code></pre>

<p>This is my <code>home.html</code> file as is with the bottom half cut out for brevity.</p>

<pre><code>{% load static %}&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta charset=""utf-8""&gt;
    &lt;title&gt;Boards&lt;/title&gt;
    &lt;link rel=""stylesheet"" href=""{% static 'css/bootstrap.min.css' %}""&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;Boards&lt;/h1&gt;
</code></pre>

<p>I have debugging set to true, and when I run my server and load it up, I am presented with the following: <code>[08/Feb/2018 19:26:02] ""GET /static/css/bootstrap.css HTTP/1.1"" 404 1675</code></p>

<p>I am also not able to visit this URL as it states a 404 error too: <code>http://127.0.0.1:8000/static/css/bootstrap.min.css</code>.</p>

<p>I'm not sure as to why Django cannot find the .css file when I have it clearly stated in the above files. No other StackOverflow questions/answers have helped me either.</p>

<p>Open to suggestions; any help is much appreciated.</p>
",4225659,969,08-02-2018 10:16,08-02-2018 10:25,0,969,47,3,20,64,"{'badge_counts': {'bronze': 47, 'silver': 20, 'gold': 3}, 'account_id': 5294049, 'is_employee': False, 'last_modified_date': 1573679992, 'last_access_date': 1596091671, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 969, 'creation_date': 1415338537, 'user_type': 'registered', 'user_id': 4225659, 'accept_rate': 64, 'link': 'https://stackoverflow.com/users/4225659/juiceb0xk', 'profile_image': 'https://lh3.googleusercontent.com/-rW9gpVSaf70/AAAAAAAAAAI/AAAAAAAAAC8/Gik6PGJChuk/photo.jpg?sz=256', 'display_name': 'juiceb0xk'}","This problem seems to be widely known, however, most are old Q&amp;A's and they do not solve my problem. I am using Django 2.0.2 and when my simple web page is not rendering my bootstrap.css file because it simply cannot find it. I am following this tutorial: https://simpleisbetterthancomplex.com/series/2017/09/11/a-complete-beginners-guide-to-django-part-2.html#static-files-setup This is my file structure: This is my static variables defined in the file. This is my file as is with the bottom half cut out for brevity. I have debugging set to true, and when I run my server and load it up, I am presented with the following: I am also not able to visit this URL as it states a 404 error too: . I'm not sure as to why Django cannot find the .css file when I have it clearly stated in the above files. No other StackOverflow questions/answers have helped me either. Open to suggestions; any help is much appreciated.","myproject/
 |-- myproject/
 |    |-- boards/
 |    |-- myproject/
 |    |-- templates/
 |    |-- static/
 |    |    +-- css/
 |    |         +-- bootstrap.min.css    &lt;-- here
 |    +-- manage.py
 settings.py STATIC_URL = '/static/'

STATICFILES_DIR = [
    os.path.join(BASE_DIR, 'static'),
]
 home.html {% load static %}&lt;!DOCTYPE html&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta charset=""utf-8""&gt;
    &lt;title&gt;Boards&lt;/title&gt;
    &lt;link rel=""stylesheet"" href=""{% static 'css/bootstrap.min.css' %}""&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;Boards&lt;/h1&gt;
 [08/Feb/2018 19:26:02] ""GET /static/css/bootstrap.css HTTP/1.1"" 404 1675 http://127.0.0.1:8000/static/css/bootstrap.min.css",16,48,0,1,
845,50132215,50360218,18054,Set dag_run.conf parameters in call to airflow test,1,<python><airflow>,13,"<p>Does anyone know if there is a way to set dag_run.conf parameters when running <code>airflow test</code> in the bash prompt?</p>

<p>For example, I've downloaded the <a href=""https://github.com/apache/incubator-airflow/blob/master/airflow/example_dags/example_trigger_target_dag.py"" rel=""noreferrer"">example_trigger_target_dag</a> from the official airflow repository and I'd like to test the <code>run_this</code> task. Usually I would do the following:</p>

<p><code>~/$ airflow test example_trigger_target_dag run_this '2018-01-01'</code></p>

<p>However running this produces the error:</p>

<pre><code>--------------------------------------------------------------------------------
Starting attempt 1 of 1
--------------------------------------------------------------------------------

[2018-05-02 10:50:01,154] {models.py:1342} INFO - Executing &lt;Task(PythonOperator): run_this&gt; on 2018-01-01 00:00:00
[2018-05-02 10:50:01,262] {models.py:1417} ERROR - 'NoneType' object has no attribute 'conf'
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in run
    result = task_copy.execute(context=context)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/operators/python_operator.py"", line 80, in execute
    return_value = self.python_callable(*self.op_args, **self.op_kwargs)
  File ""/home/annalect/uk_ds_airflow/dags/playpen/example_trigger_target_dag.py"", line 56, in run_this_func
    print(""Remotely received value of {} for key=message"".format(kwargs['dag_run'].conf['message']))
AttributeError: 'NoneType' object has no attribute 'conf'
</code></pre>

<p>I've had a go at using the <code>task_params</code> argument however I either have the wrong syntax or it doesn't achieve what I'm after as it produces the same error as above:</p>

<p><code>~/$ airflow test --task_params '{""kwargs"": {""dag_run"": {""conf"": {""message"": ""Hey world""}}}}' example_trigger_target_dag run_this '2018-01-01'</code></p>

<pre><code>[2018-05-02 11:10:58,065] {models.py:1441} INFO - Marking task as FAILED.
[2018-05-02 11:10:58,070] {models.py:1462} ERROR - 'NoneType' object has no attribute 'conf'
</code></pre>

<p>So does anyone know how to test a task that depends on <code>dag_run.conf</code> values?</p>

<p>Thanks!</p>
",4260991,4172,02-05-2018 10:18,15-05-2018 22:55,13,4172,47,6,34,68,"{'badge_counts': {'bronze': 47, 'silver': 34, 'gold': 6}, 'account_id': 5345272, 'is_employee': False, 'last_modified_date': 1671737152, 'last_access_date': 1710796926, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4172, 'creation_date': 1416224544, 'user_type': 'registered', 'user_id': 4260991, 'accept_rate': 68, 'location': 'London, United Kingdom', 'website_url': 'http://www.genebrown.co', 'link': 'https://stackoverflow.com/users/4260991/eugene-brown', 'profile_image': 'https://i.stack.imgur.com/g9LWa.png?s=256&g=1', 'display_name': 'Eugene Brown'}","Does anyone know if there is a way to set dag_run.conf parameters when running in the bash prompt? For example, I've downloaded the example_trigger_target_dag from the official airflow repository and I'd like to test the task. Usually I would do the following: However running this produces the error: I've had a go at using the argument however I either have the wrong syntax or it doesn't achieve what I'm after as it produces the same error as above: So does anyone know how to test a task that depends on values? Thanks!","airflow test run_this ~/$ airflow test example_trigger_target_dag run_this '2018-01-01' --------------------------------------------------------------------------------
Starting attempt 1 of 1
--------------------------------------------------------------------------------

[2018-05-02 10:50:01,154] {models.py:1342} INFO - Executing &lt;Task(PythonOperator): run_this&gt; on 2018-01-01 00:00:00
[2018-05-02 10:50:01,262] {models.py:1417} ERROR - 'NoneType' object has no attribute 'conf'
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in run
    result = task_copy.execute(context=context)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/operators/python_operator.py"", line 80, in execute
    return_value = self.python_callable(*self.op_args, **self.op_kwargs)
  File ""/home/annalect/uk_ds_airflow/dags/playpen/example_trigger_target_dag.py"", line 56, in run_this_func
    print(""Remotely received value of {} for key=message"".format(kwargs['dag_run'].conf['message']))
AttributeError: 'NoneType' object has no attribute 'conf'
 task_params ~/$ airflow test --task_params '{""kwargs"": {""dag_run"": {""conf"": {""message"": ""Hey world""}}}}' example_trigger_target_dag run_this '2018-01-01' [2018-05-02 11:10:58,065] {models.py:1441} INFO - Marking task as FAILED.
[2018-05-02 11:10:58,070] {models.py:1462} ERROR - 'NoneType' object has no attribute 'conf'
 dag_run.conf",8,35,0,1,
846,49841324,49841366,42476,What does calling fit() multiple times on the same model do?,3,<python><machine-learning><scikit-learn>,78,"<p>After I instantiate a scikit model (e.g. <code>LinearRegression</code>), if I call its <code>fit()</code> method multiple times (with different <code>X</code> and <code>y</code> data), what happens? Does it fit the model on the data like if I just re-instantiated the model (i.e. from scratch), or does it keep into accounts data already fitted from the previous call to <code>fit()</code>?</p>

<p>Trying with <code>LinearRegression</code> (also looking at its source code) it seems to me that every time I call <code>fit()</code>, it fits from scratch, ignoring the result of any previous call to the same method. I wonder if this true in general, and I can rely on this behavior for all models/pipelines of scikit learn.</p>
",4262324,2987,15-04-2018 11:19,15-04-2018 11:24,0,2997,35,5,28,88,"{'badge_counts': {'bronze': 35, 'silver': 28, 'gold': 5}, 'account_id': 5347071, 'is_employee': False, 'last_modified_date': 1682279400, 'last_access_date': 1711181525, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2997, 'creation_date': 1416246463, 'user_type': 'registered', 'user_id': 4262324, 'accept_rate': 88, 'website_url': '', 'link': 'https://stackoverflow.com/users/4262324/fanta', 'profile_image': 'https://i.stack.imgur.com/JPlSf.png?s=256&g=1', 'display_name': 'Fanta'}","After I instantiate a scikit model (e.g. ), if I call its method multiple times (with different and data), what happens? Does it fit the model on the data like if I just re-instantiated the model (i.e. from scratch), or does it keep into accounts data already fitted from the previous call to ? Trying with (also looking at its source code) it seems to me that every time I call , it fits from scratch, ignoring the result of any previous call to the same method. I wonder if this true in general, and I can rely on this behavior for all models/pipelines of scikit learn.",LinearRegression fit() X y fit() LinearRegression fit(),-7,3,0,0,
847,49252880,49261182,28342,"Element is not clickable at point (x,y.5) because another element obscures it",9,<python><selenium><selenium-webdriver>,20,"<p>I am trying to click on an element but getting the error:</p>

<p><code>Element is not clickable at point (x,y.5)</code></p>

<p>because another element obscures it. </p>

<p>I have already tried moving to that element first and then clicking and also changing the co-ordinates by minimizing the window and then clicking, but both methods failed. The possible duplicate question has answers which I have already tried and none of them worked for me.</p>

<p>Also, the same code is working on a different PC. </p>

<p>How to resolve it? </p>
",5491792,247,13-03-2018 09:46,13-03-2018 16:25,0,247,12,1,4,,"{'badge_counts': {'bronze': 12, 'silver': 4, 'gold': 1}, 'account_id': 7191745, 'is_employee': False, 'last_modified_date': 1607614525, 'last_access_date': 1610538259, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 247, 'creation_date': 1445915812, 'user_type': 'registered', 'user_id': 5491792, 'link': 'https://stackoverflow.com/users/5491792/shubham-srivastava', 'profile_image': 'https://lh6.googleusercontent.com/-4q4353HJrpc/AAAAAAAAAAI/AAAAAAAAAGo/zPRnNv8P_fc/photo.jpg?sz=256', 'display_name': 'Shubham srivastava'}","I am trying to click on an element but getting the error: because another element obscures it. I have already tried moving to that element first and then clicking and also changing the co-ordinates by minimizing the window and then clicking, but both methods failed. The possible duplicate question has answers which I have already tried and none of them worked for me. Also, the same code is working on a different PC. How to resolve it?","Element is not clickable at point (x,y.5)",-1,11,0,0,
848,48693069,48707532,15397,Running Flask & a Discord bot in the same application,5,<python><bots><twitch><discord.py>,13,"<p>I am building a Discord bot in Python and would like to receive HTTP requests from Twitch.tv's API (See <a href=""https://dev.twitch.tv/docs/api/webhooks-guide"" rel=""noreferrer"" title=""Twitch Webhooks Guide"">Webhooks Guide</a> &amp; <a href=""https://dev.twitch.tv/docs/api/webhooks-reference"" rel=""noreferrer"" title=""Twitch Webhooks Reference"">Webhooks Reference</a>) (To subscribe to events like; X streamer has gone live) and based on the content of the HTTP (POST or GET) request received from Twitch, do something on the Discord bot, e.g: Output a message on a text channel.</p>

<p>I am using the <a href=""https://github.com/Rapptz/discord.py"" rel=""noreferrer"" title=""discord.py"">discord.py</a> Python Discord API/Library.</p>

<p>I've looked into the matter and found that <a href=""http://flask.pocoo.org/"" rel=""noreferrer"" title=""Flask"">Flask</a> seemed like a good minimalist choice for a webserver to receive these requests on.</p>

<p>I should preface this by saying I'm very new to Python and I've never used Flask before.</p>

<p>Now. The problem is I can't seem to figure out a way to run the Flask server inside of my discord bot.</p>

<p>I've tried adding this simple code into my discord.py script:</p>

<pre><code>from flask import Flask, request
app = Flask(__name__)
@app.route('/posts', methods=['POST'])
def result():
    print(request.form['sched'])
    # Send a message to a discord text channel etc...
    return 'Received !'
</code></pre>

<p>When I run my discord.py script which looks something like this:
<em>(Stripped away some commands and features for the sake of keeping this shorter)</em></p>

<pre><code>import discord
import asyncio

from flask import Flask, request
app = Flask(__name__)
@app.route('/posts', methods=['POST'])
def result():
    print(request.form['sched'])
    # Send a message to a discord text channel etc...
    return 'Received !'

client = discord.Client()

@client.event
async def on_ready():
    print('Logged in as')
    print(client.user.name)
    print(client.user.id)
    print('------')

@client.event
async def on_message(message):

    if message.author == client.user:
        return

    content = message.content
    fullUser = message.author.name+'#'+message.author.discriminator
    print(str(message.timestamp)+"" #""+message.channel.name+"" ""+fullUser+"": ""+str(content.encode('ascii', 'ignore').decode('ascii')))
    if content.startswith('!'):

        content = content[1:]
        if content.startswith('test'):
            counter = 0
            tmp = await client.send_message(message.channel, 'Calculating messages...')
            async for log in client.logs_from(message.channel, limit=100):
                if log.author == message.author:
                    counter += 1

            await client.edit_message(tmp, 'You have {} messages.'.format(counter))

client.run('MyTokenHere')
</code></pre>

<p>It seems like if I point flask to discord.py (the above) and run it, it'll start the code, get to the ""client.run('MyTokenHere')"" part for discord, and just stop at that and run the discord bot. It's not until I exit out of the bot by doing Ctrl+C that the actual Flask server starts, but now the discord bot is disconnected and no longer does any processing.</p>

<p>The same problem persists if I were to for example add ""app.run()"" somewhere in my code (before calling ""client.run()"" which starts the Discord bot part) to launch the Flask server; It'll just run the flask, get stuck on that until I Ctrl+C out of the Flask server, then it'll proceed to start the Discord bot.
Ultimately, I need to use the Discord API and I need to be connected to the Discord API gateway and all that good jazz to actually send messages to a channel with the bot, so I don't really know what to do here.</p>

<p>So. I think I've tried my best to explain what I'm ultimately trying to achieve here, and hopefully someone can help me find a way to either make this work with Flask, or if there's a better and easier way, provide a different solution.</p>
",5513223,211,08-02-2018 19:09,09-02-2018 13:56,1,211,11,1,2,,"{'badge_counts': {'bronze': 11, 'silver': 2, 'gold': 1}, 'account_id': 7223807, 'is_employee': False, 'last_modified_date': 1573679362, 'last_access_date': 1695854475, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 211, 'creation_date': 1446400749, 'user_type': 'registered', 'user_id': 5513223, 'link': 'https://stackoverflow.com/users/5513223/haxo201', 'profile_image': 'https://www.gravatar.com/avatar/394d48300525474cc0f7cd53ca0f877f?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Haxo201'}","I am building a Discord bot in Python and would like to receive HTTP requests from Twitch.tv's API (See Webhooks Guide &amp; Webhooks Reference) (To subscribe to events like; X streamer has gone live) and based on the content of the HTTP (POST or GET) request received from Twitch, do something on the Discord bot, e.g: Output a message on a text channel. I am using the discord.py Python Discord API/Library. I've looked into the matter and found that Flask seemed like a good minimalist choice for a webserver to receive these requests on. I should preface this by saying I'm very new to Python and I've never used Flask before. Now. The problem is I can't seem to figure out a way to run the Flask server inside of my discord bot. I've tried adding this simple code into my discord.py script: When I run my discord.py script which looks something like this: (Stripped away some commands and features for the sake of keeping this shorter) It seems like if I point flask to discord.py (the above) and run it, it'll start the code, get to the ""client.run('MyTokenHere')"" part for discord, and just stop at that and run the discord bot. It's not until I exit out of the bot by doing Ctrl+C that the actual Flask server starts, but now the discord bot is disconnected and no longer does any processing. The same problem persists if I were to for example add ""app.run()"" somewhere in my code (before calling ""client.run()"" which starts the Discord bot part) to launch the Flask server; It'll just run the flask, get stuck on that until I Ctrl+C out of the Flask server, then it'll proceed to start the Discord bot. Ultimately, I need to use the Discord API and I need to be connected to the Discord API gateway and all that good jazz to actually send messages to a channel with the bot, so I don't really know what to do here. So. I think I've tried my best to explain what I'm ultimately trying to achieve here, and hopefully someone can help me find a way to either make this work with Flask, or if there's a better and easier way, provide a different solution.","from flask import Flask, request
app = Flask(__name__)
@app.route('/posts', methods=['POST'])
def result():
    print(request.form['sched'])
    # Send a message to a discord text channel etc...
    return 'Received !'
 import discord
import asyncio

from flask import Flask, request
app = Flask(__name__)
@app.route('/posts', methods=['POST'])
def result():
    print(request.form['sched'])
    # Send a message to a discord text channel etc...
    return 'Received !'

client = discord.Client()

@client.event
async def on_ready():
    print('Logged in as')
    print(client.user.name)
    print(client.user.id)
    print('------')

@client.event
async def on_message(message):

    if message.author == client.user:
        return

    content = message.content
    fullUser = message.author.name+'#'+message.author.discriminator
    print(str(message.timestamp)+"" #""+message.channel.name+"" ""+fullUser+"": ""+str(content.encode('ascii', 'ignore').decode('ascii')))
    if content.startswith('!'):

        content = content[1:]
        if content.startswith('test'):
            counter = 0
            tmp = await client.send_message(message.channel, 'Calculating messages...')
            async for log in client.logs_from(message.channel, limit=100):
                if log.author == message.author:
                    counter += 1

            await client.edit_message(tmp, 'You have {} messages.'.format(counter))

client.run('MyTokenHere')
",47,74,0,4,
849,50052570,50052789,16800,Keep a column with a categorical variable in Pandas with groupby and mean(),2,<python><pandas><pandas-groupby><categorical-data>,13,"<p>Is there a way to keep the categorical variable after <code>groupby</code> and <code>mean()</code>?
For example, given the dataframe <code>df</code>:</p>

<pre><code>              ratio    Metadata_A      Metadata_B   treatment
0      54265.937500           B10               1  AB_cmpd_01
11    107364.750000           B10               2  AB_cmpd_01
22     95766.500000           B10               3  AB_cmpd_01
24     64346.250000           B10               4  AB_cmpd_01
25     52726.333333           B10               5  AB_cmpd_01
30     65056.600000           B11               1          UT
41     78409.600000           B11               2          UT
52    133533.000000           B11               3          UT
54    102433.571429           B11               4          UT
55     82217.588235           B11               5          UT
60     89843.600000            B2               1          UT
71     98544.000000            B2               2          UT
82    179330.000000            B2               3          UT
84    107132.400000            B2               4          UT
85     73096.909091            B2               5          UT
</code></pre>

<p>I need to average over <code>ratio</code> within each of <code>Metadata_A</code>, but at the end to keep the column <code>treatment</code>:</p>

<p>Theoretically, something like: </p>

<pre><code>df.groupby(by='Metadata_A').mean().reset_index()

              ratio    Metadata_A      Metadata_B   treatment
 0     54265.937500           B10             2.5  AB_cmpd_01
 1     78409.600000           B11             2.5          UT
 2    107132.400000            B2             2.5          UT
</code></pre>

<p>However, the column <code>treatment</code> disappears after the averaging.</p>
",5550203,3006,26-04-2018 22:13,26-04-2018 22:36,0,3026,61,11,34,100,"{'badge_counts': {'bronze': 61, 'silver': 34, 'gold': 11}, 'account_id': 6860839, 'is_employee': False, 'last_modified_date': 1636494000, 'last_access_date': 1690911717, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 3026, 'creation_date': 1447237725, 'user_type': 'registered', 'user_id': 5550203, 'accept_rate': 100, 'location': 'United Kingdom', 'link': 'https://stackoverflow.com/users/5550203/arnold-klein', 'profile_image': 'https://www.gravatar.com/avatar/f9ebbbcd8a45524bd6b31f961cb3b516?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Arnold Klein'}","Is there a way to keep the categorical variable after and ? For example, given the dataframe : I need to average over within each of , but at the end to keep the column : Theoretically, something like: However, the column disappears after the averaging.","groupby mean() df               ratio    Metadata_A      Metadata_B   treatment
0      54265.937500           B10               1  AB_cmpd_01
11    107364.750000           B10               2  AB_cmpd_01
22     95766.500000           B10               3  AB_cmpd_01
24     64346.250000           B10               4  AB_cmpd_01
25     52726.333333           B10               5  AB_cmpd_01
30     65056.600000           B11               1          UT
41     78409.600000           B11               2          UT
52    133533.000000           B11               3          UT
54    102433.571429           B11               4          UT
55     82217.588235           B11               5          UT
60     89843.600000            B2               1          UT
71     98544.000000            B2               2          UT
82    179330.000000            B2               3          UT
84    107132.400000            B2               4          UT
85     73096.909091            B2               5          UT
 ratio Metadata_A treatment df.groupby(by='Metadata_A').mean().reset_index()

              ratio    Metadata_A      Metadata_B   treatment
 0     54265.937500           B10             2.5  AB_cmpd_01
 1     78409.600000           B11             2.5          UT
 2    107132.400000            B2             2.5          UT
 treatment",13,34,0,0,
850,49992329,49992422,14892,The workers in ThreadPoolExecutor is not really daemon,3,<python><multithreading><daemon><concurrent.futures>,34,"<p>The thing I cannot figure out is that although <code>ThreadPoolExecutor</code> uses daemon workers, they will still run even if main thread exit. </p>

<p>I can provide a minimal example in python3.6.4:</p>

<pre><code>import concurrent.futures
import time


def fn():
    while True:
        time.sleep(5)
        print(""Hello"")


thread_pool = concurrent.futures.ThreadPoolExecutor()
thread_pool.submit(fn)
while True:
    time.sleep(1)
    print(""Wow"")
</code></pre>

<p>Both main thread and the worker thread are infinite loops. So if I use <code>KeyboardInterrupt</code> to terminate main thread, I expect that the whole program will terminate too. But actually the worker thread is still running even though it is a daemon thread.</p>

<p>The source code of <code>ThreadPoolExecutor</code> confirms that worker threads are daemon thread:</p>

<pre><code>t = threading.Thread(target=_worker,
                     args=(weakref.ref(self, weakref_cb),
                           self._work_queue))
t.daemon = True
t.start()
self._threads.add(t)
</code></pre>

<p>Further, if I manually create a daemon thread, it works like a charm:</p>

<pre><code>from threading import Thread
import time


def fn():
    while True:
        time.sleep(5)
        print(""Hello"")


thread = Thread(target=fn)
thread.daemon = True
thread.start()
while True:
    time.sleep(1)
    print(""Wow"")
</code></pre>

<p>So I really cannot figure out this strange behavior.</p>
",5588279,19492,24-04-2018 01:54,24-04-2018 02:10,0,19522,90,11,56,55,"{'badge_counts': {'bronze': 90, 'silver': 56, 'gold': 11}, 'account_id': 7337423, 'is_employee': False, 'last_modified_date': 1662736201, 'last_access_date': 1711150197, 'reputation_change_year': 310, 'reputation_change_quarter': 310, 'reputation_change_month': 70, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 19522, 'creation_date': 1448083057, 'user_type': 'registered', 'user_id': 5588279, 'accept_rate': 55, 'website_url': '', 'link': 'https://stackoverflow.com/users/5588279/sraw', 'profile_image': 'https://www.gravatar.com/avatar/91d8baabae79c7e51f05ecb99480fe98?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Sraw'}","The thing I cannot figure out is that although uses daemon workers, they will still run even if main thread exit. I can provide a minimal example in python3.6.4: Both main thread and the worker thread are infinite loops. So if I use to terminate main thread, I expect that the whole program will terminate too. But actually the worker thread is still running even though it is a daemon thread. The source code of confirms that worker threads are daemon thread: Further, if I manually create a daemon thread, it works like a charm: So I really cannot figure out this strange behavior.","ThreadPoolExecutor import concurrent.futures
import time


def fn():
    while True:
        time.sleep(5)
        print(""Hello"")


thread_pool = concurrent.futures.ThreadPoolExecutor()
thread_pool.submit(fn)
while True:
    time.sleep(1)
    print(""Wow"")
 KeyboardInterrupt ThreadPoolExecutor t = threading.Thread(target=_worker,
                     args=(weakref.ref(self, weakref_cb),
                           self._work_queue))
t.daemon = True
t.start()
self._threads.add(t)
 from threading import Thread
import time


def fn():
    while True:
        time.sleep(5)
        print(""Hello"")


thread = Thread(target=fn)
thread.daemon = True
thread.start()
while True:
    time.sleep(1)
    print(""Wow"")
",31,54,0,0,
851,49482969,49485603,28331,What is the core difference between asyncio and trio?,1,<python><asynchronous><python-asyncio><python-trio><curio>,126,"<p>Today, I found a library named <a href=""http://trio.readthedocs.io/en/latest/index.html"" rel=""noreferrer"">trio</a> which says itself is an asynchronous API for humans. These words are a little similar with <code>requests</code>'. As <code>requests</code> is really a good library, I am wondering what is the advantages of <code>trio</code>.</p>
<p>There aren't many articles about it, I just find an <a href=""https://vorpus.org/blog/some-thoughts-on-asynchronous-api-design-in-a-post-asyncawait-world/#the-curious-effectiveness-of-curio"" rel=""noreferrer"">article</a> discussing <code>curio</code> and <code>asyncio</code>. To my surprise, <code>trio</code> says itself is even better than <code>curio</code>(next-generation curio).</p>
<p>After reading half of the article, I cannot find the core difference between these two asynchronous framework. It just gives some examples that <code>curio</code>'s implementation is more convenient than <code>asyncio</code>'s. But the underlying structure is almost the same.</p>
<p>So could someone give me a reason I have to accept that <code>trio</code> or <code>curio</code> is better than <code>asyncio</code>? Or explain more about why I should choose <code>trio</code> instead of built-in <code>asyncio</code>?</p>
",5588279,19492,26-03-2018 02:07,26-03-2018 06:58,0,19522,90,11,56,55,"{'badge_counts': {'bronze': 90, 'silver': 56, 'gold': 11}, 'account_id': 7337423, 'is_employee': False, 'last_modified_date': 1662736201, 'last_access_date': 1711150197, 'reputation_change_year': 310, 'reputation_change_quarter': 310, 'reputation_change_month': 70, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 19522, 'creation_date': 1448083057, 'user_type': 'registered', 'user_id': 5588279, 'accept_rate': 55, 'website_url': '', 'link': 'https://stackoverflow.com/users/5588279/sraw', 'profile_image': 'https://www.gravatar.com/avatar/91d8baabae79c7e51f05ecb99480fe98?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Sraw'}","Today, I found a library named trio which says itself is an asynchronous API for humans. These words are a little similar with '. As is really a good library, I am wondering what is the advantages of . There aren't many articles about it, I just find an article discussing and . To my surprise, says itself is even better than (next-generation curio). After reading half of the article, I cannot find the core difference between these two asynchronous framework. It just gives some examples that 's implementation is more convenient than 's. But the underlying structure is almost the same. So could someone give me a reason I have to accept that or is better than ? Or explain more about why I should choose instead of built-in ?",requests requests trio curio asyncio trio curio curio asyncio trio curio asyncio trio asyncio,-14,4,0,2,
852,50346326,50346820,32889,"ProgrammingError: relation ""django_session"" does not exist",4,<python><django><postgresql>,20,"<p>Got this error after changing my database from sqlite to postgresql. I've made all my settings changes: </p>

<p>Here's my settings:</p>

<pre><code>DATABASES = {
    'default': {
        'ENGINE': ""django.db.backends.postgresql_psycopg2"",
        'NAME': ""postr1"",
        'USER': ""zorgan"",
        'PASSWORD': config('DB_PASSWORD'),
        'HOST': ""localhost"",
        'PORT': '',
    }
}
</code></pre>

<p>as well as performing <code>makemigrations</code> and <code>migrations</code> which were all successful. So I'm able to succesfully start my local server:</p>

<pre><code>System check identified no issues (0 silenced).
May 15, 2018 - 08:59:39
Django version 1.11.8, using settings 'draft1.settings'
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
</code></pre>

<p>however when I go to the site it returns this error:</p>

<pre><code>ProgrammingError at /news/
relation ""django_session"" does not exist
LINE 1: ...ession_data"", ""django_session"".""expire_date"" FROM ""django_se...
</code></pre>

<p>Any idea what the problem is?</p>
",6733153,8627,15-05-2018 09:09,15-05-2018 09:33,0,8647,222,29,113,52,"{'badge_counts': {'bronze': 222, 'silver': 113, 'gold': 29}, 'account_id': 9037970, 'is_employee': False, 'last_modified_date': 1675476600, 'last_access_date': 1710982332, 'reputation_change_year': 260, 'reputation_change_quarter': 260, 'reputation_change_month': 40, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 8647, 'creation_date': 1471577112, 'user_type': 'registered', 'user_id': 6733153, 'accept_rate': 52, 'link': 'https://stackoverflow.com/users/6733153/zorgan', 'profile_image': 'https://www.gravatar.com/avatar/60dd54fc1c49693c1c961f0d3c31f500?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Zorgan'}",Got this error after changing my database from sqlite to postgresql. I've made all my settings changes: Here's my settings: as well as performing and which were all successful. So I'm able to succesfully start my local server: however when I go to the site it returns this error: Any idea what the problem is?,"DATABASES = {
    'default': {
        'ENGINE': ""django.db.backends.postgresql_psycopg2"",
        'NAME': ""postr1"",
        'USER': ""zorgan"",
        'PASSWORD': config('DB_PASSWORD'),
        'HOST': ""localhost"",
        'PORT': '',
    }
}
 makemigrations migrations System check identified no issues (0 silenced).
May 15, 2018 - 08:59:39
Django version 1.11.8, using settings 'draft1.settings'
Starting development server at http://127.0.0.1:8000/
Quit the server with CONTROL-C.
 ProgrammingError at /news/
relation ""django_session"" does not exist
LINE 1: ...ession_data"", ""django_session"".""expire_date"" FROM ""django_se...
",13,33,0,0,
853,48872234,48872281,67181,Using Apply in Pandas Lambda functions with multiple if statements,4,<python><pandas><if-statement><lambda><apply>,18,"<p>I'm trying to infer a classification according to the size of a person in a dataframe like this one:</p>

<pre><code>      Size
1     80000
2     8000000
3     8000000000
...
</code></pre>

<p>I want it to look like this:</p>

<pre><code>      Size        Classification
1     80000       &lt;1m
2     8000000     1-10m
3     8000000000  &gt;1bi
...
</code></pre>

<p>I understand that the ideal process would be to apply a lambda function like this:</p>

<pre><code>df['Classification']=df['Size'].apply(lambda x: ""&lt;1m"" if x&lt;1000000 else ""1-10m"" if 1000000&lt;x&lt;10000000 else ...)
</code></pre>

<p>I checked a few posts regarding multiple ifs in a lambda function, <a href=""https://stackoverflow.com/questions/33439434/multiple-if-statements-in-a-lambda-function"">here is an example link</a>, but that synthax is not working for me for some reason in a multiple ifs statement, but it was working in a single if condition.</p>

<p>So I tried this ""very elegant"" solution:</p>

<pre><code>df['Classification']=df['Size'].apply(lambda x: ""&lt;1m"" if x&lt;1000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""1-10m"" if 1000000 &lt; x &lt; 10000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""10-50m"" if 10000000 &lt; x &lt; 50000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""50-100m"" if 50000000 &lt; x &lt; 100000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""100-500m"" if 100000000 &lt; x &lt; 500000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""500m-1bi"" if 500000000 &lt; x &lt; 1000000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""&gt;1bi"" if 1000000000 &lt; x else pass)
</code></pre>

<p>Works out that ""pass"" seems not to apply to lambda functions as well:</p>

<pre><code>df['Classification']=df['Size'].apply(lambda x: ""&lt;1m"" if x&lt;1000000 else pass)
SyntaxError: invalid syntax
</code></pre>

<p>Any suggestions on the correct synthax for a multiple if statement inside a lambda function in an apply method in Pandas? Either multi-line or single line solutions work for me.</p>
",5606352,4564,19-02-2018 18:34,19-02-2018 18:37,0,4564,102,17,55,93,"{'badge_counts': {'bronze': 102, 'silver': 55, 'gold': 17}, 'account_id': 7364023, 'is_employee': False, 'last_modified_date': 1695081300, 'last_access_date': 1708717063, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4564, 'creation_date': 1448493888, 'user_type': 'registered', 'user_id': 5606352, 'accept_rate': 93, 'website_url': '', 'link': 'https://stackoverflow.com/users/5606352/aabujamra', 'profile_image': 'https://www.gravatar.com/avatar/794e7d24e915cd76e7a18c133bb6d393?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'aabujamra'}","I'm trying to infer a classification according to the size of a person in a dataframe like this one: I want it to look like this: I understand that the ideal process would be to apply a lambda function like this: I checked a few posts regarding multiple ifs in a lambda function, here is an example link, but that synthax is not working for me for some reason in a multiple ifs statement, but it was working in a single if condition. So I tried this ""very elegant"" solution: Works out that ""pass"" seems not to apply to lambda functions as well: Any suggestions on the correct synthax for a multiple if statement inside a lambda function in an apply method in Pandas? Either multi-line or single line solutions work for me.","      Size
1     80000
2     8000000
3     8000000000
...
       Size        Classification
1     80000       &lt;1m
2     8000000     1-10m
3     8000000000  &gt;1bi
...
 df['Classification']=df['Size'].apply(lambda x: ""&lt;1m"" if x&lt;1000000 else ""1-10m"" if 1000000&lt;x&lt;10000000 else ...)
 df['Classification']=df['Size'].apply(lambda x: ""&lt;1m"" if x&lt;1000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""1-10m"" if 1000000 &lt; x &lt; 10000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""10-50m"" if 10000000 &lt; x &lt; 50000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""50-100m"" if 50000000 &lt; x &lt; 100000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""100-500m"" if 100000000 &lt; x &lt; 500000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""500m-1bi"" if 500000000 &lt; x &lt; 1000000000 else pass)
df['Classification']=df['Size'].apply(lambda x: ""&gt;1bi"" if 1000000000 &lt; x else pass)
 df['Classification']=df['Size'].apply(lambda x: ""&lt;1m"" if x&lt;1000000 else pass)
SyntaxError: invalid syntax
",15,43,0,1,
854,50168647,52230415,88550,Multiprocessing causes Python to crash and gives an error may have been in progress in another thread when fork() was called,5,<python><python-3.x><multithreading><macos>,184,"<p>I am relatively new to Python and trying to implement a Multiprocessing module for my for loop.</p>

<p>I have an array of Image url's stored in img_urls which I need to download and apply some Google vision.</p>

<pre><code>if __name__ == '__main__':

    img_urls = [ALL_MY_Image_URLS]
    runAll(img_urls)
    print(""--- %s seconds ---"" % (time.time() - start_time)) 
</code></pre>

<p>This is my runAll() method</p>

<pre><code>def runAll(img_urls):
    num_cores = multiprocessing.cpu_count()

    print(""Image URLS  {}"",len(img_urls))
    if len(img_urls) &gt; 2:
        numberOfImages = 0
    else:
        numberOfImages = 1

    start_timeProcess = time.time()

    pool = multiprocessing.Pool()
    pool.map(annotate,img_urls)
    end_timeProcess = time.time()
    print('\n Time to complete ', end_timeProcess-start_timeProcess)

    print(full_matching_pages)


def annotate(img_path):
    file =  requests.get(img_path).content
    print(""file is"",file)
    """"""Returns web annotations given the path to an image.""""""
    print('Process Working under ',os.getpid())
    image = types.Image(content=file)
    web_detection = vision_client.web_detection(image=image).web_detection
    report(web_detection)
</code></pre>

<p>I am getting this as the warning when I run it and python crashes</p>

<pre><code>objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67567]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67567]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67568]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67568]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67569]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67569]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67571]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67571]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67572]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67572]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
</code></pre>
",5608734,2583,04-05-2018 06:36,07-09-2018 22:44,126,2593,29,4,17,75,"{'badge_counts': {'bronze': 29, 'silver': 17, 'gold': 4}, 'account_id': 7367386, 'is_employee': False, 'last_modified_date': 1699061700, 'last_access_date': 1710997268, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2593, 'creation_date': 1448545849, 'user_type': 'registered', 'user_id': 5608734, 'accept_rate': 75, 'location': 'Leeds, United Kingdom', 'website_url': 'http://divetech.in/about-us/', 'link': 'https://stackoverflow.com/users/5608734/sriteja-chilakamarri', 'profile_image': 'https://graph.facebook.com/10206948066611534/picture?type=large', 'display_name': 'SriTeja Chilakamarri'}",I am relatively new to Python and trying to implement a Multiprocessing module for my for loop. I have an array of Image url's stored in img_urls which I need to download and apply some Google vision. This is my runAll() method I am getting this as the warning when I run it and python crashes,"if __name__ == '__main__':

    img_urls = [ALL_MY_Image_URLS]
    runAll(img_urls)
    print(""--- %s seconds ---"" % (time.time() - start_time)) 
 def runAll(img_urls):
    num_cores = multiprocessing.cpu_count()

    print(""Image URLS  {}"",len(img_urls))
    if len(img_urls) &gt; 2:
        numberOfImages = 0
    else:
        numberOfImages = 1

    start_timeProcess = time.time()

    pool = multiprocessing.Pool()
    pool.map(annotate,img_urls)
    end_timeProcess = time.time()
    print('\n Time to complete ', end_timeProcess-start_timeProcess)

    print(full_matching_pages)


def annotate(img_path):
    file =  requests.get(img_path).content
    print(""file is"",file)
    """"""Returns web annotations given the path to an image.""""""
    print('Process Working under ',os.getpid())
    image = types.Image(content=file)
    web_detection = vision_client.web_detection(image=image).web_detection
    report(web_detection)
 objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67567]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67567]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67568]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67568]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67569]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67569]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67571]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67571]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67572]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67572]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
",41,57,0,0,
855,49712002,49712185,59514,Pandas dropna() function not working,2,<python><pandas><data-science>,21,"<p>I am trying to drop NA values from a pandas dataframe.</p>

<p>I have used <code>dropna()</code> (which should drop all NA rows from the dataframe). Yet, it does not work.</p>

<p>Here is the code:</p>

<pre><code>import pandas as pd
import numpy as np
prison_data = pd.read_csv('https://andrewshinsuke.me/docs/compas-scores-two-years.csv')
</code></pre>

<p>That's how you get the data frame. As the following shows, the default <code>read_csv</code> method does indeed convert the NA data points to <code>np.nan</code>.</p>

<pre><code>np.isnan(prison_data.head()['out_custody'][4])

Out[2]: True
</code></pre>

<p>Conveniently, the <code>head()</code> of the DF already contains a NaN values (in the column <code>out_custody</code>), so printing <code>prison_data.head()</code> this, you get:</p>

<pre><code>   id                name   first         last compas_screening_date   sex  

0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male
1   3         kevon dixon   kevon        dixon            2013-01-27  Male
2   4            ed philo      ed        philo            2013-04-14  Male
3   5         marcu brown   marcu        brown            2013-01-13  Male
4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male

      dob  age          age_cat              race      ...        
0  1947-04-18   69  Greater than 45             Other      ...
1  1982-01-22   34          25 - 45  African-American      ...
2  1991-05-14   24     Less than 25  African-American      ...
3  1993-01-21   23     Less than 25  African-American      ...
4  1973-01-22   43          25 - 45             Other      ...

   v_decile_score  v_score_text  v_screening_date  in_custody  out_custody  

0               1           Low        2013-08-14  2014-07-07   2014-07-14
1               1           Low        2013-01-27  2013-01-26   2013-02-05
2               3           Low        2013-04-14  2013-06-16   2013-06-16
3               6        Medium        2013-01-13         NaN          NaN
4               1           Low        2013-03-26         NaN          NaN

priors_count.1 start   end event two_year_recid
0               0     0   327     0              0
1               0     9   159     1              1
2               4     0    63     0              1
3               1     0  1174     0              0
4               2     0  1102     0              0
</code></pre>

<p>However, running <code>prison_data.dropna()</code> does not change the dataframe in any way.</p>

<pre><code>prison_data.dropna()
np.isnan(prison_data.head()['out_custody'][4])


Out[3]: True
</code></pre>
",5638083,562,07-04-2018 21:07,07-04-2018 21:32,0,562,18,1,5,83,"{'badge_counts': {'bronze': 18, 'silver': 5, 'gold': 1}, 'account_id': 7410682, 'is_employee': False, 'last_modified_date': 1659183000, 'last_access_date': 1710964603, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 562, 'creation_date': 1449209228, 'user_type': 'registered', 'user_id': 5638083, 'accept_rate': 83, 'website_url': 'https://www.gilgameshskytrooper.io', 'link': 'https://stackoverflow.com/users/5638083/gilgamesh-skytrooper', 'profile_image': 'https://i.stack.imgur.com/rZrXN.png?s=256&g=1', 'display_name': 'Gilgamesh Skytrooper'}","I am trying to drop NA values from a pandas dataframe. I have used (which should drop all NA rows from the dataframe). Yet, it does not work. Here is the code: That's how you get the data frame. As the following shows, the default method does indeed convert the NA data points to . Conveniently, the of the DF already contains a NaN values (in the column ), so printing this, you get: However, running does not change the dataframe in any way.","dropna() import pandas as pd
import numpy as np
prison_data = pd.read_csv('https://andrewshinsuke.me/docs/compas-scores-two-years.csv')
 read_csv np.nan np.isnan(prison_data.head()['out_custody'][4])

Out[2]: True
 head() out_custody prison_data.head()    id                name   first         last compas_screening_date   sex  

0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male
1   3         kevon dixon   kevon        dixon            2013-01-27  Male
2   4            ed philo      ed        philo            2013-04-14  Male
3   5         marcu brown   marcu        brown            2013-01-13  Male
4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male

      dob  age          age_cat              race      ...        
0  1947-04-18   69  Greater than 45             Other      ...
1  1982-01-22   34          25 - 45  African-American      ...
2  1991-05-14   24     Less than 25  African-American      ...
3  1993-01-21   23     Less than 25  African-American      ...
4  1973-01-22   43          25 - 45             Other      ...

   v_decile_score  v_score_text  v_screening_date  in_custody  out_custody  

0               1           Low        2013-08-14  2014-07-07   2014-07-14
1               1           Low        2013-01-27  2013-01-26   2013-02-05
2               3           Low        2013-04-14  2013-06-16   2013-06-16
3               6        Medium        2013-01-13         NaN          NaN
4               1           Low        2013-03-26         NaN          NaN

priors_count.1 start   end event two_year_recid
0               0     0   327     0              0
1               0     9   159     1              1
2               4     0    63     0              1
3               1     0  1174     0              0
4               2     0  1102     0              0
 prison_data.dropna() prison_data.dropna()
np.isnan(prison_data.head()['out_custody'][4])


Out[3]: True
",29,59,0,0,
856,49559770,49560245,19501,How do you resolve 'hidden imports not found!' warnings in pyinstaller for scipy?,1,<python><pandas><scipy><scikit-learn><pyinstaller>,13,"<p>I'm working on using pyinstaller to create an .exe for a python program that uses pandas and sklearn. The pyinstaller process completes and produces the dist folder with the executable as expected. However, when I run the .exe I get module import errors related to sklearn and scipy.</p>

<p>I created a test script (test.py) to test imports, which only imports pandas and sklearn and then prints a success message:</p>

<pre><code>import time
import pandas as pd
import sklearn

def main():
  print('hello world!')
  time.sleep(5)


if __name__ == '__main__':
  main()
</code></pre>

<p>I'm aware of pyinstaller hooks and I was able to resolve the pandas errors by adding a hook to the pyinstaller hooks directory. I added similar hooks for sklearn and scipy it looks like they're running, but in the pyinstaller output I'm getting warnings that 'Hidden import ""sklearn.utils.sparsetools._graph_validation"" not found!' and similar one for '._graph_tools'. </p>

<p>Here's the hook for scipy (hook-scipy.py):</p>

<pre><code>print('loading custome hook for scipy')

from PyInstaller.utils.hooks import collect_submodules
hiddenimports = collect_submodules('scipy') 
</code></pre>

<p>Here's a snapshot of the warnings generated from running pyinstaller</p>

<p><img src=""https://i.stack.imgur.com/aKXny.jpg"" alt=""""></p>

<p>Here's a snapshot of the error when running test.exe</p>

<p><img src=""https://i.stack.imgur.com/Gv3yu.png"" alt=""""></p>

<p>I'm working in a virtual environment where pyinstaller, pandas, sklearn, scipy and all dependencies are installed (at least I can get the regular test.py script running in this venv). Using PyInstaller 3.3.1, Python 3.6.4 on Windows 10.10.0.</p>

<p>Any help is appreciated!</p>
",5724381,811,29-03-2018 15:23,29-03-2018 15:49,0,811,11,2,9,,"{'badge_counts': {'bronze': 11, 'silver': 9, 'gold': 2}, 'account_id': 7539062, 'is_employee': False, 'last_modified_date': 1607614514, 'last_access_date': 1535636288, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 811, 'creation_date': 1451328774, 'user_type': 'registered', 'user_id': 5724381, 'location': 'Boston, MA', 'website_url': 'http://www.linkedin/in/lukewalshct', 'link': 'https://stackoverflow.com/users/5724381/luke', 'profile_image': 'https://i.stack.imgur.com/LH1ud.jpg?s=256&g=1', 'display_name': 'Luke'}","I'm working on using pyinstaller to create an .exe for a python program that uses pandas and sklearn. The pyinstaller process completes and produces the dist folder with the executable as expected. However, when I run the .exe I get module import errors related to sklearn and scipy. I created a test script (test.py) to test imports, which only imports pandas and sklearn and then prints a success message: I'm aware of pyinstaller hooks and I was able to resolve the pandas errors by adding a hook to the pyinstaller hooks directory. I added similar hooks for sklearn and scipy it looks like they're running, but in the pyinstaller output I'm getting warnings that 'Hidden import ""sklearn.utils.sparsetools._graph_validation"" not found!' and similar one for '._graph_tools'. Here's the hook for scipy (hook-scipy.py): Here's a snapshot of the warnings generated from running pyinstaller Here's a snapshot of the error when running test.exe I'm working in a virtual environment where pyinstaller, pandas, sklearn, scipy and all dependencies are installed (at least I can get the regular test.py script running in this venv). Using PyInstaller 3.3.1, Python 3.6.4 on Windows 10.10.0. Any help is appreciated!","import time
import pandas as pd
import sklearn

def main():
  print('hello world!')
  time.sleep(5)


if __name__ == '__main__':
  main()
 print('loading custome hook for scipy')

from PyInstaller.utils.hooks import collect_submodules
hiddenimports = collect_submodules('scipy') 
",13,38,2,0,
857,49979354,49979617,98004,"Python - Pip Install - Proxy Error - 'Cannot connect to proxy.', OSError'",10,<python><networking><proxy><pip>,16,"<p>I want to install some modules in a Enterprise VM in order to create some Python Scripts. I'm trying to use PIP with Proxy to do it. I'm using this command lines:</p>

<pre><code>C:\Users\user&gt;SET HTTPS_PROXY=https://user:pass@199.00.11.11:8080

C:\Users\user&gt;SET PROXY=http://user:pass@199.00.11.11:8080

C:\Users\user&gt;pip install datetime
</code></pre>

<p>To have access to my virtual machine I've this credentials:</p>

<ul>
<li><strong>USER</strong>: NAN/user </li>
<li><strong>PASS</strong>: pass</li>
</ul>

<p>But I am getting this error:</p>

<pre><code>Collecting datetime
  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Could not find a version that satisfies the requirement datetime (from versions: )
No matching distribution found for datetime
</code></pre>

<p>What I need to do in order to get the python module?</p>
",5752765,413,23-04-2018 10:53,23-04-2018 11:07,0,413,16,2,5,50,"{'badge_counts': {'bronze': 16, 'silver': 5, 'gold': 2}, 'account_id': 7576270, 'is_employee': False, 'last_modified_date': 1685982302, 'last_access_date': 1687540083, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 413, 'creation_date': 1452091142, 'user_type': 'registered', 'user_id': 5752765, 'accept_rate': 50, 'link': 'https://stackoverflow.com/users/5752765/sacvp', 'profile_image': 'https://lh4.googleusercontent.com/-NmCBQfKMB_4/AAAAAAAAAAI/AAAAAAAAAPY/20sgVvD6bw8/photo.jpg?sz=256', 'display_name': 'SaCvP'}",I want to install some modules in a Enterprise VM in order to create some Python Scripts. I'm trying to use PIP with Proxy to do it. I'm using this command lines: To have access to my virtual machine I've this credentials: USER: NAN/user PASS: pass But I am getting this error: What I need to do in order to get the python module?,"C:\Users\user&gt;SET HTTPS_PROXY=https://user:pass@199.00.11.11:8080

C:\Users\user&gt;SET PROXY=http://user:pass@199.00.11.11:8080

C:\Users\user&gt;pip install datetime
 Collecting datetime
  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy Authentication Required ( Forefront TMG requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )',))': /simple/datetime/
  Could not find a version that satisfies the requirement datetime (from versions: )
No matching distribution found for datetime
",11,29,0,0,
858,48254837,48256949,21716,Python async/await downloading a list of urls,2,<python><asynchronous><python-asyncio><aiohttp>,16,"<p>I'm trying to download over 30,000 files from a FTP server, and after some googling using asynchronous IO seemed a good idea. However, the code below failed to download any files and returns a Timeout Error. I'd really appreciate any help! Thanks!</p>

<pre><code>class pdb:
    def __init__(self):
        self.ids = []
        self.dl_id = []
        self.err_id = []


    async def download_file(self, session, url):
        try:
            with async_timeout.timeout(10):
                async with session.get(url) as remotefile:
                    if remotefile.status == 200:
                        data = await remotefile.read()
                        return {""error"": """", ""data"": data}
                    else:
                        return {""error"": remotefile.status, ""data"": """"}
        except Exception as e:
            return {""error"": e, ""data"": """"}

    async def unzip(self, session, work_queue):
        while not work_queue.empty():
            queue_url = await work_queue.get()
            print(queue_url)
            data = await self.download_file(session, queue_url)
            id = queue_url[-11:-7]
            ID = id.upper()
            if not data[""error""]:
                saved_pdb = os.path.join(""./pdb"", ID, f'{ID}.pdb')
                if ID not in self.dl_id:
                    self.dl_id.append(ID)
                with open(f""{id}.ent.gz"", 'wb') as f:
                    f.write(data[""data""].read())
                with gzip.open(f""{id}.ent.gz"", ""rb"") as inFile, open(saved_pdb, ""wb"") as outFile:
                    shutil.copyfileobj(inFile, outFile)
                os.remove(f""{id}.ent.gz"")
            else:
                self.err_id.append(ID)

    def download_queue(self, urls):
        loop = asyncio.get_event_loop()
        q = asyncio.Queue(loop=loop)
        [q.put_nowait(url) for url in urls]
        con = aiohttp.TCPConnector(limit=10)
        with aiohttp.ClientSession(loop=loop, connector=con) as session:
            tasks = [asyncio.ensure_future(self.unzip(session, q)) for _ in range(len(urls))]
            loop.run_until_complete(asyncio.gather(*tasks))
        loop.close()
</code></pre>

<p>Error message if I remove the <code>try</code> part:  </p>

<blockquote>
  <p>Traceback (most recent call last):<br>
  File ""test.py"", line 111, in <br>
      x.download_queue(urls)<br>
  File ""test.py"", line 99, in download_queue<br>
      loop.run_until_complete(asyncio.gather(*tasks))<br>
  File ""/home/yz/miniconda3/lib/python3.6/asyncio/base_events.py"", line 467, in run_until_complete<br>
      return future.result()<br>
  File ""test.py"", line 73, in unzip<br>
      data = await self.download_file(session, queue_url)<br>
  File ""test.py"", line 65, in download_file<br>
      return {""error"": remotefile.status, ""data"": """"}<br>
  File ""/home/yz/miniconda3/lib/python3.6/site- packages/async_timeout/<strong>init</strong>.py"", line 46, in <strong>exit</strong><br>
      raise asyncio.TimeoutError from None<br>
  concurrent.futures._base.TimeoutError  </p>
</blockquote>
",5925357,305,14-01-2018 22:32,15-01-2018 04:30,1,305,7,1,3,,"{'badge_counts': {'bronze': 7, 'silver': 3, 'gold': 1}, 'account_id': 7838128, 'is_employee': False, 'last_modified_date': 1678460713, 'last_access_date': 1661865010, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 305, 'creation_date': 1455460547, 'user_type': 'registered', 'user_id': 5925357, 'website_url': '', 'link': 'https://stackoverflow.com/users/5925357/yi-zhou', 'profile_image': 'https://lh6.googleusercontent.com/-RHPHl6i94_g/AAAAAAAAAAI/AAAAAAAAAUQ/frurIEvq57k/photo.jpg?sz=256', 'display_name': 'Yi Zhou'}","I'm trying to download over 30,000 files from a FTP server, and after some googling using asynchronous IO seemed a good idea. However, the code below failed to download any files and returns a Timeout Error. I'd really appreciate any help! Thanks! Error message if I remove the part: Traceback (most recent call last): File ""test.py"", line 111, in x.download_queue(urls) File ""test.py"", line 99, in download_queue loop.run_until_complete(asyncio.gather(*tasks)) File ""/home/yz/miniconda3/lib/python3.6/asyncio/base_events.py"", line 467, in run_until_complete return future.result() File ""test.py"", line 73, in unzip data = await self.download_file(session, queue_url) File ""test.py"", line 65, in download_file return {""error"": remotefile.status, ""data"": """"} File ""/home/yz/miniconda3/lib/python3.6/site- packages/async_timeout/init.py"", line 46, in exit raise asyncio.TimeoutError from None concurrent.futures._base.TimeoutError","class pdb:
    def __init__(self):
        self.ids = []
        self.dl_id = []
        self.err_id = []


    async def download_file(self, session, url):
        try:
            with async_timeout.timeout(10):
                async with session.get(url) as remotefile:
                    if remotefile.status == 200:
                        data = await remotefile.read()
                        return {""error"": """", ""data"": data}
                    else:
                        return {""error"": remotefile.status, ""data"": """"}
        except Exception as e:
            return {""error"": e, ""data"": """"}

    async def unzip(self, session, work_queue):
        while not work_queue.empty():
            queue_url = await work_queue.get()
            print(queue_url)
            data = await self.download_file(session, queue_url)
            id = queue_url[-11:-7]
            ID = id.upper()
            if not data[""error""]:
                saved_pdb = os.path.join(""./pdb"", ID, f'{ID}.pdb')
                if ID not in self.dl_id:
                    self.dl_id.append(ID)
                with open(f""{id}.ent.gz"", 'wb') as f:
                    f.write(data[""data""].read())
                with gzip.open(f""{id}.ent.gz"", ""rb"") as inFile, open(saved_pdb, ""wb"") as outFile:
                    shutil.copyfileobj(inFile, outFile)
                os.remove(f""{id}.ent.gz"")
            else:
                self.err_id.append(ID)

    def download_queue(self, urls):
        loop = asyncio.get_event_loop()
        q = asyncio.Queue(loop=loop)
        [q.put_nowait(url) for url in urls]
        con = aiohttp.TCPConnector(limit=10)
        with aiohttp.ClientSession(loop=loop, connector=con) as session:
            tasks = [asyncio.ensure_future(self.unzip(session, q)) for _ in range(len(urls))]
            loop.run_until_complete(asyncio.gather(*tasks))
        loop.close()
 try",45,69,0,0,
859,50371428,50372128,61117,"scipy curve_fit raises ""OptimizeWarning: Covariance of the parameters could not be estimated""",2,<python><scipy><curve-fitting><lmfit>,17,"<p>I am trying to fit this function to some data:</p>
<p><a href=""https://i.stack.imgur.com/22ZYx.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/22ZYx.gif"" alt=""function definition"" /></a></p>
<p>But when I use my code</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

def f(x, start, end):
    res = np.empty_like(x)
    res[x &lt; start] =-1
    res[x &gt; end] = 1
    linear = np.all([[start &lt;= x], [x &lt;= end]], axis=0)[0]
    res[linear] = np.linspace(-1., 1., num=np.sum(linear))
    return res

if __name__ == '__main__':

    xdata = np.linspace(0., 1000., 1000)
    ydata = -np.ones(1000)
    ydata[500:1000] = 1.
    ydata = ydata + np.random.normal(0., 0.25, len(ydata))

    popt, pcov = curve_fit(f, xdata, ydata, p0=[495., 505.])
    print(popt, pcov)
    plt.figure()
    plt.plot(xdata, f(xdata, *popt), 'r-', label='fit')
    plt.plot(xdata, ydata, 'b-', label='data')
    plt.show()
</code></pre>
<p>I get the warning</p>
<pre class=""lang-none prettyprint-override""><code>OptimizeWarning: Covariance of the parameters could not be estimated
</code></pre>
<p>Output:</p>
<p><a href=""https://i.stack.imgur.com/uz1lg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uz1lg.png"" alt=""plot"" /></a></p>
<p>In this example start and end should be closer to 500, but they don't change at all from my initial guess.</p>
",5963631,1867,16-05-2018 12:49,16-05-2018 13:21,0,1867,36,4,22,71,"{'badge_counts': {'bronze': 36, 'silver': 22, 'gold': 4}, 'account_id': 7894461, 'is_employee': False, 'last_modified_date': 1692036300, 'last_access_date': 1711098213, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1867, 'creation_date': 1456152751, 'user_type': 'registered', 'user_id': 5963631, 'accept_rate': 71, 'location': 'Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/5963631/jonas', 'profile_image': 'https://i.stack.imgur.com/4gxvE.png?s=256&g=1', 'display_name': 'Jonas'}","I am trying to fit this function to some data: But when I use my code I get the warning Output: In this example start and end should be closer to 500, but they don't change at all from my initial guess.","import numpy as np
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

def f(x, start, end):
    res = np.empty_like(x)
    res[x &lt; start] =-1
    res[x &gt; end] = 1
    linear = np.all([[start &lt;= x], [x &lt;= end]], axis=0)[0]
    res[linear] = np.linspace(-1., 1., num=np.sum(linear))
    return res

if __name__ == '__main__':

    xdata = np.linspace(0., 1000., 1000)
    ydata = -np.ones(1000)
    ydata[500:1000] = 1.
    ydata = ydata + np.random.normal(0., 0.25, len(ydata))

    popt, pcov = curve_fit(f, xdata, ydata, p0=[495., 505.])
    print(popt, pcov)
    plt.figure()
    plt.plot(xdata, f(xdata, *popt), 'r-', label='fit')
    plt.plot(xdata, ydata, 'b-', label='data')
    plt.show()
 OptimizeWarning: Covariance of the parameters could not be estimated
",24,35,2,2,
860,49493699,49506514,71149,access remote files on server with smb protocol python3,2,<python><python-3.x><smb>,18,"<p>I have a remote server with some files.</p>
<pre><code>smb://ftpsrv/public/
</code></pre>
<p>I can be authorized there as an anonymous user. In java I could simply write this code:</p>
<p><code>SmbFile root = new SmbFile(SMB_ROOT);</code></p>
<p>And get the ability to work with files inside (it is all I need, one row!), but I can't find how to manage with this task in Python 3, there are a lot of resources, but I think they are not relevant to my problem, because they are frequently tailored for Python 2, and old other approaches. Is there some simple way, similar to Java code above?
Or can somebody provide a real working solution if, for example, I want to access file <code>fgg.txt</code> in  <code>smb://ftpsrv/public/</code> folder. Is there really a  handy lib to tackle  this problem?</p>
<p>For example on site:</p>
<pre><code>import tempfile
from smb.SMBConnection import SMBConnection

# There will be some mechanism to capture userID, password, client_machine_name, server_name and server_ip
# client_machine_name can be an arbitary ASCII string
# server_name should match the remote machine name, or else the connection will be rejected
conn = SMBConnection(userID, password, client_machine_name, server_name, use_ntlm_v2 = True)
assert conn.connect(server_ip, 139)

file_obj = tempfile.NamedTemporaryFile()
file_attributes, filesize = conn.retrieveFile('smbtest', '/rfc1001.txt', file_obj)

# Retrieved file contents are inside file_obj
# Do what you need with the file_obj and then close it
# Note that the file obj is positioned at the end-of-file,
# so you might need to perform a file_obj.seek() if you need
# to read from the beginning
file_obj.close()
</code></pre>
<p>Do I seriously need to provide all of these details: <code>conn = SMBConnection(userID, password, client_machine_name, server_name, use_ntlm_v2 = True)</code>?</p>
",5993278,4143,26-03-2018 14:20,27-03-2018 07:14,1,4183,44,3,26,39,"{'badge_counts': {'bronze': 44, 'silver': 26, 'gold': 3}, 'account_id': 7938112, 'is_employee': False, 'last_modified_date': 1703320500, 'last_access_date': 1710973293, 'reputation_change_year': 160, 'reputation_change_quarter': 160, 'reputation_change_month': 60, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4183, 'creation_date': 1456660832, 'user_type': 'registered', 'user_id': 5993278, 'accept_rate': 39, 'location': 'Saint Petersburg, Россия', 'website_url': '', 'link': 'https://stackoverflow.com/users/5993278/alex', 'profile_image': 'https://i.stack.imgur.com/aN3Pq.jpg?s=256&g=1', 'display_name': 'Alex'}","I have a remote server with some files. I can be authorized there as an anonymous user. In java I could simply write this code: And get the ability to work with files inside (it is all I need, one row!), but I can't find how to manage with this task in Python 3, there are a lot of resources, but I think they are not relevant to my problem, because they are frequently tailored for Python 2, and old other approaches. Is there some simple way, similar to Java code above? Or can somebody provide a real working solution if, for example, I want to access file in folder. Is there really a handy lib to tackle this problem? For example on site: Do I seriously need to provide all of these details: ?","smb://ftpsrv/public/
 SmbFile root = new SmbFile(SMB_ROOT); fgg.txt smb://ftpsrv/public/ import tempfile
from smb.SMBConnection import SMBConnection

# There will be some mechanism to capture userID, password, client_machine_name, server_name and server_ip
# client_machine_name can be an arbitary ASCII string
# server_name should match the remote machine name, or else the connection will be rejected
conn = SMBConnection(userID, password, client_machine_name, server_name, use_ntlm_v2 = True)
assert conn.connect(server_ip, 139)

file_obj = tempfile.NamedTemporaryFile()
file_attributes, filesize = conn.retrieveFile('smbtest', '/rfc1001.txt', file_obj)

# Retrieved file contents are inside file_obj
# Do what you need with the file_obj and then close it
# Note that the file obj is positioned at the end-of-file,
# so you might need to perform a file_obj.seek() if you need
# to read from the beginning
file_obj.close()
 conn = SMBConnection(userID, password, client_machine_name, server_name, use_ntlm_v2 = True)",13,28,0,0,
861,49382207,49382340,65203,How to map numeric data into categories / bins in Pandas dataframe,2,<python><python-2.7><pandas><numpy><dataframe>,43,"<p>I have a pandas dataframe:</p>
<p><a href=""https://i.stack.imgur.com/HNXlY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/HNXlY.png"" alt=""SamplePandas"" /></a></p>
<p>It has around 3m rows. There are 3 kinds of <code>age_units</code>: Y, D, W  for years, Days &amp; Weeks. Any individual over 1 year old has an age unit of Y and my first grouping I want is &lt;2y old so all I have to test for in Age Units is Y...</p>
<p>I want to create a new column <code>AgeRange</code> and populate with the following ranges:</p>
<ul>
<li>&lt;2</li>
<li>2 - 18</li>
<li>18 - 35</li>
<li>35 - 65</li>
<li>65+</li>
</ul>
<p>so I wrote a function</p>
<pre class=""lang-py prettyprint-override""><code>def agerange(values):
    for i in values:
        if complete.Age_units == 'Y':
            if complete.Age &gt; 1 AND &lt; 18 return '2-18'
            elif complete.Age &gt; 17 AND &lt; 35 return '18-35'
            elif complete.Age &gt; 34 AND &lt; 65 return '35-65'
            elif complete.Age &gt; 64 return '65+'
        else return '&lt; 2'
</code></pre>
<p>I thought if I passed in the dataframe as a whole, I would get back what I needed and then could create the column I wanted something like this:</p>
<pre class=""lang-py prettyprint-override""><code>agedetails['age_range'] = ageRange(agedetails)
</code></pre>
<p>BUT when I try to run the first code to create the function I get:</p>
<pre class=""lang-none prettyprint-override""><code>  File &quot;&lt;ipython-input-124-cf39c7ce66d9&gt;&quot;, line 4
    if complete.Age &gt; 1 AND complete.Age &lt; 18 return '2-18'
                          ^
SyntaxError: invalid syntax
</code></pre>
<p>Clearly it is not accepting the AND - but I thought I heard in class I could use AND like this?  I must be mistaken but then what would be the right way to do this?</p>
<p>So after getting that error, I'm not even sure the method of passing in a dataframe will throw an error either.  I am guessing probably yes.  In which case - how would I make that work as well?</p>
<p>I am looking to learn the best method, but part of the best method for me is keeping it simple even if that means doing things in a couple of steps...</p>
",7240180,1107,20-03-2018 10:48,20-03-2018 10:55,0,1107,28,2,13,89,"{'badge_counts': {'bronze': 28, 'silver': 13, 'gold': 2}, 'account_id': 9766966, 'is_employee': False, 'last_modified_date': 1607614459, 'last_access_date': 1710734362, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1107, 'creation_date': 1480666731, 'user_type': 'registered', 'user_id': 7240180, 'accept_rate': 89, 'location': 'Auckland, New Zealand', 'website_url': '', 'link': 'https://stackoverflow.com/users/7240180/kiltannen', 'profile_image': 'https://i.stack.imgur.com/dX8NQ.jpg?s=256&g=1', 'display_name': 'kiltannen'}","I have a pandas dataframe: It has around 3m rows. There are 3 kinds of : Y, D, W for years, Days &amp; Weeks. Any individual over 1 year old has an age unit of Y and my first grouping I want is &lt;2y old so all I have to test for in Age Units is Y... I want to create a new column and populate with the following ranges: &lt;2 2 - 18 18 - 35 35 - 65 65+ so I wrote a function I thought if I passed in the dataframe as a whole, I would get back what I needed and then could create the column I wanted something like this: BUT when I try to run the first code to create the function I get: Clearly it is not accepting the AND - but I thought I heard in class I could use AND like this? I must be mistaken but then what would be the right way to do this? So after getting that error, I'm not even sure the method of passing in a dataframe will throw an error either. I am guessing probably yes. In which case - how would I make that work as well? I am looking to learn the best method, but part of the best method for me is keeping it simple even if that means doing things in a couple of steps...","age_units AgeRange def agerange(values):
    for i in values:
        if complete.Age_units == 'Y':
            if complete.Age &gt; 1 AND &lt; 18 return '2-18'
            elif complete.Age &gt; 17 AND &lt; 35 return '18-35'
            elif complete.Age &gt; 34 AND &lt; 65 return '35-65'
            elif complete.Age &gt; 64 return '65+'
        else return '&lt; 2'
 agedetails['age_range'] = ageRange(agedetails)
   File &quot;&lt;ipython-input-124-cf39c7ce66d9&gt;&quot;, line 4
    if complete.Age &gt; 1 AND complete.Age &lt; 18 return '2-18'
                          ^
SyntaxError: invalid syntax
",8,33,1,1,
862,48136025,48136198,20878,TypeError: multiple bases have instance lay-out conflict,1,<python><python-3.x><class><defaultdict>,38,"<p>I wanted to create a class out of two: <code>collections.OrderedDict</code> and <code>collections.DefaultDict</code>. So that I can get an ordered dictionary and have a default value for non existing keys being accessed. Whats are some ways to do this?</p>

<p>My solution was to create another class around the 2 classes I stated above. This makes an error due to a method in each class having the same name I think?</p>

<pre><code>from collections import defaultdict, OrderedDict
class owndic(OrderedDict, defaultdict):
    pass
</code></pre>

<p>producing</p>

<pre><code>TypeError: multiple bases have instance lay-out conflict
</code></pre>

<p>Cheers!</p>
",6046760,494,07-01-2018 09:53,07-01-2018 10:20,0,494,6,1,4,,"{'badge_counts': {'bronze': 6, 'silver': 4, 'gold': 1}, 'account_id': 4411746, 'is_employee': False, 'last_modified_date': 1617656608, 'last_access_date': 1697465337, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 494, 'creation_date': 1457639136, 'user_type': 'registered', 'user_id': 6046760, 'link': 'https://stackoverflow.com/users/6046760/user6046760', 'profile_image': 'https://www.gravatar.com/avatar/647b3ebd5a10b81c1ff6833dba4a898b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user6046760'}",I wanted to create a class out of two: and . So that I can get an ordered dictionary and have a default value for non existing keys being accessed. Whats are some ways to do this? My solution was to create another class around the 2 classes I stated above. This makes an error due to a method in each class having the same name I think? producing Cheers!,"collections.OrderedDict collections.DefaultDict from collections import defaultdict, OrderedDict
class owndic(OrderedDict, defaultdict):
    pass
 TypeError: multiple bases have instance lay-out conflict
",0,15,0,0,
863,48139676,48139708,59745,How to get the value of an element in Python + Selenium,2,<python><html><selenium><dom><selenium-webdriver>,19,"<p>I have got this HTML element in my Python (3.6.3) code (as a Selenium <em>webelement</em> of course):</p>
<pre><code>&lt;span class=&quot;ocenaCzastkowa masterTooltip&quot; style=&quot;color:#000000;&quot; alt=&quot;Kod:
pd1&lt;br/&gt;Opis: praca domowa&lt;br/&gt;Waga: 2,00&lt;br/&gt;Data: 12.09.2017&lt;br/&gt;Nauczyciel:
(NAME CENSORED)&quot;&gt;5&lt;/span&gt;
</code></pre>
<p>And I want to get the value at the end (which is 5 in this case) and I have got no idea how to get it.</p>
<p>Obviously, I can't use <code>webelement.get_attribute()</code> because I don't know the name of the attribute.</p>
",6089912,411,07-01-2018 17:23,07-01-2018 17:25,0,411,19,2,5,100,"{'badge_counts': {'bronze': 19, 'silver': 5, 'gold': 2}, 'account_id': 8080512, 'is_employee': False, 'last_modified_date': 1629204000, 'last_access_date': 1637913411, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 411, 'creation_date': 1458487225, 'user_type': 'registered', 'user_id': 6089912, 'accept_rate': 100, 'location': 'Poland', 'website_url': 'https://www.linkedin.com/in/marcel-grabowski-jurczuk-2b01b0184/', 'link': 'https://stackoverflow.com/users/6089912/czyngis', 'profile_image': 'https://i.stack.imgur.com/BO1r9.png?s=256&g=1', 'display_name': 'czyngis'}","I have got this HTML element in my Python (3.6.3) code (as a Selenium webelement of course): And I want to get the value at the end (which is 5 in this case) and I have got no idea how to get it. Obviously, I can't use because I don't know the name of the attribute.","&lt;span class=&quot;ocenaCzastkowa masterTooltip&quot; style=&quot;color:#000000;&quot; alt=&quot;Kod:
pd1&lt;br/&gt;Opis: praca domowa&lt;br/&gt;Waga: 2,00&lt;br/&gt;Data: 12.09.2017&lt;br/&gt;Nauczyciel:
(NAME CENSORED)&quot;&gt;5&lt;/span&gt;
 webelement.get_attribute()",1,7,0,0,
864,48844778,48887612,15656,Create a .obj file from 3d array in python,1,<python><unity-game-engine><.obj><marching-cubes><nifti>,14,"<p>My goal is to get a .obj file from a nifty (.nii) format using python, with the purpose of open it on Unity. I know that the ""scikit-image"" package has a module called ""measure"" which has the Marching cube algorithm implemented. I apply the marching cube algorithm to my data and I obtain the results I expect:</p>

<pre><code>verts, faces, normals, values = measure.marching_cubes_lewiner(nifty_data, 0)
</code></pre>

<p>I can then plot the data:</p>

<pre><code>fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_trisurf(verts[:, 0], verts[:,1], faces, verts[:, 2],
                linewidth=0.2, antialiased=True)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/VzvGE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/VzvGE.png"" alt=""enter image description here""></a></p>

<p>I have looked for functions to save the data (verts,faces normals, values) as a .obj but I haven't found one. Thus, I decided to build it myself. </p>

<pre><code>thefile = open('test.obj', 'w')
for item in verts:
  thefile.write(""v {0} {1} {2}\n"".format(item[0],item[1],item[2]))

for item in normals:
  thefile.write(""vn {0} {1} {2}\n"".format(item[0],item[1],item[2]))

for item in faces:
  thefile.write(""f {0}//{0} {1}//{1} {2}//{2}\n"".format(item[0],item[1],item[2]))  

thefile.close()
</code></pre>

<p>But when I import the data to unity I got the following result:</p>

<p><a href=""https://i.stack.imgur.com/VUd4F.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/VUd4F.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/s7kGg.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/s7kGg.png"" alt=""enter image description here""></a></p>

<p>So my questions are the followings:</p>

<ul>
<li>What I'm doing wrong in the .obj making process?</li>
<li>Is there a module or function that do this in a better way?</li>
<li>Is it possible at all to do what I want?</li>
</ul>

<p>Thank you.</p>

<p>More examples:</p>

<p>Python:</p>

<p><a href=""https://i.stack.imgur.com/ABLBT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ABLBT.png"" alt=""enter image description here""></a></p>

<p>Unity:</p>

<p><a href=""https://i.stack.imgur.com/4yegQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4yegQ.png"" alt=""enter image description here""></a></p>
",6130605,1014,17-02-2018 18:50,20-02-2018 14:33,3,1014,20,2,11,88,"{'badge_counts': {'bronze': 20, 'silver': 11, 'gold': 2}, 'account_id': 8140149, 'is_employee': False, 'last_modified_date': 1679706301, 'last_access_date': 1710038120, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1014, 'creation_date': 1459271121, 'user_type': 'registered', 'user_id': 6130605, 'accept_rate': 88, 'location': 'Boston, MA, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/6130605/diego-orellana', 'profile_image': 'https://www.gravatar.com/avatar/3288fc667a0d999e11086b0eed014538?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Diego Orellana'}","My goal is to get a .obj file from a nifty (.nii) format using python, with the purpose of open it on Unity. I know that the ""scikit-image"" package has a module called ""measure"" which has the Marching cube algorithm implemented. I apply the marching cube algorithm to my data and I obtain the results I expect: I can then plot the data: I have looked for functions to save the data (verts,faces normals, values) as a .obj but I haven't found one. Thus, I decided to build it myself. But when I import the data to unity I got the following result: So my questions are the followings: What I'm doing wrong in the .obj making process? Is there a module or function that do this in a better way? Is it possible at all to do what I want? Thank you. More examples: Python: Unity:","verts, faces, normals, values = measure.marching_cubes_lewiner(nifty_data, 0)
 fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_trisurf(verts[:, 0], verts[:,1], faces, verts[:, 2],
                linewidth=0.2, antialiased=True)
plt.show()
 thefile = open('test.obj', 'w')
for item in verts:
  thefile.write(""v {0} {1} {2}\n"".format(item[0],item[1],item[2]))

for item in normals:
  thefile.write(""vn {0} {1} {2}\n"".format(item[0],item[1],item[2]))

for item in faces:
  thefile.write(""f {0}//{0} {1}//{1} {2}//{2}\n"".format(item[0],item[1],item[2]))  

thefile.close()
",14,56,5,5,
865,49758189,49760073,1935,async trio way to solve Hettinger's example,2,<python><asynchronous><python-trio>,12,"<p><a href=""https://twitter.com/raymondh?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor"" rel=""noreferrer"">Raymond Hettinger</a> gave <a href=""https://www.youtube.com/watch?v=9zinZmE3Ogk"" rel=""noreferrer"">a talk on concurrency</a> in python, where one of examples looked like that:</p>

<pre><code>import urllib.request

sites = [
    'https://www.yahoo.com/',
    'http://www.cnn.com',
    'http://www.python.org',
    'http://www.jython.org',
    'http://www.pypy.org',
    'http://www.perl.org',
    'http://www.cisco.com',
    'http://www.facebook.com',
    'http://www.twitter.com',
    'http://www.macrumors.com/',
    'http://arstechnica.com/',
    'http://www.reuters.com/',
    'http://abcnews.go.com/',
    'http://www.cnbc.com/',
]

for url in sites:
    with urllib.request.urlopen(url) as u:
        page = u.read()
        print(url, len(page))
</code></pre>

<p>Essentially we go after these links and print amount of received bytes and it takes about 20 seconds to run.</p>

<p>Today I found <a href=""https://github.com/python-trio/trio"" rel=""noreferrer"">trio</a> library that has quite friendly api. But when I am trying to use it with this rather basic example I am failing to do it right.</p>

<p>first try (runs around the same 20 seconds):</p>

<pre><code>import urllib.request
import trio, time

sites = [
    'https://www.yahoo.com/',
    'http://www.cnn.com',
    'http://www.python.org',
    'http://www.jython.org',
    'http://www.pypy.org',
    'http://www.perl.org',
    'http://www.cisco.com',
    'http://www.facebook.com',
    'http://www.twitter.com',
    'http://www.macrumors.com/',
    'http://arstechnica.com/',
    'http://www.reuters.com/',
    'http://abcnews.go.com/',
    'http://www.cnbc.com/',
]


async def show_len(sites):
    t1 = time.time()
    for url in sites:
        with urllib.request.urlopen(url) as u:
            page = u.read()
            print(url, len(page))
    print(""code took to run"", time.time() - t1)

if __name__ == ""__main__"":
    trio.run(show_len, sites)
</code></pre>

<p>and second one (same speed):</p>

<pre><code>import urllib.request
import trio, time

sites = [
    'https://www.yahoo.com/',
    'http://www.cnn.com',
    'http://www.python.org',
    'http://www.jython.org',
    'http://www.pypy.org',
    'http://www.perl.org',
    'http://www.cisco.com',
    'http://www.facebook.com',
    'http://www.twitter.com',
    'http://www.macrumors.com/',
    'http://arstechnica.com/',
    'http://www.reuters.com/',
    'http://abcnews.go.com/',
    'http://www.cnbc.com/',
]

async def link_user(url):
    with urllib.request.urlopen(url) as u:
        page = u.read()
        print(url, len(page))

async def show_len(sites):
    t1 = time.time()
    for url in sites:
        await link_user(url)
    print(""code took to run"", time.time() - t1)


if __name__ == ""__main__"":
    trio.run(show_len, sites)
</code></pre>

<p>So how is this example should be dealt with using trio? </p>
",6149882,665,10-04-2018 16:02,10-04-2018 17:51,0,665,22,2,7,100,"{'badge_counts': {'bronze': 22, 'silver': 7, 'gold': 2}, 'account_id': 8168485, 'is_employee': False, 'last_modified_date': 1654806900, 'last_access_date': 1691041148, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 665, 'creation_date': 1459620386, 'user_type': 'registered', 'user_id': 6149882, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/6149882/grail-finder', 'profile_image': 'https://i.stack.imgur.com/R8Iei.jpg?s=256&g=1', 'display_name': 'Grail Finder'}","Raymond Hettinger gave a talk on concurrency in python, where one of examples looked like that: Essentially we go after these links and print amount of received bytes and it takes about 20 seconds to run. Today I found trio library that has quite friendly api. But when I am trying to use it with this rather basic example I am failing to do it right. first try (runs around the same 20 seconds): and second one (same speed): So how is this example should be dealt with using trio?","import urllib.request

sites = [
    'https://www.yahoo.com/',
    'http://www.cnn.com',
    'http://www.python.org',
    'http://www.jython.org',
    'http://www.pypy.org',
    'http://www.perl.org',
    'http://www.cisco.com',
    'http://www.facebook.com',
    'http://www.twitter.com',
    'http://www.macrumors.com/',
    'http://arstechnica.com/',
    'http://www.reuters.com/',
    'http://abcnews.go.com/',
    'http://www.cnbc.com/',
]

for url in sites:
    with urllib.request.urlopen(url) as u:
        page = u.read()
        print(url, len(page))
 import urllib.request
import trio, time

sites = [
    'https://www.yahoo.com/',
    'http://www.cnn.com',
    'http://www.python.org',
    'http://www.jython.org',
    'http://www.pypy.org',
    'http://www.perl.org',
    'http://www.cisco.com',
    'http://www.facebook.com',
    'http://www.twitter.com',
    'http://www.macrumors.com/',
    'http://arstechnica.com/',
    'http://www.reuters.com/',
    'http://abcnews.go.com/',
    'http://www.cnbc.com/',
]


async def show_len(sites):
    t1 = time.time()
    for url in sites:
        with urllib.request.urlopen(url) as u:
            page = u.read()
            print(url, len(page))
    print(""code took to run"", time.time() - t1)

if __name__ == ""__main__"":
    trio.run(show_len, sites)
 import urllib.request
import trio, time

sites = [
    'https://www.yahoo.com/',
    'http://www.cnn.com',
    'http://www.python.org',
    'http://www.jython.org',
    'http://www.pypy.org',
    'http://www.perl.org',
    'http://www.cisco.com',
    'http://www.facebook.com',
    'http://www.twitter.com',
    'http://www.macrumors.com/',
    'http://arstechnica.com/',
    'http://www.reuters.com/',
    'http://abcnews.go.com/',
    'http://www.cnbc.com/',
]

async def link_user(url):
    with urllib.request.urlopen(url) as u:
        page = u.read()
        print(url, len(page))

async def show_len(sites):
    t1 = time.time()
    for url in sites:
        await link_user(url)
    print(""code took to run"", time.time() - t1)


if __name__ == ""__main__"":
    trio.run(show_len, sites)
",85,105,0,3,
866,50091553,50091605,3039,python generators garbage collection,2,<python><generator><python-internals>,29,"<p>I think my question is related to <a href=""https://stackoverflow.com/questions/15490127/will-a-python-generator-be-garbage-collected-if-it-will-not-be-used-any-more-but"">this</a>, but not exactly similar. Consider this code:</p>

<pre><code>def countdown(n):
    try:
        while n &gt; 0:
            yield n
            n -= 1
    finally:
        print('In the finally block')

def main():
    for n in countdown(10):
        if n == 5:
            break
        print('Counting... ', n)
    print('Finished counting')

main()
</code></pre>

<p>The output of this code is:</p>

<pre><code>Counting...  10      
Counting...  9       
Counting...  8       
Counting...  7       
Counting...  6       
In the finally block 
Finished counting  
</code></pre>

<p>Is it guaranteed that the line ""In the finally block"" is going to be printed before ""Finished counting""? Or is this because of cPython implementation detail that an object will be garbage collected when the reference count reaches 0.</p>

<p>Also I am curious on how <code>finally</code> block of the <code>countdown</code> generator is executed? e.g. if I change the code of <code>main</code> to</p>

<pre><code>def main():
    c = countdown(10)
    for n in c:
        if n == 5:
            break
        print('Counting... ', n)
    print('Finished counting')
</code></pre>

<p>then I do see <code>Finished counting</code> printed before <code>In the finally block</code>. How does the garbage collector directly go to the <code>finally</code> block? I think I have always taken <code>try/except/finally</code> on its face value, but thinking in the context of generators is making me think twice about it. </p>
",6153683,2537,29-04-2018 21:37,29-04-2018 21:46,0,2537,32,2,20,81,"{'badge_counts': {'bronze': 32, 'silver': 20, 'gold': 2}, 'account_id': 155877, 'is_employee': False, 'last_modified_date': 1607614495, 'last_access_date': 1666201821, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2537, 'creation_date': 1459727677, 'user_type': 'registered', 'user_id': 6153683, 'accept_rate': 81, 'link': 'https://stackoverflow.com/users/6153683/skgbanga', 'profile_image': 'https://www.gravatar.com/avatar/433c8247303c42b6fe5cef5821f79a5a?s=256&d=identicon&r=PG', 'display_name': 'skgbanga'}","I think my question is related to this, but not exactly similar. Consider this code: The output of this code is: Is it guaranteed that the line ""In the finally block"" is going to be printed before ""Finished counting""? Or is this because of cPython implementation detail that an object will be garbage collected when the reference count reaches 0. Also I am curious on how block of the generator is executed? e.g. if I change the code of to then I do see printed before . How does the garbage collector directly go to the block? I think I have always taken on its face value, but thinking in the context of generators is making me think twice about it.","def countdown(n):
    try:
        while n &gt; 0:
            yield n
            n -= 1
    finally:
        print('In the finally block')

def main():
    for n in countdown(10):
        if n == 5:
            break
        print('Counting... ', n)
    print('Finished counting')

main()
 Counting...  10      
Counting...  9       
Counting...  8       
Counting...  7       
Counting...  6       
In the finally block 
Finished counting  
 finally countdown main def main():
    c = countdown(10)
    for n in c:
        if n == 5:
            break
        print('Counting... ', n)
    print('Finished counting')
 Finished counting In the finally block finally try/except/finally",20,45,0,1,
867,49902599,49905571,11927,Airflow latency between tasks,2,<python><airflow><directed-acyclic-graphs><airflow-scheduler>,16,"<p>As you can see  in the image : <img src=""https://i.stack.imgur.com/Bac17.png"" alt=""DAG latency between tasks]""> airflow is making too much time between tasks execution ?
it almost represents 30% of the DAG execution time.
I've changed the <code>airflow.cfg</code> file to:</p>

<pre><code>job_heartbeat_sec = 1 
scheduler_heartbeat_sec = 1
</code></pre>

<p>but I still have the same latency rate.</p>

<p>Why does it behave this way ?</p>
",6268756,507,18-04-2018 14:40,18-04-2018 17:22,0,507,12,2,5,,"{'badge_counts': {'bronze': 12, 'silver': 5, 'gold': 2}, 'account_id': 8344524, 'is_employee': False, 'last_modified_date': 1573679099, 'last_access_date': 1599747817, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 507, 'creation_date': 1461876863, 'user_type': 'registered', 'user_id': 6268756, 'link': 'https://stackoverflow.com/users/6268756/i-chorfi', 'profile_image': 'https://www.gravatar.com/avatar/fd3603fabaf94d825a644ce6aa436590?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'I.Chorfi'}",As you can see in the image : airflow is making too much time between tasks execution ? it almost represents 30% of the DAG execution time. I've changed the file to: but I still have the same latency rate. Why does it behave this way ?,"airflow.cfg job_heartbeat_sec = 1 
scheduler_heartbeat_sec = 1
",0,11,1,0,
868,50263380,50263616,14945,python: zipfile.ZipFile No such file or directory,3,<python><errno><python-zipfile>,11,"<p>There is folder path:</p>

<pre><code>P:\\2018\\Archive\\
</code></pre>

<p>There are many zipfiles I want to create programmatically, but am starting with test. I will name this test zip file ""CO_007_II.zip"" and will attempt to create in above location:</p>

<pre><code>import zipfile as zp

with zp.ZipFile(""P:\\2018\\Archive\\CO_007_II.zip"",'w') as myzip:
    myzip.write(r""P:\2018\CO_007_II"")
</code></pre>

<p>But I get error!</p>

<p>...     </p>

<pre><code>Traceback (most recent call last):
  File ""&lt;interactive input&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\ArcGIS10.2\lib\zipfile.py"", line 752, in __init__
    self.fp = open(file, modeDict[mode])
IOError: [Errno 2] No such file or directory: 'P:\\2018\\Archive\\CO_007_II.zip'
</code></pre>

<p>Is this not method for creating new zipfile? I know file does not exist. Is why I am using 'w' mode, no?</p>

<p>This is documentation:</p>

<p><a href=""https://docs.python.org/3/library/zipfile.html"" rel=""noreferrer"">https://docs.python.org/3/library/zipfile.html</a></p>

<p>It says:</p>

<p>'w' to truncate and <strong><em>write a new file</em></strong></p>

<p>Example on documentation page:</p>

<pre><code>with ZipFile('spam.zip', 'w') as myzip:
    myzip.write('eggs.txt')
</code></pre>

<p>code worked two days ago to create new zip file but did not add folder. Today nothing works! Why not? All paths valid. How do I create new zip file with python and add folders to it?</p>
",6273533,677,09-05-2018 23:22,09-05-2018 23:56,0,699,33,2,12,81,"{'badge_counts': {'bronze': 33, 'silver': 12, 'gold': 2}, 'account_id': 6865820, 'is_employee': False, 'last_modified_date': 1702692600, 'last_access_date': 1711146781, 'reputation_change_year': 32, 'reputation_change_quarter': 32, 'reputation_change_month': 32, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 699, 'creation_date': 1461968620, 'user_type': 'registered', 'user_id': 6273533, 'accept_rate': 81, 'location': 'Jupiter', 'link': 'https://stackoverflow.com/users/6273533/geojshaun', 'profile_image': 'https://i.stack.imgur.com/ztqm3.jpg?s=256&g=1', 'display_name': 'geoJshaun'}","There is folder path: There are many zipfiles I want to create programmatically, but am starting with test. I will name this test zip file ""CO_007_II.zip"" and will attempt to create in above location: But I get error! ... Is this not method for creating new zipfile? I know file does not exist. Is why I am using 'w' mode, no? This is documentation: https://docs.python.org/3/library/zipfile.html It says: 'w' to truncate and write a new file Example on documentation page: code worked two days ago to create new zip file but did not add folder. Today nothing works! Why not? All paths valid. How do I create new zip file with python and add folders to it?","P:\\2018\\Archive\\
 import zipfile as zp

with zp.ZipFile(""P:\\2018\\Archive\\CO_007_II.zip"",'w') as myzip:
    myzip.write(r""P:\2018\CO_007_II"")
 Traceback (most recent call last):
  File ""&lt;interactive input&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python27\ArcGIS10.2\lib\zipfile.py"", line 752, in __init__
    self.fp = open(file, modeDict[mode])
IOError: [Errno 2] No such file or directory: 'P:\\2018\\Archive\\CO_007_II.zip'
 with ZipFile('spam.zip', 'w') as myzip:
    myzip.write('eggs.txt')
",8,41,0,1,
869,49425727,49425795,25855,How do I drop duplicates and keep the last timestamp on pandas,1,<python><pandas><dataframe><timestamp>,16,"<p>I want to drop duplicates and keep the last  timestamp. The duplicates that want to be dropped is <code>customer_id</code> and <code>var_name</code> .Here's my data</p>

<pre><code>    customer_id  value   var_name     timestamp
    1            1       apple        2018-03-22 00:00:00.000        
    2            3       apple        2018-03-23 08:00:00.000
    2            4       apple        2018-03-24 08:00:00.000
    1            1       orange       2018-03-22 08:00:00.000
    2            3       orange       2018-03-24 08:00:00.000
    2            5       orange       2018-03-23 08:00:00.000
</code></pre>

<p>So the result will be </p>

<pre><code>    customer_id  value   var_name     timestamp
    1            1       apple        2018-03-22 00:00:00.000        
    2            4       apple        2018-03-24 08:00:00.000
    1            1       orange       2018-03-22 08:00:00.000
    2            3       orange       2018-03-24 08:00:00.000
</code></pre>
",7585973,6761,22-03-2018 10:05,22-03-2018 10:08,0,6761,72,8,39,100,"{'badge_counts': {'bronze': 72, 'silver': 39, 'gold': 8}, 'account_id': 2371019, 'is_employee': False, 'last_modified_date': 1699667400, 'last_access_date': 1709286759, 'reputation_change_year': 190, 'reputation_change_quarter': 190, 'reputation_change_month': 40, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6761, 'creation_date': 1487439830, 'user_type': 'registered', 'user_id': 7585973, 'accept_rate': 100, 'location': 'Jakarta, Indonesia', 'website_url': 'http://nabihbawazir.com', 'link': 'https://stackoverflow.com/users/7585973/nabih-bawazir', 'profile_image': 'https://www.gravatar.com/avatar/1cf9f40e4f8e5e17076d55814a9c2ad9?s=256&d=identicon&r=PG', 'display_name': 'Nabih Bawazir'}",I want to drop duplicates and keep the last timestamp. The duplicates that want to be dropped is and .Here's my data So the result will be,"customer_id var_name     customer_id  value   var_name     timestamp
    1            1       apple        2018-03-22 00:00:00.000        
    2            3       apple        2018-03-23 08:00:00.000
    2            4       apple        2018-03-24 08:00:00.000
    1            1       orange       2018-03-22 08:00:00.000
    2            3       orange       2018-03-24 08:00:00.000
    2            5       orange       2018-03-23 08:00:00.000
     customer_id  value   var_name     timestamp
    1            1       apple        2018-03-22 00:00:00.000        
    2            4       apple        2018-03-24 08:00:00.000
    1            1       orange       2018-03-22 08:00:00.000
    2            3       orange       2018-03-24 08:00:00.000
",8,19,0,0,
870,48964181,48964403,47939,How to load a pickle file from S3 to use in AWS Lambda?,4,<python><amazon-web-services><amazon-s3><aws-lambda><pickle>,30,"<p>I am currently trying to load a pickled file from S3 into AWS lambda and store it to a list (the pickle is a list).</p>

<p>Here is my code:</p>

<pre><code>import pickle
import boto3

s3 = boto3.resource('s3')
with open('oldscreenurls.pkl', 'rb') as data:
    old_list = s3.Bucket(""pythonpickles"").download_fileobj(""oldscreenurls.pkl"", data)
</code></pre>

<p>I get the following error even though the file exists:</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'oldscreenurls.pkl'
</code></pre>

<p>Any ideas?</p>
",6327717,803,24-02-2018 14:58,24-02-2018 15:24,0,803,12,2,8,,"{'badge_counts': {'bronze': 12, 'silver': 8, 'gold': 2}, 'account_id': 8432494, 'is_employee': False, 'last_modified_date': 1607614489, 'last_access_date': 1654821217, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 803, 'creation_date': 1463088089, 'user_type': 'registered', 'user_id': 6327717, 'link': 'https://stackoverflow.com/users/6327717/mifin', 'profile_image': 'https://www.gravatar.com/avatar/56d98a8a03b42c1a02bc7b60f891f07b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'mifin'}",I am currently trying to load a pickled file from S3 into AWS lambda and store it to a list (the pickle is a list). Here is my code: I get the following error even though the file exists: Any ideas?,"import pickle
import boto3

s3 = boto3.resource('s3')
with open('oldscreenurls.pkl', 'rb') as data:
    old_list = s3.Bucket(""pythonpickles"").download_fileobj(""oldscreenurls.pkl"", data)
 FileNotFoundError: [Errno 2] No such file or directory: 'oldscreenurls.pkl'
",5,18,0,0,
871,48912253,48912284,170054,aws lambda Unable to import module 'lambda_function': No module named 'requests',10,<python><amazon-web-services><logging><aws-lambda><amazon-cloudwatch>,46,"<p>I have recently started to use AWS Lambda to use triggers against some python code I have written. I currently have 2 lambda functions, both of which have been created with ZIP files. The second one I created is supposed to test the trigger events.</p>

<p>This is for testing purposes so I'm using the best code of all: </p>

<pre><code>def lambda_handler(event, context):
    print (""Hello World"")
</code></pre>

<p>However, I get this error back:</p>

<pre><code>Response:
{
  ""errorMessage"": ""Unable to import module 'lambda_function'""
}

Request ID:
""65024f16-172c-11e8-ab26-27ff3322e597""

Function Logs:
START RequestId: 65024f16-172c-11e8-ab26-27ff3322e597 Version: $LATEST
Unable to import module 'lambda_function': No module named 'requests'

END RequestId: 65024f16-172c-11e8-ab26-27ff3322e597
REPORT RequestId: 65024f16-172c-11e8-ab26-27ff3322e597  Duration: 15.93 ms  
Billed Duration: 100 ms     Memory Size: 128 MB Max Memory Used: 22 MB  
</code></pre>

<p>Everywhere I have searched for this, the answer was solved by making sure the names for the functions were correct or making sure the .zip file was readable. I have satisfied both of these conditions (the name of the file is lambda_function.py and it is in the root). </p>

<p>Alternatively, it seems like it might be an issue with the logs. I double checked my permission and I have the ability to create logs with all resources. Any other ideas what the issue might be?</p>
",6327717,803,21-02-2018 17:42,21-02-2018 17:43,0,803,12,2,8,,"{'badge_counts': {'bronze': 12, 'silver': 8, 'gold': 2}, 'account_id': 8432494, 'is_employee': False, 'last_modified_date': 1607614489, 'last_access_date': 1654821217, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 803, 'creation_date': 1463088089, 'user_type': 'registered', 'user_id': 6327717, 'link': 'https://stackoverflow.com/users/6327717/mifin', 'profile_image': 'https://www.gravatar.com/avatar/56d98a8a03b42c1a02bc7b60f891f07b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'mifin'}","I have recently started to use AWS Lambda to use triggers against some python code I have written. I currently have 2 lambda functions, both of which have been created with ZIP files. The second one I created is supposed to test the trigger events. This is for testing purposes so I'm using the best code of all: However, I get this error back: Everywhere I have searched for this, the answer was solved by making sure the names for the functions were correct or making sure the .zip file was readable. I have satisfied both of these conditions (the name of the file is lambda_function.py and it is in the root). Alternatively, it seems like it might be an issue with the logs. I double checked my permission and I have the ability to create logs with all resources. Any other ideas what the issue might be?","def lambda_handler(event, context):
    print (""Hello World"")
 Response:
{
  ""errorMessage"": ""Unable to import module 'lambda_function'""
}

Request ID:
""65024f16-172c-11e8-ab26-27ff3322e597""

Function Logs:
START RequestId: 65024f16-172c-11e8-ab26-27ff3322e597 Version: $LATEST
Unable to import module 'lambda_function': No module named 'requests'

END RequestId: 65024f16-172c-11e8-ab26-27ff3322e597
REPORT RequestId: 65024f16-172c-11e8-ab26-27ff3322e597  Duration: 15.93 ms  
Billed Duration: 100 ms     Memory Size: 128 MB Max Memory Used: 22 MB  
",15,30,0,0,
872,49655525,49656032,32458,Django 2.0 - Not a valid view function or pattern name (Customizing Auth views),2,<python><django><authentication><django-templates>,20,"<p>I´m working on a course exercise and I'm stuck for a few hours and I'm not sure what is causing the app to break, next, you will find the files involved and perhaps you can find out the solution. Thanks for your help!</p>

<p><a href=""https://i.stack.imgur.com/Hb3gD.png"" rel=""noreferrer"">Project structure</a></p>

<p>This error is being thrown when I log in:</p>

<pre><code>Internal Server Error: /account/login/

...


    django.urls.exceptions.NoReverseMatch: Reverse for 'dashboard' not found. 'dashboard' is not a valid view function or pattern name.
    [04/Apr/2018 17:12:15] ""POST /account/login/ HTTP/1.1"" 500 151978
</code></pre>

<p><strong>At the end of the <em>settings.py</em> file</strong></p>

<pre><code>from django.urls import reverse_lazy

LOGIN_REDIRECT_URL = reverse_lazy('dashboard')
LOGIN_URL = reverse_lazy('login')
LOGOUT_REDIRECT_URL = reverse_lazy('logout')
</code></pre>

<p><strong>The <em>urls.py</em> file</strong></p>

<pre><code>from django.contrib.auth import views as auth_views
from django.urls import path
from . import views

app_name = 'account'

urlpatterns = [
    # path('login/', views.user_login, name='login'),
    path('', views.dashboard, name='dashboard'),

    # login / logout urls
    path('login/', auth_views.LoginView.as_view(template_name='registration/login.html'), name='login'),
    path('logout/', auth_views.LogoutView.as_view(template_name='registration/logged_out.html'), name='logout'),
    path('logout-then-login/', auth_views.logout_then_login, name='logout_then_login'),
]
</code></pre>

<p><strong>The <em>views.py</em> file</strong></p>

<pre><code>from django.contrib.auth import authenticate, login
from django.contrib.auth.decorators import login_required
from django.http import HttpResponse
from django.shortcuts import render


@login_required
def dashboard(request):
    return render(request, 'account/dashboard.html', {'section': 'dashboard'})
</code></pre>

<p><strong>The <em>base.html</em> template</strong></p>

<pre><code>{% load staticfiles %}
&lt;!doctype html&gt;
&lt;html lang=""en""&gt;
&lt;head&gt;
    &lt;meta charset=""UTF-8""&gt;
    &lt;meta name=""viewport""
          content=""width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0""&gt;
    &lt;meta http-equiv=""X-UA-Compatible"" content=""ie=edge""&gt;
    &lt;title&gt;{% block title %}{% endblock %}&lt;/title&gt;
    &lt;link rel=""stylesheet"" href=""{% static ""css/base.css"" %}""&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div id=""header""&gt;
        &lt;span class=""logo""&gt;Bookmarks&lt;/span&gt;
        {% if request.user.is_authenticated %}
            &lt;ul class=""menu""&gt;
                &lt;li&gt; {% if section == ""dashboard"" %}class=""selected""{% endif %}&gt;&lt;a href=""{% url ""account:dashboard"" %}""&gt;My dashboard&lt;/a&gt;&lt;/li&gt;
                &lt;li&gt; {% if section == ""images"" %}class=""selected""{% endif %}&lt;a href=""#""&gt;Images&lt;/a&gt;&lt;/li&gt;
                &lt;li&gt; {% if section == ""people"" %}class=""selected""{% endif %}&lt;a href=""#""&gt;People&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
        {% endif %}

        &lt;span class=""user""&gt;
            {% if request.user.is_authenticated %}
                Hello {{ request.user.first_name }}, &lt;a href=""{% url ""account:logout %}""&gt;Logout&lt;/a&gt;
            {% else %}
                &lt;a href=""{% url ""account:dashboard"" %}""&gt;&lt;/a&gt;
            {% endif %}
        &lt;/span&gt;
    &lt;/div&gt;

    &lt;div id=""content""&gt;
        {% block content %}
        {% endblock %}
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>I appreciate your help. Thanks a lot!</p>
",6329996,453,04-04-2018 15:58,04-04-2018 16:30,0,453,14,1,6,,"{'badge_counts': {'bronze': 14, 'silver': 6, 'gold': 1}, 'account_id': 8435758, 'is_employee': False, 'last_modified_date': 1580291627, 'last_access_date': 1583002416, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 453, 'creation_date': 1463135284, 'user_type': 'registered', 'user_id': 6329996, 'link': 'https://stackoverflow.com/users/6329996/ralfillo', 'profile_image': 'https://www.gravatar.com/avatar/a53c7d5700cadaef5fb07a8976e64c27?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ralfillo'}","I´m working on a course exercise and I'm stuck for a few hours and I'm not sure what is causing the app to break, next, you will find the files involved and perhaps you can find out the solution. Thanks for your help! Project structure This error is being thrown when I log in: At the end of the settings.py file The urls.py file The views.py file The base.html template I appreciate your help. Thanks a lot!","Internal Server Error: /account/login/

...


    django.urls.exceptions.NoReverseMatch: Reverse for 'dashboard' not found. 'dashboard' is not a valid view function or pattern name.
    [04/Apr/2018 17:12:15] ""POST /account/login/ HTTP/1.1"" 500 151978
 from django.urls import reverse_lazy

LOGIN_REDIRECT_URL = reverse_lazy('dashboard')
LOGIN_URL = reverse_lazy('login')
LOGOUT_REDIRECT_URL = reverse_lazy('logout')
 from django.contrib.auth import views as auth_views
from django.urls import path
from . import views

app_name = 'account'

urlpatterns = [
    # path('login/', views.user_login, name='login'),
    path('', views.dashboard, name='dashboard'),

    # login / logout urls
    path('login/', auth_views.LoginView.as_view(template_name='registration/login.html'), name='login'),
    path('logout/', auth_views.LogoutView.as_view(template_name='registration/logged_out.html'), name='logout'),
    path('logout-then-login/', auth_views.logout_then_login, name='logout_then_login'),
]
 from django.contrib.auth import authenticate, login
from django.contrib.auth.decorators import login_required
from django.http import HttpResponse
from django.shortcuts import render


@login_required
def dashboard(request):
    return render(request, 'account/dashboard.html', {'section': 'dashboard'})
 {% load staticfiles %}
&lt;!doctype html&gt;
&lt;html lang=""en""&gt;
&lt;head&gt;
    &lt;meta charset=""UTF-8""&gt;
    &lt;meta name=""viewport""
          content=""width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0""&gt;
    &lt;meta http-equiv=""X-UA-Compatible"" content=""ie=edge""&gt;
    &lt;title&gt;{% block title %}{% endblock %}&lt;/title&gt;
    &lt;link rel=""stylesheet"" href=""{% static ""css/base.css"" %}""&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div id=""header""&gt;
        &lt;span class=""logo""&gt;Bookmarks&lt;/span&gt;
        {% if request.user.is_authenticated %}
            &lt;ul class=""menu""&gt;
                &lt;li&gt; {% if section == ""dashboard"" %}class=""selected""{% endif %}&gt;&lt;a href=""{% url ""account:dashboard"" %}""&gt;My dashboard&lt;/a&gt;&lt;/li&gt;
                &lt;li&gt; {% if section == ""images"" %}class=""selected""{% endif %}&lt;a href=""#""&gt;Images&lt;/a&gt;&lt;/li&gt;
                &lt;li&gt; {% if section == ""people"" %}class=""selected""{% endif %}&lt;a href=""#""&gt;People&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
        {% endif %}

        &lt;span class=""user""&gt;
            {% if request.user.is_authenticated %}
                Hello {{ request.user.first_name }}, &lt;a href=""{% url ""account:logout %}""&gt;Logout&lt;/a&gt;
            {% else %}
                &lt;a href=""{% url ""account:dashboard"" %}""&gt;&lt;/a&gt;
            {% endif %}
        &lt;/span&gt;
    &lt;/div&gt;

    &lt;div id=""content""&gt;
        {% block content %}
        {% endblock %}
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
",68,98,0,1,
873,49823033,63891465,4883,Conda dependencies do not install on local package build,4,<python><conda>,34,"<p>I am building a Python package using <code>conda-build</code>. Right now, my structure looks like this:</p>
<pre><code>- my_recipe/
    - meta.yaml
    - build.sh
</code></pre>
<p>And my <code>meta.yaml</code> reads thusly:</p>
<pre><code>package:
  name: my_pkg
version: &quot;0.2.0&quot;

source:
  path: ../my_pkg

requirements:
  build:
    - python
    - setuptools
  run:
    - python
    - pandas
    - numpy
    - plotly
    - matplotlib
    - pyqtgraph
    - pyopengl
    - gdal
    - scipy
    - scikit-image
</code></pre>
<p>The package itself builds correctly when I run</p>
<p><code>conda-build my_recipe/</code></p>
<p>and it installs successfully when I run</p>
<p><code>conda install -n my_env --use-local ~/miniconda3/envs/my_env/conda-bld/linux-64/my_pkg-0.2.0-py36_0.tar.bz2</code></p>
<p>However, none of the dependencies listed under <code>run</code> seem to install along with the package. For example, when I import the package in Python it says that <code>pandas</code> could not be found.</p>
<p>Are my dependencies listed in the correct location? Do I also need to list the dependencies in <code>setup.py</code>? The documentation is not very clear on where this information should be.</p>
",5249681,727,13-04-2018 18:19,14-09-2020 20:32,885,727,24,0,10,67,"{'badge_counts': {'bronze': 24, 'silver': 10, 'gold': 0}, 'account_id': 6823199, 'is_employee': False, 'last_modified_date': 1698073747, 'last_access_date': 1707087688, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 727, 'creation_date': 1440113262, 'user_type': 'registered', 'user_id': 5249681, 'accept_rate': 67, 'location': 'Olympia, WA, United States', 'website_url': 'http://brycefrank.com/', 'link': 'https://stackoverflow.com/users/5249681/bryce-frank', 'profile_image': 'https://i.stack.imgur.com/CftX5.jpg?s=256&g=1', 'display_name': 'Bryce Frank'}","I am building a Python package using . Right now, my structure looks like this: And my reads thusly: The package itself builds correctly when I run and it installs successfully when I run However, none of the dependencies listed under seem to install along with the package. For example, when I import the package in Python it says that could not be found. Are my dependencies listed in the correct location? Do I also need to list the dependencies in ? The documentation is not very clear on where this information should be.","conda-build - my_recipe/
    - meta.yaml
    - build.sh
 meta.yaml package:
  name: my_pkg
version: &quot;0.2.0&quot;

source:
  path: ../my_pkg

requirements:
  build:
    - python
    - setuptools
  run:
    - python
    - pandas
    - numpy
    - plotly
    - matplotlib
    - pyqtgraph
    - pyopengl
    - gdal
    - scipy
    - scikit-image
 conda-build my_recipe/ conda install -n my_env --use-local ~/miniconda3/envs/my_env/conda-bld/linux-64/my_pkg-0.2.0-py36_0.tar.bz2 run pandas setup.py",16,35,0,0,
874,48939795,48940635,95322,How to plot a count bar chart grouping by one categorical column and coloring by another,4,<python><pandas><bar-chart>,17,"<p>I have a dataframe that looks roughly like this:</p>
<pre><code>  Property   Name    industry
1  123     name1    industry 1
1  144     name1    industry 1
2  456     name2    industry 1
3  789     name3    industry 2
4  367     name4    industry 2
.  ...     ...      ... 
.  ...     ...      ... 
n  123     name1    industry 1
</code></pre>
<p>I want to make a bar plot that plots how many rows for each of the Names there are, and colors the bars by what industry it is. I've tried something like this:</p>
<pre><code>ax = df['name'].value_counts().plot(kind='bar',
                                    figsize=(14,8),
                                    title=&quot;Number for each Owner Name&quot;)
ax.set_xlabel(&quot;Owner Names&quot;)
ax.set_ylabel(&quot;Frequency&quot;)
</code></pre>
<p>I get the following:</p>
<p><a href=""https://i.stack.imgur.com/he7iz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/he7iz.png"" alt=""almost there"" /></a></p>
<p>My question is how do I colour the bars according the the industry column in the dataframe (and add a legend).</p>
",5377184,909,23-02-2018 01:24,23-02-2018 03:24,0,919,20,1,9,50,"{'badge_counts': {'bronze': 20, 'silver': 9, 'gold': 1}, 'account_id': 7015787, 'is_employee': False, 'last_modified_date': 1684846811, 'last_access_date': 1709412196, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 919, 'creation_date': 1443201378, 'user_type': 'registered', 'user_id': 5377184, 'accept_rate': 50, 'link': 'https://stackoverflow.com/users/5377184/tlanigan', 'profile_image': 'https://i.stack.imgur.com/clN8K.jpg?s=256&g=1', 'display_name': 'tlanigan'}","I have a dataframe that looks roughly like this: I want to make a bar plot that plots how many rows for each of the Names there are, and colors the bars by what industry it is. I've tried something like this: I get the following: My question is how do I colour the bars according the the industry column in the dataframe (and add a legend).","  Property   Name    industry
1  123     name1    industry 1
1  144     name1    industry 1
2  456     name2    industry 1
3  789     name3    industry 2
4  367     name4    industry 2
.  ...     ...      ... 
.  ...     ...      ... 
n  123     name1    industry 1
 ax = df['name'].value_counts().plot(kind='bar',
                                    figsize=(14,8),
                                    title=&quot;Number for each Owner Name&quot;)
ax.set_xlabel(&quot;Owner Names&quot;)
ax.set_ylabel(&quot;Frequency&quot;)
",12,21,1,1,
875,49505872,49505918,120406,Read JSON to pandas dataframe - ValueError: Mixing dicts with non-Series may lead to ambiguous ordering,5,<python><json><pandas>,46,"<p>I am trying to read in the JSON structure below into pandas dataframe, but it throws out the error message: </p>

<blockquote>
  <p>ValueError: Mixing dicts with non-Series may lead to ambiguous
  ordering.</p>
</blockquote>

<p><strong>Json data:</strong></p>

<pre><code>{
    ""status"": {
        ""statuscode"": 200,
        ""statusmessage"": ""Everything OK""
    },

    ""result"": [{
        ""id"": 22,
        ""club_id"": 16182
    }, {
        ""id"": 23,
        ""club_id"": 16182
    }, {
        ""id"": 24,
        ""club_id"": 16182
    }, {
        ""id"": 25,
        ""club_id"": 16182
    }, {
        ""id"": 26,
        ""club_id"": 16182
    }, {
        ""id"": 27,
        ""club_id"": 16182
    }]
}
</code></pre>

<p>How do I get this right? I have tried the script below...</p>

<pre><code>j_df = pd.read_json('json_file.json')
j_df

with open(j_file) as jsonfile:
    data = json.load(jsonfile)
</code></pre>
",7719336,3791,27-03-2018 06:35,27-03-2018 06:39,0,3791,24,4,15,100,"{'badge_counts': {'bronze': 24, 'silver': 15, 'gold': 4}, 'account_id': 10471646, 'is_employee': False, 'last_modified_date': 1607614445, 'last_access_date': 1695704998, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 3791, 'creation_date': 1489645256, 'user_type': 'registered', 'user_id': 7719336, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/7719336/userpygeo', 'profile_image': 'https://lh4.googleusercontent.com/-02qwV9PzbOo/AAAAAAAAAAI/AAAAAAAAABI/zubAT3aKQ2g/photo.jpg?sz=256', 'display_name': 'userPyGeo'}","I am trying to read in the JSON structure below into pandas dataframe, but it throws out the error message: ValueError: Mixing dicts with non-Series may lead to ambiguous ordering. Json data: How do I get this right? I have tried the script below...","{
    ""status"": {
        ""statuscode"": 200,
        ""statusmessage"": ""Everything OK""
    },

    ""result"": [{
        ""id"": 22,
        ""club_id"": 16182
    }, {
        ""id"": 23,
        ""club_id"": 16182
    }, {
        ""id"": 24,
        ""club_id"": 16182
    }, {
        ""id"": 25,
        ""club_id"": 16182
    }, {
        ""id"": 26,
        ""club_id"": 16182
    }, {
        ""id"": 27,
        ""club_id"": 16182
    }]
}
 j_df = pd.read_json('json_file.json')
j_df

with open(j_file) as jsonfile:
    data = json.load(jsonfile)
",29,45,0,0,
876,50138615,50138677,271676,WebDriverException: unknown error: cannot find Chrome binary error with Selenium in Python for older versions of Google Chrome,12,<python><google-chrome><selenium-webdriver><selenium-chromedriver>,47,"<p>For compatibility reasons I prefer to use Chrome version 55.0.2883.75 with Chromedriver v. 2.26. I downloaded the older version of chrome from <a href=""https://www.slimjet.com/chrome/google-chrome-old-version.php"" rel=""nofollow noreferrer"">https://www.slimjet.com/chrome/google-chrome-old-version.php</a> and Chromedriver 2.26 from <a href=""https://chromedriver.storage.googleapis.com/index.html?path=2.26/"" rel=""nofollow noreferrer"">https://chromedriver.storage.googleapis.com/index.html?path=2.26/</a>.</p>
<p>I am using the following code to attempt to set my Chrome binary location:</p>
<pre><code>from selenium import webdriver
from selenium.webdriver.chrome.options import Options

options = Options()
options.binary_location = &quot;C:\\Program Files\\Chrome\\chrome64_55.0.2883.75\\chrome.exe&quot;
driver = webdriver.Chrome('chromedriver.exe', chrome_options = options)
</code></pre>
<p>However, when I attempt to launch the WebDriver Python returns the following error:</p>
<pre><code>WebDriverException: unknown error: cannot find Chrome binary
(Driver info: chromedriver=2.26.436362
(5476ec6bf7ccbada1734a0cdec7d570bb042aa30),platform=Windows NT 10.0.14393 x86_64)
</code></pre>
<p>I have tried searching through similar questions and answers, but have not had any luck so far.</p>
",5405592,623,02-05-2018 15:45,02-05-2018 15:49,0,623,11,1,8,83,"{'badge_counts': {'bronze': 11, 'silver': 8, 'gold': 1}, 'account_id': 7059864, 'is_employee': False, 'last_modified_date': 1707350182, 'last_access_date': 1709169149, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 623, 'creation_date': 1443917372, 'user_type': 'registered', 'user_id': 5405592, 'accept_rate': 83, 'link': 'https://stackoverflow.com/users/5405592/venetian', 'profile_image': 'https://www.gravatar.com/avatar/95da553a0ee87a512a48da74513f3271?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Venetian'}","For compatibility reasons I prefer to use Chrome version 55.0.2883.75 with Chromedriver v. 2.26. I downloaded the older version of chrome from https://www.slimjet.com/chrome/google-chrome-old-version.php and Chromedriver 2.26 from https://chromedriver.storage.googleapis.com/index.html?path=2.26/. I am using the following code to attempt to set my Chrome binary location: However, when I attempt to launch the WebDriver Python returns the following error: I have tried searching through similar questions and answers, but have not had any luck so far.","from selenium import webdriver
from selenium.webdriver.chrome.options import Options

options = Options()
options.binary_location = &quot;C:\\Program Files\\Chrome\\chrome64_55.0.2883.75\\chrome.exe&quot;
driver = webdriver.Chrome('chromedriver.exe', chrome_options = options)
 WebDriverException: unknown error: cannot find Chrome binary
(Driver info: chromedriver=2.26.436362
(5476ec6bf7ccbada1734a0cdec7d570bb042aa30),platform=Windows NT 10.0.14393 x86_64)
",7,15,0,2,
877,49724954,49734613,2840,How are PyTorch's tensors implemented?,2,<python><python-3.x><rust><pytorch><tensor>,16,"<p>I am building my own Tensor class in Rust, and I am trying to make it like PyTorch's implementation. </p>

<p><em>What is the most efficient way to store tensors programmatically, but, specifically, in a strongly typed language like Rust?</em> <em>Are there any resources that provide good insights into how this is done?</em></p>

<p>I am currently building a contiguous array, so that, given dimensions of <code>3 x 3 x 3</code>, my array would just have <code>3^3</code> elements in it, which would represent the tensor. However, this does make some of the mathematical operations and manipulations of the array harder.</p>

<p>The dimension of the tensor should be dynamic, so that I could have a tensor with <code>n</code> dimensions.</p>
",5476495,751,09-04-2018 02:59,09-04-2018 13:50,0,751,20,0,6,62,"{'badge_counts': {'bronze': 20, 'silver': 6, 'gold': 0}, 'account_id': 7167952, 'is_employee': False, 'last_modified_date': 1622769601, 'last_access_date': 1572536820, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 751, 'creation_date': 1445528183, 'user_type': 'registered', 'user_id': 5476495, 'accept_rate': 62, 'location': 'London', 'website_url': 'http://ryanmaugin.github.io', 'link': 'https://stackoverflow.com/users/5476495/ryanm', 'profile_image': 'https://i.stack.imgur.com/HmZ5k.png?s=256&g=1', 'display_name': 'RyanM'}","I am building my own Tensor class in Rust, and I am trying to make it like PyTorch's implementation. What is the most efficient way to store tensors programmatically, but, specifically, in a strongly typed language like Rust? Are there any resources that provide good insights into how this is done? I am currently building a contiguous array, so that, given dimensions of , my array would just have elements in it, which would represent the tensor. However, this does make some of the mathematical operations and manipulations of the array harder. The dimension of the tensor should be dynamic, so that I could have a tensor with dimensions.",3 x 3 x 3 3^3 n,-3,7,0,0,
878,49442523,49442658,4027,Prevent showing debugging log info inside ipython shell,7,<python><python-3.x><scrapy><ipython>,19,"<p>I'm using scrapy shell inside virtualenv. IPython is installed inside virtualenv. When I start scrapy shell using</p>

<pre><code> scrapy shell 'https://example.com'
</code></pre>

<p>and press tab for autocomplete suggestions, it shows a lot of debug information. How can I disable this?</p>

<pre><code>In [1]: from scra2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff parser start
2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff parser calculated
2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff: line_lengths old: 1, new: 1
2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff replace old[1:1] new[1:1]
2018-03-23 10:05:45 [parso.python.diff] DEBUG: parse_part from 1 to 1 (to 0 in part parser)
2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff parser end
</code></pre>

<p><a href=""https://i.stack.imgur.com/kKwOM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kKwOM.png"" alt=""enter image description here""></a></p>
",6619424,1963,23-03-2018 04:47,23-03-2018 05:01,0,1963,47,2,30,88,"{'badge_counts': {'bronze': 47, 'silver': 30, 'gold': 2}, 'account_id': 8863691, 'is_employee': False, 'last_modified_date': 1648877855, 'last_access_date': 1710842470, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1963, 'creation_date': 1469100503, 'user_type': 'registered', 'user_id': 6619424, 'accept_rate': 88, 'location': 'Kochi, Kerala, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/6619424/arun', 'profile_image': 'https://i.stack.imgur.com/RYh9j.jpg?s=256&g=1', 'display_name': 'Arun'}","I'm using scrapy shell inside virtualenv. IPython is installed inside virtualenv. When I start scrapy shell using and press tab for autocomplete suggestions, it shows a lot of debug information. How can I disable this?"," scrapy shell 'https://example.com'
 In [1]: from scra2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff parser start
2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff parser calculated
2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff: line_lengths old: 1, new: 1
2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff replace old[1:1] new[1:1]
2018-03-23 10:05:45 [parso.python.diff] DEBUG: parse_part from 1 to 1 (to 0 in part parser)
2018-03-23 10:05:45 [parso.python.diff] DEBUG: diff parser end
",5,16,1,1,
879,48587997,48589225,18307,"Matplotlib Pie Graph with 'All Other Categories""",1,<python><matplotlib>,16,"<p>I have created a matplotlib pie chart:</p>

<pre><code>df.plot(kind='pie', subplots=True, figsize=(6, 4))
</code></pre>

<p>My dataframe consists of two columns - Country and Value (% distribution) and has about 25 countries listed. I would like to only plot the top 10 countries by values (by highest %) and within the plot, calculate the remaining countries % value and give it the title of 'All Other Countries'. How do I do this using matplotlib using the .plot function?</p>

<pre><code>Country   Value
Albania    4%
Brazil     3%
Denmark    5%
France     10%
Mexico     3%
Nigeria    15%
Spain      4%
U.S.       5%
</code></pre>
",6627176,705,02-02-2018 17:32,02-02-2018 18:58,0,705,24,4,11,69,"{'badge_counts': {'bronze': 24, 'silver': 11, 'gold': 4}, 'account_id': 8874489, 'is_employee': False, 'last_modified_date': 1607614477, 'last_access_date': 1556721199, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 705, 'creation_date': 1469220002, 'user_type': 'registered', 'user_id': 6627176, 'accept_rate': 69, 'website_url': '', 'link': 'https://stackoverflow.com/users/6627176/spacedinosaur10', 'profile_image': 'https://www.gravatar.com/avatar/8a7ab0bc1f888409502ac4238fe18c35?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'spacedinosaur10'}","I have created a matplotlib pie chart: My dataframe consists of two columns - Country and Value (% distribution) and has about 25 countries listed. I would like to only plot the top 10 countries by values (by highest %) and within the plot, calculate the remaining countries % value and give it the title of 'All Other Countries'. How do I do this using matplotlib using the .plot function?","df.plot(kind='pie', subplots=True, figsize=(6, 4))
 Country   Value
Albania    4%
Brazil     3%
Denmark    5%
France     10%
Mexico     3%
Nigeria    15%
Spain      4%
U.S.       5%
",8,17,0,0,
880,49610908,49612480,27671,Exporting a PostgreSQL query to a csv file using Python,4,<python><sql><postgresql><export-to-csv><psycopg2>,20,"<p>I need to export some rows from a table in a PostgreSQL database to a .csv file using a Python script:</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import sys, psycopg2

...

    conn = psycopg2.connect(""dbname=dbname user=user password=password"")
    cur = conn.cursor()

    sql = ""\copy (SELECT * FROM table WHERE month=6) TO '/mnt/results/month/table.csv' WITH CSV DELIMITER ';';""
    cur.execute(sql)
    cur.close()

...
</code></pre>

<p>But when I run the script I get this:</p>

<pre><code>Syntax error at or near «\»
LINE 1: \copy (SELECT * FROM TABLE WHERE month=6) TO '...
</code></pre>

<p>Does anyone know what can be wrong or give me a tip about?</p>
",6634159,435,02-04-2018 12:00,02-04-2018 13:43,0,435,14,1,3,,"{'badge_counts': {'bronze': 14, 'silver': 3, 'gold': 1}, 'account_id': 3799451, 'is_employee': False, 'last_modified_date': 1695432300, 'last_access_date': 1710932061, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 435, 'creation_date': 1469437051, 'user_type': 'registered', 'user_id': 6634159, 'location': 'Spain', 'link': 'https://stackoverflow.com/users/6634159/ra%c3%bal-casado', 'profile_image': 'https://i.stack.imgur.com/RrGyv.jpg?s=256&g=1', 'display_name': 'Ra&#250;l Casado'}",I need to export some rows from a table in a PostgreSQL database to a .csv file using a Python script: But when I run the script I get this: Does anyone know what can be wrong or give me a tip about?,"#!/usr/bin/python
# -*- coding: utf-8 -*-

import sys, psycopg2

...

    conn = psycopg2.connect(""dbname=dbname user=user password=password"")
    cur = conn.cursor()

    sql = ""\copy (SELECT * FROM table WHERE month=6) TO '/mnt/results/month/table.csv' WITH CSV DELIMITER ';';""
    cur.execute(sql)
    cur.close()

...
 Syntax error at or near «\»
LINE 1: \copy (SELECT * FROM TABLE WHERE month=6) TO '...
",15,26,0,0,
881,48197234,48197296,3969,Unnest (explode) a Pandas Series,8,<python><pandas><dataframe>,15,"<p>I have:</p>

<pre><code>df = pd.DataFrame({'col1': ['asdf', 'xy', 'q'], 'col2': [1, 2, 3]})

   col1  col2
0  asdf     1
1    xy     2
2     q     3
</code></pre>

<p>I'd like to take the ""combinatoric product"" of each letter from the strings in <code>col1</code>, with each elementwise int in <code>col2</code>.  I.e.:</p>

<pre><code>  col1  col2
0    a    1
1    s    1
2    d    1
3    f    1
4    x    2
5    y    2
6    q    3
</code></pre>

<p>Current method:</p>

<pre><code>from itertools import product

pieces = []
for _, s in df.iterrows():
    letters = list(s.col1)
    prods = list(product(letters, [s.col2]))
    pieces.append(pd.DataFrame(prods))

pd.concat(pieces)
</code></pre>

<p>Any more efficient workarounds?</p>
",7954504,39578,10-01-2018 22:36,10-01-2018 22:43,0,39618,244,34,154,99,"{'badge_counts': {'bronze': 244, 'silver': 154, 'gold': 34}, 'account_id': 10815338, 'is_employee': False, 'last_modified_date': 1709973300, 'last_access_date': 1705676794, 'reputation_change_year': 301, 'reputation_change_quarter': 301, 'reputation_change_month': 81, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 39618, 'creation_date': 1493765210, 'user_type': 'registered', 'user_id': 7954504, 'accept_rate': 99, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/7954504/brad-solomon', 'profile_image': 'https://i.stack.imgur.com/6POkt.jpg?s=256&g=1', 'display_name': 'Brad Solomon'}","I have: I'd like to take the ""combinatoric product"" of each letter from the strings in , with each elementwise int in . I.e.: Current method: Any more efficient workarounds?","df = pd.DataFrame({'col1': ['asdf', 'xy', 'q'], 'col2': [1, 2, 3]})

   col1  col2
0  asdf     1
1    xy     2
2     q     3
 col1 col2   col1  col2
0    a    1
1    s    1
2    d    1
3    f    1
4    x    2
5    y    2
6    q    3
 from itertools import product

pieces = []
for _, s in df.iterrows():
    letters = list(s.col1)
    prods = list(product(letters, [s.col2]))
    pieces.append(pd.DataFrame(prods))

pd.concat(pieces)
",18,36,0,0,
882,48327567,48331777,8892,Removing horizontal underlines,4,<python><c++><opencv><tesseract>,29,"<p>I am attempting to pull text from a few hundred JPGs that contain information on capital punishment records; the JPGs are hosted by the Texas Department of Criminal Justice (TDCJ).  Below is an example snippet with personally identifiable information removed.</p>

<p><a href=""https://i.stack.imgur.com/WfWFu.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WfWFu.jpg"" alt=""enter image description here""></a></p>

<p><strong>I've identified the underlines as being the impediment to proper OCR</strong>--if I go in, screenshot a sub-snippet and manually white-out lines, the resulting OCR through <a href=""https://github.com/madmaze/pytesseract"" rel=""noreferrer"">pytesseract</a> is very good.  But with underlines present, it's extremely poor.</p>

<p>How can I best remove these horizontal lines?  What I have tried:</p>

<ul>
<li>Started on OpenCV doc's walkthrough: <a href=""https://docs.opencv.org/3.2.0/d1/dee/tutorial_moprh_lines_detection.html"" rel=""noreferrer"">Extract horizontal and vertical lines by using morphological operations</a>.  Got stuck pretty quickly, because I know zero C++.</li>
<li>Followed along with <a href=""https://stackoverflow.com/q/46274961/7954504"">Removing Horizontal Lines in image</a> - ended up with an illegible string.</li>
<li>Followed along with <a href=""https://stackoverflow.com/questions/19094642/removing-long-horizontal-vertical-lines-from-edge-image-using-opencv"">Removing long horizontal/vertical lines from edge image using OpenCV</a> - wasn't able to get the intuition behind sizing the array of zeros here.</li>
</ul>

<p>Tagging this question with <a href=""/questions/tagged/c%2b%2b"" class=""post-tag"" title=""show questions tagged &#39;c++&#39;"" rel=""tag"">c++</a> in the hope that someone could help to translate Step 5 of the <a href=""https://docs.opencv.org/3.2.0/d1/dee/tutorial_moprh_lines_detection.html"" rel=""noreferrer"">docs walkthrough</a> to Python.  I've tried a batch of transformations such as Hugh Line Transform, but I am feeling around in the dark within a library and area I have zero prior experience with.</p>

<pre><code>import cv2

# Inverted grayscale
img = cv2.imread('rsnippet.jpg', cv2.IMREAD_GRAYSCALE)
img = cv2.bitwise_not(img)

# Transform inverted grayscale to binary
th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                            cv2.THRESH_BINARY, 15, -2)

# An alternative; Not sure if `th` or `th2` is optimal here
th2 = cv2.threshold(img, 170, 255, cv2.THRESH_BINARY)[1]

# Create corresponding structure element for horizontal lines.
# Start by cloning th/th2.
horiz = th.copy()
r, c = horiz.shape

# Lost after here - not understanding intuition behind sizing/partitioning
</code></pre>
",7954504,39578,18-01-2018 17:57,18-01-2018 23:08,0,39618,244,34,154,99,"{'badge_counts': {'bronze': 244, 'silver': 154, 'gold': 34}, 'account_id': 10815338, 'is_employee': False, 'last_modified_date': 1709973300, 'last_access_date': 1705676794, 'reputation_change_year': 301, 'reputation_change_quarter': 301, 'reputation_change_month': 81, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 39618, 'creation_date': 1493765210, 'user_type': 'registered', 'user_id': 7954504, 'accept_rate': 99, 'location': 'USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/7954504/brad-solomon', 'profile_image': 'https://i.stack.imgur.com/6POkt.jpg?s=256&g=1', 'display_name': 'Brad Solomon'}","I am attempting to pull text from a few hundred JPGs that contain information on capital punishment records; the JPGs are hosted by the Texas Department of Criminal Justice (TDCJ). Below is an example snippet with personally identifiable information removed. I've identified the underlines as being the impediment to proper OCR--if I go in, screenshot a sub-snippet and manually white-out lines, the resulting OCR through pytesseract is very good. But with underlines present, it's extremely poor. How can I best remove these horizontal lines? What I have tried: Started on OpenCV doc's walkthrough: Extract horizontal and vertical lines by using morphological operations. Got stuck pretty quickly, because I know zero C++. Followed along with Removing Horizontal Lines in image - ended up with an illegible string. Followed along with Removing long horizontal/vertical lines from edge image using OpenCV - wasn't able to get the intuition behind sizing the array of zeros here. Tagging this question with c++ in the hope that someone could help to translate Step 5 of the docs walkthrough to Python. I've tried a batch of transformations such as Hugh Line Transform, but I am feeling around in the dark within a library and area I have zero prior experience with.","import cv2

# Inverted grayscale
img = cv2.imread('rsnippet.jpg', cv2.IMREAD_GRAYSCALE)
img = cv2.bitwise_not(img)

# Transform inverted grayscale to binary
th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                            cv2.THRESH_BINARY, 15, -2)

# An alternative; Not sure if `th` or `th2` is optimal here
th2 = cv2.threshold(img, 170, 255, cv2.THRESH_BINARY)[1]

# Create corresponding structure element for horizontal lines.
# Start by cloning th/th2.
horiz = th.copy()
r, c = horiz.shape

# Lost after here - not understanding intuition behind sizing/partitioning
",18,36,1,7,
883,48140989,48141688,23207,Keras LSTM input dimension setting,1,<python><tensorflow><deep-learning><keras><lstm>,20,"<p>I was trying to train a LSTM model using keras but I think I got something wrong here.</p>

<p>I got an error of </p>

<blockquote>
  <p>ValueError: Error when checking input: expected lstm_17_input to have
  3 dimensions, but got array with shape (10000, 0, 20)</p>
</blockquote>

<p>while my code looks like </p>

<pre><code>model = Sequential()
model.add(LSTM(256, activation=""relu"", dropout=0.25, recurrent_dropout=0.25, input_shape=(None, 20, 64)))
model.add(Dense(1, activation=""sigmoid""))
model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])
model.fit(X_train, y_train,
      batch_size=batch_size,
      epochs=10)
</code></pre>

<p>where <code>X_train</code> has a shape of <code>(10000, 20)</code> and the first few data points are like </p>

<pre><code>array([[ 0,  0,  0, ..., 40, 40,  9],
   [ 0,  0,  0, ..., 33, 20, 51],
   [ 0,  0,  0, ..., 54, 54, 50],
...
</code></pre>

<p>and <code>y_train</code> has a shape of <code>(10000, )</code>, which is a binary (0/1) label array.</p>

<p>Could someone point out where I was wrong here?</p>
",6659095,1544,07-01-2018 19:56,07-01-2018 21:23,0,1574,41,6,24,85,"{'badge_counts': {'bronze': 41, 'silver': 24, 'gold': 6}, 'account_id': 8923650, 'is_employee': False, 'last_modified_date': 1658768700, 'last_access_date': 1696287238, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 30, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1574, 'creation_date': 1469937332, 'user_type': 'registered', 'user_id': 6659095, 'accept_rate': 85, 'link': 'https://stackoverflow.com/users/6659095/mr-cysl', 'profile_image': 'https://www.gravatar.com/avatar/bd3b34a70334f50314f564ac0311bb4e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Mr.cysl'}","I was trying to train a LSTM model using keras but I think I got something wrong here. I got an error of ValueError: Error when checking input: expected lstm_17_input to have 3 dimensions, but got array with shape (10000, 0, 20) while my code looks like where has a shape of and the first few data points are like and has a shape of , which is a binary (0/1) label array. Could someone point out where I was wrong here?","model = Sequential()
model.add(LSTM(256, activation=""relu"", dropout=0.25, recurrent_dropout=0.25, input_shape=(None, 20, 64)))
model.add(Dense(1, activation=""sigmoid""))
model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])
model.fit(X_train, y_train,
      batch_size=batch_size,
      epochs=10)
 X_train (10000, 20) array([[ 0,  0,  0, ..., 40, 40,  9],
   [ 0,  0,  0, ..., 33, 20, 51],
   [ 0,  0,  0, ..., 54, 54, 50],
...
 y_train (10000, )",7,33,0,0,
884,48588490,48589029,4844,How can I use Regex to find a string of characters in alphabetical order using Python?,6,<python><regex><python-3.x>,12,"<p>So I have a challenge I'm working on - find the longest string of alphabetical characters in a string. For example, ""abcghiijkyxz"" should result in ""ghiijk"" (Yes the i is doubled). </p>

<p>I've been doing quite a bit with loops to solve the problem - iterating over the entire string, then for each character, starting a second loop using lower and ord. No help needed writing that loop.</p>

<p>However, it was suggested to me that Regex would be great for this sort of thing. My regex is weak (I know how to grab a static set, my look-forwards knowledge extends to knowing they exist). How would I write a Regex to look forward, and check future characters for being next in alphabetical order? Or is the suggestion to use Regex not practical for this type of thing?</p>

<p>Edit: The general consensus seems to be that Regex is indeed terrible for this type of thing. </p>
",6836407,1225,02-02-2018 18:05,02-02-2018 18:43,0,1225,35,1,19,72,"{'badge_counts': {'bronze': 35, 'silver': 19, 'gold': 1}, 'account_id': 9186539, 'is_employee': False, 'last_modified_date': 1607614470, 'last_access_date': 1709912126, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1225, 'creation_date': 1473960959, 'user_type': 'registered', 'user_id': 6836407, 'accept_rate': 72, 'location': 'California, United States', 'link': 'https://stackoverflow.com/users/6836407/selkie', 'profile_image': 'https://www.gravatar.com/avatar/9cc871e15260d406e9f71cd32c2988f5?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Selkie'}","So I have a challenge I'm working on - find the longest string of alphabetical characters in a string. For example, ""abcghiijkyxz"" should result in ""ghiijk"" (Yes the i is doubled). I've been doing quite a bit with loops to solve the problem - iterating over the entire string, then for each character, starting a second loop using lower and ord. No help needed writing that loop. However, it was suggested to me that Regex would be great for this sort of thing. My regex is weak (I know how to grab a static set, my look-forwards knowledge extends to knowing they exist). How would I write a Regex to look forward, and check future characters for being next in alphabetical order? Or is the suggestion to use Regex not practical for this type of thing? Edit: The general consensus seems to be that Regex is indeed terrible for this type of thing.",,0,7,0,0,
885,49265280,49265583,60389,Get text from qtextedit and assign it to a variable,1,<python><pyqt><pyqt5><qtextedit>,20,"<p>When I try to get the text from the qtextedit created with PyQt5 Designer I get an error or ""Python stop working"" and the script stop automatically. I've tried multiple solutions but nothing works. I have to assign the text from the qtextedit to a variable, for check if the process run or no. That's the code generated by PyQt5:</p>

<pre><code>from PyQt5 import QtCore, QtGui, QtWidgets
import psutil
class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName(""MainWindow"")
        MainWindow.resize(221, 157)
        MainWindow.setMinimumSize(QtCore.QSize(221, 157))
        MainWindow.setMaximumSize(QtCore.QSize(221, 157))
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName(""centralwidget"")
        self.cercaprocesso = QtWidgets.QPushButton(self.centralwidget)
        self.cercaprocesso.setGeometry(QtCore.QRect(0, 80, 221, 31))
        font = QtGui.QFont()
        font.setFamily(""MS Shell Dlg 2"")
        font.setPointSize(10)
        font.setBold(True)
        font.setWeight(75)
        self.cercaprocesso.setFont(font)
        self.cercaprocesso.setObjectName(""pushButton"")
        self.label = QtWidgets.QLabel(self.centralwidget)
        self.label.setGeometry(QtCore.QRect(0, 0, 221, 40))
        font = QtGui.QFont()
        font.setPointSize(10)
        font.setBold(True)
        font.setWeight(75)
        self.label.setFont(font)
        self.label.setAlignment(QtCore.Qt.AlignCenter)
        self.label.setObjectName(""label"")
        self.textbox = QtWidgets.QTextEdit(self.centralwidget)
        self.textbox.setGeometry(QtCore.QRect(30, 40, 161, 31))
        self.textbox.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOff)
        self.textbox.setObjectName(""textEdit"")
        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 221, 21))
        self.menubar.setObjectName(""menubar"")
        self.menucredit = QtWidgets.QMenu(self.menubar)
        self.menucredit.setObjectName(""menucredit"")
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName(""statusbar"")
        MainWindow.setStatusBar(self.statusbar)
        self.actionbot_created_by_andrea1980345_inforge_user = QtWidgets.QAction(MainWindow)     
        self.actionbot_created_by_andrea1980345_inforge_user.setObjectName (""actionbot"")
        self.menucredit.addAction(self.actionbot_created_by_andrea1980345_inforge_user)
        self.menubar.addAction(self.menucredit.menuAction())

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)
</code></pre>

<p>Button Code here:</p>

<pre><code>def retranslateUi(self, MainWindow):
    _translate = QtCore.QCoreApplication.translate
    MainWindow.setWindowTitle(_translate(""MainWindow"", ""andrea1980345 Bot""))
    self.cercaprocesso.setText(_translate(""MainWindow"", ""Cerca Processo""))
    self.label.setText(_translate(""MainWindow"", ""Inserire il nome del processo""))
    self.menucredit.setTitle(_translate(""MainWindow"", ""Credit""))
    self.actionbot_created_by_andrea1980345_inforge_user.setText(_translate(""MainWindow"", ""bot""))
    self.cercaprocesso.clicked.connect(self.search)
</code></pre>

<p>Here the code for get the text from the textbox and assign to a variable:</p>

<pre><code>def search(self):
    textboxValue = self.textbox.text()
    for pid in psutil.pids():  # Controlla se il processo è attivo
        listapid = psutil.Process(pid)
        if listapid.name() == textboxValue:
            print('Processo trovato')
    self.textbox.setText("""")
</code></pre>

<p>End code:</p>

<pre><code>if __name__ == ""__main__"":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())
</code></pre>
",6923483,207,13-03-2018 20:24,13-03-2018 20:46,0,207,9,1,2,,"{'badge_counts': {'bronze': 9, 'silver': 2, 'gold': 1}, 'account_id': 9326317, 'is_employee': False, 'last_modified_date': 1581143407, 'last_access_date': 1545259874, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 207, 'creation_date': 1475619696, 'user_type': 'registered', 'user_id': 6923483, 'link': 'https://stackoverflow.com/users/6923483/abunis', 'profile_image': 'https://www.gravatar.com/avatar/40a84a3b9129b1a5195395b1d102010a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Abunis'}","When I try to get the text from the qtextedit created with PyQt5 Designer I get an error or ""Python stop working"" and the script stop automatically. I've tried multiple solutions but nothing works. I have to assign the text from the qtextedit to a variable, for check if the process run or no. That's the code generated by PyQt5: Button Code here: Here the code for get the text from the textbox and assign to a variable: End code:","from PyQt5 import QtCore, QtGui, QtWidgets
import psutil
class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName(""MainWindow"")
        MainWindow.resize(221, 157)
        MainWindow.setMinimumSize(QtCore.QSize(221, 157))
        MainWindow.setMaximumSize(QtCore.QSize(221, 157))
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName(""centralwidget"")
        self.cercaprocesso = QtWidgets.QPushButton(self.centralwidget)
        self.cercaprocesso.setGeometry(QtCore.QRect(0, 80, 221, 31))
        font = QtGui.QFont()
        font.setFamily(""MS Shell Dlg 2"")
        font.setPointSize(10)
        font.setBold(True)
        font.setWeight(75)
        self.cercaprocesso.setFont(font)
        self.cercaprocesso.setObjectName(""pushButton"")
        self.label = QtWidgets.QLabel(self.centralwidget)
        self.label.setGeometry(QtCore.QRect(0, 0, 221, 40))
        font = QtGui.QFont()
        font.setPointSize(10)
        font.setBold(True)
        font.setWeight(75)
        self.label.setFont(font)
        self.label.setAlignment(QtCore.Qt.AlignCenter)
        self.label.setObjectName(""label"")
        self.textbox = QtWidgets.QTextEdit(self.centralwidget)
        self.textbox.setGeometry(QtCore.QRect(30, 40, 161, 31))
        self.textbox.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOff)
        self.textbox.setObjectName(""textEdit"")
        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 221, 21))
        self.menubar.setObjectName(""menubar"")
        self.menucredit = QtWidgets.QMenu(self.menubar)
        self.menucredit.setObjectName(""menucredit"")
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName(""statusbar"")
        MainWindow.setStatusBar(self.statusbar)
        self.actionbot_created_by_andrea1980345_inforge_user = QtWidgets.QAction(MainWindow)     
        self.actionbot_created_by_andrea1980345_inforge_user.setObjectName (""actionbot"")
        self.menucredit.addAction(self.actionbot_created_by_andrea1980345_inforge_user)
        self.menubar.addAction(self.menucredit.menuAction())

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)
 def retranslateUi(self, MainWindow):
    _translate = QtCore.QCoreApplication.translate
    MainWindow.setWindowTitle(_translate(""MainWindow"", ""andrea1980345 Bot""))
    self.cercaprocesso.setText(_translate(""MainWindow"", ""Cerca Processo""))
    self.label.setText(_translate(""MainWindow"", ""Inserire il nome del processo""))
    self.menucredit.setTitle(_translate(""MainWindow"", ""Credit""))
    self.actionbot_created_by_andrea1980345_inforge_user.setText(_translate(""MainWindow"", ""bot""))
    self.cercaprocesso.clicked.connect(self.search)
 def search(self):
    textboxValue = self.textbox.text()
    for pid in psutil.pids():  # Controlla se il processo è attivo
        listapid = psutil.Process(pid)
        if listapid.name() == textboxValue:
            print('Processo trovato')
    self.textbox.setText("""")
 if __name__ == ""__main__"":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())
",68,87,0,0,
886,49494299,49494729,12404,Call an async function in an normal function,1,<python><python-3.x><asynchronous><python-asyncio>,13,"<p>I'm pretty new to the asyncio for python 3.6</p>

<p>So the thing is I have a class and I want to init some property in there.
And one of the property is the return value from an async function.</p>

<p>What's the best practice to do this?</p>

<ol>
<li><p>Call event_loop one time in the init function to get the return value?</p></li>
<li><p>Make the __init__ function async? and run it in the event loop?</p></li>
</ol>

<p>Cheers!</p>

UPDATE AGAIN:

<p>Following is my code:</p>

<pre><code>import asyncio
import aioredis
from datetime import datetime

class C:
    def __init__(self):
        self.a = 1
        self.b = 2
        self.r = None
        asyncio.get_event_loop().run_until_complete(self._async_init())

    async def _async_init(self):
        # this is the property I want, which returns from an async function
        self.r = await aioredis.create_redis('redis://localhost:6379')

    async def heart_beat(self):
        while True:
            await self.r.publish('test_channel', datetime.now().__str__())
            await asyncio.sleep(10)

    def run(self):
        asyncio.get_event_loop().run_until_complete(self.heart_beat())

c=C()
c.run()
</code></pre>
",7003310,4734,26-03-2018 14:47,26-03-2018 15:07,0,4774,26,4,16,88,"{'badge_counts': {'bronze': 26, 'silver': 16, 'gold': 4}, 'account_id': 9415427, 'is_employee': False, 'last_modified_date': 1640997900, 'last_access_date': 1711145897, 'reputation_change_year': 240, 'reputation_change_quarter': 240, 'reputation_change_month': 80, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 4774, 'creation_date': 1476211290, 'user_type': 'registered', 'user_id': 7003310, 'accept_rate': 88, 'link': 'https://stackoverflow.com/users/7003310/qichao-he', 'profile_image': 'https://www.gravatar.com/avatar/53ff8ab967bf1f158da78f01c7e9f1fb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'qichao_he'}",I'm pretty new to the asyncio for python 3.6 So the thing is I have a class and I want to init some property in there. And one of the property is the return value from an async function. What's the best practice to do this? Call event_loop one time in the init function to get the return value? Make the __init__ function async? and run it in the event loop? Cheers! UPDATE AGAIN: Following is my code:,"import asyncio
import aioredis
from datetime import datetime

class C:
    def __init__(self):
        self.a = 1
        self.b = 2
        self.r = None
        asyncio.get_event_loop().run_until_complete(self._async_init())

    async def _async_init(self):
        # this is the property I want, which returns from an async function
        self.r = await aioredis.create_redis('redis://localhost:6379')

    async def heart_beat(self):
        while True:
            await self.r.publish('test_channel', datetime.now().__str__())
            await asyncio.sleep(10)

    def run(self):
        asyncio.get_event_loop().run_until_complete(self.heart_beat())

c=C()
c.run()
",24,44,0,0,
887,50311732,50311904,22069,pyspark equivalence of `df.loc`?,3,<python><pandas><apache-spark><dataframe><pyspark>,12,"<p>I am looking for pyspark equivalence of pandas dataframe. 
In particular, I want to do the following operation on pyspark dataframe</p>

<pre><code># in pandas dataframe, I can do the following operation
# assuming df = pandas dataframe
index = df['column_A'] &gt; 0.0
amount = sum(df.loc[index, 'column_B'] * df.loc[index, 'column_C']) 
        / sum(df.loc[index, 'column_C'])
</code></pre>

<p>I am wondering what is the pyspark equivalence of doing this to the pyspark dataframe?</p>
",7092612,1111,13-05-2018 00:25,13-05-2018 01:12,0,1121,27,5,15,68,"{'badge_counts': {'bronze': 27, 'silver': 15, 'gold': 5}, 'account_id': 9546969, 'is_employee': False, 'last_modified_date': 1686964500, 'last_access_date': 1707472263, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1121, 'creation_date': 1477856460, 'user_type': 'registered', 'user_id': 7092612, 'accept_rate': 68, 'website_url': '', 'link': 'https://stackoverflow.com/users/7092612/wrek', 'profile_image': 'https://www.gravatar.com/avatar/feded8d3ed8dca160f793ff782cb5226?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'wrek'}","I am looking for pyspark equivalence of pandas dataframe. In particular, I want to do the following operation on pyspark dataframe I am wondering what is the pyspark equivalence of doing this to the pyspark dataframe?","# in pandas dataframe, I can do the following operation
# assuming df = pandas dataframe
index = df['column_A'] &gt; 0.0
amount = sum(df.loc[index, 'column_B'] * df.loc[index, 'column_C']) 
        / sum(df.loc[index, 'column_C'])
",4,11,0,0,
888,50421287,50421288,76681,"PIP: ""Cannot uninstall 'ipython'. It is a distutils installed project and thus we cannot accurately determine...""",4,<python><pip><installation><spyder>,29,"<p>Trying to install <a href=""https://pypi.org/project/spyder/"" rel=""noreferrer"">spyder</a> using <code>pip</code>:</p>

<pre><code>pip install spyder
</code></pre>

<p>This error pops up:</p>

<blockquote>
  <p>Cannot uninstall 'ipython'. It is a distutils installed project and
  thus we cannot accurately determine which files belong to it which
  would lead to only a partial uninstall.</p>
</blockquote>
",7148573,1590,19-05-2018 02:36,19-05-2018 02:36,0,1590,27,3,16,,"{'badge_counts': {'bronze': 27, 'silver': 16, 'gold': 3}, 'account_id': 7528089, 'is_employee': False, 'last_modified_date': 1695431700, 'last_access_date': 1705158133, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1590, 'creation_date': 1478918418, 'user_type': 'registered', 'user_id': 7148573, 'location': 'S&#227;o Carlos - State of S&#227;o Paulo, Brazil', 'website_url': 'http://raphael.neocities.org', 'link': 'https://stackoverflow.com/users/7148573/raphael', 'profile_image': 'https://i.stack.imgur.com/T91qn.jpg?s=256&g=1', 'display_name': 'Raphael'}",Trying to install spyder using : This error pops up: Cannot uninstall 'ipython'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.,"pip pip install spyder
",-1,12,0,1,
889,48493755,48494469,79428,Keras AttributeError: 'list' object has no attribute 'ndim',3,<python><tensorflow><machine-learning><keras><jupyter-notebook>,27,"<p>I'm running a Keras neural network model in Jupyter Notebook (Python 3.6)</p>

<p>I get the following error</p>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'ndim'</p>
</blockquote>

<p>after calling the .fit() method from Keras.model</p>

<pre><code>model  = Sequential()
model.add(Dense(5, input_dim=len(X_data[0]), activation='sigmoid' ))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
model.fit(X_data, y_data, epochs=20, batch_size=10)
</code></pre>

<p>I checked the requirements.txt file for Keras (in Anaconda3) and the numpy, scipy, and six module versions are all up to date.</p>

<p>What can explain this AttributeError?</p>

<p>The full error message is the following (seems to be somewhat related to Numpy):</p>

<blockquote>
  <p>--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
  last)  in ()
        3 model.add(Dense(1, activation = 'sigmoid'))
        4 model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
  ----> 5 model.fit(X_data, y_data, epochs=20, batch_size=10)</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\models.py in fit(self, x, y,
  batch_size, epochs, verbose, callbacks, validation_split,
  validation_data, shuffle, class_weight, sample_weight, initial_epoch,
  steps_per_epoch, validation_steps, **kwargs)
      963                               initial_epoch=initial_epoch,
      964                               steps_per_epoch=steps_per_epoch,
  --> 965                               validation_steps=validation_steps)
      966 
      967     def evaluate(self, x=None, y=None,</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\engine\training.py in fit(self, x,
  y, batch_size, epochs, verbose, callbacks, validation_split,
  validation_data, shuffle, class_weight, sample_weight, initial_epoch,
  steps_per_epoch, validation_steps, **kwargs)    1591<br>
  class_weight=class_weight,    1592             check_batch_axis=False,
  -> 1593             batch_size=batch_size)    1594         # Prepare validation data.    1595         do_validation = False</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\engine\training.py in
  _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)    1424<br>
  self._feed_input_shapes,    1425<br>
  check_batch_axis=False,
  -> 1426                                     exception_prefix='input')    1427         y = _standardize_input_data(y, self._feed_output_names,<br>
  1428                                     output_shapes,</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\engine\training.py in
  _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
       68     elif isinstance(data, list):
       69         data = [x.values if x.<strong>class</strong>.<strong>name</strong> == 'DataFrame' else x for x in data]
  ---> 70         data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]
       71     else:
       72         data = data.values if data.<strong>class</strong>.<strong>name</strong> == 'DataFrame' else data</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\engine\training.py in
  (.0)
       68     elif isinstance(data, list):
       69         data = [x.values if x.<strong>class</strong>.<strong>name</strong> == 'DataFrame' else x for x in data]
  ---> 70         data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]
       71     else:
       72         data = data.values if data.<strong>class</strong>.<strong>name</strong> == 'DataFrame' else data</p>
  
  <p>AttributeError: 'list' object has no attribute 'ndim'</p>
</blockquote>
",7201349,1322,29-01-2018 02:55,29-01-2018 04:35,0,1322,22,2,15,86,"{'badge_counts': {'bronze': 22, 'silver': 15, 'gold': 2}, 'account_id': 9708689, 'is_employee': False, 'last_modified_date': 1703338200, 'last_access_date': 1710825662, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1322, 'creation_date': 1479920567, 'user_type': 'registered', 'user_id': 7201349, 'accept_rate': 86, 'website_url': 'https://www.linkedin.com/in/laurence-liang-innovator/', 'link': 'https://stackoverflow.com/users/7201349/larry', 'profile_image': 'https://www.gravatar.com/avatar/3fd4388325d411bfde857169f3c8a34b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Larry'}","I'm running a Keras neural network model in Jupyter Notebook (Python 3.6) I get the following error AttributeError: 'list' object has no attribute 'ndim' after calling the .fit() method from Keras.model I checked the requirements.txt file for Keras (in Anaconda3) and the numpy, scipy, and six module versions are all up to date. What can explain this AttributeError? The full error message is the following (seems to be somewhat related to Numpy): --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) in () 3 model.add(Dense(1, activation = 'sigmoid')) 4 model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc']) ----> 5 model.fit(X_data, y_data, epochs=20, batch_size=10) ~\Anaconda3\lib\site-packages\keras\models.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs) 963 initial_epoch=initial_epoch, 964 steps_per_epoch=steps_per_epoch, --> 965 validation_steps=validation_steps) 966 967 def evaluate(self, x=None, y=None, ~\Anaconda3\lib\site-packages\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs) 1591 class_weight=class_weight, 1592 check_batch_axis=False, -> 1593 batch_size=batch_size) 1594 # Prepare validation data. 1595 do_validation = False ~\Anaconda3\lib\site-packages\keras\engine\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size) 1424 self._feed_input_shapes, 1425 check_batch_axis=False, -> 1426 exception_prefix='input') 1427 y = _standardize_input_data(y, self._feed_output_names, 1428 output_shapes, ~\Anaconda3\lib\site-packages\keras\engine\training.py in _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix) 68 elif isinstance(data, list): 69 data = [x.values if x.class.name == 'DataFrame' else x for x in data] ---> 70 data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data] 71 else: 72 data = data.values if data.class.name == 'DataFrame' else data ~\Anaconda3\lib\site-packages\keras\engine\training.py in (.0) 68 elif isinstance(data, list): 69 data = [x.values if x.class.name == 'DataFrame' else x for x in data] ---> 70 data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data] 71 else: 72 data = data.values if data.class.name == 'DataFrame' else data AttributeError: 'list' object has no attribute 'ndim'","model  = Sequential()
model.add(Dense(5, input_dim=len(X_data[0]), activation='sigmoid' ))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
model.fit(X_data, y_data, epochs=20, batch_size=10)
",4,72,0,0,
890,48377214,48389451,101829,"RuntimeError: dimension out of range (expected to be in range of [-1, 0], but got 1)",5,<python><machine-learning><pytorch>,28,"<p>Im using a Pytorch Unet model to which i am feeding in a image as input and along with that i am feeding the label as the input image mask and traning the dataset on it.
The Unet model i have picked up from somewhere else, and i am using the cross-entropy loss as a loss function but i get this dimension out of range error,</p>
<pre><code>RuntimeError                              
Traceback (most recent call last)
&lt;ipython-input-358-fa0ef49a43ae&gt; in &lt;module&gt;()
     16 for epoch in range(0, num_epochs):
     17     # train for one epoch
---&gt; 18     curr_loss = train(train_loader, model, criterion, epoch, num_epochs)
     19 
     20     # store best loss and save a model checkpoint

&lt;ipython-input-356-1bd6c6c281fb&gt; in train(train_loader, model, criterion, epoch, num_epochs)
     16         # measure loss
     17         print (outputs.size(),labels.size())
---&gt; 18         loss = criterion(outputs, labels)
     19         losses.update(loss.data[0], images.size(0))
     20 

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in     _ _call__(self, *input, **kwargs)
    323         for hook in self._forward_pre_hooks.values():
    324             hook(self, input)
--&gt; 325         result = self.forward(*input, **kwargs)
    326         for hook in self._forward_hooks.values():
    327             hook_result = hook(self, input, result)

&lt;ipython-input-355-db66abcdb074&gt; in forward(self, logits, targets)
      9         probs_flat = probs.view(-1)
     10         targets_flat = targets.view(-1)
---&gt; 11         return self.crossEntropy_loss(probs_flat, targets_flat)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in     __call__(self, *input, **kwargs)
    323         for hook in self._forward_pre_hooks.values():
    324             hook(self, input)
  --&gt; 325         result = self.forward(*input, **kwargs)
    326         for hook in self._forward_hooks.values():
    327             hook_result = hook(self, input, result)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py in f orward(self, input, target)
    599         _assert_no_grad(target)
    600         return F.cross_entropy(input, target, self.weight, self.size_average,
--&gt; 601                                self.ignore_index, self.reduce)
    602 
    603 

/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in     cross_entropy(input, target, weight, size_average, ignore_index, reduce)
   1138         &gt;&gt;&gt; loss.backward()
   1139     &quot;&quot;&quot;
-&gt; 1140     return nll_loss(log_softmax(input, 1), target, weight, size_average, ignore_index, reduce)
   1141 
   1142 

/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in     log_softmax(input, dim, _stacklevel)
    784     if dim is None:
    785         dim = _get_softmax_dim('log_softmax', input.dim(),      _stacklevel)
--&gt; 786     return torch._C._nn.log_softmax(input, dim)
    787 
    788 

RuntimeError: dimension out of range (expected to be in range of [-1, 0], but got 1)
</code></pre>
<p>Part of my code looks like this</p>
<pre><code>class crossEntropy(nn.Module):
    def __init__(self, weight = None, size_average = True):
        super(crossEntropy, self).__init__()
        self.crossEntropy_loss = nn.CrossEntropyLoss(weight, size_average)
        
    def forward(self, logits, targets):
        probs = F.sigmoid(logits)
        probs_flat = probs.view(-1)
        targets_flat = targets.view(-1)
        return self.crossEntropy_loss(probs_flat, targets_flat)


class UNet(nn.Module):
    def __init__(self, imsize):
        super(UNet, self).__init__()
        self.imsize = imsize

        self.activation = F.relu
        
        self.pool1 = nn.MaxPool2d(2)
        self.pool2 = nn.MaxPool2d(2)
        self.pool3 = nn.MaxPool2d(2)
        self.pool4 = nn.MaxPool2d(2)
        self.conv_block1_64 = UNetConvBlock(4, 64)
        self.conv_block64_128 = UNetConvBlock(64, 128)
        self.conv_block128_256 = UNetConvBlock(128, 256)
        self.conv_block256_512 = UNetConvBlock(256, 512)
        self.conv_block512_1024 = UNetConvBlock(512, 1024)

        self.up_block1024_512 = UNetUpBlock(1024, 512)
        self.up_block512_256 = UNetUpBlock(512, 256)
        self.up_block256_128 = UNetUpBlock(256, 128)
        self.up_block128_64 = UNetUpBlock(128, 64)

        self.last = nn.Conv2d(64, 2, 1)


    def forward(self, x):
        block1 = self.conv_block1_64(x)
        pool1 = self.pool1(block1)

        block2 = self.conv_block64_128(pool1)
        pool2 = self.pool2(block2)

        block3 = self.conv_block128_256(pool2)
        pool3 = self.pool3(block3)

        block4 = self.conv_block256_512(pool3)
        pool4 = self.pool4(block4)

        block5 = self.conv_block512_1024(pool4)

        up1 = self.up_block1024_512(block5, block4)

        up2 = self.up_block512_256(up1, block3)

        up3 = self.up_block256_128(up2, block2)

        up4 = self.up_block128_64(up3, block1)

        return F.log_softmax(self.last(up4))
</code></pre>
",8176285,9249,22-01-2018 08:18,22-01-2018 20:06,0,9339,68,14,40,73,"{'badge_counts': {'bronze': 68, 'silver': 40, 'gold': 14}, 'account_id': 11138885, 'is_employee': False, 'last_modified_date': 1671307500, 'last_access_date': 1591387367, 'reputation_change_year': 310, 'reputation_change_quarter': 310, 'reputation_change_month': 100, 'reputation_change_week': 50, 'reputation_change_day': 0, 'reputation': 9339, 'creation_date': 1497711892, 'user_type': 'registered', 'user_id': 8176285, 'accept_rate': 73, 'link': 'https://stackoverflow.com/users/8176285/ryan', 'profile_image': 'https://www.gravatar.com/avatar/51535938bd9cf87d44432562af9eb914?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Ryan '}","Im using a Pytorch Unet model to which i am feeding in a image as input and along with that i am feeding the label as the input image mask and traning the dataset on it. The Unet model i have picked up from somewhere else, and i am using the cross-entropy loss as a loss function but i get this dimension out of range error, Part of my code looks like this","RuntimeError                              
Traceback (most recent call last)
&lt;ipython-input-358-fa0ef49a43ae&gt; in &lt;module&gt;()
     16 for epoch in range(0, num_epochs):
     17     # train for one epoch
---&gt; 18     curr_loss = train(train_loader, model, criterion, epoch, num_epochs)
     19 
     20     # store best loss and save a model checkpoint

&lt;ipython-input-356-1bd6c6c281fb&gt; in train(train_loader, model, criterion, epoch, num_epochs)
     16         # measure loss
     17         print (outputs.size(),labels.size())
---&gt; 18         loss = criterion(outputs, labels)
     19         losses.update(loss.data[0], images.size(0))
     20 

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in     _ _call__(self, *input, **kwargs)
    323         for hook in self._forward_pre_hooks.values():
    324             hook(self, input)
--&gt; 325         result = self.forward(*input, **kwargs)
    326         for hook in self._forward_hooks.values():
    327             hook_result = hook(self, input, result)

&lt;ipython-input-355-db66abcdb074&gt; in forward(self, logits, targets)
      9         probs_flat = probs.view(-1)
     10         targets_flat = targets.view(-1)
---&gt; 11         return self.crossEntropy_loss(probs_flat, targets_flat)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in     __call__(self, *input, **kwargs)
    323         for hook in self._forward_pre_hooks.values():
    324             hook(self, input)
  --&gt; 325         result = self.forward(*input, **kwargs)
    326         for hook in self._forward_hooks.values():
    327             hook_result = hook(self, input, result)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py in f orward(self, input, target)
    599         _assert_no_grad(target)
    600         return F.cross_entropy(input, target, self.weight, self.size_average,
--&gt; 601                                self.ignore_index, self.reduce)
    602 
    603 

/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in     cross_entropy(input, target, weight, size_average, ignore_index, reduce)
   1138         &gt;&gt;&gt; loss.backward()
   1139     &quot;&quot;&quot;
-&gt; 1140     return nll_loss(log_softmax(input, 1), target, weight, size_average, ignore_index, reduce)
   1141 
   1142 

/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in     log_softmax(input, dim, _stacklevel)
    784     if dim is None:
    785         dim = _get_softmax_dim('log_softmax', input.dim(),      _stacklevel)
--&gt; 786     return torch._C._nn.log_softmax(input, dim)
    787 
    788 

RuntimeError: dimension out of range (expected to be in range of [-1, 0], but got 1)
 class crossEntropy(nn.Module):
    def __init__(self, weight = None, size_average = True):
        super(crossEntropy, self).__init__()
        self.crossEntropy_loss = nn.CrossEntropyLoss(weight, size_average)
        
    def forward(self, logits, targets):
        probs = F.sigmoid(logits)
        probs_flat = probs.view(-1)
        targets_flat = targets.view(-1)
        return self.crossEntropy_loss(probs_flat, targets_flat)


class UNet(nn.Module):
    def __init__(self, imsize):
        super(UNet, self).__init__()
        self.imsize = imsize

        self.activation = F.relu
        
        self.pool1 = nn.MaxPool2d(2)
        self.pool2 = nn.MaxPool2d(2)
        self.pool3 = nn.MaxPool2d(2)
        self.pool4 = nn.MaxPool2d(2)
        self.conv_block1_64 = UNetConvBlock(4, 64)
        self.conv_block64_128 = UNetConvBlock(64, 128)
        self.conv_block128_256 = UNetConvBlock(128, 256)
        self.conv_block256_512 = UNetConvBlock(256, 512)
        self.conv_block512_1024 = UNetConvBlock(512, 1024)

        self.up_block1024_512 = UNetUpBlock(1024, 512)
        self.up_block512_256 = UNetUpBlock(512, 256)
        self.up_block256_128 = UNetUpBlock(256, 128)
        self.up_block128_64 = UNetUpBlock(128, 64)

        self.last = nn.Conv2d(64, 2, 1)


    def forward(self, x):
        block1 = self.conv_block1_64(x)
        pool1 = self.pool1(block1)

        block2 = self.conv_block64_128(pool1)
        pool2 = self.pool2(block2)

        block3 = self.conv_block128_256(pool2)
        pool3 = self.pool3(block3)

        block4 = self.conv_block256_512(pool3)
        pool4 = self.pool4(block4)

        block5 = self.conv_block512_1024(pool4)

        up1 = self.up_block1024_512(block5, block4)

        up2 = self.up_block512_256(up1, block3)

        up3 = self.up_block256_128(up2, block2)

        up4 = self.up_block128_64(up3, block1)

        return F.log_softmax(self.last(up4))
",116,123,0,0,
891,48669514,48671384,38834,Difference between reverse() and reverse_lazy() in Django,7,<python><django><django-views><url-routing>,58,"<p>I understand that we can use <code>reverse()</code> in FBV and <code>reverse_lazy()</code> in CBV. I understand that we have to use <code>reverse_lazy()</code> in CBV as the urls are not loaded when the file is imported (Ref: <a href=""https://stackoverflow.com/questions/45649804/reverse-lazy-and-url-loading"">Reverse_lazy and URL Loading?</a>)</p>

<p>What I don't understand is: </p>

<p>How are the urls loaded when we call <code>reverse</code> from the FBV? As when we import the views at the top of the <code>urls.py</code> in a Django app, <code>urlpatterns</code> list is yet to be evaluated. How does <code>reverse()</code> for FBV work but not for CBV?</p>
",7235253,1754,07-02-2018 17:08,07-02-2018 18:59,0,1764,22,2,14,33,"{'badge_counts': {'bronze': 22, 'silver': 14, 'gold': 2}, 'account_id': 9759812, 'is_employee': False, 'last_modified_date': 1607614459, 'last_access_date': 1709905555, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1764, 'creation_date': 1480582450, 'user_type': 'registered', 'user_id': 7235253, 'accept_rate': 33, 'location': 'Navi Mumbai, Maharashtra, India', 'website_url': 'http://ryucoder.in', 'link': 'https://stackoverflow.com/users/7235253/ryucoder', 'profile_image': 'https://i.stack.imgur.com/edQ1N.jpg?s=256&g=1', 'display_name': 'RyuCoder'}","I understand that we can use in FBV and in CBV. I understand that we have to use in CBV as the urls are not loaded when the file is imported (Ref: Reverse_lazy and URL Loading?) What I don't understand is: How are the urls loaded when we call from the FBV? As when we import the views at the top of the in a Django app, list is yet to be evaluated. How does for FBV work but not for CBV?",reverse() reverse_lazy() reverse_lazy() reverse urls.py urlpatterns reverse(),-7,5,0,1,
892,48668369,48668450,12265,How to add Dropout in Keras functional model?,1,<python><machine-learning><neural-network><keras><recurrent-neural-network>,11,"<p>Let's say I have an LSTM layer in Keras like this:</p>

<pre><code>x = Input(shape=(input_shape), dtype='int32')

x = LSTM(128,return_sequences=True)(x)
</code></pre>

<p>Now I am trying to add Dropout to this layer using:</p>

<pre><code>X = Dropout(0.5)
</code></pre>

<p>but this gives error, which I am assuming the above line is redefining X instead of adding Dropout to it. 
How to fix this?</p>
",7242276,499,07-02-2018 16:11,07-02-2018 16:15,0,499,21,2,9,0,"{'badge_counts': {'bronze': 21, 'silver': 9, 'gold': 2}, 'account_id': 9770062, 'is_employee': False, 'last_modified_date': 1648863000, 'last_access_date': 1667846455, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 499, 'creation_date': 1480696869, 'user_type': 'registered', 'user_id': 7242276, 'accept_rate': 0, 'link': 'https://stackoverflow.com/users/7242276/a-razavi', 'profile_image': 'https://www.gravatar.com/avatar/919bec0dfd07a327c4fac8fce5fd0340?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'A.Razavi'}","Let's say I have an LSTM layer in Keras like this: Now I am trying to add Dropout to this layer using: but this gives error, which I am assuming the above line is redefining X instead of adding Dropout to it. How to fix this?","x = Input(shape=(input_shape), dtype='int32')

x = LSTM(128,return_sequences=True)(x)
 X = Dropout(0.5)
",2,14,0,0,
893,50314440,50314470,10954,What does the name of the ord() function stand for?,2,<python><unicode><built-in><ordinal><ord>,37,"<p>The official Python documentation explains <code>ord(c)</code>  </p>

<blockquote>
  <p>ord(c):<br>
  Given a string representing one Unicode character, return an integer representing the Unicode code point of that character. For example, ord('a') returns the integer 97 and ord('€') (Euro sign) returns 8364. This is the inverse of chr().</p>
</blockquote>

<p>It does not specify the meaning of <code>ord</code>, google searches are not helpful.</p>

<p>What's the origin of it?</p>
",7301792,20751,13-05-2018 08:58,13-05-2018 09:04,0,20801,149,21,86,98,"{'badge_counts': {'bronze': 149, 'silver': 86, 'gold': 21}, 'collectives': [{'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 9857812, 'is_employee': False, 'last_modified_date': 1709313000, 'last_access_date': 1710283988, 'reputation_change_year': 360, 'reputation_change_quarter': 360, 'reputation_change_month': 60, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 20801, 'creation_date': 1481801529, 'user_type': 'registered', 'user_id': 7301792, 'accept_rate': 98, 'location': 'Beijing, 北京市中国', 'website_url': '', 'link': 'https://stackoverflow.com/users/7301792/wizard', 'profile_image': 'https://i.stack.imgur.com/K6k1M.png?s=256&g=1', 'display_name': 'Wizard'}","The official Python documentation explains ord(c): Given a string representing one Unicode character, return an integer representing the Unicode code point of that character. For example, ord('a') returns the integer 97 and ord('€') (Euro sign) returns 8364. This is the inverse of chr(). It does not specify the meaning of , google searches are not helpful. What's the origin of it?",ord(c) ord,-2,10,0,0,
894,49921128,49921616,52142,Selenium can't click element because other element obscures it,4,<python><selenium><element>,28,"<p><strong>Set-up</strong></p>

<p>I'm using Python 3.x and Selenium to fill out a query field and subsequently click the search button,</p>

<pre><code># element containing the product search bar and buttons
search_area = el_id('Products').find_element_by_class_name('searchArea')

# insert name of file to be duplicated
name_field = search_area.find_element_by_xpath(""//input[@type='text']"")
name_field.clear()
name_field.send_keys('to_be_duplicated')  

# click search button
search_area.find_element_by_xpath('span/a[1]').click()
</code></pre>

<p>where <code>el_id(x) = browser.find_element_by_id(x)</code>.</p>

<hr>

<p><strong>Problem</strong></p>

<p>Executing the code above gives the following error, </p>

<pre><code>ElementClickInterceptedException: Element &lt;a class=""button button-fleft searchButton"" href=""#""&gt; is not clickable at point (577.6166763305664,225.06666564941406) because another element &lt;div class=""blockUI blockOverlay""&gt; obscures it
</code></pre>

<p>I can solve this error by inserting a hard wait before grabbing and clicking the button, like so,</p>

<pre><code># click search button
time.sleep(1)
search_area.find_element_by_xpath('span/a[1]').click()
</code></pre>

<p>But I rather solve it differently, so I followed <a href=""https://stackoverflow.com/a/48441919/7326714"">this answer</a> and did the following, </p>

<pre><code># click search button
search_button = search_area.find_element_by_xpath('span/a[1]')
WebDriverWait(driver, 10).until_not(EC.visibility_of_element_located((By.XPATH, 
""//*[@id=""Products""]/tbody/tr[1]/td/div/input"")))
search_button.click()
</code></pre>

<p>But I got exactly the same error. </p>

<p>I also tried <a href=""https://stackoverflow.com/a/37303115/7326714"">this answer</a>, but same error. </p>

<p>How do I solve this?</p>
",7326714,1911,19-04-2018 12:27,19-04-2018 12:51,0,1911,66,6,33,82,"{'badge_counts': {'bronze': 66, 'silver': 33, 'gold': 6}, 'account_id': 8925974, 'is_employee': False, 'last_modified_date': 1710554400, 'last_access_date': 1694421857, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1911, 'creation_date': 1482338076, 'user_type': 'registered', 'user_id': 7326714, 'accept_rate': 82, 'website_url': 'http://www.rentindicator.com', 'link': 'https://stackoverflow.com/users/7326714/lucspan', 'profile_image': 'https://www.gravatar.com/avatar/99cb825a3421c01b6f87b604d55a14eb?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'LucSpan'}","Set-up I'm using Python 3.x and Selenium to fill out a query field and subsequently click the search button, where . Problem Executing the code above gives the following error, I can solve this error by inserting a hard wait before grabbing and clicking the button, like so, But I rather solve it differently, so I followed this answer and did the following, But I got exactly the same error. I also tried this answer, but same error. How do I solve this?","# element containing the product search bar and buttons
search_area = el_id('Products').find_element_by_class_name('searchArea')

# insert name of file to be duplicated
name_field = search_area.find_element_by_xpath(""//input[@type='text']"")
name_field.clear()
name_field.send_keys('to_be_duplicated')  

# click search button
search_area.find_element_by_xpath('span/a[1]').click()
 el_id(x) = browser.find_element_by_id(x) ElementClickInterceptedException: Element &lt;a class=""button button-fleft searchButton"" href=""#""&gt; is not clickable at point (577.6166763305664,225.06666564941406) because another element &lt;div class=""blockUI blockOverlay""&gt; obscures it
 # click search button
time.sleep(1)
search_area.find_element_by_xpath('span/a[1]').click()
 # click search button
search_button = search_area.find_element_by_xpath('span/a[1]')
WebDriverWait(driver, 10).until_not(EC.visibility_of_element_located((By.XPATH, 
""//*[@id=""Products""]/tbody/tr[1]/td/div/input"")))
search_button.click()
",14,48,0,2,
895,49671215,49691590,56618,"Cache entry deserialization failed, entry ignored",6,<python><windows><ssl><scikit-learn><installation>,37,"<pre><code>C:\Users\deypr&gt;pip3 install sklearn

Collecting sklearn

  Cache entry deserialization failed, entry ignored

  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Could not fetch URL https://pypi.python.org/simple/sklearn/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.python.org', port=443): Max retries exceeded with url: /simple/sklearn/ (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)) - skipping

 Could not find a version that satisfies the requirement sklearn (from versions: )
No matching distribution found for sklearn
</code></pre>

<p>I am getting this error whenever trying to install any <strong>python3</strong> package.</p>

<ol>
<li><p>What could be the possible reasons?</p></li>
<li><p>How to fix it ?</p></li>
</ol>
",8241887,696,05-04-2018 11:27,06-04-2018 11:17,1,696,16,1,8,,"{'badge_counts': {'bronze': 16, 'silver': 8, 'gold': 1}, 'account_id': 11235544, 'is_employee': False, 'last_modified_date': 1672823100, 'last_access_date': 1677059673, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 696, 'creation_date': 1498930139, 'user_type': 'registered', 'user_id': 8241887, 'location': 'Bangalore, Karnataka, India', 'website_url': 'https://www.linkedin.com/in/pronomita-dey-491286107/', 'link': 'https://stackoverflow.com/users/8241887/pronomita-dey', 'profile_image': 'https://www.gravatar.com/avatar/58e970e0116e4d420476765f62f08b29?s=256&d=identicon&r=PG', 'display_name': 'Pronomita Dey'}",I am getting this error whenever trying to install any python3 package. What could be the possible reasons? How to fix it ?,"C:\Users\deypr&gt;pip3 install sklearn

Collecting sklearn

  Cache entry deserialization failed, entry ignored

  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)': /simple/sklearn/

  Could not fetch URL https://pypi.python.org/simple/sklearn/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.python.org', port=443): Max retries exceeded with url: /simple/sklearn/ (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_ACCESS_DENIED] tlsv1 alert access denied (_ssl.c:777)'),)) - skipping

 Could not find a version that satisfies the requirement sklearn (from versions: )
No matching distribution found for sklearn
",19,28,0,0,
896,49225211,54915668,12896,Keras custom loss implementation : ValueError: An operation has `None` for gradient,1,<python><tensorflow><keras><backend><algebraic-number>,14,"<p>I'm trying to implement this loss function : <a href=""https://i.stack.imgur.com/8ZtEt.png"" rel=""nofollow noreferrer"">MCFD_loss_function</a>
from this document (P6) : <a href=""http://www.faculty.ucr.edu/~taelee/paper/lossfunctions.pdf"" rel=""nofollow noreferrer"">Loss functions</a></p>

<p>So I created a new function like this : </p>

<pre><code>def mcfd_loss(y_true, y_pred):
    return K.sum( # ∑
        K.cast(
            K.greater( # only values greater than 0 (+ float32 cast)
                  K.dot(K.sign(y_pred),  # π
                        K.sign(y_true))
           , 0)
        , 'float32')
    )
</code></pre>

<p>But when I start training this error is raised :</p>

<blockquote>
  <p>ValueError: An operation has <code>None</code> for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.</p>
</blockquote>

<p>I don't know which point I missed. The error seems to be raised because I use greater function. I don't know what does this error mean and how to correct my problem.</p>

<p>Thanks.</p>
",7369642,337,11-03-2018 21:19,27-02-2019 22:41,353,335,19,1,3,,"{'badge_counts': {'bronze': 19, 'silver': 3, 'gold': 1}, 'account_id': 9959996, 'is_employee': False, 'last_modified_date': 1707525606, 'last_access_date': 1709472297, 'reputation_change_year': -2, 'reputation_change_quarter': -2, 'reputation_change_month': -2, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 335, 'creation_date': 1483450252, 'user_type': 'registered', 'user_id': 7369642, 'location': 'Paris, France', 'website_url': '', 'link': 'https://stackoverflow.com/users/7369642/bastien-enjalbert', 'profile_image': 'https://www.gravatar.com/avatar/a40e113d2a4339d923788cf08f28795b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'bastien enjalbert'}","I'm trying to implement this loss function : MCFD_loss_function from this document (P6) : Loss functions So I created a new function like this : But when I start training this error is raised : ValueError: An operation has for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval. I don't know which point I missed. The error seems to be raised because I use greater function. I don't know what does this error mean and how to correct my problem. Thanks.","def mcfd_loss(y_true, y_pred):
    return K.sum( # ∑
        K.cast(
            K.greater( # only values greater than 0 (+ float32 cast)
                  K.dot(K.sign(y_pred),  # π
                        K.sign(y_true))
           , 0)
        , 'float32')
    )
 None",7,25,0,2,
897,48247490,48249910,44385,Django Rest-Framework nested serializer order,5,<python><json><django><rest><django-rest-framework>,52,"<p>Is there a way to order a nested serializer <code>_set</code>, for example order by <code>pk</code> or <code>time-stamp</code>.</p>

<p>So basically order <code>song_set</code> shown in the json data below from the most recent to the latest object created, in this case by <code>order_by('-timestamp')</code> or <code>order_by('-pk')</code>.</p>

<p><strong>Json data</strong></p>

<pre><code>{
    ""pk"": 151,
    ""album_name"": ""Name"",
    ""song_set"": [
         {
           pk: 3,
           timestamp: '5 seconds'
         },
         {
           pk: 2,
           timestamp: '10 seconds'
         },
         {
           pk: 1,
           timestamp: '15 seconds'
         }
    ]
}
</code></pre>

<p><strong>Model</strong></p>

<pre><code>class Album(models.Model):
    album_name     = models.CharField(max_length=100, blank=True)


class Song(models.Model):
    album          = models.ForeignKey('album.Album', default=1)
    timestamp      = models.DateTimeField(auto_now_add=True, auto_now=False)
</code></pre>

<p><strong>Serializer</strong></p>

<pre><code>class SongListSerializer(HyperlinkedModelSerializer):
    class Meta:
        model = Song
        fields = [
            'pk',
            'timestamp'
        ]

class AlbumSerializer(HyperlinkedModelSerializer):
    song_set = SongListSerializer(many=True, read_only=True)
    class Meta:
        model = Album
        fields = [
            'pk',
            'timestamp',
            'song_set'
        ]
</code></pre>
",8329222,661,14-01-2018 07:23,14-01-2018 13:11,0,661,15,1,8,56,"{'badge_counts': {'bronze': 15, 'silver': 8, 'gold': 1}, 'account_id': 11361116, 'is_employee': False, 'last_modified_date': 1692408900, 'last_access_date': 1535714796, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 661, 'creation_date': 1500441368, 'user_type': 'registered', 'user_id': 8329222, 'accept_rate': 56, 'link': 'https://stackoverflow.com/users/8329222/laa', 'profile_image': 'https://www.gravatar.com/avatar/5cebfa951337cdd813babb79110648d9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'laa'}","Is there a way to order a nested serializer , for example order by or . So basically order shown in the json data below from the most recent to the latest object created, in this case by or . Json data Model Serializer","_set pk time-stamp song_set order_by('-timestamp') order_by('-pk') {
    ""pk"": 151,
    ""album_name"": ""Name"",
    ""song_set"": [
         {
           pk: 3,
           timestamp: '5 seconds'
         },
         {
           pk: 2,
           timestamp: '10 seconds'
         },
         {
           pk: 1,
           timestamp: '15 seconds'
         }
    ]
}
 class Album(models.Model):
    album_name     = models.CharField(max_length=100, blank=True)


class Song(models.Model):
    album          = models.ForeignKey('album.Album', default=1)
    timestamp      = models.DateTimeField(auto_now_add=True, auto_now=False)
 class SongListSerializer(HyperlinkedModelSerializer):
    class Meta:
        model = Song
        fields = [
            'pk',
            'timestamp'
        ]

class AlbumSerializer(HyperlinkedModelSerializer):
    song_set = SongListSerializer(many=True, read_only=True)
    class Meta:
        model = Album
        fields = [
            'pk',
            'timestamp',
            'song_set'
        ]
",33,57,0,0,
898,49390842,49399421,95246,Cross Entropy in PyTorch,4,<python><machine-learning><pytorch><loss>,65,"<p>Cross entropy formula:</p>
<p><a href=""https://i.stack.imgur.com/W3xm0.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/W3xm0.gif"" alt=""enter image description here"" /></a></p>
<p>But why does the following give <code>loss = 0.7437</code> instead of <code>loss = 0</code> (since <code>1*log(1) = 0</code>)?</p>

<pre class=""lang-python prettyprint-override""><code>import torch
import torch.nn as nn
from torch.autograd import Variable

output = Variable(torch.FloatTensor([0,0,0,1])).view(1, -1)
target = Variable(torch.LongTensor([3]))

criterion = nn.CrossEntropyLoss()
loss = criterion(output, target)
print(loss)
</code></pre>
",7483494,22773,20-03-2018 17:39,21-03-2018 06:13,1,22823,107,20,91,78,"{'badge_counts': {'bronze': 107, 'silver': 91, 'gold': 20}, 'collectives': [{'collective': {'tags': ['firebase-invites', 'google-app-engine-deploy', 'firebase-machine-learning', 'google-cloud-profiler', 'google-cloud-messaging', 'google-cloud-endpoints-v2', 'firebase-analytics', 'google-prediction', 'google-container-optimized-os', 'google-cloud-functions', 'bigtable', 'firebase-app-distribution', 'google-cloud-build', 'google-cloud-node', 'google-cloud-ai', 'google-cloud-tpu', 'google-app-engine-python', 'google-cloud-ml', 'google-cloud-deploy', 'google-cloud-network-load-balancer', 'google-cloud-metrics', 'google-compute-engine', 'google-cloud-data-fusion', 'google-cloud-run', 'firebaseui', 'google-analytics-firebase', 'firebase-admin', 'google-cloud-storage-r', 'google-cloud-bigtable', 'google-cloud-router', 'google-cloud-python', 'google-container-builder', 'google-cloud-api-gateway', 'firebase-predictions', 'google-cloud-workstations', 'google-cloud-iam', 'firebase-database', 'google-cloud-logging', 'google-cloud-language', 'google-cloud-firestore', 'google-cloud-datalab', 'google-cloud-internal-load-balancer', 'google-cloud-print', 'firebase-app-check', 'google-cloud-monitoring', 'google-cloud-shell', 'firebase', 'cordova-plugin-firebasex', 'google-app-engine-patch', 'google-cloud-url-maps', 'google-cloud-debugger', 'google-cloud-marketplace', 'google-cloud-test-lab', 'google-cloud-trace', 'google-cloud-billing', 'google-cloud-transcoder', 'google-cloud-automl-nl', 'google-cloud-shell-editor', 'google-cloud-cdn', 'google-cloud-spanner-emulator', 'google-cloud-launcher', 'google-app-engine', 'google-cloud-memorystore', 'google-cloud-ops-agent', 'google-cloud-talent-solution', 'firebase-test-lab', 'google-cloud-source-repos', 'firebase-queue', 'google-cloud-armor', 'jib', 'nativescript-firebase', 'looker', 'google-cloud-dataflow', 'google-cloud-filestore', 'firebase-ab-testing', 'google-cloud-sql', 'google-cloud-code', 'dialogflow-es-fulfillment', 'google-cloud-dataproc-metastore', 'google-cloud-console', 'google-anthos', 'google-container-os', 'google-cloud-automl', 'google-cloud-speech', 'google-cloud-identity-aware-proxy', 'google-cloud-print-privet', 'firebase-in-app-messaging', 'google-cloud-php-client', 'react-redux-firebase', 'firebase-app-indexing', 'google-cloud-visualstudio', 'firebase-console', 'google-cloud-instances', 'maven-jib', 'google-cloud-endpoints', 'firebase-authentication', 'apigee', 'google-cloud-ai-platform-pipelines', 'google-cloud-repository', 'dialogflow-es', 'google-cloud-cpp', 'google-cloud-scheduler', 'firebase-util', 'google-cloud-healthcare', 'google-cloud-translate', 'google-bigquery', 'google-cloud-spanner', 'google-cloud-powershell', 'google-cloud-networking', 'google-translate', 'google-dataflow', 'firebasesimplelogin', 'firebase-remote-config', 'google-cloud-dns', 'google-cloud-dlp', 'google-cloud-dataproc', 'google-cloud-nl', 'google-fusion-tables', 'google-kubernetes-engine', 'firebase-cloud-messaging', 'google-cloud-search', 'google-cloud-recommendation', 'firebase-hosting', 'firebase-job-dispatcher', 'google-app-engine-go', 'google-cloud-resource-manager', 'dialogflow-cx', 'firebase-performance', 'firebase-security', 'google-cloud-stackdriver', 'google-cloud-registry', 'google-cloud-interconnect', 'firebase-admob', 'looker-studio', 'google-cloud-load-balancer', 'google-cloud-datastore', 'google-cloud-http-load-balancer', 'google-cloud-instance-template', 'firebase-cli', 'firebase-storage', 'firebase-crash-reporting', 'google-cloud-ml-engine', 'google-cloud-pubsublite', 'google-cloud-robotics', 'google-container-registry', 'google-cloud-vpn', 'firebase-realtime-database', 'google-migrate-for-compute-engine', 'gcloud', 'firebase-assistant', 'firebase-polymer', 'google-app-engine-launch', 'google-cloud-vertex-ai', 'google-cloud-tasks', 'google-cloud-storage', 'google-cloud-identity', 'firebase-notifications', 'google-cloud-sdk', 'firebase-mlkit', 'firebase-extensions', 'google-cloud-platform', 'firebase-dynamic-links', 'google-cloud-tools', 'google-cloud-pubsub', 'recaptcha-enterprise', 'google-cloud-intellij', 'firebase-tools', 'google-cloud-dataprep', 'google-app-engine-golang', 'google-cloud-kms', 'google-cloud-vision', 'rest-firebase', 'cloud-document-ai', 'google-cloud-iot', 'google-app-engine-php', 'google-cloud-proxy', 'vertex-ai-search', 'google-cloud-error-reporting', 'react-native-firebase', 'redux-saga-firebase', 'google-cloud-composer', 'google-cloud-webrisk', 'google-cloud-save', 'stackdriver', 'apigee-baas', 'google-cloud-data-transfer', 'google-cloud-asset-inventory'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 10129219, 'is_employee': False, 'last_modified_date': 1696597200, 'last_access_date': 1711121701, 'reputation_change_year': 510, 'reputation_change_quarter': 510, 'reputation_change_month': 120, 'reputation_change_week': 30, 'reputation_change_day': 10, 'reputation': 22823, 'creation_date': 1485622269, 'user_type': 'registered', 'user_id': 7483494, 'accept_rate': 78, 'location': 'Germany', 'website_url': '', 'link': 'https://stackoverflow.com/users/7483494/mbt', 'profile_image': 'https://i.stack.imgur.com/tRSwM.png?s=256&g=1', 'display_name': 'MBT'}",Cross entropy formula: But why does the following give instead of (since )?,"loss = 0.7437 loss = 0 1*log(1) = 0 import torch
import torch.nn as nn
from torch.autograd import Variable

output = Variable(torch.FloatTensor([0,0,0,1])).view(1, -1)
target = Variable(torch.LongTensor([3]))

criterion = nn.CrossEntropyLoss()
loss = criterion(output, target)
print(loss)
",6,15,1,1,
899,49956883,49957003,3431,Efficient random generator for very large range (in python),3,<python><performance><generator><shuffle>,14,"<p>I am trying to create a generator that returns numbers in a given range that pass a particular test given by a function <code>foo</code>. However I would like the numbers to be tested in a random order. The following code will achieve this:</p>

<pre><code>from random import shuffle

def MyGenerator(foo, num):
    order = list(range(num))
    shuffle(order)
    for i in order:
        if foo(i):
            yield i
</code></pre>

<p><strong>The Problem</strong></p>

<p>The problem with this solution is that sometimes the range will be quite large (<code>num</code> might be of the order <code>10**8</code> and upwards). This function can become slow, having such a large list in memory. I have tried to avoid this problem, with the following code:</p>

<pre><code>from random import randint    

def MyGenerator(foo, num):
    tried = set()
    while len(tried) &lt;= num - 1:
        i = randint(0, num-1)
        if i in tried:
            continue
        tried.add(i)
        if foo(i):
            yield i
</code></pre>

<p>This works well most of the time, since in most cases <code>num</code> will be quite large, <code>foo</code> will pass a reasonable number of numbers and the total number of times the <code>__next__</code> method will be called will be relatively small (say, a maximum of 200 often much smaller). Therefore its reasonable likely we stumble upon a value that passes the <code>foo</code> test and the size of <code>tried</code> never gets large. (Even if it only passes 10% of the time, we wouldn't expect <code>tried</code> to get larger than about 2000 roughly.)</p>

<p>However, when <code>num</code> is small (close to the number of times that the <code>__next__</code> method is called, or <code>foo</code> fails most of the time, the above solution becomes very inefficient - randomly guessing numbers until it guesses one that isn't in <code>tried</code>.</p>

<p><strong>My attempted solution...</strong></p>

<p>I was hoping to use some kind of function that maps the numbers <code>0,1,2,..., n</code> onto themselves in a roughly random way. (This isn't being used for any security purposes and so doesn't matter if it isn't the most 'random' function in the world). The function here (<a href=""https://stackoverflow.com/questions/48205823/create-a-random-bijective-function-which-has-same-domain-and-range"">Create a random bijective function which has same domain and range</a>) maps signed 32-bit integers onto themselves, but I am not sure how to adapt the mapping to a smaller range. Given <code>num</code> I don't even need a bijection on <code>0,1,..num</code> just a value of <code>n</code> larger than and 'close' to <code>num</code> (using whatever definition of close you see fit). Then I can do the following:</p>

<pre><code>def mix_function_factory(num):
    # something here???
    def foo(index):
        # something else here??
    return foo

def MyGenerator(foo, num):
    mix_function = mix_function_factory(num):
    for i in range(num):
        index = mix_function(i)
        if index &lt;= num:
            if foo(index):
                yield index
</code></pre>

<p>(so long as the bijection isn't on a set of numbers massively larger than <code>num</code> the number of times <code>index &lt;= num</code> isn't True will be small).</p>

<p><strong>My Question</strong></p>

<p>Can you think of one of the following:</p>

<ul>
<li>A potential solution for <code>mix_function_factory</code> or even a few other potential functions for <code>mix_function</code> that I could attempt to generalise for different values of <code>num</code>?</li>
<li>A better way of solving the original problem?</li>
</ul>

<p>Many thanks in advance....</p>
",7549907,6525,21-04-2018 14:38,21-04-2018 14:54,0,6545,42,4,25,100,"{'badge_counts': {'bronze': 42, 'silver': 25, 'gold': 4}, 'account_id': 5901208, 'is_employee': False, 'last_modified_date': 1674297900, 'last_access_date': 1710842809, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 6545, 'creation_date': 1486813202, 'user_type': 'registered', 'user_id': 7549907, 'accept_rate': 100, 'location': 'London', 'website_url': '', 'link': 'https://stackoverflow.com/users/7549907/tim-mccurrach', 'profile_image': 'https://www.gravatar.com/avatar/29473460bbc1b265961d33279d837412?s=256&d=identicon&r=PG', 'display_name': 'tim-mccurrach'}","I am trying to create a generator that returns numbers in a given range that pass a particular test given by a function . However I would like the numbers to be tested in a random order. The following code will achieve this: The Problem The problem with this solution is that sometimes the range will be quite large ( might be of the order and upwards). This function can become slow, having such a large list in memory. I have tried to avoid this problem, with the following code: This works well most of the time, since in most cases will be quite large, will pass a reasonable number of numbers and the total number of times the method will be called will be relatively small (say, a maximum of 200 often much smaller). Therefore its reasonable likely we stumble upon a value that passes the test and the size of never gets large. (Even if it only passes 10% of the time, we wouldn't expect to get larger than about 2000 roughly.) However, when is small (close to the number of times that the method is called, or fails most of the time, the above solution becomes very inefficient - randomly guessing numbers until it guesses one that isn't in . My attempted solution... I was hoping to use some kind of function that maps the numbers onto themselves in a roughly random way. (This isn't being used for any security purposes and so doesn't matter if it isn't the most 'random' function in the world). The function here (Create a random bijective function which has same domain and range) maps signed 32-bit integers onto themselves, but I am not sure how to adapt the mapping to a smaller range. Given I don't even need a bijection on just a value of larger than and 'close' to (using whatever definition of close you see fit). Then I can do the following: (so long as the bijection isn't on a set of numbers massively larger than the number of times isn't True will be small). My Question Can you think of one of the following: A potential solution for or even a few other potential functions for that I could attempt to generalise for different values of ? A better way of solving the original problem? Many thanks in advance....","foo from random import shuffle

def MyGenerator(foo, num):
    order = list(range(num))
    shuffle(order)
    for i in order:
        if foo(i):
            yield i
 num 10**8 from random import randint    

def MyGenerator(foo, num):
    tried = set()
    while len(tried) &lt;= num - 1:
        i = randint(0, num-1)
        if i in tried:
            continue
        tried.add(i)
        if foo(i):
            yield i
 num foo __next__ foo tried tried num __next__ foo tried 0,1,2,..., n num 0,1,..num n num def mix_function_factory(num):
    # something here???
    def foo(index):
        # something else here??
    return foo

def MyGenerator(foo, num):
    mix_function = mix_function_factory(num):
    for i in range(num):
        index = mix_function(i)
        if index &lt;= num:
            if foo(index):
                yield index
 num index &lt;= num mix_function_factory mix_function num",6,64,0,1,
900,49532873,49532919,22312,What is the difference between `sep` and `delimiter` attributes in pandas.read_csv() method?,2,<python><pandas><delimiter>,19,"<p>What is the difference between <code>sep</code> and <code>delimiter</code> attributes in <code>pandas.read_csv()</code> method? </p>

<p>Also what is the situation when I would choose one over the other?</p>

<p>In documentation I read something about Python builtin sniffer tool, also in delimiter, it says <em>alternative argument name for sep</em>, then why cant we have only one attribute?</p>
",6355150,990,28-03-2018 11:05,28-03-2018 11:08,0,990,33,1,14,42,"{'badge_counts': {'bronze': 33, 'silver': 14, 'gold': 1}, 'account_id': 8473176, 'is_employee': False, 'last_modified_date': 1648485327, 'last_access_date': 1710520647, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 990, 'creation_date': 1463647743, 'user_type': 'registered', 'user_id': 6355150, 'accept_rate': 42, 'location': 'India', 'website_url': '', 'link': 'https://stackoverflow.com/users/6355150/gadaadhaarigeek', 'profile_image': 'https://i.stack.imgur.com/1EsgN.jpg?s=256&g=1', 'display_name': 'GadaaDhaariGeek'}","What is the difference between and attributes in method? Also what is the situation when I would choose one over the other? In documentation I read something about Python builtin sniffer tool, also in delimiter, it says alternative argument name for sep, then why cant we have only one attribute?",sep delimiter pandas.read_csv(),-3,5,0,0,
901,49153020,49153393,25925,How to dump a collection to json file using pymongo,7,<python><json><mongodb><pymongo-3.x>,17,"<p>I am trying to dump a collection to .json file but after looking in pymongo tutorial I can not find any thing that relates to it.</p>

<p>Tutorial link: <a href=""https://api.mongodb.com/python/current/tutorial.html"" rel=""noreferrer"">https://api.mongodb.com/python/current/tutorial.html</a></p>
",6367591,189,07-03-2018 13:22,07-03-2018 13:43,0,189,7,1,1,,"{'badge_counts': {'bronze': 7, 'silver': 1, 'gold': 1}, 'account_id': 8491362, 'is_employee': False, 'last_modified_date': 1573679071, 'last_access_date': 1529116683, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 189, 'creation_date': 1463923853, 'user_type': 'registered', 'user_id': 6367591, 'link': 'https://stackoverflow.com/users/6367591/anhng', 'profile_image': 'https://www.gravatar.com/avatar/460802ab51759d149fee1e5aa34bbab0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'AnhNg'}",I am trying to dump a collection to .json file but after looking in pymongo tutorial I can not find any thing that relates to it. Tutorial link: https://api.mongodb.com/python/current/tutorial.html,,0,3,0,1,
902,50087098,50087199,183704,Permission denied error by installing matplotlib,4,<python><opencv><matplotlib><pip><failed-installation>,43,"<p>I installed <strong>opencv</strong> with all dependencies. After the installation I tried to import <strong>matplotlib</strong> for a simple example.</p>

<p>Then I got the following error, when I tried to install matplotlib via pip with <code>pip install matplotlib</code>:</p>

<pre><code>Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/local/lib/python3.5/dist-packages/kiwisolver.cpython-35m-x86_64-linux-gnu.so'
Consider using the `--user` option or check the permissions.
</code></pre>

<p>What can I do to install matplotlib?</p>
",6390021,567,29-04-2018 13:27,29-04-2018 13:39,0,567,13,1,5,,"{'badge_counts': {'bronze': 13, 'silver': 5, 'gold': 1}, 'account_id': 8524083, 'is_employee': False, 'last_modified_date': 1635939000, 'last_access_date': 1696795172, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 567, 'creation_date': 1464338497, 'user_type': 'registered', 'user_id': 6390021, 'link': 'https://stackoverflow.com/users/6390021/maximilian-von-unwerth', 'profile_image': 'https://www.gravatar.com/avatar/c2e73b48cb4353a63aa4d69b0eaadd47?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Maximilian von Unwerth'}","I installed opencv with all dependencies. After the installation I tried to import matplotlib for a simple example. Then I got the following error, when I tried to install matplotlib via pip with : What can I do to install matplotlib?","pip install matplotlib Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/local/lib/python3.5/dist-packages/kiwisolver.cpython-35m-x86_64-linux-gnu.so'
Consider using the `--user` option or check the permissions.
",0,9,0,0,
903,49081819,49542826,1059,Python property descriptor design: why copy rather than mutate?,3,<python><inheritance><properties><mutators>,27,"<p>I was looking at how Python implements the <a href=""https://docs.python.org/2/howto/descriptor.html#properties"" rel=""noreferrer"">property descriptor</a> internally. According to the docs <code>property()</code> is implemented in terms of the descriptor protocol, reproducing it here for convenience:</p>

<pre><code>class Property(object):
    ""Emulate PyProperty_Type() in Objects/descrobject.c""

    def __init__(self, fget=None, fset=None, fdel=None, doc=None):
        self.fget = fget
        self.fset = fset
        self.fdel = fdel
        if doc is None and fget is not None:
            doc = fget.__doc__
        self.__doc__ = doc

    def __get__(self, obj, objtype=None):
        if obj is None:
            return self
        if self.fget is None:
            raise AttributeError(""unreadable attribute"")
        return self.fget(obj)

    def __set__(self, obj, value):
        if self.fset is None:
            raise AttributeError(""can't set attribute"")
        self.fset(obj, value)

    def __delete__(self, obj):
        if self.fdel is None:
            raise AttributeError(""can't delete attribute"")
        self.fdel(obj)

    def getter(self, fget):
        return type(self)(fget, self.fset, self.fdel, self.__doc__)

    def setter(self, fset):
        return type(self)(self.fget, fset, self.fdel, self.__doc__)

    def deleter(self, fdel):
        return type(self)(self.fget, self.fset, fdel, self.__doc__)
</code></pre>

<p>My question is: why aren't the last three methods implemented as follows:</p>

<pre><code>    def getter(self, fget):
        self.fget = fget
        return self

    def setter(self, fset):
        self.fset = fset
        return self

    def deleter(self, fdel):
        self.fdel= fdel
        return self
</code></pre>

<p>Is there a reason for returing new instances of property, internally pointing to basically the same get and set functions?</p>
",6481805,1202,03-03-2018 07:29,28-03-2018 19:38,25,1202,16,0,14,60,"{'badge_counts': {'bronze': 16, 'silver': 14, 'gold': 0}, 'account_id': 8658398, 'is_employee': False, 'last_modified_date': 1641991800, 'last_access_date': 1648813360, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1202, 'creation_date': 1466220871, 'user_type': 'registered', 'user_id': 6481805, 'accept_rate': 60, 'website_url': '', 'link': 'https://stackoverflow.com/users/6481805/nesdis', 'profile_image': 'https://www.gravatar.com/avatar/602cace92d9ec930e9aebc7d5a4a302a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'nesdis'}","I was looking at how Python implements the property descriptor internally. According to the docs is implemented in terms of the descriptor protocol, reproducing it here for convenience: My question is: why aren't the last three methods implemented as follows: Is there a reason for returing new instances of property, internally pointing to basically the same get and set functions?","property() class Property(object):
    ""Emulate PyProperty_Type() in Objects/descrobject.c""

    def __init__(self, fget=None, fset=None, fdel=None, doc=None):
        self.fget = fget
        self.fset = fset
        self.fdel = fdel
        if doc is None and fget is not None:
            doc = fget.__doc__
        self.__doc__ = doc

    def __get__(self, obj, objtype=None):
        if obj is None:
            return self
        if self.fget is None:
            raise AttributeError(""unreadable attribute"")
        return self.fget(obj)

    def __set__(self, obj, value):
        if self.fset is None:
            raise AttributeError(""can't set attribute"")
        self.fset(obj, value)

    def __delete__(self, obj):
        if self.fdel is None:
            raise AttributeError(""can't delete attribute"")
        self.fdel(obj)

    def getter(self, fget):
        return type(self)(fget, self.fset, self.fdel, self.__doc__)

    def setter(self, fset):
        return type(self)(self.fget, fset, self.fdel, self.__doc__)

    def deleter(self, fdel):
        return type(self)(self.fget, self.fset, fdel, self.__doc__)
     def getter(self, fget):
        self.fget = fget
        return self

    def setter(self, fset):
        self.fset = fset
        return self

    def deleter(self, fdel):
        self.fdel= fdel
        return self
",44,56,0,1,
904,49147774,49147859,168838,"What is ""random-state"" in sklearn.model_selection.train_test_split example?",5,<python><numpy><machine-learning><random><scikit-learn>,39,"<p>Can someone explain me what <code>random_state</code> means in below example?</p>
<pre><code>import numpy as np
from sklearn.model_selection import train_test_split
X, y = np.arange(10).reshape((5, 2)), range(5)


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42) 
</code></pre>
<p>Why is it hard coded to 42?</p>
",6482145,1622,07-03-2018 09:00,07-03-2018 09:04,0,1622,33,2,16,100,"{'badge_counts': {'bronze': 33, 'silver': 16, 'gold': 2}, 'account_id': 8658944, 'is_employee': False, 'last_modified_date': 1700271600, 'last_access_date': 1710761825, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1622, 'creation_date': 1466232304, 'user_type': 'registered', 'user_id': 6482145, 'accept_rate': 100, 'location': 'Bhayandar East, Mira Bhayandar, Maharashtra, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/6482145/saurabh', 'profile_image': 'https://lh5.googleusercontent.com/-8yPdfuEgiyQ/AAAAAAAAAAI/AAAAAAAAACc/CDfou3nvYNM/photo.jpg?sz=256', 'display_name': 'Saurabh'}",Can someone explain me what means in below example? Why is it hard coded to 42?,"random_state import numpy as np
from sklearn.model_selection import train_test_split
X, y = np.arange(10).reshape((5, 2)), range(5)


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42) 
",5,10,0,0,
905,48056977,48057026,17090,Pandas merge TypeError: object of type 'NoneType' has no len(),1,<python><pandas>,12,"<p>I'm experimenting with pandas merge left_on and right_on params.
According to <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""noreferrer"">Documentation 1</a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/merging.html"" rel=""noreferrer"">Documentation 2</a></p>

<p>Documentation 1: states that left_on and right_on are field names to join on in left/right DataFrame.
Documentation 2: Columns from the left DataFrame to use as <strong>keys</strong></p>

<p>What does <strong>keys</strong> means?</p>

<p>Following documentation 1:</p>

<pre><code>left_frame = pd.DataFrame({'key': range(5),
                       'left_value': ['a', 'b', 'c', 'd', 'e']})
right_frame = pd.DataFrame({'key': range(2,7),
                       'right_value': ['f', 'g', 'h', 'i', 'j']})
</code></pre>

<p>I did this:</p>

<pre><code>df = pd.merge(left_frame,right_frame,how='right',right_on='key')
</code></pre>

<p>left_frame has 'key' as field name, but yet it returns </p>

<pre><code>TypeError: object of type 'NoneType' has no len()
</code></pre>
",8496110,562,02-01-2018 07:15,02-01-2018 07:19,0,562,23,2,8,55,"{'badge_counts': {'bronze': 23, 'silver': 8, 'gold': 2}, 'account_id': 11597971, 'is_employee': False, 'last_modified_date': 1583401005, 'last_access_date': 1632904714, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 562, 'creation_date': 1503330892, 'user_type': 'registered', 'user_id': 8496110, 'accept_rate': 55, 'location': 'Malaysia', 'website_url': '', 'link': 'https://stackoverflow.com/users/8496110/chia-yi', 'profile_image': 'https://i.stack.imgur.com/RhLoB.png?s=256&g=1', 'display_name': 'Chia Yi'}","I'm experimenting with pandas merge left_on and right_on params. According to Documentation 1 and Documentation 2 Documentation 1: states that left_on and right_on are field names to join on in left/right DataFrame. Documentation 2: Columns from the left DataFrame to use as keys What does keys means? Following documentation 1: I did this: left_frame has 'key' as field name, but yet it returns","left_frame = pd.DataFrame({'key': range(5),
                       'left_value': ['a', 'b', 'c', 'd', 'e']})
right_frame = pd.DataFrame({'key': range(2,7),
                       'right_value': ['f', 'g', 'h', 'i', 'j']})
 df = pd.merge(left_frame,right_frame,how='right',right_on='key')
 TypeError: object of type 'NoneType' has no len()
",3,25,0,2,
906,50185027,50185096,15540,Why are single type constraints disallowed in Python?,1,<python><types><python-3.6>,76,"<p>Suppose you want to constrain a type variable to implement a certain interface. You might write something like so:</p>

<pre><code>from typing import TypeVar, Callable

T = TypeVar('T', Callable)

class Foo(Generic[T]):
    ...

&gt;&gt; TypeError: A single constraint is not allowed
</code></pre>

<p>Why is Python unhappy about this use of type constraints? <a href=""https://www.python.org/dev/peps/pep-0484/#generics"" rel=""noreferrer"">PEP 484</a> and the <a href=""https://github.com/python/typing/blob/master/src/typing.py#L511"" rel=""noreferrer"">Python source code</a> are unhelpful in this regard.</p>

<p>Note: in my particular case I am interested in constraining a type variable to implement an abstract base class, but the principle is the same.</p>
",6536722,1318,05-05-2018 02:04,05-05-2018 02:18,0,1318,17,1,8,,"{'badge_counts': {'bronze': 17, 'silver': 8, 'gold': 1}, 'account_id': 8739147, 'is_employee': False, 'last_modified_date': 1616601901, 'last_access_date': 1687932321, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1318, 'creation_date': 1467353623, 'user_type': 'registered', 'user_id': 6536722, 'location': 'San Francisco, CA, USA', 'website_url': 'https://tdb-alcorn.github.io', 'link': 'https://stackoverflow.com/users/6536722/alcorn', 'profile_image': 'https://lh5.googleusercontent.com/-HG3oZ42hr3M/AAAAAAAAAAI/AAAAAAAAHKg/NUsi29KpnFk/photo.jpg?sz=256', 'display_name': 'alcorn'}","Suppose you want to constrain a type variable to implement a certain interface. You might write something like so: Why is Python unhappy about this use of type constraints? PEP 484 and the Python source code are unhelpful in this regard. Note: in my particular case I am interested in constraining a type variable to implement an abstract base class, but the principle is the same.","from typing import TypeVar, Callable

T = TypeVar('T', Callable)

class Foo(Generic[T]):
    ...

&gt;&gt; TypeError: A single constraint is not allowed
",7,15,0,2,
907,48173168,48174220,7647,Use both sample_weight and class_weight simultaneously,2,<python><tensorflow><keras>,11,"<p>My dataset already has weighted examples. And in this binary classification I also have far more of the first class compared to the second.</p>
<p>Can I use both <code>sample_weight</code> and further re-weight it with <code>class_weight</code> in the <code>model.fit()</code> function?</p>
<p>Or do I first make a new array of new_weights and pass it to the fit function as <code>sample_weight</code>?</p>
<p>Edit:</p>
<p>TO further clarify, I already have individual weights for each sample in my dataset, and to further add to the complexity, the total sum of sample weights of the first class is far more than the total sample weights of the second class.</p>
<p>For example I currently have:</p>
<blockquote>
<p>y = [0,0,0,0,1,1]</p>
<p>sample_weights = [0.01,0.03,0.05,0.02, 0.01,0.02]</p>
</blockquote>
<p>so the <em>sum of weights</em> for <em>class '0'</em> is <strong>0.11</strong> and for <em>class '1'</em> is  <strong>0.03</strong>. So I should have:</p>
<blockquote>
<p>class_weight = {0 : 1. , 1: 0.11/0.03}</p>
</blockquote>
<p>I need to use both <code>sample_weight</code> AND <code>class_weight</code> features. If one overrides the other then I will have to create new sample_weights and then use <code>fit()</code> or <code>train_on_batch()</code>.</p>
<p>So my question is, can I use both, or does one override the other?</p>
",7867665,882,09-01-2018 16:59,09-01-2018 18:04,0,882,28,0,7,100,"{'badge_counts': {'bronze': 28, 'silver': 7, 'gold': 0}, 'account_id': 10687613, 'is_employee': False, 'last_modified_date': 1654307700, 'last_access_date': 1675827429, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 882, 'creation_date': 1492178182, 'user_type': 'registered', 'user_id': 7867665, 'accept_rate': 100, 'location': 'Europe', 'link': 'https://stackoverflow.com/users/7867665/user7867665', 'profile_image': 'https://www.gravatar.com/avatar/256aec4ed3913a66e4a9785f8b000b80?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user7867665'}","My dataset already has weighted examples. And in this binary classification I also have far more of the first class compared to the second. Can I use both and further re-weight it with in the function? Or do I first make a new array of new_weights and pass it to the fit function as ? Edit: TO further clarify, I already have individual weights for each sample in my dataset, and to further add to the complexity, the total sum of sample weights of the first class is far more than the total sample weights of the second class. For example I currently have: y = [0,0,0,0,1,1] sample_weights = [0.01,0.03,0.05,0.02, 0.01,0.02] so the sum of weights for class '0' is 0.11 and for class '1' is 0.03. So I should have: class_weight = {0 : 1. , 1: 0.11/0.03} I need to use both AND features. If one overrides the other then I will have to create new sample_weights and then use or . So my question is, can I use both, or does one override the other?",sample_weight class_weight model.fit() sample_weight sample_weight class_weight fit() train_on_batch(),-8,16,0,0,
908,49616399,49616915,64496,Windows: Anaconda 'python' is not recognized as an internal or external command on CMD (Updated),6,<python><pip><anaconda><virtualenv><conda>,20,"<p>This is starting to get aggravating. I'm getting the typical  'python' is not recognized as an internal or external command... message when trying to run python on the command line. This is a common issue, and I've found plenty of posts saying to fix it by adding python folder (<code>C:\Users\ftake\Anaconda3</code>) my Windows path (restarted my PC after that) but nothing worked for me. The problem is that I've already Anaconda 1.8.2 installed on my computer and when trying to run python on CMD an error message occurs saying that python is not recognized as an internal...</p>

<p>I've tried to install <code>pyinstaller</code> (or any other Python packages) using pip, a message occurs too 'pip' is not recognized as an internal or external command...
Even though when trying to run (on CMD) the following command to create a virtual environment: <code>virtualenv --python=python3.6.3 &lt;env-name&gt;</code></p>

<p>I'm using:
OS: Windows 10 Pro.
Anaconda Version: 1.8.2</p>
",7916257,393,02-04-2018 18:08,02-04-2018 18:45,0,393,12,1,3,,"{'badge_counts': {'bronze': 12, 'silver': 3, 'gold': 1}, 'account_id': 10759342, 'is_employee': False, 'last_modified_date': 1673402100, 'last_access_date': 1697827419, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 393, 'creation_date': 1493072725, 'user_type': 'registered', 'user_id': 7916257, 'location': 'Moscow, Russia', 'website_url': '', 'link': 'https://stackoverflow.com/users/7916257/fouzi-takelait', 'profile_image': 'https://www.gravatar.com/avatar/55b4a42c9fe813d55985dc3de5a9ef12?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Fouzi TAKELAIT'}","This is starting to get aggravating. I'm getting the typical 'python' is not recognized as an internal or external command... message when trying to run python on the command line. This is a common issue, and I've found plenty of posts saying to fix it by adding python folder () my Windows path (restarted my PC after that) but nothing worked for me. The problem is that I've already Anaconda 1.8.2 installed on my computer and when trying to run python on CMD an error message occurs saying that python is not recognized as an internal... I've tried to install (or any other Python packages) using pip, a message occurs too 'pip' is not recognized as an internal or external command... Even though when trying to run (on CMD) the following command to create a virtual environment: I'm using: OS: Windows 10 Pro. Anaconda Version: 1.8.2",C:\Users\ftake\Anaconda3 pyinstaller virtualenv --python=python3.6.3 &lt;env-name&gt;,-3,8,0,0,
909,48824381,48824685,2628,gcloud ml-engine local predict RuntimeError: Bad magic number in .pyc file,6,<python><tensorflow><google-cloud-platform><gcloud><google-cloud-ml>,12,"<p>My objective is to make predictions on google cloud ml engine.</p>

<p>I installed gcloud sdk on linux ubuntu 16.04LT following <a href=""https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu"" rel=""noreferrer"">Google instructions</a>.
I already have a machine learning trained model.
I using python version anaconda python 3.5. </p>

<p>I run:</p>

<pre><code>gcloud ml-engine local predict --model-dir={MY_MODEL_DIR} --json-instances={MY_INPUT_JSON_INSTANCE}
</code></pre>

<p>I received the message: ERROR: </p>

<blockquote>
  <p>(gcloud.ml-engine.local.predict) RuntimeError: Bad magic number in .pyc file</p>
</blockquote>

<p>Below is all the stack trace:</p>

<pre><code>DEBUG: (gcloud.ml-engine.local.predict) RuntimeError: Bad magic number in .pyc file
Traceback (most recent call last):
  File ""/usr/lib/google-cloud-sdk/lib/googlecloudsdk/calliope/cli.py"", line 797, in Execute
    resources = calliope_command.Run(cli=self, args=args)
  File ""/usr/lib/google-cloud-sdk/lib/googlecloudsdk/calliope/backend.py"", line 757, in Run
    resources = command_instance.Run(args)
  File ""/usr/lib/google-cloud-sdk/lib/surface/ml_engine/local/predict.py"", line 65, in Run
    args.text_instances)
  File ""/usr/lib/google-cloud-sdk/lib/googlecloudsdk/command_lib/ml_engine/local_utils.py"", line 89, in RunPredict
    raise LocalPredictRuntimeError(err)
LocalPredictRuntimeError: RuntimeError: Bad magic number in .pyc file
ERROR: (gcloud.ml-engine.local.predict) RuntimeError: Bad magic number in .pyc file
Evaluation ended**
</code></pre>
",7985942,679,16-02-2018 10:09,16-02-2018 10:24,0,679,18,2,8,,"{'badge_counts': {'bronze': 18, 'silver': 8, 'gold': 2}, 'collectives': [{'collective': {'tags': ['google-cloud-save', 'firebase-admob', 'google-cloud-identity-aware-proxy', 'google-cloud-node', 'google-dataflow', 'jib', 'google-cloud-search', 'google-cloud-spanner', 'google-fusion-tables', 'google-cloud-data-fusion', 'google-cloud-interconnect', 'google-cloud-dataproc-metastore', 'google-cloud-repository', 'google-cloud-error-reporting', 'google-cloud-api-gateway', 'google-cloud-spanner-emulator', 'google-cloud-filestore', 'google-cloud-sql', 'google-cloud-endpoints-v2', 'firebase-dynamic-links', 'firebase-job-dispatcher', 'google-cloud-datalab', 'google-container-registry', 'gcloud', 'google-cloud-dataproc', 'google-app-engine-go', 'google-cloud-vpn', 'google-cloud-storage', 'google-cloud-workstations', 'google-cloud-tpu', 'google-app-engine-launch', 'google-cloud-instances', 'google-cloud-ai-platform-pipelines', 'google-cloud-iot', 'firebase-machine-learning', 'cordova-plugin-firebasex', 'google-cloud-resource-manager', 'dialogflow-es-fulfillment', 'firebase-polymer', 'google-cloud-logging', 'google-cloud-stackdriver', 'firebase-app-check', 'google-cloud-scheduler', 'firebase-console', 'firebase-performance', 'firebase-cli', 'firebase-queue', 'google-cloud-registry', 'google-cloud-composer', 'firebase-extensions', 'google-cloud-visualstudio', 'google-cloud-healthcare', 'google-cloud-webrisk', 'google-cloud-source-repos', 'google-bigquery', 'google-cloud-vision', 'google-cloud-metrics', 'google-cloud-shell-editor', 'google-cloud-network-load-balancer', 'google-cloud-deploy', 'google-cloud-instance-template', 'rest-firebase', 'google-cloud-messaging', 'firebase-predictions', 'google-cloud-url-maps', 'google-cloud-platform', 'google-cloud-translate', 'google-app-engine-php', 'google-cloud-ai', 'google-cloud-cdn', 'google-cloud-vertex-ai', 'google-cloud-marketplace', 'google-cloud-memorystore', 'firebase-in-app-messaging', 'google-cloud-networking', 'google-cloud-tasks', 'firebase-database', 'firebase-cloud-messaging', 'firebase-assistant', 'google-migrate-for-compute-engine', 'google-cloud-http-load-balancer', 'dialogflow-cx', 'firebase-mlkit', 'google-cloud-language', 'firebase-app-distribution', 'google-kubernetes-engine', 'react-redux-firebase', 'firebase-authentication', 'google-cloud-storage-r', 'google-cloud-automl', 'google-cloud-run', 'firebase-test-lab', 'google-cloud-billing', 'google-cloud-profiler', 'stackdriver', 'google-cloud-print', 'google-app-engine-python', 'google-compute-engine', 'firebase-remote-config', 'google-cloud-monitoring', 'apigee-baas', 'dialogflow-es', 'google-cloud-print-privet', 'maven-jib', 'google-cloud-pubsublite', 'firebase-analytics', 'google-cloud-shell', 'firebase-app-indexing', 'firebase-util', 'google-cloud-kms', 'firebase-ab-testing', 'google-cloud-internal-load-balancer', 'google-cloud-ml-engine', 'google-cloud-code', 'google-cloud-launcher', 'firebaseui', 'firebase-crash-reporting', 'firebasesimplelogin', 'looker', 'google-cloud-test-lab', 'firebase-invites', 'google-cloud-endpoints', 'vertex-ai-search', 'google-cloud-dlp', 'firebase-notifications', 'google-cloud-dns', 'google-anthos', 'google-cloud-cpp', 'google-cloud-talent-solution', 'firebase-admin', 'google-cloud-proxy', 'google-cloud-asset-inventory', 'google-cloud-intellij', 'firebase-storage', 'google-app-engine-golang', 'google-cloud-nl', 'looker-studio', 'google-cloud-build', 'google-cloud-trace', 'google-cloud-pubsub', 'google-app-engine-deploy', 'google-app-engine', 'google-translate', 'google-cloud-recommendation', 'google-container-os', 'google-cloud-bigtable', 'google-cloud-dataflow', 'nativescript-firebase', 'google-app-engine-patch', 'firebase', 'apigee', 'google-cloud-firestore', 'recaptcha-enterprise', 'google-cloud-dataprep', 'google-container-builder', 'firebase-tools', 'react-native-firebase', 'google-cloud-console', 'google-prediction', 'google-cloud-powershell', 'google-cloud-debugger', 'google-cloud-python', 'google-cloud-php-client', 'google-cloud-ops-agent', 'google-cloud-identity', 'firebase-realtime-database', 'bigtable', 'google-cloud-load-balancer', 'google-cloud-tools', 'redux-saga-firebase', 'google-cloud-datastore', 'google-cloud-data-transfer', 'google-cloud-armor', 'firebase-security', 'google-cloud-ml', 'google-cloud-robotics', 'google-cloud-speech', 'google-cloud-iam', 'google-cloud-sdk', 'google-cloud-functions', 'google-cloud-automl-nl', 'cloud-document-ai', 'google-container-optimized-os', 'firebase-hosting', 'google-analytics-firebase', 'google-cloud-router', 'google-cloud-transcoder'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 10861087, 'is_employee': False, 'last_modified_date': 1579342810, 'last_access_date': 1711023934, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 679, 'creation_date': 1494330054, 'user_type': 'registered', 'user_id': 7985942, 'location': 'Aparecida de Goi&#226;nia, State of Goi&#225;s, Brazil', 'website_url': '', 'link': 'https://stackoverflow.com/users/7985942/randolfo', 'profile_image': 'https://lh4.googleusercontent.com/-F1YArRAhOdQ/AAAAAAAAAAI/AAAAAAAANSE/uHTlpglYjI0/photo.jpg?sz=256', 'display_name': 'Randolfo'}",My objective is to make predictions on google cloud ml engine. I installed gcloud sdk on linux ubuntu 16.04LT following Google instructions. I already have a machine learning trained model. I using python version anaconda python 3.5. I run: I received the message: ERROR: (gcloud.ml-engine.local.predict) RuntimeError: Bad magic number in .pyc file Below is all the stack trace:,"gcloud ml-engine local predict --model-dir={MY_MODEL_DIR} --json-instances={MY_INPUT_JSON_INSTANCE}
 DEBUG: (gcloud.ml-engine.local.predict) RuntimeError: Bad magic number in .pyc file
Traceback (most recent call last):
  File ""/usr/lib/google-cloud-sdk/lib/googlecloudsdk/calliope/cli.py"", line 797, in Execute
    resources = calliope_command.Run(cli=self, args=args)
  File ""/usr/lib/google-cloud-sdk/lib/googlecloudsdk/calliope/backend.py"", line 757, in Run
    resources = command_instance.Run(args)
  File ""/usr/lib/google-cloud-sdk/lib/surface/ml_engine/local/predict.py"", line 65, in Run
    args.text_instances)
  File ""/usr/lib/google-cloud-sdk/lib/googlecloudsdk/command_lib/ml_engine/local_utils.py"", line 89, in RunPredict
    raise LocalPredictRuntimeError(err)
LocalPredictRuntimeError: RuntimeError: Bad magic number in .pyc file
ERROR: (gcloud.ml-engine.local.predict) RuntimeError: Bad magic number in .pyc file
Evaluation ended**
",12,33,0,1,
910,49456831,49456899,2333,Multiple sets of duplicate records from a pandas dataframe,3,<python><pandas><dataframe><group-by><pandas-groupby>,11,"<p>How to get all the existing duplicated sets of records(based on a column) from a dataframe?</p>

<p>I got a dataframe as follows:</p>

<pre><code>flight_id | from_location  | to_location |  schedule |  
1         |   Vancouver    |   Toronto   |   3-Jan   |  
2         |   Amsterdam    |   Tokyo     |   15-Feb  |  
4         |   Fairbanks    |   Glasgow   |   12-Jan  |  
9         |   Halmstad     |   Athens    |   21-Jan  |  
3         |   Brisbane     |   Lisbon    |   4-Feb   |  
4         | Johannesburg   |   Venice    |   23-Jan  |
9         | LosAngeles     |  Perth      |   3-Mar   |
</code></pre>

<p>Here flight_id is the column on which I need to check duplicates. And there are 2 sets of duplicates.</p>

<p>Output for this specific example should look like--<code>[(2,5),(3,6)]</code>. List of tuples of record index values</p>
",7987878,1687,23-03-2018 19:09,23-03-2018 19:15,0,1687,29,3,18,40,"{'badge_counts': {'bronze': 29, 'silver': 18, 'gold': 3}, 'account_id': 10863773, 'is_employee': False, 'last_modified_date': 1607614438, 'last_access_date': 1625239455, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1687, 'creation_date': 1494353785, 'user_type': 'registered', 'user_id': 7987878, 'accept_rate': 40, 'website_url': '', 'link': 'https://stackoverflow.com/users/7987878/kingz', 'profile_image': 'https://www.gravatar.com/avatar/3c34f4c17f87203d56c83ee2102e8a4b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Kingz'}",How to get all the existing duplicated sets of records(based on a column) from a dataframe? I got a dataframe as follows: Here flight_id is the column on which I need to check duplicates. And there are 2 sets of duplicates. Output for this specific example should look like--. List of tuples of record index values,"flight_id | from_location  | to_location |  schedule |  
1         |   Vancouver    |   Toronto   |   3-Jan   |  
2         |   Amsterdam    |   Tokyo     |   15-Feb  |  
4         |   Fairbanks    |   Glasgow   |   12-Jan  |  
9         |   Halmstad     |   Athens    |   21-Jan  |  
3         |   Brisbane     |   Lisbon    |   4-Feb   |  
4         | Johannesburg   |   Venice    |   23-Jan  |
9         | LosAngeles     |  Perth      |   3-Mar   |
 [(2,5),(3,6)]",6,17,0,0,
911,50109996,50110336,7997,what's the difference between tf.constant and tf.convert_to_tensor,1,<python><tensorflow>,16,"<p><code>tf.to_float(tf.convert_to_tensor(python_object))</code> used many times in Tensorflow object detection api like <a href=""https://github.com/tensorflow/models/blob/cb1567e87cd92ab0394a0f7b0d5c1cb226a2cbde/research/object_detection/anchor_generators/grid_anchor_generator.py#L56-L65"" rel=""noreferrer"">grid_anchor_generator</a>. normaly I'll use <code>tf.constant(python_object, dtype=tf.float32)</code>. I'm wondering the difference between them. Thanks</p>
",8037585,1470,01-05-2018 00:38,01-05-2018 01:34,0,1470,22,1,13,,"{'badge_counts': {'bronze': 22, 'silver': 13, 'gold': 1}, 'account_id': 10935835, 'is_employee': False, 'last_modified_date': 1607614436, 'last_access_date': 1707701380, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1470, 'creation_date': 1495208523, 'user_type': 'registered', 'user_id': 8037585, 'location': 'Shanghai CN', 'website_url': '', 'link': 'https://stackoverflow.com/users/8037585/bugkiller', 'profile_image': 'https://i.stack.imgur.com/p6250.jpg?s=256&g=1', 'display_name': 'BugKiller'}",used many times in Tensorflow object detection api like grid_anchor_generator. normaly I'll use . I'm wondering the difference between them. Thanks,"tf.to_float(tf.convert_to_tensor(python_object)) tf.constant(python_object, dtype=tf.float32)",-2,1,0,1,
912,49434712,49434747,11392,Pandas groupby on a column of lists,1,<python><python-3.x><pandas><pandas-groupby>,11,"<p>I have a <code>pandas</code> dataframe with a column that contains <code>lists</code>:</p>

<pre><code>df = pd.DataFrame({'List': [['once', 'upon'], ['once', 'upon'], ['a', 'time'], ['there', 'was'], ['a', 'time']], 'Count': [2, 3, 4, 1, 2]})

Count   List
2    [once, upon]
3    [once, upon]
4    [a, time]
1    [there, was]
2    [a, time]
</code></pre>

<p>How can I combine the <code>List</code> columns and sum the <code>Count</code> columns? The expected result is: </p>

<pre><code>Count   List
5     [once, upon]
6     [a, time]
1     [there, was]
</code></pre>

<p>I've tried: </p>

<pre><code>df.groupby('List')['Count'].sum()
</code></pre>

<p>which results in: </p>

<pre><code>TypeError: unhashable type: 'list'
</code></pre>
",8062181,401,22-03-2018 17:16,22-03-2018 17:18,0,411,14,1,4,83,"{'badge_counts': {'bronze': 14, 'silver': 4, 'gold': 1}, 'account_id': 10972416, 'is_employee': False, 'last_modified_date': 1659603300, 'last_access_date': 1710866956, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 411, 'creation_date': 1495668670, 'user_type': 'registered', 'user_id': 8062181, 'accept_rate': 83, 'website_url': '', 'link': 'https://stackoverflow.com/users/8062181/luxo-jr', 'profile_image': 'https://www.gravatar.com/avatar/98c85cee3ce3e500b142895ae5a3935a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Luxo_Jr'}",I have a dataframe with a column that contains : How can I combine the columns and sum the columns? The expected result is: I've tried: which results in:,"pandas lists df = pd.DataFrame({'List': [['once', 'upon'], ['once', 'upon'], ['a', 'time'], ['there', 'was'], ['a', 'time']], 'Count': [2, 3, 4, 1, 2]})

Count   List
2    [once, upon]
3    [once, upon]
4    [a, time]
1    [there, was]
2    [a, time]
 List Count Count   List
5     [once, upon]
6     [a, time]
1     [there, was]
 df.groupby('List')['Count'].sum()
 TypeError: unhashable type: 'list'
",6,29,0,0,
913,48899051,48899141,123439,How to drop a specific column of csv file while reading it using pandas?,6,<python><pandas><csv><dataframe>,74,"<p>I need to remove a <strong>column</strong> with label <strong>name</strong> at the time of loading a csv using <code>pandas</code>. I am reading csv as follows and want to add parameters inside it to do so. Thanks.</p>

<p><code>pd.read_csv(""sample.csv"")</code></p>

<p>I know this to do after reading csv:</p>

<pre><code>df.drop('name', axis=1)
</code></pre>
",8145200,880,21-02-2018 05:59,21-02-2018 06:06,0,880,11,1,6,,"{'badge_counts': {'bronze': 11, 'silver': 6, 'gold': 1}, 'account_id': 11092819, 'is_employee': False, 'last_modified_date': 1595019543, 'last_access_date': 1609854467, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 880, 'creation_date': 1497199204, 'user_type': 'registered', 'user_id': 8145200, 'location': 'India', 'website_url': '', 'link': 'https://stackoverflow.com/users/8145200/anon-george', 'profile_image': 'https://lh6.googleusercontent.com/-hOfKsvrBlzI/AAAAAAAAAAI/AAAAAAAAAWA/TTqlBVE0XWU/photo.jpg?sz=256', 'display_name': 'Anon George'}",I need to remove a column with label name at the time of loading a csv using . I am reading csv as follows and want to add parameters inside it to do so. Thanks. I know this to do after reading csv:,"pandas pd.read_csv(""sample.csv"") df.drop('name', axis=1)
",-2,8,0,0,
914,48283295,48283398,10645,"How to remove case-insensitive duplicates from a list, while maintaining the original list order?",6,<python><list>,13,"<p>I have a list of strings such as:</p>

<pre><code>myList = [""paper"", ""Plastic"", ""aluminum"", ""PAPer"", ""tin"", ""glass"", ""tin"", ""PAPER"", ""Polypropylene Plastic""]
</code></pre>

<p>I want this outcome (and this is the only acceptable outcome):</p>

<pre><code>myList = [""paper"", ""Plastic"", ""aluminum"", ""tin"", ""glass"", ""Polypropylene Plastic""]
</code></pre>

<p>Note that if an item (<code>""Polypropylene Plastic""</code>) happens to contain another item (<code>""Plastic""</code>), I would still like to retain both items. So, the cases can be different, but the item must be a letter-for-letter match, for it to be removed.</p>

<p>The original list order must be retained. All duplicates <em>after the first instance</em> of that item should be removed. The original case of that first instance should be preserved, as well as the original cases of all non-duplicate items.</p>

<p>I've searched and only found questions that address one need or the other, not both.</p>
",8840617,524,16-01-2018 14:16,16-01-2018 14:22,0,524,23,3,8,86,"{'badge_counts': {'bronze': 23, 'silver': 8, 'gold': 3}, 'account_id': 12098493, 'is_employee': False, 'last_modified_date': 1635555300, 'last_access_date': 1700775366, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 524, 'creation_date': 1509059827, 'user_type': 'registered', 'user_id': 8840617, 'accept_rate': 86, 'link': 'https://stackoverflow.com/users/8840617/crickets', 'profile_image': 'https://www.gravatar.com/avatar/311d19eb992b615168e2c9946473e4e3?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Crickets'}","I have a list of strings such as: I want this outcome (and this is the only acceptable outcome): Note that if an item () happens to contain another item (), I would still like to retain both items. So, the cases can be different, but the item must be a letter-for-letter match, for it to be removed. The original list order must be retained. All duplicates after the first instance of that item should be removed. The original case of that first instance should be preserved, as well as the original cases of all non-duplicate items. I've searched and only found questions that address one need or the other, not both.","myList = [""paper"", ""Plastic"", ""aluminum"", ""PAPer"", ""tin"", ""glass"", ""tin"", ""PAPER"", ""Polypropylene Plastic""]
 myList = [""paper"", ""Plastic"", ""aluminum"", ""tin"", ""glass"", ""Polypropylene Plastic""]
 ""Polypropylene Plastic"" ""Plastic""",-2,15,0,0,
915,48304305,48304415,318605,Anaconda / Python: Change Anaconda Prompt User Path,8,<python><anaconda><filepath><prompt>,51,"<p>I want to change my Anaconda Prompt User file path.  Currently it is as follows:</p>

<p><a href=""https://i.stack.imgur.com/1YNDA.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1YNDA.jpg"" alt=""enter image description here""></a></p>

<p>I want it to change to: C:\Users\u354590</p>

<p>How do I do this? </p>

<p>The current version of anaconda I have is:</p>

<pre><code>Python 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]
</code></pre>
",7077532,5094,17-01-2018 15:14,17-01-2018 15:19,0,5094,120,22,70,64,"{'badge_counts': {'bronze': 120, 'silver': 70, 'gold': 22}, 'account_id': 9523508, 'is_employee': False, 'last_modified_date': 1703901000, 'last_access_date': 1710700614, 'reputation_change_year': 132, 'reputation_change_quarter': 132, 'reputation_change_month': 22, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 5094, 'creation_date': 1477519901, 'user_type': 'registered', 'user_id': 7077532, 'accept_rate': 64, 'location': 'Dallas', 'link': 'https://stackoverflow.com/users/7077532/pinenuts0', 'profile_image': 'https://www.gravatar.com/avatar/c3df487fabbb407245e9493fe448088c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'PineNuts0'}",I want to change my Anaconda Prompt User file path. Currently it is as follows: I want it to change to: C:\Users\u354590 How do I do this? The current version of anaconda I have is:,"Python 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]
",0,12,1,1,
916,50070877,50070904,33264,Postgres Psycopg2 Create Table,1,<python><database><postgresql><python-3.6><postgresql-10>,22,"<p>I am new to Postgres and Python. I try to create a simple user table but I don't know why it isn't created.
The error message doesn't appear,</p>
<pre><code>    #!/usr/bin/python
    import psycopg2
    
    try:
        conn = psycopg2.connect(database = &quot;projetofinal&quot;, user = &quot;postgres&quot;, password = &quot;admin&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;)
    except:
        print(&quot;I am unable to connect to the database&quot;) 
    
    cur = conn.cursor()
    try:
        cur.execute(&quot;CREATE TABLE test (id serial PRIMARY KEY, num integer, data varchar);&quot;)
    except:
        print(&quot;I can't drop our test database!&quot;)
    
    conn.close()
    cur.close()
</code></pre>
<p>Any help or hint would be appreciated.
Thank you.</p>
",8162483,353,27-04-2018 22:06,27-04-2018 22:10,0,353,12,1,2,0,"{'badge_counts': {'bronze': 12, 'silver': 2, 'gold': 1}, 'account_id': 11118572, 'is_employee': False, 'last_modified_date': 1675881824, 'last_access_date': 1711118824, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 353, 'creation_date': 1497468113, 'user_type': 'registered', 'user_id': 8162483, 'accept_rate': 0, 'location': 'Portugal', 'website_url': '', 'link': 'https://stackoverflow.com/users/8162483/ricardo-pinto', 'profile_image': 'https://graph.facebook.com/1576508389036507/picture?type=large', 'display_name': 'Ricardo Pinto'}","I am new to Postgres and Python. I try to create a simple user table but I don't know why it isn't created. The error message doesn't appear, Any help or hint would be appreciated. Thank you.","    #!/usr/bin/python
    import psycopg2
    
    try:
        conn = psycopg2.connect(database = &quot;projetofinal&quot;, user = &quot;postgres&quot;, password = &quot;admin&quot;, host = &quot;localhost&quot;, port = &quot;5432&quot;)
    except:
        print(&quot;I am unable to connect to the database&quot;) 
    
    cur = conn.cursor()
    try:
        cur.execute(&quot;CREATE TABLE test (id serial PRIMARY KEY, num integer, data varchar);&quot;)
    except:
        print(&quot;I can't drop our test database!&quot;)
    
    conn.close()
    cur.close()
",15,21,0,0,
917,48758383,48758432,29254,All intermediate steps should be transformers and implement fit and transform,3,<python><machine-learning><scikit-learn><feature-selection>,13,"<p>I am implementing a pipeline using important features selection and then using the same features to train my random forest classifier. Following is my code.</p>
<pre class=""lang-py prettyprint-override""><code>m = ExtraTreesClassifier(n_estimators = 10)
m.fit(train_cv_x,train_cv_y)
sel = SelectFromModel(m, prefit=True)
X_new = sel.transform(train_cv_x)
clf = RandomForestClassifier(5000)

model = Pipeline([('m', m),('sel', sel),('X_new', X_new),('clf', clf),])
params = {'clf__max_features': ['auto', 'sqrt', 'log2']}

gs = GridSearchCV(model, params)
gs.fit(train_cv_x,train_cv_y)
</code></pre>
<p>So <code>X_new</code> are the new features selected via <code>SelectFromModel</code> and <code>sel.transform</code>. Then I want to train my RF using the new features selected.</p>
<p>I am getting the following error:</p>
<pre class=""lang-none prettyprint-override""><code>All intermediate steps should be transformers and implement fit and transform, 
ExtraTreesClassifier ...
</code></pre>
",8163412,1387,13-02-2018 01:59,13-02-2018 02:08,0,1387,45,4,21,79,"{'badge_counts': {'bronze': 45, 'silver': 21, 'gold': 4}, 'account_id': 11120387, 'is_employee': False, 'last_modified_date': 1706925000, 'last_access_date': 1703194019, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1387, 'creation_date': 1497488105, 'user_type': 'registered', 'user_id': 8163412, 'accept_rate': 79, 'website_url': '', 'link': 'https://stackoverflow.com/users/8163412/stupid420', 'profile_image': 'https://www.gravatar.com/avatar/29545335bcc8a5f9cb60ed17be4bcf4e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Stupid420'}",I am implementing a pipeline using important features selection and then using the same features to train my random forest classifier. Following is my code. So are the new features selected via and . Then I want to train my RF using the new features selected. I am getting the following error:,"m = ExtraTreesClassifier(n_estimators = 10)
m.fit(train_cv_x,train_cv_y)
sel = SelectFromModel(m, prefit=True)
X_new = sel.transform(train_cv_x)
clf = RandomForestClassifier(5000)

model = Pipeline([('m', m),('sel', sel),('X_new', X_new),('clf', clf),])
params = {'clf__max_features': ['auto', 'sqrt', 'log2']}

gs = GridSearchCV(model, params)
gs.fit(train_cv_x,train_cv_y)
 X_new SelectFromModel sel.transform All intermediate steps should be transformers and implement fit and transform, 
ExtraTreesClassifier ...
",8,18,0,0,
918,48417867,48418031,19531,access to numbers in classification_report - sklearn,5,<python><dictionary><scikit-learn><classification>,25,"<p>This is a simple example of a <code>classification_report</code> in <code>sklearn</code>:</p>
<pre><code>from sklearn.metrics import classification_report
y_true = [0, 1, 2, 2, 2]
y_pred = [0, 0, 2, 2, 1]
target_names = ['class 0', 'class 1', 'class 2']
print(classification_report(y_true, y_pred, target_names=target_names))
#             precision    recall  f1-score   support
#
#    class 0       0.50      1.00      0.67         1
#    class 1       0.00      0.00      0.00         1
#    class 2       1.00      0.67      0.80         3
#
#avg / total       0.70      0.60      0.61         5
</code></pre>
<p>I want to have access to avg/total row. For instance, I want to extract the <code>f1-score</code> from the report, which is 0.61.</p>
<p>How can I have access to the number in <code>classification_report</code>?</p>
",8899386,4072,24-01-2018 08:27,24-01-2018 08:36,0,4082,48,6,28,94,"{'badge_counts': {'bronze': 48, 'silver': 28, 'gold': 6}, 'account_id': 12192399, 'is_employee': False, 'last_modified_date': 1694346300, 'last_access_date': 1702650388, 'reputation_change_year': 150, 'reputation_change_quarter': 150, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 4082, 'creation_date': 1510049215, 'user_type': 'registered', 'user_id': 8899386, 'accept_rate': 94, 'website_url': 'https://hadijahanshahi.com', 'link': 'https://stackoverflow.com/users/8899386/hadij', 'profile_image': 'https://i.stack.imgur.com/BpHdY.png?s=256&g=1', 'display_name': 'Hadij'}","This is a simple example of a in : I want to have access to avg/total row. For instance, I want to extract the from the report, which is 0.61. How can I have access to the number in ?","classification_report sklearn from sklearn.metrics import classification_report
y_true = [0, 1, 2, 2, 2]
y_pred = [0, 0, 2, 2, 1]
target_names = ['class 0', 'class 1', 'class 2']
print(classification_report(y_true, y_pred, target_names=target_names))
#             precision    recall  f1-score   support
#
#    class 0       0.50      1.00      0.67         1
#    class 1       0.00      0.00      0.00         1
#    class 2       1.00      0.67      0.80         3
#
#avg / total       0.70      0.60      0.61         5
 f1-score classification_report",7,16,0,0,
919,49969006,50005651,27597,save and load keras.callbacks.History,5,<python><deep-learning><keras><generator>,18,"<p>I'm training a deep neural net using Keras and looking for a way to save and later load the history object which is of <code>keras.callbacks.History</code> type. Here's the setup:</p>

<pre><code>history_model_1 = model_1.fit_generator(train_generator,
                          steps_per_epoch=100,
                          epochs=20,
                          validation_data=validation_generator,
                          validation_steps=50)
</code></pre>

<p><code>history_model_1</code> is the variable I want to be saved and loaded during another Python session.  </p>
",8201676,1378,22-04-2018 17:55,24-04-2018 15:33,2,1388,44,4,20,,"{'badge_counts': {'bronze': 44, 'silver': 20, 'gold': 4}, 'account_id': 11176136, 'is_employee': False, 'last_modified_date': 1653097200, 'last_access_date': 1710880301, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 1388, 'creation_date': 1498157780, 'user_type': 'registered', 'user_id': 8201676, 'location': 'Warsaw, Poland', 'website_url': '', 'link': 'https://stackoverflow.com/users/8201676/balkon16', 'profile_image': 'https://www.gravatar.com/avatar/5ec8e7cd1673c9ec4c0523b433a5bd95?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'balkon16'}",I'm training a deep neural net using Keras and looking for a way to save and later load the history object which is of type. Here's the setup: is the variable I want to be saved and loaded during another Python session.,"keras.callbacks.History history_model_1 = model_1.fit_generator(train_generator,
                          steps_per_epoch=100,
                          epochs=20,
                          validation_data=validation_generator,
                          validation_steps=50)
 history_model_1",2,10,0,0,
920,48275759,48276166,10952,"AWS elastic search error ""[Errno 8] nodename nor servname provided, or not known.""",1,<python><aws-elasticsearch>,20,"<p>I created one AWS elasticsearch instance. I want to access it using a python script. I specified my AWS configuration (access key, secret key, region). I am using below code to access the AWS ES instance:</p>

<pre><code>from elasticsearch import Elasticsearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth

AWS_ACCESS_KEY = '**************'
AWS_SECRET_KEY = '*****************'
region = 'us-east-1'
service = 'es'

awsauth = AWS4Auth(AWS_ACCESS_KEY, AWS_SECRET_KEY, region, service)

host = 'https://kbckjsdkcdn.us-east-1.es.amazonaws.com' # For example, my-test-domain.us-east-1.es.amazonaws.com

es = Elasticsearch(
    hosts = [{'host': host, 'port': 443}],
    http_auth = awsauth,
    use_ssl = True,
    verify_certs = True,
    connection_class = RequestsHttpConnection
)

print es.info()
</code></pre>

<p>When I am running the above code, I am getting following error:</p>

<pre><code>elasticsearch.exceptions.ConnectionError:  ConnectionError(HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //search-opendata-2xd6pwilq5sv4ahomcuaiyxmqe.us-east-1.es.amazonaws.com:443/ (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x10ee72310&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))) caused by: ConnectionError(HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //search-opendata-2xd6pwilq5sv4ahomcuaiyxmqe.us-east-1.es.amazonaws.com:443/ (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x10ee72310&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',)))
</code></pre>

<p>How can I resolve this error?</p>

<p>Thanks</p>
",8228723,1948,16-01-2018 07:11,16-01-2018 07:39,0,1958,35,5,21,29,"{'badge_counts': {'bronze': 35, 'silver': 21, 'gold': 5}, 'account_id': 11216159, 'is_employee': False, 'last_modified_date': 1691202001, 'last_access_date': 1633960110, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1958, 'creation_date': 1498688655, 'user_type': 'registered', 'user_id': 8228723, 'accept_rate': 29, 'location': 'Bhopal, Madhya Pradesh, India', 'link': 'https://stackoverflow.com/users/8228723/neha', 'profile_image': 'https://www.gravatar.com/avatar/e5bf1a2a091d955fdfa2e1c9427008ed?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'neha'}","I created one AWS elasticsearch instance. I want to access it using a python script. I specified my AWS configuration (access key, secret key, region). I am using below code to access the AWS ES instance: When I am running the above code, I am getting following error: How can I resolve this error? Thanks","from elasticsearch import Elasticsearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth

AWS_ACCESS_KEY = '**************'
AWS_SECRET_KEY = '*****************'
region = 'us-east-1'
service = 'es'

awsauth = AWS4Auth(AWS_ACCESS_KEY, AWS_SECRET_KEY, region, service)

host = 'https://kbckjsdkcdn.us-east-1.es.amazonaws.com' # For example, my-test-domain.us-east-1.es.amazonaws.com

es = Elasticsearch(
    hosts = [{'host': host, 'port': 443}],
    http_auth = awsauth,
    use_ssl = True,
    verify_certs = True,
    connection_class = RequestsHttpConnection
)

print es.info()
 elasticsearch.exceptions.ConnectionError:  ConnectionError(HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //search-opendata-2xd6pwilq5sv4ahomcuaiyxmqe.us-east-1.es.amazonaws.com:443/ (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x10ee72310&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))) caused by: ConnectionError(HTTPSConnectionPool(host='https', port=443): Max retries exceeded with url: //search-opendata-2xd6pwilq5sv4ahomcuaiyxmqe.us-east-1.es.amazonaws.com:443/ (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x10ee72310&gt;: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',)))
",20,33,0,0,
921,49228744,49228822,57256,AttributeError: module 'attr' has no attribute 's',2,<python>,43,"<pre><code>&gt;&gt;&gt; import attr
&gt;&gt;&gt; @attr.s
... class SmartClass(object):
...     a=attr.ib()
...     b=attr.ib()
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: module 'attr' has no attribute 's'
&gt;&gt;&gt; 
</code></pre>

<p>I don't understand why it's not working. I have installed this module using pip and it was installed properly, but still it shows an error in the implementation part.</p>
",8236985,994,12-03-2018 05:38,12-03-2018 05:45,0,994,29,2,14,100,"{'badge_counts': {'bronze': 29, 'silver': 14, 'gold': 2}, 'account_id': 11227871, 'is_employee': False, 'last_modified_date': 1706188800, 'last_access_date': 1711101798, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 994, 'creation_date': 1498821378, 'user_type': 'registered', 'user_id': 8236985, 'accept_rate': 100, 'location': 'Bangalore, Karnataka, India', 'website_url': 'https://arcticoak2.github.io/', 'link': 'https://stackoverflow.com/users/8236985/arctic-oak', 'profile_image': 'https://i.stack.imgur.com/4JZPu.jpg?s=256&g=1', 'display_name': 'arctic_Oak'}","I don't understand why it's not working. I have installed this module using pip and it was installed properly, but still it shows an error in the implementation part.","&gt;&gt;&gt; import attr
&gt;&gt;&gt; @attr.s
... class SmartClass(object):
...     a=attr.ib()
...     b=attr.ib()
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: module 'attr' has no attribute 's'
&gt;&gt;&gt; 
",9,13,0,0,
922,48301186,48301637,35596,Cropping Concave polygon from Image using Opencv python,3,<python><opencv><image-processing><crop>,29,"<p>How can I crop a concave polygon from an image. My Input image look like 
<img src=""https://i.stack.imgur.com/qrJTd.png"" alt=""this"">.</p>

<p>and the coordinates of <strong>closed</strong> polygon are 
[10,150],[150,100],[300,150],[350,100],[310,20],[35,10]. I want region bounded by concave polygon to be cropped using opencv. I searched for other similar questions but I did not able to find correct answer. That's why I am asking it ? Can you help me.</p>

<p>Any help would be highly appreciated.!!!</p>
",8237470,346,17-01-2018 12:29,17-01-2018 12:51,0,346,9,1,3,,"{'badge_counts': {'bronze': 9, 'silver': 3, 'gold': 1}, 'account_id': 11228567, 'is_employee': False, 'last_modified_date': 1666347001, 'last_access_date': 1564826963, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 346, 'creation_date': 1498827918, 'user_type': 'registered', 'user_id': 8237470, 'location': 'NIT Road, Mavoor, Calicut, Kerala, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/8237470/himanshu-tiwari', 'profile_image': 'https://www.gravatar.com/avatar/7a8e07ca077dc74712c37b1ed680e123?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Himanshu Tiwari'}","How can I crop a concave polygon from an image. My Input image look like . and the coordinates of closed polygon are [10,150],[150,100],[300,150],[350,100],[310,20],[35,10]. I want region bounded by concave polygon to be cropped using opencv. I searched for other similar questions but I did not able to find correct answer. That's why I am asking it ? Can you help me. Any help would be highly appreciated.!!!",,0,7,1,0,
923,48603339,48603374,20759,How to call JavaScript function using BeautifulSoup and Python,3,<javascript><python><selenium><web-scraping><urllib>,12,"<p>I am performing web scraping to grab data from a website as part of my project. I can make the request and grab the data which is present in the dom. However, some data is getting rendered on javascript onClick function.</p>
<p>One way could be, using the selenium to click on the link (which calls the javascript function) and grab the rendered data, but this process is time-consuming, and I don't want to open the browser.</p>
<p>Is there any way other than selenium to achieve this?</p>
<p>Website: <a href=""http://catalog.fullerton.edu/preview_entity.php?catoid=16&amp;ent_oid=1849"" rel=""nofollow noreferrer"">http://catalog.fullerton.edu/preview_entity.php?catoid=16&amp;ent_oid=1849</a></p>
<p>In the <em>courses</em> section of this webpage, all the courses are hyperlinks, and as soon as someone clicks on the courses, a javascript method gets called. I need the data which gets rendered after the javascript function call.</p>
",8323260,707,04-02-2018 00:00,04-02-2018 00:04,0,707,33,1,12,64,"{'badge_counts': {'bronze': 33, 'silver': 12, 'gold': 1}, 'account_id': 11352859, 'is_employee': False, 'last_modified_date': 1658539800, 'last_access_date': 1632077296, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 707, 'creation_date': 1500357762, 'user_type': 'registered', 'user_id': 8323260, 'accept_rate': 64, 'location': 'Northern California, CA, USA', 'website_url': '', 'link': 'https://stackoverflow.com/users/8323260/miserable', 'profile_image': 'https://www.gravatar.com/avatar/b4e790fdfa970f8a74322e651ea2be59?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'miserable'}","I am performing web scraping to grab data from a website as part of my project. I can make the request and grab the data which is present in the dom. However, some data is getting rendered on javascript onClick function. One way could be, using the selenium to click on the link (which calls the javascript function) and grab the rendered data, but this process is time-consuming, and I don't want to open the browser. Is there any way other than selenium to achieve this? Website: http://catalog.fullerton.edu/preview_entity.php?catoid=16&amp;ent_oid=1849 In the courses section of this webpage, all the courses are hyperlinks, and as soon as someone clicks on the courses, a javascript method gets called. I need the data which gets rendered after the javascript function call.",,0,5,0,1,
924,49878953,49918082,13018,Issues listening incoming messages in websocket client on Python 3.6,1,<python><websocket>,15,"<p>I'm trying to build a <strong>websocket</strong> client on <strong>Python</strong> using websockets package from here: <a href=""https://websockets.readthedocs.io/en/stable/api.html#module-websockets.client"" rel=""noreferrer"">Websockets 4.0 API</a></p>

<p>I'm using this way instead of example code because I want to create a websocket client class object, and use it as gateway.</p>

<p>I'm having issues with my listener method (receiveMessage) on client side, which raises a ConnectionClose  exception at execution. I think maybe there is any problem with the loop.</p>

<p>This is the simple webSocket client I've tried to build:</p>

<pre><code>import websockets

class WebSocketClient():

    def __init__(self):
        pass

    async def connect(self):
        '''
            Connecting to webSocket server

            websockets.client.connect returns a WebSocketClientProtocol, which is used to send and receive messages
        '''
        self.connection = await websockets.client.connect('ws://127.0.0.1:8765')
        if self.connection.open:
            print('Connection stablished. Client correcly connected')
            # Send greeting
            await self.sendMessage('Hey server, this is webSocket client')
            # Enable listener
            await self.receiveMessage()


    async def sendMessage(self, message):
        '''
            Sending message to webSocket server
        '''
        await self.connection.send(message)

    async def receiveMessage(self):
        '''
            Receiving all server messages and handling them
        '''
        while True:
            message = await self.connection.recv()
            print('Received message from server: ' + str(message))
</code></pre>

<p>And this is the main:</p>

<pre><code>'''
    Main file
'''

import asyncio
from webSocketClient import WebSocketClient

if __name__ == '__main__':
    # Creating client object
    client = WebSocketClient()
    loop = asyncio.get_event_loop()
    loop.run_until_complete(client.connect())
    loop.run_forever()
    loop.close()
</code></pre>

<p>To test incoming messages listener, server sends two messages to client when it stablishes the connection.</p>

<p>Client connects correctly to server, and sends the greeting. However, when client receives both messages, it raises a <strong>ConnectionClosed exception</strong> with code 1000 (no reason). </p>

<p>If I remove the loop in the receiveMessage client method, client does not raise any exception, but it only receives one message, so I suppose I need a loop to keep listener alive, but I don't know exactly where or how.</p>

<p>Any solution? </p>

<p>Thanks in advance.</p>

<p><strong>EDIT:</strong> I realize that client closes connection (and breaks loop) when it receives all pending messages from server. However, I want client keeps alive listening future messages.</p>

<p>In addition, I've tried to add another function whose task is to send a 'heartbeat' to server, but client closes connection anyway.</p>
",8357886,441,17-04-2018 13:04,19-04-2018 09:56,2,441,12,1,6,,"{'badge_counts': {'bronze': 12, 'silver': 6, 'gold': 1}, 'account_id': 11400901, 'is_employee': False, 'last_modified_date': 1677809100, 'last_access_date': 1530606175, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 441, 'creation_date': 1500900978, 'user_type': 'registered', 'user_id': 8357886, 'location': 'M&#225;laga, Espa&#241;a', 'link': 'https://stackoverflow.com/users/8357886/charliehollow', 'profile_image': 'https://www.gravatar.com/avatar/dfcc75c8458ab6d395b171ef02cbddc6?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'CharlieHollow'}","I'm trying to build a websocket client on Python using websockets package from here: Websockets 4.0 API I'm using this way instead of example code because I want to create a websocket client class object, and use it as gateway. I'm having issues with my listener method (receiveMessage) on client side, which raises a ConnectionClose exception at execution. I think maybe there is any problem with the loop. This is the simple webSocket client I've tried to build: And this is the main: To test incoming messages listener, server sends two messages to client when it stablishes the connection. Client connects correctly to server, and sends the greeting. However, when client receives both messages, it raises a ConnectionClosed exception with code 1000 (no reason). If I remove the loop in the receiveMessage client method, client does not raise any exception, but it only receives one message, so I suppose I need a loop to keep listener alive, but I don't know exactly where or how. Any solution? Thanks in advance. EDIT: I realize that client closes connection (and breaks loop) when it receives all pending messages from server. However, I want client keeps alive listening future messages. In addition, I've tried to add another function whose task is to send a 'heartbeat' to server, but client closes connection anyway.","import websockets

class WebSocketClient():

    def __init__(self):
        pass

    async def connect(self):
        '''
            Connecting to webSocket server

            websockets.client.connect returns a WebSocketClientProtocol, which is used to send and receive messages
        '''
        self.connection = await websockets.client.connect('ws://127.0.0.1:8765')
        if self.connection.open:
            print('Connection stablished. Client correcly connected')
            # Send greeting
            await self.sendMessage('Hey server, this is webSocket client')
            # Enable listener
            await self.receiveMessage()


    async def sendMessage(self, message):
        '''
            Sending message to webSocket server
        '''
        await self.connection.send(message)

    async def receiveMessage(self):
        '''
            Receiving all server messages and handling them
        '''
        while True:
            message = await self.connection.recv()
            print('Received message from server: ' + str(message))
 '''
    Main file
'''

import asyncio
from webSocketClient import WebSocketClient

if __name__ == '__main__':
    # Creating client object
    client = WebSocketClient()
    loop = asyncio.get_event_loop()
    loop.run_until_complete(client.connect())
    loop.run_forever()
    loop.close()
",47,76,0,1,
925,48342098,48342205,309056,How to check python anaconda version installed on Windows 10 PC?,3,<python><anaconda>,78,"<p>I have a Windows 10 PC with python anaconda installed. The latest anaconda version is v5.0.1</p>

<p>I would like to find out whether the PC has the latest version v5.0.1 installed and whether it is 32-bit/64bit or python 2.7/3.6. How do I do that?</p>

<p><a href=""https://www.anaconda.com/download/"" rel=""noreferrer"">https://www.anaconda.com/download/</a></p>
",7518091,3439,19-01-2018 13:21,19-01-2018 13:27,0,3459,43,6,24,91,"{'badge_counts': {'bronze': 43, 'silver': 24, 'gold': 6}, 'account_id': 10181486, 'is_employee': False, 'last_modified_date': 1698458700, 'last_access_date': 1695452243, 'reputation_change_year': 140, 'reputation_change_quarter': 140, 'reputation_change_month': 40, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 3459, 'creation_date': 1486275793, 'user_type': 'registered', 'user_id': 7518091, 'accept_rate': 91, 'location': 'Singapore', 'website_url': '', 'link': 'https://stackoverflow.com/users/7518091/user1315789', 'profile_image': 'https://www.gravatar.com/avatar/f8af9d177a327eaf22be1651863a103a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user1315789'}",I have a Windows 10 PC with python anaconda installed. The latest anaconda version is v5.0.1 I would like to find out whether the PC has the latest version v5.0.1 installed and whether it is 32-bit/64bit or python 2.7/3.6. How do I do that? https://www.anaconda.com/download/,,0,5,0,1,
926,48749364,48772153,46053,"How to fix the error ""failed to parse date field "" in Elasticsearch",1,<python><parsing><elasticsearch><timestamp>,15,"<p>I tried to do a query in Elasticsearch via python. I want to get all values in the last one hour from now. For this I wrote this script:</p>
<pre><code>import time
from elasticsearch import Elasticsearch
from datetime import datetime, timedelta

es = Elasticsearch()
index = &quot;standalone&quot;

filename = &quot;2017-12-22V2.csv&quot;

Timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
one_hour_from_now = datetime.now() - timedelta(hours=1)
one_hour_from_now = one_hour_from_now.strftime('%Y-%m-%d %H:%M:%S')


query = {&quot;query&quot;:{&quot;bool&quot;:{&quot;must&quot;:{&quot;range&quot;:{&quot;Time&quot;:{&quot;gt&quot;:one_hour_from_now,&quot;lt&quot;:Timestamp}}},&quot;must_not&quot;:[],&quot;should&quot;:[]}},&quot;from&quot;:0,&quot;size&quot;:10,&quot;sort&quot;:[],&quot;aggs&quot;:{}}


ret = es.search(index, body=query)
print(&quot;ret&quot;, ret)
</code></pre>
<p>When I execute it I get this error:</p>
<pre><code> es.search exception:  TransportError(400, 'search_phase_execution_exception', 'failed to parse date field [2018-02-12 15:50:26] with format [strict_date_optional_time||epoch_millis]')
</code></pre>
<p>This is the structure of my ES index:
<a href=""https://i.stack.imgur.com/mLwY4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mLwY4.jpg"" alt=""Elasticsearch structure "" /></a></p>
<p>Can someone help me please</p>
<p>Thank you</p>
",7617545,529,12-02-2018 15:01,13-02-2018 17:13,1,529,18,4,8,82,"{'badge_counts': {'bronze': 18, 'silver': 8, 'gold': 4}, 'account_id': 10326124, 'is_employee': False, 'last_modified_date': 1586069860, 'last_access_date': 1677571883, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 529, 'creation_date': 1487946807, 'user_type': 'registered', 'user_id': 7617545, 'accept_rate': 82, 'location': 'Germany', 'link': 'https://stackoverflow.com/users/7617545/ahmyohlin', 'profile_image': 'https://www.gravatar.com/avatar/d9415b666fd19ad82cab2b0998aa5124?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'AhmyOhlin'}",I tried to do a query in Elasticsearch via python. I want to get all values in the last one hour from now. For this I wrote this script: When I execute it I get this error: This is the structure of my ES index: Can someone help me please Thank you,"import time
from elasticsearch import Elasticsearch
from datetime import datetime, timedelta

es = Elasticsearch()
index = &quot;standalone&quot;

filename = &quot;2017-12-22V2.csv&quot;

Timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
one_hour_from_now = datetime.now() - timedelta(hours=1)
one_hour_from_now = one_hour_from_now.strftime('%Y-%m-%d %H:%M:%S')


query = {&quot;query&quot;:{&quot;bool&quot;:{&quot;must&quot;:{&quot;range&quot;:{&quot;Time&quot;:{&quot;gt&quot;:one_hour_from_now,&quot;lt&quot;:Timestamp}}},&quot;must_not&quot;:[],&quot;should&quot;:[]}},&quot;from&quot;:0,&quot;size&quot;:10,&quot;sort&quot;:[],&quot;aggs&quot;:{}}


ret = es.search(index, body=query)
print(&quot;ret&quot;, ret)
  es.search exception:  TransportError(400, 'search_phase_execution_exception', 'failed to parse date field [2018-02-12 15:50:26] with format [strict_date_optional_time||epoch_millis]')
",18,28,1,1,
927,48067514,48067785,154851,'utf-8' codec can't decode byte 0xa0 in position 4276: invalid start byte,6,<python><csv><encoding><utf-8>,46,"<p>I try to read and print the following file: txt.tsv (<a href=""https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2017q3_notes.zip"" rel=""noreferrer"">https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2017q3_notes.zip</a>)</p>
<p>According to the SEC the data set is provided in a single encoding, as follows:</p>
<blockquote>
<p>Tab Delimited Value (.txt): utf-8, tab-delimited, \n- terminated lines, with the first line containing the field names in lowercase.</p>
</blockquote>
<p>My current code:</p>
<pre><code>import csv

with open('txt.tsv') as tsvfile:
    reader = csv.DictReader(tsvfile, dialect='excel-tab')
    for row in reader:
        print(row)
</code></pre>
<p>All attempts ended with the following error message:</p>
<blockquote>
<p>'utf-8' codec can't decode byte 0xa0 in position 4276: invalid start byte</p>
</blockquote>
<p>I am a bit lost. Can anyone help me?</p>
",9165417,463,02-01-2018 20:36,02-01-2018 21:00,0,463,5,1,4,,"{'badge_counts': {'bronze': 5, 'silver': 4, 'gold': 1}, 'account_id': 12603579, 'is_employee': False, 'last_modified_date': 1573678432, 'last_access_date': 1612542538, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 463, 'creation_date': 1514924240, 'user_type': 'registered', 'user_id': 9165417, 'link': 'https://stackoverflow.com/users/9165417/vital', 'profile_image': 'https://lh4.googleusercontent.com/-WaFrLKXd-Z0/AAAAAAAAAAI/AAAAAAAABvc/4j-zLD2biO8/photo.jpg?sz=256', 'display_name': 'Vital'}","I try to read and print the following file: txt.tsv (https://www.sec.gov/files/dera/data/financial-statement-and-notes-data-sets/2017q3_notes.zip) According to the SEC the data set is provided in a single encoding, as follows: Tab Delimited Value (.txt): utf-8, tab-delimited, \n- terminated lines, with the first line containing the field names in lowercase. My current code: All attempts ended with the following error message: 'utf-8' codec can't decode byte 0xa0 in position 4276: invalid start byte I am a bit lost. Can anyone help me?","import csv

with open('txt.tsv') as tsvfile:
    reader = csv.DictReader(tsvfile, dialect='excel-tab')
    for row in reader:
        print(row)
",5,18,0,1,
928,49504886,49505622,49545,Converting a geopandas geodataframe into a pandas dataframe,1,<python><pandas><geopandas>,42,"<p>What is the most efficient way to convert a geopandas geodataframe into a pandas dataframe?  Below is the method I use, is there another method which is more efficient or better in general at not generating errors?</p>

<pre><code>import geopandas as gpd
import pandas as pd

# assuming I have a shapefile named shp1.shp
gdf1 = gpd.read_file('shp1.shp')

# then for the conversion, I drop the last column (geometry) and specify the column names for the new df
df1 = pd.DataFrame(gdf1.iloc[:,:-1].values, columns = list(gdf1.columns.values)[:-1] )
</code></pre>
",7644596,985,27-03-2018 05:18,27-03-2018 06:20,0,995,21,2,10,,"{'badge_counts': {'bronze': 21, 'silver': 10, 'gold': 2}, 'account_id': 10365406, 'is_employee': False, 'last_modified_date': 1701608400, 'last_access_date': 1711000423, 'reputation_change_year': 8, 'reputation_change_quarter': 8, 'reputation_change_month': 9, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 995, 'creation_date': 1488418523, 'user_type': 'registered', 'user_id': 7644596, 'location': 'Brisbane QLD, Australia', 'link': 'https://stackoverflow.com/users/7644596/jberrio', 'profile_image': 'https://www.gravatar.com/avatar/8c35dfbd61abcbf82e7cef2a1aebf0b9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'jberrio'}","What is the most efficient way to convert a geopandas geodataframe into a pandas dataframe? Below is the method I use, is there another method which is more efficient or better in general at not generating errors?","import geopandas as gpd
import pandas as pd

# assuming I have a shapefile named shp1.shp
gdf1 = gpd.read_file('shp1.shp')

# then for the conversion, I drop the last column (geometry) and specify the column names for the new df
df1 = pd.DataFrame(gdf1.iloc[:,:-1].values, columns = list(gdf1.columns.values)[:-1] )
",7,11,0,0,
929,50022252,50022310,5500,What's the meaning of `f` and `m` in PyCharm auto-completion?,3,<python><pycharm>,12,"<p>In the PyCharm, the auto-completion there are <code>f</code> and <code>m</code>. </p>

<p>What's the meaning of <code>f</code> and <code>m</code>?</p>

<p><kbd><a href=""https://i.stack.imgur.com/4TEZk.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4TEZk.jpg"" alt=""enter image description here""></a></kbd></p>

<p>Does the <code>f</code> means function?</p>
",7646621,3452,25-04-2018 12:22,25-04-2018 12:25,0,3482,84,13,43,63,"{'badge_counts': {'bronze': 84, 'silver': 43, 'gold': 13}, 'account_id': 10368178, 'is_employee': False, 'last_modified_date': 1709950200, 'last_access_date': 1705048607, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 40, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 3482, 'creation_date': 1488449358, 'user_type': 'registered', 'user_id': 7646621, 'accept_rate': 63, 'website_url': '', 'link': 'https://stackoverflow.com/users/7646621/qg-java-17137', 'profile_image': 'https://i.stack.imgur.com/L3kxr.jpg?s=256&g=1', 'display_name': 'qg_java_17137'}","In the PyCharm, the auto-completion there are and . What's the meaning of and ? Does the means function?",f m f m f,-5,7,1,1,
930,50014827,50017842,47391,Write Python DataFrame as CSV into Azure Blob,5,<python><azure><azure-storage><azure-blob-storage>,16,"<p>I have got two questions on reading and writing Python objects from/to Azure blob storage.</p>
<ol>
<li><p>Can someone tell me how to write Python dataframe as csv file directly into Azure Blob without storing it locally?</p>
<p>I tried using the functions <code>create_blob_from_text</code> and <code>create_blob_from_stream</code> but none of them works.</p>
<p>Converting dataframe to string and using <code>create_blob_from_text</code> function
writes the file into the blob but as a plain string but not as csv.</p>
<pre><code>df_b = df.to_string()
block_blob_service.create_blob_from_text('test', 'OutFilePy.csv', df_b)  
</code></pre>
</li>
<li><p>How to directly read a json file in Azure blob storage directly into Python?</p>
</li>
</ol>
",7676703,955,25-04-2018 05:47,25-04-2018 08:47,0,955,41,4,19,73,"{'badge_counts': {'bronze': 41, 'silver': 19, 'gold': 4}, 'account_id': 9090225, 'is_employee': False, 'last_modified_date': 1706943600, 'last_access_date': 1708309985, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 955, 'creation_date': 1488956986, 'user_type': 'registered', 'user_id': 7676703, 'accept_rate': 73, 'website_url': '', 'link': 'https://stackoverflow.com/users/7676703/angisen', 'profile_image': 'https://www.gravatar.com/avatar/360bb426ba9ff8d9315f15661fb71bd0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'AngiSen'}",I have got two questions on reading and writing Python objects from/to Azure blob storage. Can someone tell me how to write Python dataframe as csv file directly into Azure Blob without storing it locally? I tried using the functions and but none of them works. Converting dataframe to string and using function writes the file into the blob but as a plain string but not as csv. How to directly read a json file in Azure blob storage directly into Python?,"create_blob_from_text create_blob_from_stream create_blob_from_text df_b = df.to_string()
block_blob_service.create_blob_from_text('test', 'OutFilePy.csv', df_b)  
",-2,13,0,0,
931,48134598,48134620,68825,x.shape[0] vs x[0].shape in NumPy,4,<python><arrays><numpy>,16,"<p>Let say, I have an array with </p>

<p><code>x.shape = (10,1024)</code></p>

<p>when I try to print x[0].shape</p>

<pre><code>x[0].shape
</code></pre>

<p>it prints 1024</p>

<p>and when I print x.shape[0] </p>

<pre><code>x.shape[0]
</code></pre>

<p>it prints 10</p>

<p>I know it's a silly question, and maybe there is another question like this, but can someone explain it to me ?</p>
",9181323,695,07-01-2018 05:27,07-01-2018 05:31,0,695,16,1,11,,"{'badge_counts': {'bronze': 16, 'silver': 11, 'gold': 1}, 'account_id': 12629131, 'is_employee': False, 'last_modified_date': 1624703400, 'last_access_date': 1711165619, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 695, 'creation_date': 1515246796, 'user_type': 'registered', 'user_id': 9181323, 'link': 'https://stackoverflow.com/users/9181323/kevin-chandra', 'profile_image': 'https://www.gravatar.com/avatar/5b4275bafecbb1f8e9600461fd63a4d2?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Kevin Chandra'}","Let say, I have an array with when I try to print x[0].shape it prints 1024 and when I print x.shape[0] it prints 10 I know it's a silly question, and maybe there is another question like this, but can someone explain it to me ?","x.shape = (10,1024) x[0].shape
 x.shape[0]
",-1,19,0,0,
932,49503748,49504376,29156,Save and load model optimizer state,7,<python><tensorflow><machine-learning><keras>,26,"<p>I have a set of fairly complicated models that I am training and I am looking for a way to save and load the model optimizer states. The ""trainer models"" consist of different combinations of several other ""weight models"", of which some have shared weights, some have frozen weights depending on the trainer, etc. It is a bit too complicated of an example to share, but in short, I am not able to use <code>model.save('model_file.h5')</code> and <code>keras.models.load_model('model_file.h5')</code> when stopping and starting my training. </p>

<p>Using <code>model.load_weights('weight_file.h5')</code> works fine for testing my model if the training has finished, but if I attempt to continue training the model using this method, the loss does not come even close to returning to its last location. I have read that this is because the optimizer state is not saved using this method which makes sense. However, I need a method for saving and loading the states of the optimizers of my trainer models. It seems as though keras once had a <code>model.optimizer.get_sate()</code> and <code>model.optimizer.set_sate()</code> that would accomplish what I am after, but that does not seem to be the case anymore (at least for the Adam optimizer). Are there any other solutions with the current Keras?</p>
",7687401,847,27-03-2018 03:06,27-03-2018 04:29,0,847,22,2,8,,"{'badge_counts': {'bronze': 22, 'silver': 8, 'gold': 2}, 'account_id': 10426227, 'is_employee': False, 'last_modified_date': 1618851047, 'last_access_date': 1585866462, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 847, 'creation_date': 1489102886, 'user_type': 'registered', 'user_id': 7687401, 'location': 'Victoria, BC, Canada', 'link': 'https://stackoverflow.com/users/7687401/starnetter', 'profile_image': 'https://www.gravatar.com/avatar/5fa467cb7520dd5008f226f934701c41?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Starnetter'}","I have a set of fairly complicated models that I am training and I am looking for a way to save and load the model optimizer states. The ""trainer models"" consist of different combinations of several other ""weight models"", of which some have shared weights, some have frozen weights depending on the trainer, etc. It is a bit too complicated of an example to share, but in short, I am not able to use and when stopping and starting my training. Using works fine for testing my model if the training has finished, but if I attempt to continue training the model using this method, the loss does not come even close to returning to its last location. I have read that this is because the optimizer state is not saved using this method which makes sense. However, I need a method for saving and loading the states of the optimizers of my trainer models. It seems as though keras once had a and that would accomplish what I am after, but that does not seem to be the case anymore (at least for the Adam optimizer). Are there any other solutions with the current Keras?",model.save('model_file.h5') keras.models.load_model('model_file.h5') model.load_weights('weight_file.h5') model.optimizer.get_sate() model.optimizer.set_sate(),-5,3,0,0,
933,48234672,48711917,8865,How to use same unit test for different implementations in python?,4,<python><unit-testing><automated-tests><tdd><python-unittest>,11,"<p>I am developing multiple functions that answer a same problem but using different algorithm.</p>

<p>So the same input for all functions should generate the same output, that's why I wnted to use the same unit tests instead of having to create multiple tests with the same logic.</p>

<p>I was using the <strong>Python unittest framework</strong>, and I wanted to use an <em>abstract test class</em> to have the generic tests defined with a <em>function</em> variable so that I could just instantiate that generic function with the one I want to test in another normal test class. But it seems I can't instantiate the <em>function</em> variable in the child class.</p>

<p>So here is an example abstract class with generic tests for multiple functions.</p>

<pre><code>class AbstractTestCase():

    def test_generic_input_one(self):
        result = self.function(""input 1"")
        self.assertFalse(result)

    def test_generic_input_two(self):
        result = self.function(""input 2"")
        self.assertTrue(result)
</code></pre>

<p>And here you would have a specific test class for the <code>function_a</code> that inherits the generic tests from the <code>AbstractTestCase</code> class and that implements its own.</p>

<pre><code>class TestsFunctionA(AbstractTestCase, unittest.TestCase):

    def setUp(self):
        self.function = function_a

    def test_specific_input(self):
        result = self.assertTrue(self.function(""specific input""))
        self.assertTrue(result)
</code></pre>

<p>I am pretty sure it can be done, but I can't seem to find an example to see how to implement it. I would like to avoid code duplication.</p>

<p>What should be the simplest and best way to do it ?</p>
",7747942,6351,12-01-2018 22:06,09-02-2018 18:21,28,6371,83,8,69,,"{'badge_counts': {'bronze': 83, 'silver': 69, 'gold': 8}, 'account_id': 10512827, 'is_employee': False, 'last_modified_date': 1698181800, 'last_access_date': 1709928173, 'reputation_change_year': 208, 'reputation_change_quarter': 208, 'reputation_change_month': 40, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 6371, 'creation_date': 1490128743, 'user_type': 'registered', 'user_id': 7747942, 'location': 'Canada', 'website_url': 'https://sylhare.github.io/', 'link': 'https://stackoverflow.com/users/7747942/sylhare', 'profile_image': 'https://lh6.googleusercontent.com/-5jXKyAbmi_M/AAAAAAAAAAI/AAAAAAAAAFQ/AewmU0ev_68/photo.jpg?sz=256', 'display_name': 'Sylhare'}","I am developing multiple functions that answer a same problem but using different algorithm. So the same input for all functions should generate the same output, that's why I wnted to use the same unit tests instead of having to create multiple tests with the same logic. I was using the Python unittest framework, and I wanted to use an abstract test class to have the generic tests defined with a function variable so that I could just instantiate that generic function with the one I want to test in another normal test class. But it seems I can't instantiate the function variable in the child class. So here is an example abstract class with generic tests for multiple functions. And here you would have a specific test class for the that inherits the generic tests from the class and that implements its own. I am pretty sure it can be done, but I can't seem to find an example to see how to implement it. I would like to avoid code duplication. What should be the simplest and best way to do it ?","class AbstractTestCase():

    def test_generic_input_one(self):
        result = self.function(""input 1"")
        self.assertFalse(result)

    def test_generic_input_two(self):
        result = self.function(""input 2"")
        self.assertTrue(result)
 function_a AbstractTestCase class TestsFunctionA(AbstractTestCase, unittest.TestCase):

    def setUp(self):
        self.function = function_a

    def test_specific_input(self):
        result = self.assertTrue(self.function(""specific input""))
        self.assertTrue(result)
",13,34,0,0,
934,49795525,49795599,40863,TypeError: Object of type 'int32' is not JSON serializable,3,<python><ajax><django><django-views>,14,"<p>I have to select those symptoms which is in the dictionary with the symptoms already posted.It works fine.But for some symptoms typeError is showing in command prompt and also all are getting printed in command prompt but not in html page.
Here is my code</p>

<p>views.py</p>

<pre><code>def predict(request):
sym=request.POST.getlist('symptoms[]')
sym=list(map(int,sym))
diseaseArray=[]
diseaseArray=np.array(diseaseArray,dtype=int)
dictArray=[]
for dicti in dictionary:
    if (set(sym)&lt;= set(dicti['symptoms']) and len(sym)!= 0) or [x for x in sym if x in dicti['primary']]:
        diseaseArray=np.append(diseaseArray,dicti['primary'])
        diseaseArray=np.append(diseaseArray,dicti['symptoms'])
diseaseArray=list(set(diseaseArray))
print(diseaseArray)
for i in diseaseArray:
    if i not in sym:
        dict={'id':i}
        dictArray.append(dict)
        print(dictArray)
for j in dictArray:
    symptoms=Symptom.objects.get(syd=j['id'])
    j['name']=symptoms.symptoms
    print(j['name'])
print(len(dictArray))
return JsonResponse(dictArray,safe=False)
</code></pre>

<p>template</p>

<pre><code>$('.js-example-basic-multiple').change(function(){
  $('#suggestion-list').html('');
  $('#suggestion').removeClass('invisible');

  $.ajax({
  url:""/predict"",
  method:""post"",
  data:{
    symptoms: $('.js-example-basic-multiple').val(),
  },
  success: function(data){

      data.forEach(function(disease){
      console.log(disease.name)
        $('#suggestion-list').append('&lt;li&gt;'+disease.name+'&lt;li&gt;')
        $('#suggestion-list').removeClass('invisible');
    });



  }

  });
</code></pre>
",7765871,261,12-04-2018 11:36,12-04-2018 11:40,0,261,16,1,3,44,"{'badge_counts': {'bronze': 16, 'silver': 3, 'gold': 1}, 'account_id': 10539112, 'is_employee': False, 'last_modified_date': 1658538302, 'last_access_date': 1661800840, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 261, 'creation_date': 1490435225, 'user_type': 'registered', 'user_id': 7765871, 'accept_rate': 44, 'location': 'Kerala, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/7765871/najmath', 'profile_image': 'https://i.stack.imgur.com/JbulL.jpg?s=256&g=1', 'display_name': 'najmath'}",I have to select those symptoms which is in the dictionary with the symptoms already posted.It works fine.But for some symptoms typeError is showing in command prompt and also all are getting printed in command prompt but not in html page. Here is my code views.py template,"def predict(request):
sym=request.POST.getlist('symptoms[]')
sym=list(map(int,sym))
diseaseArray=[]
diseaseArray=np.array(diseaseArray,dtype=int)
dictArray=[]
for dicti in dictionary:
    if (set(sym)&lt;= set(dicti['symptoms']) and len(sym)!= 0) or [x for x in sym if x in dicti['primary']]:
        diseaseArray=np.append(diseaseArray,dicti['primary'])
        diseaseArray=np.append(diseaseArray,dicti['symptoms'])
diseaseArray=list(set(diseaseArray))
print(diseaseArray)
for i in diseaseArray:
    if i not in sym:
        dict={'id':i}
        dictArray.append(dict)
        print(dictArray)
for j in dictArray:
    symptoms=Symptom.objects.get(syd=j['id'])
    j['name']=symptoms.symptoms
    print(j['name'])
print(len(dictArray))
return JsonResponse(dictArray,safe=False)
 $('.js-example-basic-multiple').change(function(){
  $('#suggestion-list').html('');
  $('#suggestion').removeClass('invisible');

  $.ajax({
  url:""/predict"",
  method:""post"",
  data:{
    symptoms: $('.js-example-basic-multiple').val(),
  },
  success: function(data){

      data.forEach(function(disease){
      console.log(disease.name)
        $('#suggestion-list').append('&lt;li&gt;'+disease.name+'&lt;li&gt;')
        $('#suggestion-list').removeClass('invisible');
    });



  }

  });
",44,56,0,0,
935,49048111,49048223,72134,How to get the duration of video using OpenCV,6,<python><opencv><video>,37,"<p>I can only get the number of frames <code>CAP_PROP_FRAME_COUNT</code> using OpenCV.</p>
<p>However, I cannot find the parameter to get the duration of the video using OpenCV.</p>
<p>How to do that?</p>
<p>Thank you very much.</p>
",7819796,774,01-03-2018 10:54,01-03-2018 11:01,0,784,15,1,9,75,"{'badge_counts': {'bronze': 15, 'silver': 9, 'gold': 1}, 'account_id': 9578760, 'is_employee': False, 'last_modified_date': 1691199000, 'last_access_date': 1665725297, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 784, 'creation_date': 1491387196, 'user_type': 'registered', 'user_id': 7819796, 'accept_rate': 75, 'link': 'https://stackoverflow.com/users/7819796/frankie', 'profile_image': 'https://www.gravatar.com/avatar/015f91bd8b6aeaca2829199d9b7a6782?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Frankie'}","I can only get the number of frames using OpenCV. However, I cannot find the parameter to get the duration of the video using OpenCV. How to do that? Thank you very much.",CAP_PROP_FRAME_COUNT,-1,4,0,0,
936,48541444,48590850,20905,Pandas filtering for multiple substrings in series,3,<python><string><pandas><dataframe><series>,46,"<p>I need to filter rows in a <code>pandas</code> dataframe so that a specific string column contains at least one of a list of provided substrings. The substrings may have unusual / regex characters. The comparison should not involve regex and is case insensitive.</p>

<p>For example:</p>

<pre><code>lst = ['kdSj;af-!?', 'aBC+dsfa?\-', 'sdKaJg|dksaf-*']
</code></pre>

<p>I currently apply the mask like this:</p>

<pre><code>mask = np.logical_or.reduce([df[col].str.contains(i, regex=False, case=False) for i in lst])
df = df[mask]
</code></pre>

<p>My dataframe is large (~1mio rows) and <code>lst</code> has length 100. Is there a more efficient way? For example, if the first item in <code>lst</code> is found, we should not have to test any subsequent strings for that row.</p>
",9209546,161813,31-01-2018 11:48,02-02-2018 21:04,2,162051,348,35,286,100,"{'badge_counts': {'bronze': 348, 'silver': 286, 'gold': 35}, 'account_id': 12672477, 'is_employee': False, 'last_modified_date': 1710009300, 'last_access_date': 1710982305, 'reputation_change_year': 884, 'reputation_change_quarter': 884, 'reputation_change_month': 294, 'reputation_change_week': 120, 'reputation_change_day': 0, 'reputation': 162051, 'creation_date': 1515768442, 'user_type': 'registered', 'user_id': 9209546, 'accept_rate': 100, 'location': 'London, UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/9209546/jpp', 'profile_image': 'https://i.stack.imgur.com/qk9vC.jpg?s=256&g=1', 'display_name': 'jpp'}","I need to filter rows in a dataframe so that a specific string column contains at least one of a list of provided substrings. The substrings may have unusual / regex characters. The comparison should not involve regex and is case insensitive. For example: I currently apply the mask like this: My dataframe is large (~1mio rows) and has length 100. Is there a more efficient way? For example, if the first item in is found, we should not have to test any subsequent strings for that row.","pandas lst = ['kdSj;af-!?', 'aBC+dsfa?\-', 'sdKaJg|dksaf-*']
 mask = np.logical_or.reduce([df[col].str.contains(i, regex=False, case=False) for i in lst])
df = df[mask]
 lst lst",-2,14,0,0,
937,49757871,49758140,5039,pd.Timestamp versus np.datetime64: are they interchangeable for selected uses?,1,<python><arrays><pandas><numpy><datetime>,16,"<p>This question is motivated by <a href=""https://stackoverflow.com/a/49719901/9209546"">an answer</a> to a <a href=""https://stackoverflow.com/questions/49719627/boolean-mask-from-pandas-datetime-index-using-loc-accessor"">question on improving performance</a> when performing comparisons with <code>DatetimeIndex</code> in <code>pandas</code>.</p>

<p>The solution converts the <code>DatetimeIndex</code> to a <code>numpy</code> array via <code>df.index.values</code> and compares the array to a <code>np.datetime64</code> object. This appears to be the most efficient way to retrieve the Boolean array from this comparison.</p>

<p>The feedback on this question from one of the developers of <code>pandas</code> was: ""These are not the same generally. Offering up a numpy solution is often a special case and not recommended.""</p>

<p>My questions are:</p>

<ol>
<li>Are they interchangeable for a subset of operations? I appreciate
<code>DatetimeIndex</code> offers more functionality, but I require only basic functionality such as slicing and indexing.</li>
<li>Are there any documented differences in <em>result</em> for operations that are translatable to <code>numpy</code>?</li>
</ol>

<p>In my research, I found some posts which mention ""not always compatible"" - but none of them seem to have any conclusive references / documentation, or specify why/when generally they are incompatible. Many other posts use the <code>numpy</code> representation without comment.</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/38676418/pandas-datetimeindex-indexing-dtype-datetime64-vs-timestamp"">Pandas DatetimeIndex indexing dtype: datetime64 vs Timestamp</a></li>
<li><a href=""https://stackoverflow.com/questions/13227865/how-to-convert-from-pandas-datetimeindex-to-numpy-datetime64"">How to convert from pandas.DatetimeIndex to numpy.datetime64?</a></li>
</ul>
",9209546,161813,10-04-2018 15:44,10-04-2018 16:00,0,162051,348,35,286,100,"{'badge_counts': {'bronze': 348, 'silver': 286, 'gold': 35}, 'account_id': 12672477, 'is_employee': False, 'last_modified_date': 1710009300, 'last_access_date': 1710982305, 'reputation_change_year': 884, 'reputation_change_quarter': 884, 'reputation_change_month': 294, 'reputation_change_week': 120, 'reputation_change_day': 0, 'reputation': 162051, 'creation_date': 1515768442, 'user_type': 'registered', 'user_id': 9209546, 'accept_rate': 100, 'location': 'London, UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/9209546/jpp', 'profile_image': 'https://i.stack.imgur.com/qk9vC.jpg?s=256&g=1', 'display_name': 'jpp'}","This question is motivated by an answer to a question on improving performance when performing comparisons with in . The solution converts the to a array via and compares the array to a object. This appears to be the most efficient way to retrieve the Boolean array from this comparison. The feedback on this question from one of the developers of was: ""These are not the same generally. Offering up a numpy solution is often a special case and not recommended."" My questions are: Are they interchangeable for a subset of operations? I appreciate offers more functionality, but I require only basic functionality such as slicing and indexing. Are there any documented differences in result for operations that are translatable to ? In my research, I found some posts which mention ""not always compatible"" - but none of them seem to have any conclusive references / documentation, or specify why/when generally they are incompatible. Many other posts use the representation without comment. Pandas DatetimeIndex indexing dtype: datetime64 vs Timestamp How to convert from pandas.DatetimeIndex to numpy.datetime64?",DatetimeIndex pandas DatetimeIndex numpy df.index.values np.datetime64 pandas DatetimeIndex numpy numpy,-10,20,0,4,
938,49259580,49259581,13972,Replace values in a pandas series via dictionary efficiently,1,<python><python-3.x><pandas><performance><dictionary>,31,"<p>How to replace values in a Pandas series <code>s</code> via a dictionary <code>d</code> has been asked and re-asked many times.</p>

<p>The recommended method (<a href=""https://stackoverflow.com/a/40855794/9209546"">1</a>, <a href=""https://stackoverflow.com/a/40528697/9209546"">2</a>, <a href=""https://stackoverflow.com/a/45355744/9209546"">3</a>, <a href=""https://stackoverflow.com/a/20250996/9209546"">4</a>) is to either use <code>s.replace(d)</code> or, occasionally, use <code>s.map(d)</code> if all your series values are found in the dictionary keys.</p>

<p>However, performance using <code>s.replace</code> is often unreasonably slow, often 5-10x slower than a simple list comprehension.</p>

<p>The alternative, <code>s.map(d)</code> has good performance, but is only recommended when all keys are found in the dictionary.</p>

<p>Why is <code>s.replace</code> so slow and how can performance be improved?</p>

<pre><code>import pandas as pd, numpy as np

df = pd.DataFrame({'A': np.random.randint(0, 1000, 1000000)})
lst = df['A'].values.tolist()

##### TEST 1 #####

d = {i: i+1 for i in range(1000)}

%timeit df['A'].replace(d)                          # 1.98s
%timeit [d[i] for i in lst]                         # 134ms

##### TEST 2 #####

d = {i: i+1 for i in range(10)}

%timeit df['A'].replace(d)                          # 20.1ms
%timeit [d.get(i, i) for i in lst]                  # 243ms
</code></pre>

<p><strong>Note:</strong> This question is not marked as a duplicate because it is looking for specific advice on <em>when to use</em> different methods given different datasets. This is explicit in the answer and is an aspect not usually addressed in other questions.</p>
",9209546,161813,13-03-2018 15:08,13-03-2018 15:08,0,162051,348,35,286,100,"{'badge_counts': {'bronze': 348, 'silver': 286, 'gold': 35}, 'account_id': 12672477, 'is_employee': False, 'last_modified_date': 1710009300, 'last_access_date': 1710982305, 'reputation_change_year': 884, 'reputation_change_quarter': 884, 'reputation_change_month': 294, 'reputation_change_week': 120, 'reputation_change_day': 0, 'reputation': 162051, 'creation_date': 1515768442, 'user_type': 'registered', 'user_id': 9209546, 'accept_rate': 100, 'location': 'London, UK', 'website_url': '', 'link': 'https://stackoverflow.com/users/9209546/jpp', 'profile_image': 'https://i.stack.imgur.com/qk9vC.jpg?s=256&g=1', 'display_name': 'jpp'}","How to replace values in a Pandas series via a dictionary has been asked and re-asked many times. The recommended method (1, 2, 3, 4) is to either use or, occasionally, use if all your series values are found in the dictionary keys. However, performance using is often unreasonably slow, often 5-10x slower than a simple list comprehension. The alternative, has good performance, but is only recommended when all keys are found in the dictionary. Why is so slow and how can performance be improved? Note: This question is not marked as a duplicate because it is looking for specific advice on when to use different methods given different datasets. This is explicit in the answer and is an aspect not usually addressed in other questions.","s d s.replace(d) s.map(d) s.replace s.map(d) s.replace import pandas as pd, numpy as np

df = pd.DataFrame({'A': np.random.randint(0, 1000, 1000000)})
lst = df['A'].values.tolist()

##### TEST 1 #####

d = {i: i+1 for i in range(1000)}

%timeit df['A'].replace(d)                          # 1.98s
%timeit [d[i] for i in lst]                         # 134ms

##### TEST 2 #####

d = {i: i+1 for i in range(10)}

%timeit df['A'].replace(d)                          # 20.1ms
%timeit [d.get(i, i) for i in lst]                  # 243ms
",10,31,0,4,
939,49585038,49586985,8986,"Cython: when should I define a string as char*, str, or bytes?",2,<python><string><python-3.x><cython>,13,"<p>When defining a variable type that will hold a string in Cython + Python 3, I can use (at least):</p>

<pre><code>cdef char* mystring = ""foo""
cdef str mystring = ""foo""
cdef bytes mystring = ""foo""
</code></pre>

<p>The <a href=""https://cython.readthedocs.io/en/latest/src/tutorial/strings.html"" rel=""noreferrer"">documentation page on strings</a> is unclear on this -- it mostly gives examples using char* and bytes, and frankly I'm having a lot of difficulty understanding it.</p>

<p>In my case the strings will be coming from a Python3 program and are assumed to be unicode. They will be used as dict keys and function arguments, but I will do no further manipulation on them.  Needless to say I am trying to maximize speed.</p>

<p><a href=""https://stackoverflow.com/questions/23064141/optimizing-strings-in-cython"">This question</a> suggests that under Python2.7 and without Unicode, typing as <code>str</code> makes string manipulation code run SLOWER than with no typing at all. (But that's not necessarily relevant here since I won't be doing much string manipulation.)</p>

<p>What are the advantages and disadvantages of each of these options?</p>
",8589122,865,31-03-2018 06:44,31-03-2018 11:01,0,865,14,0,8,,"{'badge_counts': {'bronze': 14, 'silver': 8, 'gold': 0}, 'account_id': 11133765, 'is_employee': False, 'last_modified_date': 1607614423, 'last_access_date': 1538890979, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 865, 'creation_date': 1505079850, 'user_type': 'registered', 'user_id': 8589122, 'link': 'https://stackoverflow.com/users/8589122/right2clicky', 'profile_image': 'https://www.gravatar.com/avatar/f6b37e4589ef9dbece584d99a8ab7e65?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'right2clicky'}","When defining a variable type that will hold a string in Cython + Python 3, I can use (at least): The documentation page on strings is unclear on this -- it mostly gives examples using char* and bytes, and frankly I'm having a lot of difficulty understanding it. In my case the strings will be coming from a Python3 program and are assumed to be unicode. They will be used as dict keys and function arguments, but I will do no further manipulation on them. Needless to say I am trying to maximize speed. This question suggests that under Python2.7 and without Unicode, typing as makes string manipulation code run SLOWER than with no typing at all. (But that's not necessarily relevant here since I won't be doing much string manipulation.) What are the advantages and disadvantages of each of these options?","cdef char* mystring = ""foo""
cdef str mystring = ""foo""
cdef bytes mystring = ""foo""
 str",1,14,0,2,
940,48266106,48266238,62378,Missing handler error in AWS Lambda,3,<python><amazon-web-services><aws-lambda>,23,"<p>My apologies for basic question. I am completely new to AWS as well as Python. I am trying to do sample code given in <a href=""https://boto3.readthedocs.io/en/latest/guide/migrations3.html#accessing-a-bucket"" rel=""noreferrer"">https://boto3.readthedocs.io/en/latest/guide/migrations3.html#accessing-a-bucket</a> but facing a error.</p>

<pre><code>import botocore
import boto3
s3 = boto3.resource('s3')
bucket = s3.Bucket('bucketname')
exists = True


try:
    s3.meta.client.head_bucket(Bucket='bucketname')
except botocore.exceptions.ClientError as e:
    # If a client error is thrown, then check that it was a 404 error.
    # If it was a 404 error, then the bucket does not exist.
    error_code = int(e.response['Error']['Code'])
    if error_code == 404:
        exists = False 
</code></pre>

<p>Error in logs is</p>

<blockquote>
  <p>""errorMessage"": ""Handler 'lambda_handler' missing on module
  'lambda_function'""</p>
</blockquote>
",9220116,233,15-01-2018 15:36,15-01-2018 15:43,0,233,4,1,2,,"{'badge_counts': {'bronze': 4, 'silver': 2, 'gold': 1}, 'account_id': 12689478, 'is_employee': False, 'last_modified_date': 1573678423, 'last_access_date': 1516439688, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 233, 'creation_date': 1516029789, 'user_type': 'registered', 'user_id': 9220116, 'link': 'https://stackoverflow.com/users/9220116/chpsam', 'profile_image': 'https://www.gravatar.com/avatar/453fd6dbac8fb621f17dc2278501f125?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'chpsam'}","My apologies for basic question. I am completely new to AWS as well as Python. I am trying to do sample code given in https://boto3.readthedocs.io/en/latest/guide/migrations3.html#accessing-a-bucket but facing a error. Error in logs is ""errorMessage"": ""Handler 'lambda_handler' missing on module 'lambda_function'""","import botocore
import boto3
s3 = boto3.resource('s3')
bucket = s3.Bucket('bucketname')
exists = True


try:
    s3.meta.client.head_bucket(Bucket='bucketname')
except botocore.exceptions.ClientError as e:
    # If a client error is thrown, then check that it was a 404 error.
    # If it was a 404 error, then the bucket does not exist.
    error_code = int(e.response['Error']['Code'])
    if error_code == 404:
        exists = False 
",14,25,0,1,
941,49844925,49844955,39677,Openpyxl.utils.exceptions.IllegalcharacterError,5,<python><openpyxl>,21,"<p>I have the following python code to write processed words into excel file. The words are about 7729</p>
<pre><code>From openpyxl import *
book=Workbook ()
sheet=book.active
sheet.title=&quot;test&quot;
for x in range (7729):
    sheet.cell (row=1,column=x+1).value=x
book.save ('test.xlsx')
</code></pre>
<p>This is the what the code I used looks like, but when I run it, it gives me an error that says</p>
<pre><code>openpyxl.utils.exceptions.IllegalCharacterError
</code></pre>
<p>This is my first time using this module, I would appreciate any kind of help.</p>
",8616724,897,15-04-2018 17:38,15-04-2018 17:42,0,897,30,1,7,,"{'badge_counts': {'bronze': 30, 'silver': 7, 'gold': 1}, 'account_id': 11774863, 'is_employee': False, 'last_modified_date': 1692007800, 'last_access_date': 1689052302, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 897, 'creation_date': 1505508387, 'user_type': 'registered', 'user_id': 8616724, 'website_url': '', 'link': 'https://stackoverflow.com/users/8616724/ehm', 'profile_image': 'https://www.gravatar.com/avatar/a3b4df11f7c98e9c2074e798e7489b53?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'EHM'}","I have the following python code to write processed words into excel file. The words are about 7729 This is the what the code I used looks like, but when I run it, it gives me an error that says This is my first time using this module, I would appreciate any kind of help.","From openpyxl import *
book=Workbook ()
sheet=book.active
sheet.title=&quot;test&quot;
for x in range (7729):
    sheet.cell (row=1,column=x+1).value=x
book.save ('test.xlsx')
 openpyxl.utils.exceptions.IllegalCharacterError
",6,13,0,0,
942,48128714,48128826,99243,How to make an Inner Join in django?,5,<python><mysql><django><orm><inner-join>,39,"<p>I want to show in an Html the name of the city, state, and country of a publication. But they are in different tables.</p>

<p>Here is my <strong>models.py</strong></p>

<pre><code>class country(models.Model):
    country_name = models.CharField(max_length=200, null=True)
    country_subdomain = models.CharField(max_length=3, null=True)
    def __str__(self):
        return self.country_name

class countrystate(models.Model):
    state_name = models.CharField(max_length=200, null=True)
    country = models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    importance = models.IntegerField(null=True)
    def __str__(self):
        return self.state_name

class city(models.Model):
    city_name = models.CharField(max_length=200, null=True)
    countrystate = models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    def __str__(self):
        return self.city_name

class publication(models.Model):
    user = ForeignKey(users, on_delete=models.CASCADE, null=False)
    title= models.CharField(max_length=300, null=True)
    country=models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    countrystate=models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    city=models.ForeignKey(city, on_delete=models.CASCADE, null=True)

    def __str__(self):
        return self.title
</code></pre>

<p>Here is my views.py</p>

<pre><code>def publications(request):
    mypublications = publication.objects.filter(user_id=request.session['account_id'])
    dic.update({""plist"": mypublications })
    return render(request, 'blog/mypublications.html', dic)
</code></pre>

<p>In a django view, what is the equivalent of the next sql query?</p>

<pre><code>SELECT p.user_id, p.title, c.cuntry_id, c.country_name, s.state_id, s.state_name, y.city_id, y.city_name FROM publication AS p
INNER JOIN country AS c ON c.id = p.country_id
INNER JOIN countrystate AS s ON s.id = p.countrystate_id
INNER JOIN city AS y ON y.id = p.city_id
</code></pre>
",8747207,1419,06-01-2018 15:17,06-01-2018 15:29,0,1429,58,8,35,40,"{'badge_counts': {'bronze': 58, 'silver': 35, 'gold': 8}, 'account_id': 11954417, 'is_employee': False, 'last_modified_date': 1703319301, 'last_access_date': 1710882775, 'reputation_change_year': 80, 'reputation_change_quarter': 80, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1429, 'creation_date': 1507566047, 'user_type': 'registered', 'user_id': 8747207, 'accept_rate': 40, 'location': 'Guatemala', 'link': 'https://stackoverflow.com/users/8747207/sergio-mendez', 'profile_image': 'https://lh5.googleusercontent.com/-R1BCkXtE1t4/AAAAAAAAAAI/AAAAAAAAABk/04aQ8PYOW6s/photo.jpg?sz=256', 'display_name': 'Sergio Mendez'}","I want to show in an Html the name of the city, state, and country of a publication. But they are in different tables. Here is my models.py Here is my views.py In a django view, what is the equivalent of the next sql query?","class country(models.Model):
    country_name = models.CharField(max_length=200, null=True)
    country_subdomain = models.CharField(max_length=3, null=True)
    def __str__(self):
        return self.country_name

class countrystate(models.Model):
    state_name = models.CharField(max_length=200, null=True)
    country = models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    importance = models.IntegerField(null=True)
    def __str__(self):
        return self.state_name

class city(models.Model):
    city_name = models.CharField(max_length=200, null=True)
    countrystate = models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    def __str__(self):
        return self.city_name

class publication(models.Model):
    user = ForeignKey(users, on_delete=models.CASCADE, null=False)
    title= models.CharField(max_length=300, null=True)
    country=models.ForeignKey(country, on_delete=models.CASCADE, null=True)
    countrystate=models.ForeignKey(countrystate, on_delete=models.CASCADE, null=True)
    city=models.ForeignKey(city, on_delete=models.CASCADE, null=True)

    def __str__(self):
        return self.title
 def publications(request):
    mypublications = publication.objects.filter(user_id=request.session['account_id'])
    dic.update({""plist"": mypublications })
    return render(request, 'blog/mypublications.html', dic)
 SELECT p.user_id, p.title, c.cuntry_id, c.country_name, s.state_id, s.state_name, y.city_id, y.city_name FROM publication AS p
INNER JOIN country AS c ON c.id = p.country_id
INNER JOIN countrystate AS s ON s.id = p.countrystate_id
INNER JOIN city AS y ON y.id = p.city_id
",33,49,0,0,
943,49008074,49009008,33115,How to create a neural network for regression?,1,<python><numpy><machine-learning><neural-network><keras>,17,"<p>I am trying to use Keras to make a neural network. The data I am using is <a href=""https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics"" rel=""noreferrer"">https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics</a>. My code is as follows:</p>

<pre><code>import numpy as np
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split

data = np.genfromtxt(r""""""file location"""""", delimiter=',')

model = Sequential()
model.add(Dense(32, activation = 'relu', input_dim = 6))
model.add(Dense(1,))
model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])

Y = data[:,-1]
X = data[:, :-1]
</code></pre>

<p>From here I have tried using model.fit(X, Y), but the accuracy of the model appears to remain at 0. I am new to Keras so this is probably an easy solution, apologies in advance.</p>

<p>My question is what is the best way to add regression to the model so that the accuracy increases? Thanks in advance.</p>
",8788938,173,27-02-2018 11:53,27-02-2018 12:42,0,173,5,1,1,,"{'badge_counts': {'bronze': 5, 'silver': 1, 'gold': 1}, 'account_id': 12012877, 'is_employee': False, 'last_modified_date': 1573678498, 'last_access_date': 1558345788, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 173, 'creation_date': 1508230776, 'user_type': 'registered', 'user_id': 8788938, 'link': 'https://stackoverflow.com/users/8788938/es1927', 'profile_image': 'https://www.gravatar.com/avatar/aa3af0f3baf16b6c980dcbd780c46e98?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ES1927'}","I am trying to use Keras to make a neural network. The data I am using is https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics. My code is as follows: From here I have tried using model.fit(X, Y), but the accuracy of the model appears to remain at 0. I am new to Keras so this is probably an easy solution, apologies in advance. My question is what is the best way to add regression to the model so that the accuracy increases? Thanks in advance.","import numpy as np
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split

data = np.genfromtxt(r""""""file location"""""", delimiter=',')

model = Sequential()
model.add(Dense(32, activation = 'relu', input_dim = 6))
model.add(Dense(1,))
model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])

Y = data[:,-1]
X = data[:, :-1]
",13,21,0,1,
944,48796169,48796572,110972,How to fix ipykernel_launcher.py: error: unrecognized arguments in jupyter?,9,<python><python-3.x><tensorflow><jupyter-notebook><jupyter>,31,"<p>I am following this tensorflow <a href=""https://www.tensorflow.org/get_started/get_started_for_beginners"" rel=""noreferrer"">tutorial</a> after two days setting up the environment I finally could run <code>premade_estimator.py</code> using cmd</p>

<p><a href=""https://i.stack.imgur.com/DmlmX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DmlmX.png"" alt=""""></a></p>

<p>but when I try to run the same code in a jupyter notebook I am getting this error:</p>

<blockquote>
<pre><code>usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]
                             [--train_steps TRAIN_STEPS]

ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json
</code></pre>
  
  <p>An exception has occurred, use %tb to see the full traceback.</p>

<pre><code>SystemExit: 2

C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: 
UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
</code></pre>
</blockquote>

<p>I have tried to fix it without success using:</p>

<pre><code>pip install --ignore-installed --upgrade jupyter

pip install ipykernel
python -m ipykernel install

conda install notebook ipykernel
ipython kernelspec install-self
</code></pre>

<p>Any idea will be appreciate! Thanks!</p>
",8828524,2373,14-02-2018 21:03,14-02-2018 21:30,0,2373,33,3,15,,"{'badge_counts': {'bronze': 33, 'silver': 15, 'gold': 3}, 'account_id': 12074496, 'is_employee': False, 'last_modified_date': 1666222200, 'last_access_date': 1710963531, 'reputation_change_year': 40, 'reputation_change_quarter': 40, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 2373, 'creation_date': 1508890906, 'user_type': 'registered', 'user_id': 8828524, 'location': 'Portland, OR, United States', 'website_url': 'https://www.youtube.com/channel/UCtFs5crf5WY8eMoWrSpONdg', 'link': 'https://stackoverflow.com/users/8828524/virtualdvid', 'profile_image': 'https://lh6.googleusercontent.com/-Z001KmBq-2Y/AAAAAAAAAAI/AAAAAAAAIlU/paK7lD3iHuw/photo.jpg?sz=256', 'display_name': 'virtualdvid'}","I am following this tensorflow tutorial after two days setting up the environment I finally could run using cmd but when I try to run the same code in a jupyter notebook I am getting this error: An exception has occurred, use %tb to see the full traceback. I have tried to fix it without success using: Any idea will be appreciate! Thanks!","premade_estimator.py usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]
                             [--train_steps TRAIN_STEPS]

ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json
 SystemExit: 2

C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: 
UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
 pip install --ignore-installed --upgrade jupyter

pip install ipykernel
python -m ipykernel install

conda install notebook ipykernel
ipython kernelspec install-self
",11,34,1,2,
945,49658308,49658338,21719,How does the logical `and` operator work with integers?,2,<python>,24,"<p>So, I was playing with the interpreter, and typed in the following:</p>

<pre><code>In [95]: 1 and 2
Out[95]: 2

In [96]: 1 and 5
Out[96]: 5

In [97]: 234324 and 2
Out[97]: 2

In [98]: 234324 and 22343243242
Out[98]: 22343243242L

In [99]: 1 or 2 and 9
Out[99]: 1
</code></pre>

<p>Initially I thought that it has to do with False and True values, because:</p>

<pre><code>In [101]: True + True
Out[101]: 2

In [102]: True * 5
Out[102]: 5
</code></pre>

<p>But that doesn't seem related, because False is always 0, and it seems from the trials above that it isn't the biggest value that is being outputted.</p>

<p>I can't see the pattern here honestly, and couldn't find anything in the documentation (honestly, I didn't really know how to effectively look for it). </p>

<p>So, how does </p>

<pre><code>int(x) [logical operation] int(y)
</code></pre>

<p>work in Python? </p>
",8840464,769,04-04-2018 18:49,04-04-2018 18:51,0,769,19,2,8,40,"{'badge_counts': {'bronze': 19, 'silver': 8, 'gold': 2}, 'account_id': 2158248, 'is_employee': False, 'last_modified_date': 1630678198, 'last_access_date': 1631559028, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 769, 'creation_date': 1509055915, 'user_type': 'registered', 'user_id': 8840464, 'accept_rate': 40, 'website_url': '', 'link': 'https://stackoverflow.com/users/8840464/oyed', 'profile_image': 'https://www.gravatar.com/avatar/6c30bc0d6e421b360c0b7019065ebbca?s=256&d=identicon&r=PG', 'display_name': 'oyed'}","So, I was playing with the interpreter, and typed in the following: Initially I thought that it has to do with False and True values, because: But that doesn't seem related, because False is always 0, and it seems from the trials above that it isn't the biggest value that is being outputted. I can't see the pattern here honestly, and couldn't find anything in the documentation (honestly, I didn't really know how to effectively look for it). So, how does work in Python?","In [95]: 1 and 2
Out[95]: 2

In [96]: 1 and 5
Out[96]: 5

In [97]: 234324 and 2
Out[97]: 2

In [98]: 234324 and 22343243242
Out[98]: 22343243242L

In [99]: 1 or 2 and 9
Out[99]: 1
 In [101]: True + True
Out[101]: 2

In [102]: True * 5
Out[102]: 5
 int(x) [logical operation] int(y)
",17,37,0,0,
946,48079364,56552098,4371,Wrapping text not working in matplotlib,1,<python><matplotlib><text>,15,"<p>I'm attempting to wrap text using wrap=True but it doesn't seem to be working for me. Running the example from matplotlib below:</p>

<pre><code>import matplotlib.pyplot as plt

fig = plt.figure()
plt.axis([0, 10, 0, 10])
t = ""This is a really long string that I'd rather have wrapped so that it""\
    "" doesn't go outside of the figure, but if it's long enough it will go""\
    "" off the top or bottom!""
plt.text(4, 1, t, ha='left', rotation=15, wrap=True)
plt.text(6, 5, t, ha='left', rotation=15, wrap=True)
plt.text(5, 5, t, ha='right', rotation=-15, wrap=True)
plt.text(5, 10, t, fontsize=18, style='oblique', ha='center',
         va='top', wrap=True)
plt.text(3, 4, t, family='serif', style='italic', ha='right', wrap=True)
plt.text(-1, 0, t, ha='left', rotation=-15, wrap=True)

plt.show()
</code></pre>

<p>gets me this:</p>

<p><strong>Text wrapping gone wrong</strong></p>

<p><a href=""https://i.stack.imgur.com/YCagz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YCagz.png"" alt=""enter image description here""></a></p>

<p>Any ideas on what's the issue?</p>
",8879643,309,03-01-2018 14:29,11-06-2019 21:47,524,319,13,1,4,100,"{'badge_counts': {'bronze': 13, 'silver': 4, 'gold': 1}, 'account_id': 12162465, 'is_employee': False, 'last_modified_date': 1573678483, 'last_access_date': 1632547418, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 319, 'creation_date': 1509700925, 'user_type': 'registered', 'user_id': 8879643, 'accept_rate': 100, 'link': 'https://stackoverflow.com/users/8879643/atoe', 'profile_image': 'https://www.gravatar.com/avatar/2865f7828f727f40127b918f145b5256?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'AToe'}",I'm attempting to wrap text using wrap=True but it doesn't seem to be working for me. Running the example from matplotlib below: gets me this: Text wrapping gone wrong Any ideas on what's the issue?,"import matplotlib.pyplot as plt

fig = plt.figure()
plt.axis([0, 10, 0, 10])
t = ""This is a really long string that I'd rather have wrapped so that it""\
    "" doesn't go outside of the figure, but if it's long enough it will go""\
    "" off the top or bottom!""
plt.text(4, 1, t, ha='left', rotation=15, wrap=True)
plt.text(6, 5, t, ha='left', rotation=15, wrap=True)
plt.text(5, 5, t, ha='right', rotation=-15, wrap=True)
plt.text(5, 10, t, fontsize=18, style='oblique', ha='center',
         va='top', wrap=True)
plt.text(3, 4, t, family='serif', style='italic', ha='right', wrap=True)
plt.text(-1, 0, t, ha='left', rotation=-15, wrap=True)

plt.show()
",15,27,1,1,
947,48607319,48607346,38828,Rotating strings in Python,5,<python><string>,11,"<p>I was trying to make the string <code>HELLO</code> to <code>OHELL</code> in Python. But couldn't get any way to rotate it without working with loops. How to code for it in just 1-2 lines so that I could get the desired pattern?</p>
",8880799,123,04-02-2018 10:55,04-02-2018 10:58,0,123,6,1,1,,"{'badge_counts': {'bronze': 6, 'silver': 1, 'gold': 1}, 'account_id': 12163983, 'is_employee': False, 'last_modified_date': 1573678483, 'last_access_date': 1607345612, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 123, 'creation_date': 1509714694, 'user_type': 'registered', 'user_id': 8880799, 'link': 'https://stackoverflow.com/users/8880799/mohit-gidwani', 'profile_image': 'https://www.gravatar.com/avatar/71a733098f8c6f8c47a049ede79da543?s=256&d=identicon&r=PG', 'display_name': 'Mohit Gidwani'}",I was trying to make the string to in Python. But couldn't get any way to rotate it without working with loops. How to code for it in just 1-2 lines so that I could get the desired pattern?,HELLO OHELL,-2,1,0,0,
948,48051100,48052002,55278,python pandas merge multiple csv files,4,<python><pandas><csv><datetime>,15,"<p>I have around 600 csv file datasets, all have the very same column names [‘DateTime’, ‘Actual’, ‘Consensus’, ‘Previous’, ‘Revised’], all economic indicators and all-time series data sets.</p>

<p>the aim is to merge them all together in one csv file.</p>

<p>With ‘DateTime’ as an index.</p>

<p>The way I wanted this file to indexed in is the time line way which means let’s say the first event in the first csv dated in 12/18/2017 10:00:00 and first event in the second csv dated in 12/29/2017 09:00:00 and first event in the third csv dated in 12/20/2017 09:00:00.</p>

<p>So, I want to index them the later first and the newer after it, etc. despite the source csv it originally from.</p>

<p>I tried to merge just 3 of them as an experiment and the problem is the ‘DateTime’ because it prints the 3 of them together like this ('12/18/2017 10:00:00', '12/29/2017 09:00:00', '12/20/2017 09:00:00')
Here is the code:</p>

<pre><code>import pandas as pd


df1 = pd.read_csv(""E:\Business\Economic Indicators\Consumer Price Index - Core (YoY) - European Monetary Union.csv"")
df2 = pd.read_csv(""E:\Business\Economic Indicators\Private loans (YoY) - European Monetary Union.csv"")
df3 = pd.read_csv(""E:\Business\Economic Indicators\Current Account s.a - European Monetary Union.csv"")

df = pd.concat([df1, df2, df3], axis=1, join='inner')
df.set_index('DateTime', inplace=True)

print(df.head())
df.to_csv('df.csv')
</code></pre>
",8893169,615,01-01-2018 15:56,01-01-2018 18:03,0,615,22,3,9,89,"{'badge_counts': {'bronze': 22, 'silver': 9, 'gold': 3}, 'account_id': 12182001, 'is_employee': False, 'last_modified_date': 1689991500, 'last_access_date': 1664109326, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 615, 'creation_date': 1509958634, 'user_type': 'registered', 'user_id': 8893169, 'accept_rate': 89, 'location': 'Plano, TX, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/8893169/sayed-gouda', 'profile_image': 'https://www.gravatar.com/avatar/9ceab6a59b595e341df6c7b03fa4a0f8?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Sayed Gouda'}","I have around 600 csv file datasets, all have the very same column names [‘DateTime’, ‘Actual’, ‘Consensus’, ‘Previous’, ‘Revised’], all economic indicators and all-time series data sets. the aim is to merge them all together in one csv file. With ‘DateTime’ as an index. The way I wanted this file to indexed in is the time line way which means let’s say the first event in the first csv dated in 12/18/2017 10:00:00 and first event in the second csv dated in 12/29/2017 09:00:00 and first event in the third csv dated in 12/20/2017 09:00:00. So, I want to index them the later first and the newer after it, etc. despite the source csv it originally from. I tried to merge just 3 of them as an experiment and the problem is the ‘DateTime’ because it prints the 3 of them together like this ('12/18/2017 10:00:00', '12/29/2017 09:00:00', '12/20/2017 09:00:00') Here is the code:","import pandas as pd


df1 = pd.read_csv(""E:\Business\Economic Indicators\Consumer Price Index - Core (YoY) - European Monetary Union.csv"")
df2 = pd.read_csv(""E:\Business\Economic Indicators\Private loans (YoY) - European Monetary Union.csv"")
df3 = pd.read_csv(""E:\Business\Economic Indicators\Current Account s.a - European Monetary Union.csv"")

df = pd.concat([df1, df2, df3], axis=1, join='inner')
df.set_index('DateTime', inplace=True)

print(df.head())
df.to_csv('df.csv')
",11,26,0,0,
949,49308530,49308695,46157,Missing values in Time Series in python,4,<python><pandas><nan><imputation>,25,"<p>I have a time series dataframe, the dataframe is quite big and contain some missing values in the 2 columns('Humidity' and 'Pressure'). I would like to impute this missing values in a clever way, for example using the value of the nearest neighbor or the average of the previous and following timestamp.Is there an easy way to do it? I have tried with fancyimpute but the dataset contain around 180000 examples and give a memory error <a href=""https://i.stack.imgur.com/WCdT4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WCdT4.png"" alt=""enter image description here""></a></p>
",8901144,1225,15-03-2018 20:22,15-03-2018 20:32,0,1225,31,3,19,100,"{'badge_counts': {'bronze': 31, 'silver': 19, 'gold': 3}, 'account_id': 12195263, 'is_employee': False, 'last_modified_date': 1679706900, 'last_access_date': 1679686131, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1225, 'creation_date': 1510069245, 'user_type': 'registered', 'user_id': 8901144, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/8901144/marco', 'profile_image': 'https://graph.facebook.com/10213582154335791/picture?type=large', 'display_name': 'Marco'}","I have a time series dataframe, the dataframe is quite big and contain some missing values in the 2 columns('Humidity' and 'Pressure'). I would like to impute this missing values in a clever way, for example using the value of the nearest neighbor or the average of the previous and following timestamp.Is there an easy way to do it? I have tried with fancyimpute but the dataset contain around 180000 examples and give a memory error",,0,1,1,1,
950,50267185,50267205,30798,Iterate over pandas series,1,<python><pandas><series>,20,"<p>I want to travel round the series index</p>
<pre><code>In [44]: type(ed1)
Out[44]: pandas.core.series.Series

In [43]: for _, row  in ed1.iterrows():
...:     print(row.name)
</code></pre>
<p>and I get this error:</p>
<pre><code>  AttributeError: 'Series' object has no attribute 'iterrows'
</code></pre>
<p>Does series has any methods like iterrows?</p>
",8911320,233,10-05-2018 06:51,10-05-2018 06:52,0,233,9,1,2,,"{'badge_counts': {'bronze': 9, 'silver': 2, 'gold': 1}, 'account_id': 12209047, 'is_employee': False, 'last_modified_date': 1573678478, 'last_access_date': 1700458518, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 233, 'creation_date': 1510202797, 'user_type': 'registered', 'user_id': 8911320, 'link': 'https://stackoverflow.com/users/8911320/alan', 'profile_image': 'https://www.gravatar.com/avatar/b526af642e50eba7dbcc9eea91884a9b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Alan'}",I want to travel round the series index and I get this error: Does series has any methods like iterrows?,"In [44]: type(ed1)
Out[44]: pandas.core.series.Series

In [43]: for _, row  in ed1.iterrows():
...:     print(row.name)
   AttributeError: 'Series' object has no attribute 'iterrows'
",4,11,0,0,
951,48416511,48417012,64018,How to convert UTC to EST with Python and take care of daylight saving automatically?,5,<python><datetime><datetime-format><python-datetime>,31,"<p>If I have a bunch of data with date &amp; time in <code>UTC format</code>, how can I convert them to <code>EST</code>. </p>

<p>It can determine when they will be <code>-4(in summer)</code> and -<code>5(in winter)</code> automatically every year? 
Thanks</p>
",9008162,746,24-01-2018 06:57,24-01-2018 07:29,0,746,20,2,8,89,"{'badge_counts': {'bronze': 20, 'silver': 8, 'gold': 2}, 'account_id': 12348209, 'is_employee': False, 'last_modified_date': 1679487609, 'last_access_date': 1710099056, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 746, 'creation_date': 1511633672, 'user_type': 'registered', 'user_id': 9008162, 'accept_rate': 89, 'website_url': '', 'link': 'https://stackoverflow.com/users/9008162/saga', 'profile_image': 'https://www.gravatar.com/avatar/09ac12b6b900e64fa1c51907f1497abc?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'saga'}","If I have a bunch of data with date &amp; time in , how can I convert them to . It can determine when they will be and - automatically every year? Thanks",UTC format EST -4(in summer) 5(in winter),-4,4,0,0,
952,49481345,49481583,11637,Is there a way to automatically correct the color balance?,1,<python><image-processing><scikit-image>,11,"<p>I'm looking for a solution to automatically adjust the <a href=""https://en.wikipedia.org/wiki/Color_balance"" rel=""noreferrer"">color balance</a>?<br>
I would like to use the pictures for color analysis and comparison, therefore color balance is important.<br>
I was hoping a feature such as <a href=""http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html"" rel=""noreferrer"">histogram equalization with scikit-image</a> to correct and balance the colors. The result doesn't have to look good or natural, it must just always be the same.  </p>

<p>Does anyone have a solution ?</p>
",9027428,782,25-03-2018 21:53,25-03-2018 22:20,0,792,25,1,6,57,"{'badge_counts': {'bronze': 25, 'silver': 6, 'gold': 1}, 'account_id': 12382397, 'is_employee': False, 'last_modified_date': 1701481500, 'last_access_date': 1710195749, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 792, 'creation_date': 1511980515, 'user_type': 'registered', 'user_id': 9027428, 'accept_rate': 57, 'location': 'Northport, AL, United States', 'website_url': '', 'link': 'https://stackoverflow.com/users/9027428/laurent-r', 'profile_image': 'https://www.gravatar.com/avatar/43ff757561b140276d5a37323c8d5519?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Laurent R'}","I'm looking for a solution to automatically adjust the color balance? I would like to use the pictures for color analysis and comparison, therefore color balance is important. I was hoping a feature such as histogram equalization with scikit-image to correct and balance the colors. The result doesn't have to look good or natural, it must just always be the same. Does anyone have a solution ?",,0,5,0,2,
953,49151325,49159742,20098,How to penalize False Negatives more than False Positives,2,<python><machine-learning><scikit-learn>,20,"<p>From the business perspective, false negatives lead to about tenfold higher costs (real money) than false positives. Given my standard binary classification models (logit, random forest, etc.), how can I incorporate this into my model? </p>

<p>Do I have to change (weight) the loss function in favor of the 'preferred' error (FP) ? If so, how to do that?</p>
",9032335,631,07-03-2018 11:55,07-03-2018 19:22,0,631,20,2,8,,"{'badge_counts': {'bronze': 20, 'silver': 8, 'gold': 2}, 'account_id': 12390038, 'is_employee': False, 'last_modified_date': 1674870300, 'last_access_date': 1710843123, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 631, 'creation_date': 1512029527, 'user_type': 'registered', 'user_id': 9032335, 'link': 'https://stackoverflow.com/users/9032335/ivegotaquestion', 'profile_image': 'https://www.gravatar.com/avatar/4a1543756e62c31836cc133bbfb6a520?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'ivegotaquestion'}","From the business perspective, false negatives lead to about tenfold higher costs (real money) than false positives. Given my standard binary classification models (logit, random forest, etc.), how can I incorporate this into my model? Do I have to change (weight) the loss function in favor of the 'preferred' error (FP) ? If so, how to do that?",,0,3,0,0,
954,48687857,48688342,83842,Python - Json List to Pandas Dataframe,2,<python><json><pandas><dataframe><python-requests>,31,"<p>I've a json list and I can't convert to Pandas dataframe (various rows and 19 columns)</p>
<p>Link to response : <a href=""https://www.byma.com.ar/wp-admin/admin-ajax.php?action=get_historico_simbolo&amp;simbolo=INAG&amp;fecha=01-02-2018"" rel=""nofollow noreferrer"">https://www.byma.com.ar/wp-admin/admin-ajax.php?action=get_historico_simbolo&amp;simbolo=INAG&amp;fecha=01-02-2018</a></p>
<p>json response:</p>
<pre class=""lang-none prettyprint-override""><code>[
    {&quot;Apertura&quot;:35,&quot;Apertura_Homogeneo&quot;:35,&quot;Cantidad_Operaciones&quot;:1,&quot;Cierre&quot;:35,&quot;Cierre_Homogeneo&quot;:35,&quot;Denominacion&quot;:&quot;INSUMOS AGROQUIMICOS S.A.&quot;,&quot;Fecha&quot;:&quot;02\/02\/2018&quot;,&quot;Maximo&quot;:35,&quot;Maximo_Homogeneo&quot;:35,&quot;Minimo&quot;:35,&quot;Minimo_Homogeneo&quot;:35,&quot;Monto_Operado_Pesos&quot;:175,&quot;Promedio&quot;:35,&quot;Promedio_Homogeneo&quot;:35,&quot;Simbolo&quot;:&quot;INAG&quot;,&quot;Variacion&quot;:-5.15,&quot;Variacion_Homogeneo&quot;:0,&quot;Vencimiento&quot;:&quot;48hs&quot;,&quot;Volumen_Nominal&quot;:5},
    {&quot;Apertura&quot;:34.95,&quot;Apertura_Homogeneo&quot;:34.95,&quot;Cantidad_Operaciones&quot;:2,&quot;Cierre&quot;:34.95,&quot;Cierre_Homogeneo&quot;:34.95,&quot;Denominacion&quot;:&quot;INSUMOS AGROQUIMICOS S.A.&quot;,&quot;Fecha&quot;:&quot;05\/02\/2018&quot;,&quot;Maximo&quot;:34.95,&quot;Maximo_Homogeneo&quot;:34.95,&quot;Minimo&quot;:34.95,&quot;Minimo_Homogeneo&quot;:34.95,&quot;Monto_Operado_Pesos&quot;:5243,&quot;Promedio&quot;:-79228162514264337593543950335,&quot;Promedio_Homogeneo&quot;:-79228162514264337593543950335,&quot;Simbolo&quot;:&quot;INAG&quot;,&quot;Variacion&quot;:-0.14,&quot;Variacion_Homogeneo&quot;:-0.14,&quot;Vencimiento&quot;:&quot;48hs&quot;,&quot;Volumen_Nominal&quot;:150},
    {&quot;Apertura&quot;:32.10,&quot;Apertura_Homogeneo&quot;:32.10,&quot;Cantidad_Operaciones&quot;:2,&quot;Cierre&quot;:32.10,&quot;Cierre_Homogeneo&quot;:32.10,&quot;Denominacion&quot;:&quot;INSUMOS AGROQUIMICOS S.A.&quot;,&quot;Fecha&quot;:&quot;07\/02\/2018&quot;,&quot;Maximo&quot;:32.10,&quot;Maximo_Homogeneo&quot;:32.10,&quot;Minimo&quot;:32.10,&quot;Minimo_Homogeneo&quot;:32.10,&quot;Monto_Operado_Pesos&quot;:98756,&quot;Promedio&quot;:32.10,&quot;Promedio_Homogeneo&quot;:32.10,&quot;Simbolo&quot;:&quot;INAG&quot;,&quot;Variacion&quot;:-8.16,&quot;Variacion_Homogeneo&quot;:-8.88,&quot;Vencimiento&quot;:&quot;48hs&quot;,&quot;Volumen_Nominal&quot;:3076}
]
</code></pre>
<p>I use the next piece of code to convert this json to dataframe:</p>
<pre><code>def getFinanceHistoricalStockFromByma(tickerList): 
     dataFrameHistorical = pd.DataFrame()  
     for item in tickerList:
         url = 'https://www.byma.com.ar/wp-admin/admin-ajax.php?action=get_historico_simbolo&amp;simbolo=' + item + '&amp;fecha=01-02-2018'
         response = requests.get(url)
         if response.content : print 'ok info Historical Stock'
         data = response.json()                
         dfItem = jsonToDataFrame(data)                
         dataFrameHistorical = dataFrameHistorical.append(dfItem, ignore_index=True)    
    return dataFrameHistorical

def jsonToDataFrame(jsonStr):    
     return json_normalize(jsonStr)    
</code></pre>
<p>The result of <code>json_normalize</code> is 1 row and a lot of columns. How can I convert this json response to 1 row per list?</p>
",9057672,338,08-02-2018 14:29,08-02-2018 14:54,0,348,9,1,3,,"{'badge_counts': {'bronze': 9, 'silver': 3, 'gold': 1}, 'account_id': 12438170, 'is_employee': False, 'last_modified_date': 1646447100, 'last_access_date': 1647956662, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 348, 'creation_date': 1512494074, 'user_type': 'registered', 'user_id': 9057672, 'location': 'Argentina', 'link': 'https://stackoverflow.com/users/9057672/adrian-ezequiel-martinez', 'profile_image': 'https://graph.facebook.com/1414057738716870/picture?type=large', 'display_name': 'Adrian Ezequiel Martinez'}",I've a json list and I can't convert to Pandas dataframe (various rows and 19 columns) Link to response : https://www.byma.com.ar/wp-admin/admin-ajax.php?action=get_historico_simbolo&amp;simbolo=INAG&amp;fecha=01-02-2018 json response: I use the next piece of code to convert this json to dataframe: The result of is 1 row and a lot of columns. How can I convert this json response to 1 row per list?,"[
    {&quot;Apertura&quot;:35,&quot;Apertura_Homogeneo&quot;:35,&quot;Cantidad_Operaciones&quot;:1,&quot;Cierre&quot;:35,&quot;Cierre_Homogeneo&quot;:35,&quot;Denominacion&quot;:&quot;INSUMOS AGROQUIMICOS S.A.&quot;,&quot;Fecha&quot;:&quot;02\/02\/2018&quot;,&quot;Maximo&quot;:35,&quot;Maximo_Homogeneo&quot;:35,&quot;Minimo&quot;:35,&quot;Minimo_Homogeneo&quot;:35,&quot;Monto_Operado_Pesos&quot;:175,&quot;Promedio&quot;:35,&quot;Promedio_Homogeneo&quot;:35,&quot;Simbolo&quot;:&quot;INAG&quot;,&quot;Variacion&quot;:-5.15,&quot;Variacion_Homogeneo&quot;:0,&quot;Vencimiento&quot;:&quot;48hs&quot;,&quot;Volumen_Nominal&quot;:5},
    {&quot;Apertura&quot;:34.95,&quot;Apertura_Homogeneo&quot;:34.95,&quot;Cantidad_Operaciones&quot;:2,&quot;Cierre&quot;:34.95,&quot;Cierre_Homogeneo&quot;:34.95,&quot;Denominacion&quot;:&quot;INSUMOS AGROQUIMICOS S.A.&quot;,&quot;Fecha&quot;:&quot;05\/02\/2018&quot;,&quot;Maximo&quot;:34.95,&quot;Maximo_Homogeneo&quot;:34.95,&quot;Minimo&quot;:34.95,&quot;Minimo_Homogeneo&quot;:34.95,&quot;Monto_Operado_Pesos&quot;:5243,&quot;Promedio&quot;:-79228162514264337593543950335,&quot;Promedio_Homogeneo&quot;:-79228162514264337593543950335,&quot;Simbolo&quot;:&quot;INAG&quot;,&quot;Variacion&quot;:-0.14,&quot;Variacion_Homogeneo&quot;:-0.14,&quot;Vencimiento&quot;:&quot;48hs&quot;,&quot;Volumen_Nominal&quot;:150},
    {&quot;Apertura&quot;:32.10,&quot;Apertura_Homogeneo&quot;:32.10,&quot;Cantidad_Operaciones&quot;:2,&quot;Cierre&quot;:32.10,&quot;Cierre_Homogeneo&quot;:32.10,&quot;Denominacion&quot;:&quot;INSUMOS AGROQUIMICOS S.A.&quot;,&quot;Fecha&quot;:&quot;07\/02\/2018&quot;,&quot;Maximo&quot;:32.10,&quot;Maximo_Homogeneo&quot;:32.10,&quot;Minimo&quot;:32.10,&quot;Minimo_Homogeneo&quot;:32.10,&quot;Monto_Operado_Pesos&quot;:98756,&quot;Promedio&quot;:32.10,&quot;Promedio_Homogeneo&quot;:32.10,&quot;Simbolo&quot;:&quot;INAG&quot;,&quot;Variacion&quot;:-8.16,&quot;Variacion_Homogeneo&quot;:-8.88,&quot;Vencimiento&quot;:&quot;48hs&quot;,&quot;Volumen_Nominal&quot;:3076}
]
 def getFinanceHistoricalStockFromByma(tickerList): 
     dataFrameHistorical = pd.DataFrame()  
     for item in tickerList:
         url = 'https://www.byma.com.ar/wp-admin/admin-ajax.php?action=get_historico_simbolo&amp;simbolo=' + item + '&amp;fecha=01-02-2018'
         response = requests.get(url)
         if response.content : print 'ok info Historical Stock'
         data = response.json()                
         dfItem = jsonToDataFrame(data)                
         dataFrameHistorical = dataFrameHistorical.append(dfItem, ignore_index=True)    
    return dataFrameHistorical

def jsonToDataFrame(jsonStr):    
     return json_normalize(jsonStr)    
 json_normalize",15,25,0,1,
955,49237663,49237705,36854,Passing a function with multiple arguments to DataFrame.apply,3,<python><function><pandas><multiple-arguments>,31,"<p>Suppose I have a dataframe like this:</p>

<pre><code>df = pd.DataFrame([['foo', 'x'], ['bar', 'y']], columns=['A', 'B'])


       A    B
0    foo    x
1    bar    y
</code></pre>

<p>I know how to use a single argument function with Apply when it comes to dataframes, like this:</p>

<pre><code>def some_func(row):
    return '{0}-{1}'.format(row['A'], row['B'])

df['C'] = df.apply(some_func, axis=1)

df


       A    B        C
0    foo    x    foo-x
1    bar    y    bar-y
</code></pre>

<p>How can I use apply on dataframes when they involve multiple input arguments? Here's an example of what I want:</p>

<pre><code>def some_func(row, var1):
    return '{0}-{1}-{2}'.format(row['A'], row['B'], var1)

df['C'] = df.apply(some_func(row, var1='DOG'), axis=1)

df


       A    B            C
0    foo    x    foo-x-DOG
1    bar    y    bar-y-DOG
</code></pre>

<p>I'm not looking for work-arounds to solve this one particular example, just how to do something like this in general. Any advice would be well appreciated, thanks.</p>
",9094443,375,12-03-2018 14:26,12-03-2018 14:29,0,385,5,1,3,,"{'badge_counts': {'bronze': 5, 'silver': 3, 'gold': 1}, 'account_id': 12493691, 'is_employee': False, 'last_modified_date': 1652470500, 'last_access_date': 1563364578, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 10, 'reputation_change_day': 10, 'reputation': 385, 'creation_date': 1513176430, 'user_type': 'registered', 'user_id': 9094443, 'location': 'Allston, Boston, MA, United States', 'link': 'https://stackoverflow.com/users/9094443/michael-henry', 'profile_image': 'https://lh5.googleusercontent.com/-HpZbKKXqKqo/AAAAAAAAAAI/AAAAAAAAAFc/SP0G59-XsZI/photo.jpg?sz=256', 'display_name': 'Michael Henry'}","Suppose I have a dataframe like this: I know how to use a single argument function with Apply when it comes to dataframes, like this: How can I use apply on dataframes when they involve multiple input arguments? Here's an example of what I want: I'm not looking for work-arounds to solve this one particular example, just how to do something like this in general. Any advice would be well appreciated, thanks.","df = pd.DataFrame([['foo', 'x'], ['bar', 'y']], columns=['A', 'B'])


       A    B
0    foo    x
1    bar    y
 def some_func(row):
    return '{0}-{1}'.format(row['A'], row['B'])

df['C'] = df.apply(some_func, axis=1)

df


       A    B        C
0    foo    x    foo-x
1    bar    y    bar-y
 def some_func(row, var1):
    return '{0}-{1}-{2}'.format(row['A'], row['B'], var1)

df['C'] = df.apply(some_func(row, var1='DOG'), axis=1)

df


       A    B            C
0    foo    x    foo-x-DOG
1    bar    y    bar-y-DOG
",25,41,0,0,
956,49051017,49051253,36938,Year Field in Django,4,<python><django><django-models>,26,"<p>I want my users to enter their birth year. I don't want them to type the same in the form rather select the year from available options. I known that I can do something like this in my model if I needed to date instead of year:</p>

<pre><code>class MyModel(models.Model):

    birthday = models.DateField(null=True, blank=True)
</code></pre>

<p>I can do this in forms to let the user choose date from datepicker.</p>

<pre><code>    birthday = forms.fields.DateField(widget=forms.widgets.DateInput(attrs={'type': 'date'}))
</code></pre>

<p>For year, I can use a <code>CharField/IntegerField</code> with <code>choices</code> similar to what has been done in this <a href=""https://stackoverflow.com/a/24656072/8414030"">SO</a> answer. </p>

<pre><code>import datetime
YEAR_CHOICES = [(r,r) for r in range(1984, datetime.date.today().year+1)]

year = models.IntegerField(_('year'), choices=YEAR_CHOICES, default=datetime.datetime.now().year)
</code></pre>

<p>The problem, however, is that change of current year from say, 2018 to 2019, will not change the available options.</p>

<p>Can you help or provide hints to achieve what I want to do?</p>
",8414030,763,01-03-2018 13:44,01-03-2018 13:57,0,763,23,2,12,71,"{'badge_counts': {'bronze': 23, 'silver': 12, 'gold': 2}, 'account_id': 11479473, 'is_employee': False, 'last_modified_date': 1607614427, 'last_access_date': 1708101878, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 763, 'creation_date': 1501793223, 'user_type': 'registered', 'user_id': 8414030, 'accept_rate': 71, 'location': 'Delhi, India', 'website_url': '', 'link': 'https://stackoverflow.com/users/8414030/inquilabee', 'profile_image': 'https://www.gravatar.com/avatar/1c887354d195a9566fe058b8aac730f3?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'inquilabee'}","I want my users to enter their birth year. I don't want them to type the same in the form rather select the year from available options. I known that I can do something like this in my model if I needed to date instead of year: I can do this in forms to let the user choose date from datepicker. For year, I can use a with similar to what has been done in this SO answer. The problem, however, is that change of current year from say, 2018 to 2019, will not change the available options. Can you help or provide hints to achieve what I want to do?","class MyModel(models.Model):

    birthday = models.DateField(null=True, blank=True)
     birthday = forms.fields.DateField(widget=forms.widgets.DateInput(attrs={'type': 'date'}))
 CharField/IntegerField choices import datetime
YEAR_CHOICES = [(r,r) for r in range(1984, datetime.date.today().year+1)]

year = models.IntegerField(_('year'), choices=YEAR_CHOICES, default=datetime.datetime.now().year)
",3,23,0,1,
957,49351071,49354182,23996,load image dataset (folder or zip) located in Google Drive to Google Colab?,4,<python><neural-network><google-colaboratory>,15,"<p>I have a dataset of images on my Google Drive. I have this dataset both in a compressed .zip version and an uncompressed folder.</p>

<p>I want to train a CNN using Google Colab. How can I tell Colab where the images in my Google Drive are?</p>

<ol>
<li><p><a href=""https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb"" rel=""noreferrer"">official tutorial does not help me as it only shows how to upload single files, not a folder with 10000 images as in my case.</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/49088159/add-a-folder-with-20k-of-images-into-google-colaboratory"">Then I found this answer, but the solution is not finished, or at least I did not understand how to go on from unzipping. Unfortunately I am unable to comment this answer as I don't have enough ""stackoverflow points""</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory"">I also found this thread, but here all the answer use other tools, such as Github or dropbox</a></p></li>
</ol>

<p>I hope someone could explain me what I need to do or tell me where to find help.</p>

<p>Edit1: </p>

<p><a href=""https://stackoverflow.com/questions/48860586/how-to-upload-and-save-large-data-to-google-colaboratory-from-local-drive"">I have found yet another thread asking the same question as mine:</a> Sadly, of the 3 answers, two refer to Kaggle, which I don't know and don't use. The third answer provides two links. The first link refers to the 3rd thread I linked, and the second link only explains how to upload single files manually.</p>
",9439097,3423,18-03-2018 17:48,19-03-2018 00:20,1,3463,54,5,32,,"{'badge_counts': {'bronze': 54, 'silver': 32, 'gold': 5}, 'account_id': 12488109, 'is_employee': False, 'last_modified_date': 1653647003, 'last_access_date': 1711099970, 'reputation_change_year': 190, 'reputation_change_quarter': 190, 'reputation_change_month': 60, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 3463, 'creation_date': 1520098797, 'user_type': 'registered', 'user_id': 9439097, 'website_url': '', 'link': 'https://stackoverflow.com/users/9439097/charelf', 'profile_image': 'https://i.stack.imgur.com/2cIBT.png?s=256&g=1', 'display_name': 'charelf'}","I have a dataset of images on my Google Drive. I have this dataset both in a compressed .zip version and an uncompressed folder. I want to train a CNN using Google Colab. How can I tell Colab where the images in my Google Drive are? official tutorial does not help me as it only shows how to upload single files, not a folder with 10000 images as in my case. Then I found this answer, but the solution is not finished, or at least I did not understand how to go on from unzipping. Unfortunately I am unable to comment this answer as I don't have enough ""stackoverflow points"" I also found this thread, but here all the answer use other tools, such as Github or dropbox I hope someone could explain me what I need to do or tell me where to find help. Edit1: I have found yet another thread asking the same question as mine: Sadly, of the 3 answers, two refer to Kaggle, which I don't know and don't use. The third answer provides two links. The first link refers to the 3rd thread I linked, and the second link only explains how to upload single files manually.",,0,15,0,4,
958,48651891,48653758,25153,Longest Common Subsequence in Python,4,<python><algorithm><dynamic-programming>,12,"<p>I am trying to find the longest common subsequence between two strings.</p>

<p>I watched this tutoial <a href=""https://www.youtube.com/watch?v=NnD96abizww"" rel=""noreferrer"">https://www.youtube.com/watch?v=NnD96abizww</a></p>

<p>and wrote:</p>

<pre><code># Longest Common Subsequence

def lcs(s1, s2):
    matrix = [ [0 for x in range(len(s2))] for x in range(len(s1)) ]
    cs = """"
    for i in range(len(s1)):
        for j in range(len(s2)):
            if s1[i]==s2[j]:
                if i==0 or j==0:
                    matrix[i][j] = 1
                    cs += s1[i]
                else:
                    matrix[i][j] = matrix[i-1][j-1] + 1
                    cs += s1[i]
            else:
                if i==0 or j==0:
                    matrix[i][j] = 0
                else:
                    matrix[i][j] = max(matrix[i-1][j], matrix[i][j-1])

    return matrix[len(s1)-1][len(s2)-1], cs


print(lcs(""abcdaf"", ""acbcf""))  



I get (3, 'abccaf')
</code></pre>

<p>This is clearly wrong it should be 4 abcf.</p>

<p>Not sure what step is going wrong. One general question is how long does it take usually for programmers to ""get"" these kind of questions?</p>
",9151730,783,06-02-2018 21:00,06-02-2018 23:44,0,783,24,6,13,58,"{'badge_counts': {'bronze': 24, 'silver': 13, 'gold': 6}, 'account_id': 12581047, 'is_employee': False, 'last_modified_date': 1607614411, 'last_access_date': 1541046858, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 783, 'creation_date': 1514521327, 'user_type': 'registered', 'user_id': 9151730, 'accept_rate': 58, 'link': 'https://stackoverflow.com/users/9151730/mourinho', 'profile_image': 'https://www.gravatar.com/avatar/db373d99b8c09c425344cb1bd0f4f0b0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'mourinho'}","I am trying to find the longest common subsequence between two strings. I watched this tutoial https://www.youtube.com/watch?v=NnD96abizww and wrote: This is clearly wrong it should be 4 abcf. Not sure what step is going wrong. One general question is how long does it take usually for programmers to ""get"" these kind of questions?","# Longest Common Subsequence

def lcs(s1, s2):
    matrix = [ [0 for x in range(len(s2))] for x in range(len(s1)) ]
    cs = """"
    for i in range(len(s1)):
        for j in range(len(s2)):
            if s1[i]==s2[j]:
                if i==0 or j==0:
                    matrix[i][j] = 1
                    cs += s1[i]
                else:
                    matrix[i][j] = matrix[i-1][j-1] + 1
                    cs += s1[i]
            else:
                if i==0 or j==0:
                    matrix[i][j] = 0
                else:
                    matrix[i][j] = max(matrix[i-1][j], matrix[i][j-1])

    return matrix[len(s1)-1][len(s2)-1], cs


print(lcs(""abcdaf"", ""acbcf""))  



I get (3, 'abccaf')
",27,39,0,1,
959,49704364,49704855,153069,Make python3 as my default python on Mac,5,<python><python-3.x><macos><terminal><homebrew>,117,"<p>What I'm trying to do here is to <strong>make python3 as my default python.</strong> Except the python 2.7 which automatically installed on mac, I installed <strong>python3</strong> with <strong>homebrew</strong>. This is the website that I'm following. <a href=""http://docs.python-guide.org/en/latest/starting/install3/osx/#install3-osx"" rel=""noreferrer"">http://docs.python-guide.org/en/latest/starting/install3/osx/#install3-osx</a></p>
<p>I guess I followed every instruction well, got xcode freshly installed, Command line tools, and homebrew. But here's my little confusion occurs.</p>
<blockquote>
<p>The script will explain what changes it will make and prompt you before the installation begins. Once you’ve installed Homebrew, insert the Homebrew directory at the top of your PATH environment variable. You can do this by adding the following line at the bottom of your ~/.profile file</p>
<p><strong>export PATH=/usr/local/bin:/usr/local/sbin:$PATH</strong></p>
</blockquote>
<p>I was really confused what this was, but I concluded that I should just add this following line at the bottom of ~/.profile file. So I opened the <strong>~/.profile</strong> file by <strong>open .profile</strong> in the terminal, and added following line at the bottom. And now it looks like this.</p>
<pre><code>export PATH=/usr/local/bin:/usr/local/sbin:$PATH
# Setting PATH for Python 3.6
# The original version is saved in .profile.pysave
export PATH=/usr/local/bin:/usr/local/sbin:$PATH
</code></pre>
<p>And then I did <strong>brew install python</strong>, and was hoping to see <strong>python3</strong> when I do <strong>python --version.</strong>
But it just shows me <strong>python 2.7.10.</strong> I want my default python to be <strong>python3</strong> not 2.7</p>
<p>And I found a little clue from the website.</p>
<blockquote>
<p>Do I have a Python 3 installed?</p>
</blockquote>
<pre><code>$ python --version
Python 3.6.4
</code></pre>
<blockquote>
<p>If you still see 2.7 ensure in <strong>PATH /usr/local/bin/ takes pecedence over /usr/bin/</strong></p>
</blockquote>
<p>Maybe it has to do something with <strong>PATH?</strong> Could someone explain in simple English what <strong>PATH</strong> exactly is and how I could make my default python to be python3 when I run <strong>python --version</strong> in the terminal?</p>
",8424406,1423,07-04-2018 05:52,07-04-2018 07:08,0,1423,12,2,11,,"{'badge_counts': {'bronze': 12, 'silver': 11, 'gold': 2}, 'account_id': 11494615, 'is_employee': False, 'last_modified_date': 1681809900, 'last_access_date': 1652348139, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1423, 'creation_date': 1502018999, 'user_type': 'registered', 'user_id': 8424406, 'location': 'Daegu, South Korea', 'website_url': '', 'link': 'https://stackoverflow.com/users/8424406/sambo-kim', 'profile_image': 'https://www.gravatar.com/avatar/f3e95342be92a82f63077736f5e08f53?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Sambo Kim'}","What I'm trying to do here is to make python3 as my default python. Except the python 2.7 which automatically installed on mac, I installed python3 with homebrew. This is the website that I'm following. http://docs.python-guide.org/en/latest/starting/install3/osx/#install3-osx I guess I followed every instruction well, got xcode freshly installed, Command line tools, and homebrew. But here's my little confusion occurs. The script will explain what changes it will make and prompt you before the installation begins. Once you’ve installed Homebrew, insert the Homebrew directory at the top of your PATH environment variable. You can do this by adding the following line at the bottom of your ~/.profile file export PATH=/usr/local/bin:/usr/local/sbin:$PATH I was really confused what this was, but I concluded that I should just add this following line at the bottom of ~/.profile file. So I opened the ~/.profile file by open .profile in the terminal, and added following line at the bottom. And now it looks like this. And then I did brew install python, and was hoping to see python3 when I do python --version. But it just shows me python 2.7.10. I want my default python to be python3 not 2.7 And I found a little clue from the website. Do I have a Python 3 installed? If you still see 2.7 ensure in PATH /usr/local/bin/ takes pecedence over /usr/bin/ Maybe it has to do something with PATH? Could someone explain in simple English what PATH exactly is and how I could make my default python to be python3 when I run python --version in the terminal?","export PATH=/usr/local/bin:/usr/local/sbin:$PATH
# Setting PATH for Python 3.6
# The original version is saved in .profile.pysave
export PATH=/usr/local/bin:/usr/local/sbin:$PATH
 $ python --version
Python 3.6.4
",4,25,0,1,
960,48768650,48768953,65353,Groupby sum and count on multiple columns in python,4,<python><python-3.x><python-2.7><pandas><pandas-groupby>,14,"<p>I have a pandas dataframe that looks like this</p>

<pre><code>ID     country   month   revenue  profit   ebit
234    USA       201409   10        5       3
344    USA       201409    9        7       2
532    UK        201410    20       10      5
129    Canada    201411    15       10      5
</code></pre>

<p>I want to group by ID, country, month and count the IDs per month and country and sum the revenue, profit, ebit.
The output for the above data would be:</p>

<pre><code> country   month    revenue   profit  ebit   count
   USA     201409     19        12      5      2
   UK      201409     20        10      5      1
   Canada  201411     15        10      5      1
</code></pre>

<p>I have tried different variations of groupby, sum and count functions of pandas but I am unable to figure out how to apply groupby sum and count all together to give the result as shown. Please share any ideas that you might have. Thanks! </p>
",9169205,405,13-02-2018 14:03,13-02-2018 14:21,0,405,15,1,5,100,"{'badge_counts': {'bronze': 15, 'silver': 5, 'gold': 1}, 'account_id': 12609537, 'is_employee': False, 'last_modified_date': 1697248500, 'last_access_date': 1697635135, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 405, 'creation_date': 1514999202, 'user_type': 'registered', 'user_id': 9169205, 'accept_rate': 100, 'website_url': '', 'link': 'https://stackoverflow.com/users/9169205/n91', 'profile_image': 'https://www.gravatar.com/avatar/e41c1eb19df9c2fd00ce996edc6a266c?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'N91'}","I have a pandas dataframe that looks like this I want to group by ID, country, month and count the IDs per month and country and sum the revenue, profit, ebit. The output for the above data would be: I have tried different variations of groupby, sum and count functions of pandas but I am unable to figure out how to apply groupby sum and count all together to give the result as shown. Please share any ideas that you might have. Thanks!","ID     country   month   revenue  profit   ebit
234    USA       201409   10        5       3
344    USA       201409    9        7       2
532    UK        201410    20       10      5
129    Canada    201411    15       10      5
  country   month    revenue   profit  ebit   count
   USA     201409     19        12      5      2
   UK      201409     20        10      5      1
   Canada  201411     15        10      5      1
",7,19,0,0,
961,50140371,50140851,17726,SciPy skewnormal fitting,2,<python><scipy>,14,"<p>I am trying to fit data into a skew normal distribution using the <em>SciPy Skewnorm</em> package.</p>

<p>However, I am failing to understand the usage properly as I cannot find proper documentation or examples on this matter.</p>

<p>On the help section I found <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skewnorm.html"" rel=""noreferrer"">Documentation</a> and trying to use <code>skewnorm.fit()</code> along with <code>skewnorm.pdf()</code> to fit data into a model and use that model to output a distribution and compare with the original data. </p>

<p>Please let me know if anyone can help with this.</p>

<pre><code>from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# choose some parameters
a, loc, scale = 5.3, -0.1, 2.2
# draw a sample
data = stats.skewnorm(a, loc, scale).rvs(1000)
# estimate parameters from sample
ae, loce, scalee = stats.skewnorm.fit(data)
# Plot the PDF.
plt.figure()
plt.hist(data, bins=100, normed=True, alpha=0.6, color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = stats.skewnorm.pdf(x,ae, loce, scalee)#.rvs(100)
plt.plot(x, p, 'k', linewidth=2)
</code></pre>

<p>Output: </p>

<p><img src=""https://i.stack.imgur.com/9E7rt.png"" alt=""enter image description here""></p>
",9173788,164,02-05-2018 17:29,02-05-2018 17:59,0,174,12,1,1,,"{'badge_counts': {'bronze': 12, 'silver': 1, 'gold': 1}, 'account_id': 12616530, 'is_employee': False, 'last_modified_date': 1619471700, 'last_access_date': 1703351838, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 174, 'creation_date': 1515083293, 'user_type': 'registered', 'user_id': 9173788, 'location': 'Windsor, ON, Canada', 'website_url': 'https://www.linkedin.com/in/afshinr/', 'link': 'https://stackoverflow.com/users/9173788/afshin-rahimi', 'profile_image': 'https://i.stack.imgur.com/Dlkic.jpg?s=256&g=1', 'display_name': 'Afshin Rahimi'}","I am trying to fit data into a skew normal distribution using the SciPy Skewnorm package. However, I am failing to understand the usage properly as I cannot find proper documentation or examples on this matter. On the help section I found Documentation and trying to use along with to fit data into a model and use that model to output a distribution and compare with the original data. Please let me know if anyone can help with this. Output:","skewnorm.fit() skewnorm.pdf() from scipy import stats
import matplotlib.pyplot as plt
import numpy as np

# choose some parameters
a, loc, scale = 5.3, -0.1, 2.2
# draw a sample
data = stats.skewnorm(a, loc, scale).rvs(1000)
# estimate parameters from sample
ae, loce, scalee = stats.skewnorm.fit(data)
# Plot the PDF.
plt.figure()
plt.hist(data, bins=100, normed=True, alpha=0.6, color='g')
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = stats.skewnorm.pdf(x,ae, loce, scalee)#.rvs(100)
plt.plot(x, p, 'k', linewidth=2)
",14,30,1,1,
962,48122798,48123430,17677,"""OSError: [Errno 22] Invalid argument"" when read()ing a huge file",2,<python><python-3.x><macos><file-io>,18,"<p>I'm trying to write a small script that prints the checksum of a file (using some code from <a href=""https://gist.github.com/Zireael-N/ed36997fd1a967d78cb2"" rel=""noreferrer"">https://gist.github.com/Zireael-N/ed36997fd1a967d78cb2</a>):</p>

<pre><code>import sys
import os
import hashlib

file = '/Users/Me/Downloads/2017-11-29-raspbian-stretch.img'

with open(file, 'rb') as f:
    contents = f.read()
    print('SHA256 of file is %s' % hashlib.sha256(contents).hexdigest())
</code></pre>

<p>But I'm getting the following error message:</p>

<pre><code>Traceback (most recent call last):
  File ""checksum.py"", line 8, in &lt;module&gt;
    contents = f.read()
OSError: [Errno 22] Invalid argument
</code></pre>

<p>What am I doing wrong? I'm using python 3 on macOS High Sierra</p>
",9179656,201,05-01-2018 23:39,06-01-2018 01:26,1,201,7,1,2,,"{'badge_counts': {'bronze': 7, 'silver': 2, 'gold': 1}, 'account_id': 12626064, 'is_employee': False, 'last_modified_date': 1573678430, 'last_access_date': 1636327870, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 201, 'creation_date': 1515194710, 'user_type': 'registered', 'user_id': 9179656, 'link': 'https://stackoverflow.com/users/9179656/hallvard', 'profile_image': 'https://www.gravatar.com/avatar/036f30c0a99e0e5d7e637d71f22bfcf9?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Hallvard'}",I'm trying to write a small script that prints the checksum of a file (using some code from https://gist.github.com/Zireael-N/ed36997fd1a967d78cb2): But I'm getting the following error message: What am I doing wrong? I'm using python 3 on macOS High Sierra,"import sys
import os
import hashlib

file = '/Users/Me/Downloads/2017-11-29-raspbian-stretch.img'

with open(file, 'rb') as f:
    contents = f.read()
    print('SHA256 of file is %s' % hashlib.sha256(contents).hexdigest())
 Traceback (most recent call last):
  File ""checksum.py"", line 8, in &lt;module&gt;
    contents = f.read()
OSError: [Errno 22] Invalid argument
",11,22,0,1,
963,49299574,49299687,84999,How to list all installed Jupyter kernels?,2,<python><conda>,68,"<p>Listing all the available environments is as simple as:</p>
<pre class=""lang-none prettyprint-override""><code>$ conda env list
</code></pre>
<p>Now how does one list the currently installed kernels, without having to go to the path:</p>
<pre class=""lang-none prettyprint-override""><code>$ ls /home/{{user}}/.local/share/jupyter/kernels/
</code></pre>
",9187350,17747,15-03-2018 12:31,15-03-2018 12:38,0,17807,56,8,35,20,"{'badge_counts': {'bronze': 56, 'silver': 35, 'gold': 8}, 'account_id': 12639214, 'is_employee': False, 'last_modified_date': 1689003000, 'last_access_date': 1711073415, 'reputation_change_year': 460, 'reputation_change_quarter': 460, 'reputation_change_month': 110, 'reputation_change_week': 20, 'reputation_change_day': 0, 'reputation': 17807, 'creation_date': 1515405174, 'user_type': 'registered', 'user_id': 9187350, 'accept_rate': 20, 'location': 'Washington DC, US', 'website_url': 'http://renatodamas.com', 'link': 'https://stackoverflow.com/users/9187350/renatodamas', 'profile_image': 'https://i.stack.imgur.com/PNJAq.png?s=256&g=1', 'display_name': 'renatodamas'}","Listing all the available environments is as simple as: Now how does one list the currently installed kernels, without having to go to the path:","$ conda env list
 $ ls /home/{{user}}/.local/share/jupyter/kernels/
",0,6,0,0,
964,48904183,53887102,12312,import pyodbc failure mac os,3,<python><python-3.x><macos><pyodbc>,17,"<p>I've tried to install pyodbc on mac, but I got this error</p>

<pre><code>Traceback (most recent call last):
  File ""Untitled.py"", line 1, in &lt;module&gt;
    import pyodbc
ImportError: dlopen(/usr/local/lib/python3.6/site-packages/pyodbc.cpython-36m-darwin.so, 2): Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib
  Referenced from: /usr/local/lib/python3.6/site-packages/pyodbc.cpython-36m-darwin.so
  Reason: image not found
</code></pre>

<p>What could it be and how can I solve it?</p>
",8428347,325,21-02-2018 10:59,21-12-2018 15:15,303,325,14,1,3,,"{'badge_counts': {'bronze': 14, 'silver': 3, 'gold': 1}, 'account_id': 11441660, 'is_employee': False, 'last_modified_date': 1654266850, 'last_access_date': 1709846453, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 325, 'creation_date': 1502103324, 'user_type': 'registered', 'user_id': 8428347, 'website_url': '', 'link': 'https://stackoverflow.com/users/8428347/volkoshkursk', 'profile_image': 'https://www.gravatar.com/avatar/ed1244e1af693602930a08fab774795a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'volkoshkursk'}","I've tried to install pyodbc on mac, but I got this error What could it be and how can I solve it?","Traceback (most recent call last):
  File ""Untitled.py"", line 1, in &lt;module&gt;
    import pyodbc
ImportError: dlopen(/usr/local/lib/python3.6/site-packages/pyodbc.cpython-36m-darwin.so, 2): Library not loaded: /usr/local/opt/unixodbc/lib/libodbc.2.dylib
  Referenced from: /usr/local/lib/python3.6/site-packages/pyodbc.cpython-36m-darwin.so
  Reason: image not found
",5,11,0,0,
965,49854796,49854821,19079,Select the inverse index in pd.Dataframe,3,<python><pandas><dataframe><indexing>,19,"<p>How to select the inverse index in pd.DataFrame by using <code>loc</code> or <code>iloc</code>?</p>

<p>I tried <code>df.loc[!my_index,my_feature]</code> but fail. </p>

<p>And <code>df.loc[[ind for ind in df.index.tolist() if ind not in my_index],my_feature]</code> looks too dull. Any better idea?</p>
",8482467,1249,16-04-2018 10:13,16-04-2018 10:15,0,1249,27,3,14,56,"{'badge_counts': {'bronze': 27, 'silver': 14, 'gold': 3}, 'account_id': 11577657, 'is_employee': False, 'last_modified_date': 1696757100, 'last_access_date': 1711097158, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1249, 'creation_date': 1503044027, 'user_type': 'registered', 'user_id': 8482467, 'accept_rate': 56, 'website_url': '', 'link': 'https://stackoverflow.com/users/8482467/garvey', 'profile_image': 'https://www.gravatar.com/avatar/72793ff6fb29e785c951406c98f80985?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Garvey'}",How to select the inverse index in pd.DataFrame by using or ? I tried but fail. And looks too dull. Any better idea?,"loc iloc df.loc[!my_index,my_feature] df.loc[[ind for ind in df.index.tolist() if ind not in my_index],my_feature]",-4,5,0,0,
966,48987959,48991515,76552,classification metrics can't handle a mix of continuous-multioutput and multi-label-indicator targets,2,<python><keras>,21,"<p>I have created an ANN with numerical inputs and a single categorical output which is one hot encoded to be 1 of 19 categories. I set my output layer to have 19 units. I don't know how to perform the confusion matrix now nor how to classifier.predict() in light of this rather than a single binary output. I keep getting an error saying classification metrics can't handle a mix of continuous-multioutput and multi-label-indicator targets. Not sure how to proceed.</p>

<pre><code>#Importing Datasets
dataset=pd.read_csv('Data.csv')
x = dataset.iloc[:,1:36].values # lower bound independent variable to upper bound in a matrix (in this case only 1 column 'NC')
y = dataset.iloc[:,36:].values # dependent variable vector
print(x.shape)
print(y.shape)

#One Hot Encoding fuel rail column
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_y= LabelEncoder()
y[:,0]=labelencoder_y.fit_transform(y[:,0])
onehotencoder= OneHotEncoder(categorical_features=[0])
y = onehotencoder.fit_transform(y).toarray()
print(y[:,0:])

print(x.shape)
print (y.shape)


#splitting data into Training and Test Data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.1,random_state=0)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
#x_train = sc.fit_transform(x_train)
#x_test=sc.transform(x_test)
y_train = sc.fit_transform(y_train)
y_test=sc.transform(y_test)

# PART2 - Making ANN, deep neural network

#Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense


#Initialising ANN
classifier = Sequential()
#Adding the input layer and first hidden layer
classifier.add(Dense(activation= 'relu', input_dim =35, units=2, kernel_initializer=""uniform""))#rectifier activation function, include all input with one hot encoding
#Adding second hidden layer
classifier.add(Dense(activation= 'relu', units=2, kernel_initializer=""uniform"")) #rectifier activation function
#Adding the Output Layer
classifier.add(Dense(activation='softmax', units=19, kernel_initializer=""uniform"")) 
#Compiling ANN - stochastic gradient descent
classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])#stochastic gradient descent

#Fit ANN to training set

#PART 3 - Making predictions and evaluating the model
#Fitting classifier to the training set
classifier.fit(x_train, y_train, batch_size=10, epochs=100)#original batch is 10 and epoch is 100

#Predicting the Test set rules
y_pred = classifier.predict(x_test)
y_pred = (y_pred &gt; 0.5) #greater than 0.50 on scale 0 to 1
print(y_pred)

#Making confusion matrix that checks accuracy of the model
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
</code></pre>
",8512104,265,26-02-2018 11:54,26-02-2018 15:09,0,265,9,1,2,40,"{'badge_counts': {'bronze': 9, 'silver': 2, 'gold': 1}, 'account_id': 11620697, 'is_employee': False, 'last_modified_date': 1573678549, 'last_access_date': 1553081139, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 265, 'creation_date': 1503583423, 'user_type': 'registered', 'user_id': 8512104, 'accept_rate': 40, 'link': 'https://stackoverflow.com/users/8512104/user8512104', 'profile_image': 'https://www.gravatar.com/avatar/f10615b519d6fa1362072fcf552f0e4b?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user8512104'}",I have created an ANN with numerical inputs and a single categorical output which is one hot encoded to be 1 of 19 categories. I set my output layer to have 19 units. I don't know how to perform the confusion matrix now nor how to classifier.predict() in light of this rather than a single binary output. I keep getting an error saying classification metrics can't handle a mix of continuous-multioutput and multi-label-indicator targets. Not sure how to proceed.,"#Importing Datasets
dataset=pd.read_csv('Data.csv')
x = dataset.iloc[:,1:36].values # lower bound independent variable to upper bound in a matrix (in this case only 1 column 'NC')
y = dataset.iloc[:,36:].values # dependent variable vector
print(x.shape)
print(y.shape)

#One Hot Encoding fuel rail column
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_y= LabelEncoder()
y[:,0]=labelencoder_y.fit_transform(y[:,0])
onehotencoder= OneHotEncoder(categorical_features=[0])
y = onehotencoder.fit_transform(y).toarray()
print(y[:,0:])

print(x.shape)
print (y.shape)


#splitting data into Training and Test Data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.1,random_state=0)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
#x_train = sc.fit_transform(x_train)
#x_test=sc.transform(x_test)
y_train = sc.fit_transform(y_train)
y_test=sc.transform(y_test)

# PART2 - Making ANN, deep neural network

#Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense


#Initialising ANN
classifier = Sequential()
#Adding the input layer and first hidden layer
classifier.add(Dense(activation= 'relu', input_dim =35, units=2, kernel_initializer=""uniform""))#rectifier activation function, include all input with one hot encoding
#Adding second hidden layer
classifier.add(Dense(activation= 'relu', units=2, kernel_initializer=""uniform"")) #rectifier activation function
#Adding the Output Layer
classifier.add(Dense(activation='softmax', units=19, kernel_initializer=""uniform"")) 
#Compiling ANN - stochastic gradient descent
classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])#stochastic gradient descent

#Fit ANN to training set

#PART 3 - Making predictions and evaluating the model
#Fitting classifier to the training set
classifier.fit(x_train, y_train, batch_size=10, epochs=100)#original batch is 10 and epoch is 100

#Predicting the Test set rules
y_pred = classifier.predict(x_test)
y_pred = (y_pred &gt; 0.5) #greater than 0.50 on scale 0 to 1
print(y_pred)

#Making confusion matrix that checks accuracy of the model
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
",63,67,0,0,
967,49538185,49538384,40781,Purpose of `numpy.log1p( )`?,4,<python><numpy>,40,"<p>I just came across one of these <a href=""https://www.kaggle.com/yadavsarthak/you-got-this-feature-engineering-and-lasso"" rel=""noreferrer"">Kernels</a> and couldn't understand what does <code>numpy.log1p()</code> do in the third pipeline of this code (House Prediction dataset in Kaggle).</p>
<p>Numpy documentation said</p>
<p>  <em>Returns</em>:<br />
  - An array with natural logarithmic value of x + 1<br />
  - where x belongs to all elements of input array.</p>
<p>What is the purpose of finding log with one added while finding skewness of original and transformed array of same features? What does it actually do?</p>
",8543989,564,28-03-2018 15:12,28-03-2018 15:23,0,574,13,1,5,,"{'badge_counts': {'bronze': 13, 'silver': 5, 'gold': 1}, 'account_id': 11667802, 'is_employee': False, 'last_modified_date': 1590646918, 'last_access_date': 1710937358, 'reputation_change_year': 30, 'reputation_change_quarter': 30, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 574, 'creation_date': 1504191118, 'user_type': 'registered', 'user_id': 8543989, 'location': 'Karachi, Pakistan', 'website_url': 'http://none', 'link': 'https://stackoverflow.com/users/8543989/sabah', 'profile_image': 'https://i.stack.imgur.com/ESqvp.jpg?s=256&g=1', 'display_name': 'Sabah'}",I just came across one of these Kernels and couldn't understand what does do in the third pipeline of this code (House Prediction dataset in Kaggle). Numpy documentation said Returns: - An array with natural logarithmic value of x + 1 - where x belongs to all elements of input array. What is the purpose of finding log with one added while finding skewness of original and transformed array of same features? What does it actually do?,numpy.log1p(),-1,6,0,1,
968,48646684,48647001,13456,Pandas: conditional shift,3,<python><pandas><datetime><data-analysis>,19,"<p>There is a way to shift a dataframe column dependently on the condition on two other columns? something like:</p>

<pre><code>df[""cumulated_closed_value""] = df.groupby(""user"").['close_cumsum'].shiftWhile(df['close_time']&gt;df['open_time])
</code></pre>

<p>I have figured out a way to do this but it's inefficient:</p>

<p><strong>1)Load data and create the column to shift</strong> </p>

<pre><code>df=pd.read_csv('data.csv')
df.sort_values(['user','close_time'],inplace=True)
df['close_cumsum']=df.groupby('user')['value'].cumsum()
df.sort_values(['user','open_time'],inplace=True)
print(df)
</code></pre>

<p>output:</p>

<pre><code>   user  open_time close_time  value  close_cumsum
0     1 2017-01-01 2017-03-01      5            18
1     1 2017-01-02 2017-02-01      6             6
2     1 2017-02-03 2017-02-05      7            13
3     1 2017-02-07 2017-04-01      3            21
4     1 2017-09-07 2017-09-11      1            22
5     2 2018-01-01 2018-02-01     15            15
6     2 2018-03-01 2018-04-01      3            18
</code></pre>

<p><strong>2) shift the column with a self-join and some filters</strong></p>

<p>Self-join (this is memory inefficient)   <code>df2=pd.merge(df[['user','open_time']],df[['user','close_time','close_cumsum']], on='user')</code></p>

<p>filter for 'close_time' &lt; 'open_time'. Then get the row with the max close_time</p>

<pre><code>df2=df2[df2['close_time']&lt;df2['open_time']]
idx = df2.groupby(['user','open_time'])['close_time'].transform(max) == df2['close_time']
df2=df2[idx]
</code></pre>

<p><strong>3)merge with the original dataset:</strong></p>

<pre><code>df3=pd.merge(df[['user','open_time','close_time','value']],df2[['user','open_time','close_cumsum']],how='left')
print(df3)
</code></pre>

<p>output:</p>

<pre><code>   user  open_time close_time  value  close_cumsum
0     1 2017-01-01 2017-03-01      5           NaN
1     1 2017-01-02 2017-02-01      6           NaN
2     1 2017-02-03 2017-02-05      7           6.0
3     1 2017-02-07 2017-04-01      3          13.0
4     1 2017-09-07 2017-09-11      1          21.0
5     2 2018-01-01 2018-02-01     15           NaN
6     2 2018-03-01 2018-04-01      3          15.0
</code></pre>

<p><strong>There is a more pandas way to get the same result?</strong></p>

<p><strong>Edit:</strong> I have added one data line to make the case more clear.
My goal is to get the sum of all transactions closed before the opening time of the new transaction</p>
",9220463,621,06-02-2018 15:38,06-02-2018 15:56,0,621,22,1,7,,"{'badge_counts': {'bronze': 22, 'silver': 7, 'gold': 1}, 'account_id': 12690017, 'is_employee': False, 'last_modified_date': 1705669500, 'last_access_date': 1709933126, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 621, 'creation_date': 1516035152, 'user_type': 'registered', 'user_id': 9220463, 'website_url': '', 'link': 'https://stackoverflow.com/users/9220463/riccardo-nizzolo', 'profile_image': 'https://www.gravatar.com/avatar/f9ea57eee3d0e8ba19d9eb8fb1b98b64?s=256&d=identicon&r=PG', 'display_name': 'riccardo nizzolo'}",There is a way to shift a dataframe column dependently on the condition on two other columns? something like: I have figured out a way to do this but it's inefficient: 1)Load data and create the column to shift output: 2) shift the column with a self-join and some filters Self-join (this is memory inefficient) filter for 'close_time' &lt; 'open_time'. Then get the row with the max close_time 3)merge with the original dataset: output: There is a more pandas way to get the same result? Edit: I have added one data line to make the case more clear. My goal is to get the sum of all transactions closed before the opening time of the new transaction,"df[""cumulated_closed_value""] = df.groupby(""user"").['close_cumsum'].shiftWhile(df['close_time']&gt;df['open_time])
 df=pd.read_csv('data.csv')
df.sort_values(['user','close_time'],inplace=True)
df['close_cumsum']=df.groupby('user')['value'].cumsum()
df.sort_values(['user','open_time'],inplace=True)
print(df)
    user  open_time close_time  value  close_cumsum
0     1 2017-01-01 2017-03-01      5            18
1     1 2017-01-02 2017-02-01      6             6
2     1 2017-02-03 2017-02-05      7            13
3     1 2017-02-07 2017-04-01      3            21
4     1 2017-09-07 2017-09-11      1            22
5     2 2018-01-01 2018-02-01     15            15
6     2 2018-03-01 2018-04-01      3            18
 df2=pd.merge(df[['user','open_time']],df[['user','close_time','close_cumsum']], on='user') df2=df2[df2['close_time']&lt;df2['open_time']]
idx = df2.groupby(['user','open_time'])['close_time'].transform(max) == df2['close_time']
df2=df2[idx]
 df3=pd.merge(df[['user','open_time','close_time','value']],df2[['user','open_time','close_cumsum']],how='left')
print(df3)
    user  open_time close_time  value  close_cumsum
0     1 2017-01-01 2017-03-01      5           NaN
1     1 2017-01-02 2017-02-01      6           NaN
2     1 2017-02-03 2017-02-05      7           6.0
3     1 2017-02-07 2017-04-01      3          13.0
4     1 2017-09-07 2017-09-11      1          21.0
5     2 2018-01-01 2018-02-01     15           NaN
6     2 2018-03-01 2018-04-01      3          15.0
",20,61,0,0,
969,48373845,48373959,30635,Loading model with custom loss + keras,5,<python><keras>,44,"<p>In Keras, if you need to have a custom loss with additional parameters, we can use it like mentioned on <a href=""https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras"">https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras</a></p>

<pre><code>def penalized_loss(noise):
    def loss(y_true, y_pred):
        return K.mean(K.square(y_pred - y_true) - K.square(y_true - noise), axis=-1)
    return loss
</code></pre>

<p>The above method works when I am training the model. However, once the model is trained I am having difficulty in loading the model. When I try to use the custom_objects parameter in load_model like below</p>

<pre><code>model = load_model(modelFile, custom_objects={'penalized_loss': penalized_loss} )
</code></pre>

<p>it complains <code>ValueError: Unknown loss function:loss</code></p>

<p>Is there any way to pass in the loss function as one of the custom losses in <code>custom_objects</code> ? From what I can gather, the inner function is not in the namespace during load_model call. Is there any easier way to load the model or use a custom loss with additional parameters</p>
",9249320,543,22-01-2018 02:15,22-01-2018 02:36,0,543,4,1,4,,"{'badge_counts': {'bronze': 4, 'silver': 4, 'gold': 1}, 'account_id': 9635260, 'is_employee': False, 'last_modified_date': 1611789486, 'last_access_date': 1516589069, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 543, 'creation_date': 1516586877, 'user_type': 'registered', 'user_id': 9249320, 'link': 'https://stackoverflow.com/users/9249320/jason', 'profile_image': 'https://www.gravatar.com/avatar/36ff96e5fa814b7ee02f6d5f99b9f015?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jason'}","In Keras, if you need to have a custom loss with additional parameters, we can use it like mentioned on https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras The above method works when I am training the model. However, once the model is trained I am having difficulty in loading the model. When I try to use the custom_objects parameter in load_model like below it complains Is there any way to pass in the loss function as one of the custom losses in ? From what I can gather, the inner function is not in the namespace during load_model call. Is there any easier way to load the model or use a custom loss with additional parameters","def penalized_loss(noise):
    def loss(y_true, y_pred):
        return K.mean(K.square(y_pred - y_true) - K.square(y_true - noise), axis=-1)
    return loss
 model = load_model(modelFile, custom_objects={'penalized_loss': penalized_loss} )
 ValueError: Unknown loss function:loss custom_objects",1,16,0,1,
970,49371931,49372657,73613,UnicodeError: UTF-16 stream does not start with BOM,4,<python><csv><error-handling>,12,"<p>I have trouble reading the csv file by python.
My csv file has Korean and numbers. </p>

<p>Below is my python code.</p>

<pre><code>import csv
import codecs
csvreader = csv.reader(codecs.open('1.csv', 'rU', 'utf-16'))
for row in csvreader:
    print(row)
</code></pre>

<p>First, there was a UnicodeDecodeError when I enter ""for row in csvreader"" line in the above code.</p>

<p>So I used the code below then the problem seemed to be solved</p>

<pre><code>csvreader = csv.reader(codecs.open('1.csv', 'rU', 'utf-16'))
</code></pre>

<p>Then I ran into NULL byte error. Then I can't figure out what's wrong with the csv file.</p>

<p>[update] I don't think I changed anything from the previous code but my program shows ""UnicodeError: UTF-16 stream does not start with BOM""</p>

<p>When I open the csv by excel I can see the table in proper format (image attached at the botton)
 but when I open it in sublime Text, below is a snippet of what I get.</p>

<pre><code>504b 0304 1400 0600 0800 0000 2100 6322
f979 7701 0000 d405 0000 1300 0802 5b43
6f6e 7465 6e74 5f54 7970 6573 5d2e 786d
6c20 a204 0228 a000 0200 0000 0000 0000
0000 0000 0000 0000 0000 0000 0000 0000
</code></pre>

<p>If you need more information about my file, let me know!</p>

<p>I appreciate your help.
Thanks in advance :)</p>

<p>csv file shown in excel</p>

<p><a href=""https://i.stack.imgur.com/ufFLM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ufFLM.png"" alt=""enter image description here""></a></p>

<p>csv file shown in sublime text
<a href=""https://i.stack.imgur.com/dsm2b.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dsm2b.png"" alt=""enter image description here""></a></p>
",9251988,159,19-03-2018 20:33,19-03-2018 21:31,0,159,8,1,2,,"{'badge_counts': {'bronze': 8, 'silver': 2, 'gold': 1}, 'account_id': 12310067, 'is_employee': False, 'last_modified_date': 1573678417, 'last_access_date': 1536336832, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 159, 'creation_date': 1516630507, 'user_type': 'registered', 'user_id': 9251988, 'link': 'https://stackoverflow.com/users/9251988/py11', 'profile_image': 'https://www.gravatar.com/avatar/0b65f7ed4cb9e527383ea5b04d6ae54e?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Py11'}","I have trouble reading the csv file by python. My csv file has Korean and numbers. Below is my python code. First, there was a UnicodeDecodeError when I enter ""for row in csvreader"" line in the above code. So I used the code below then the problem seemed to be solved Then I ran into NULL byte error. Then I can't figure out what's wrong with the csv file. [update] I don't think I changed anything from the previous code but my program shows ""UnicodeError: UTF-16 stream does not start with BOM"" When I open the csv by excel I can see the table in proper format (image attached at the botton) but when I open it in sublime Text, below is a snippet of what I get. If you need more information about my file, let me know! I appreciate your help. Thanks in advance :) csv file shown in excel csv file shown in sublime text","import csv
import codecs
csvreader = csv.reader(codecs.open('1.csv', 'rU', 'utf-16'))
for row in csvreader:
    print(row)
 csvreader = csv.reader(codecs.open('1.csv', 'rU', 'utf-16'))
 504b 0304 1400 0600 0800 0000 2100 6322
f979 7701 0000 d405 0000 1300 0802 5b43
6f6e 7465 6e74 5f54 7970 6573 5d2e 786d
6c20 a204 0228 a000 0200 0000 0000 0000
0000 0000 0000 0000 0000 0000 0000 0000
",8,44,2,2,
971,50321549,50322355,41233,map values in a dataframe from a dictionary using pyspark,3,<python><apache-spark><pyspark>,12,"<p>I want to know how to map values in a specific column in a dataframe. </p>

<p>I have a dataframe which looks like:</p>

<pre><code>df = sc.parallelize([('india','japan'),('usa','uruguay')]).toDF(['col1','col2'])

+-----+-------+
| col1|   col2|
+-----+-------+
|india|  japan|
|  usa|uruguay|
+-----+-------+
</code></pre>

<p>I have a dictionary from where I want to map the values.</p>

<pre><code>dicts = sc.parallelize([('india','ind'), ('usa','us'),('japan','jpn'),('uruguay','urg')])
</code></pre>

<p>The output I want is:</p>

<pre><code>+-----+-------+--------+--------+
| col1|   col2|col1_map|col2_map|
+-----+-------+--------+--------+
|india|  japan|     ind|     jpn|
|  usa|uruguay|      us|     urg|
+-----+-------+--------+--------+
</code></pre>

<p>I have tried using the <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=sort#pyspark.RDD.lookup"" rel=""noreferrer""><code>lookup function</code></a> but it doesn't work. It throws error SPARK-5063. Following is my approach which failed:</p>

<pre><code>def map_val(x):
    return dicts.lookup(x)[0]

myfun = udf(lambda x: map_val(x), StringType())

df = df.withColumn('col1_map', myfun('col1')) # doesn't work
df = df.withColumn('col2_map', myfun('col2')) # doesn't work
</code></pre>
",9299259,20811,13-05-2018 23:36,14-05-2018 02:19,1,20851,41,5,22,,"{'badge_counts': {'bronze': 41, 'silver': 22, 'gold': 5}, 'collectives': [{'collective': {'tags': ['google-cloud-save', 'firebase-admob', 'google-cloud-identity-aware-proxy', 'google-cloud-node', 'google-dataflow', 'jib', 'google-cloud-search', 'google-cloud-spanner', 'google-fusion-tables', 'google-cloud-data-fusion', 'google-cloud-interconnect', 'google-cloud-dataproc-metastore', 'google-cloud-repository', 'google-cloud-error-reporting', 'google-cloud-api-gateway', 'google-cloud-spanner-emulator', 'google-cloud-filestore', 'google-cloud-sql', 'google-cloud-endpoints-v2', 'firebase-dynamic-links', 'firebase-job-dispatcher', 'google-cloud-datalab', 'google-container-registry', 'gcloud', 'google-cloud-dataproc', 'google-app-engine-go', 'google-cloud-vpn', 'google-cloud-storage', 'google-cloud-workstations', 'google-cloud-tpu', 'google-app-engine-launch', 'google-cloud-instances', 'google-cloud-ai-platform-pipelines', 'google-cloud-iot', 'firebase-machine-learning', 'cordova-plugin-firebasex', 'google-cloud-resource-manager', 'dialogflow-es-fulfillment', 'firebase-polymer', 'google-cloud-logging', 'google-cloud-stackdriver', 'firebase-app-check', 'google-cloud-scheduler', 'firebase-console', 'firebase-performance', 'firebase-cli', 'firebase-queue', 'google-cloud-registry', 'google-cloud-composer', 'firebase-extensions', 'google-cloud-visualstudio', 'google-cloud-healthcare', 'google-cloud-webrisk', 'google-cloud-source-repos', 'google-bigquery', 'google-cloud-vision', 'google-cloud-metrics', 'google-cloud-shell-editor', 'google-cloud-network-load-balancer', 'google-cloud-deploy', 'google-cloud-instance-template', 'rest-firebase', 'google-cloud-messaging', 'firebase-predictions', 'google-cloud-url-maps', 'google-cloud-platform', 'google-cloud-translate', 'google-app-engine-php', 'google-cloud-ai', 'google-cloud-cdn', 'google-cloud-vertex-ai', 'google-cloud-marketplace', 'google-cloud-memorystore', 'firebase-in-app-messaging', 'google-cloud-networking', 'google-cloud-tasks', 'firebase-database', 'firebase-cloud-messaging', 'firebase-assistant', 'google-migrate-for-compute-engine', 'google-cloud-http-load-balancer', 'dialogflow-cx', 'firebase-mlkit', 'google-cloud-language', 'firebase-app-distribution', 'google-kubernetes-engine', 'react-redux-firebase', 'firebase-authentication', 'google-cloud-storage-r', 'google-cloud-automl', 'google-cloud-run', 'firebase-test-lab', 'google-cloud-billing', 'google-cloud-profiler', 'stackdriver', 'google-cloud-print', 'google-app-engine-python', 'google-compute-engine', 'firebase-remote-config', 'google-cloud-monitoring', 'apigee-baas', 'dialogflow-es', 'google-cloud-print-privet', 'maven-jib', 'google-cloud-pubsublite', 'firebase-analytics', 'google-cloud-shell', 'firebase-app-indexing', 'firebase-util', 'google-cloud-kms', 'firebase-ab-testing', 'google-cloud-internal-load-balancer', 'google-cloud-ml-engine', 'google-cloud-code', 'google-cloud-launcher', 'firebaseui', 'firebase-crash-reporting', 'firebasesimplelogin', 'looker', 'google-cloud-test-lab', 'firebase-invites', 'google-cloud-endpoints', 'vertex-ai-search', 'google-cloud-dlp', 'firebase-notifications', 'google-cloud-dns', 'google-anthos', 'google-cloud-cpp', 'google-cloud-talent-solution', 'firebase-admin', 'google-cloud-proxy', 'google-cloud-asset-inventory', 'google-cloud-intellij', 'firebase-storage', 'google-app-engine-golang', 'google-cloud-nl', 'looker-studio', 'google-cloud-build', 'google-cloud-trace', 'google-cloud-pubsub', 'google-app-engine-deploy', 'google-app-engine', 'google-translate', 'google-cloud-recommendation', 'google-container-os', 'google-cloud-bigtable', 'google-cloud-dataflow', 'nativescript-firebase', 'google-app-engine-patch', 'firebase', 'apigee', 'google-cloud-firestore', 'recaptcha-enterprise', 'google-cloud-dataprep', 'google-container-builder', 'firebase-tools', 'react-native-firebase', 'google-cloud-console', 'google-prediction', 'google-cloud-powershell', 'google-cloud-debugger', 'google-cloud-python', 'google-cloud-php-client', 'google-cloud-ops-agent', 'google-cloud-identity', 'firebase-realtime-database', 'bigtable', 'google-cloud-load-balancer', 'google-cloud-tools', 'redux-saga-firebase', 'google-cloud-datastore', 'google-cloud-data-transfer', 'google-cloud-armor', 'firebase-security', 'google-cloud-ml', 'google-cloud-robotics', 'google-cloud-speech', 'google-cloud-iam', 'google-cloud-sdk', 'google-cloud-functions', 'google-cloud-automl-nl', 'cloud-document-ai', 'google-container-optimized-os', 'firebase-hosting', 'google-analytics-firebase', 'google-cloud-router', 'google-cloud-transcoder'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}], 'account_id': 12855909, 'is_employee': False, 'last_modified_date': 1699822201, 'last_access_date': 1708268457, 'reputation_change_year': 360, 'reputation_change_quarter': 360, 'reputation_change_month': 90, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 20851, 'creation_date': 1517480061, 'user_type': 'registered', 'user_id': 9299259, 'location': 'Berlin, Germany', 'website_url': 'https://www.linkedin.com/in/saraswatmanish/', 'link': 'https://stackoverflow.com/users/9299259/yolo', 'profile_image': 'https://i.stack.imgur.com/7A5cq.jpg?s=256&g=1', 'display_name': 'YOLO'}",I want to know how to map values in a specific column in a dataframe. I have a dataframe which looks like: I have a dictionary from where I want to map the values. The output I want is: I have tried using the but it doesn't work. It throws error SPARK-5063. Following is my approach which failed:,"df = sc.parallelize([('india','japan'),('usa','uruguay')]).toDF(['col1','col2'])

+-----+-------+
| col1|   col2|
+-----+-------+
|india|  japan|
|  usa|uruguay|
+-----+-------+
 dicts = sc.parallelize([('india','ind'), ('usa','us'),('japan','jpn'),('uruguay','urg')])
 +-----+-------+--------+--------+
| col1|   col2|col1_map|col2_map|
+-----+-------+--------+--------+
|india|  japan|     ind|     jpn|
|  usa|uruguay|      us|     urg|
+-----+-------+--------+--------+
 lookup function def map_val(x):
    return dicts.lookup(x)[0]

myfun = udf(lambda x: map_val(x), StringType())

df = df.withColumn('col1_map', myfun('col1')) # doesn't work
df = df.withColumn('col2_map', myfun('col2')) # doesn't work
",17,39,0,1,
972,48624415,49479143,58214,"how to fix CMake Error in CMakeLists.txt: Generator NMake Makefiles does not support platform specification, but platform x64 was specified",8,<python><windows><cmake>,27,"<p>I want to install dlib using <em>pip install dlib</em> using cmd in windows 10
But it is showing following three errors:
 CMake Error in CMakeLists.txt:
      Generator</p>

<pre><code>    NMake Makefiles

  does not support platform specification, but platform

    x64

  was specified.


CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage
CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage
</code></pre>

<p>INFORMATION:
pip 9.0.1 from d:\python36\lib\site-packages (python 3.6)
cmake 0.9.0
windows 10 pro(64-bit)
Version:  10.0.16299 Build 16299 </p>
",9317030,310,05-02-2018 14:08,25-03-2018 18:07,48,310,8,1,3,,"{'badge_counts': {'bronze': 8, 'silver': 3, 'gold': 1}, 'account_id': 12882392, 'is_employee': False, 'last_modified_date': 1573678406, 'last_access_date': 1638977671, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 310, 'creation_date': 1517839088, 'user_type': 'registered', 'user_id': 9317030, 'website_url': '', 'link': 'https://stackoverflow.com/users/9317030/owase-sayyad', 'profile_image': 'https://www.gravatar.com/avatar/138880a20cfccc1208ed897a602244af?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Owase Sayyad'}",I want to install dlib using pip install dlib using cmd in windows 10 But it is showing following three errors: CMake Error in CMakeLists.txt: Generator INFORMATION: pip 9.0.1 from d:\python36\lib\site-packages (python 3.6) cmake 0.9.0 windows 10 pro(64-bit) Version: 10.0.16299 Build 16299,"    NMake Makefiles

  does not support platform specification, but platform

    x64

  was specified.


CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage
CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage
",10,23,0,0,
973,48783894,48783962,2411,Python splitting list to sublists at given start/end keywords,5,<python><list><loops><sublist>,15,"<p>If I were to have a list, say </p>

<pre><code>lst = ['hello', 'foo', 'test', 'world', 'bar', 'idk']
</code></pre>

<p>I'd like to split it into a sublist with <code>'foo'</code> and <code>'bar'</code> as start and end keywords, so that I would get</p>

<pre><code>lst = ['hello', ['foo', 'test', 'world', 'bar'], 'idk']
</code></pre>

<p>The way I am currently doing this is as follows.</p>

<pre><code>def findLoop(t):   
    inds = [index for index, item in enumerate(t) if item in [""FOO"", ""BAR""]]
    centre = inds[(len(inds)/2)-1:(len(inds)/2)+1]
    newCentre = t[centre[0]:centre[1]+1]
    return t[:centre[0]] + [newCentre] + t[centre[1]+1:]

def getLoops(t):
    inds = len([index for index, item in enumerate(t) if item in [""FOO"", ""BAR""]])
    for i in range(inds):
        t = findLoop(t)
    return t
</code></pre>

<p>This looks a bit messy, but it works very well for nested start/end keywords, so sublists can be formed inside of sublists, but it does not work for multiple start/end keywords not being inside eachother. Being nested is not important yet, so any help would be appreciated.</p>
",9319953,265,14-02-2018 09:40,14-02-2018 09:44,0,265,8,0,1,,"{'badge_counts': {'bronze': 8, 'silver': 1, 'gold': 0}, 'account_id': 9519040, 'is_employee': False, 'last_modified_date': 1573678406, 'last_access_date': 1600570064, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 265, 'creation_date': 1517891006, 'user_type': 'registered', 'user_id': 9319953, 'location': 'whitehead', 'link': 'https://stackoverflow.com/users/9319953/leo-whitehead', 'profile_image': 'https://www.gravatar.com/avatar/4d534726133231fc750e1134a6ea23d8?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Leo Whitehead'}","If I were to have a list, say I'd like to split it into a sublist with and as start and end keywords, so that I would get The way I am currently doing this is as follows. This looks a bit messy, but it works very well for nested start/end keywords, so sublists can be formed inside of sublists, but it does not work for multiple start/end keywords not being inside eachother. Being nested is not important yet, so any help would be appreciated.","lst = ['hello', 'foo', 'test', 'world', 'bar', 'idk']
 'foo' 'bar' lst = ['hello', ['foo', 'test', 'world', 'bar'], 'idk']
 def findLoop(t):   
    inds = [index for index, item in enumerate(t) if item in [""FOO"", ""BAR""]]
    centre = inds[(len(inds)/2)-1:(len(inds)/2)+1]
    newCentre = t[centre[0]:centre[1]+1]
    return t[:centre[0]] + [newCentre] + t[centre[1]+1:]

def getLoops(t):
    inds = len([index for index, item in enumerate(t) if item in [""FOO"", ""BAR""]])
    for i in range(inds):
        t = findLoop(t)
    return t
",8,26,0,0,
974,49060525,51163568,10691,Install jupyterlab in pip3 throws 'TypeError: expected string or bytes-like object',3,<python><python-3.x><jupyter-notebook><jupyter-lab>,12,"<p><strong>Background</strong></p>

<ul>
<li><p>I was trying to install Jupyter Lab using <code>pip3 install jupyterlab</code>, and it threw the same error.</p></li>
<li><p>I did not figure it out. Found a workaround using <code>pip2 install jupyterlab</code> -- it worked, but clearly was a bandaid fix.</p></li>
<li><p>tried running <code>import pandas as pd</code> in the python3 notebook (inside jupyter lab); module was not found.</p></li>
<li><p>I am using Cygwin as my linux emulator on a PC.</p></li>
<li><p>I prefer python3.x over python 2.7.</p></li>
</ul>

<p><strong>My attempts to resolve the issue</strong></p>

<ul>
<li><p><em>Similar reported JupyterLab post</em> - I found this post here: <a href=""https://stackoverflow.com/questions/35253338/import-pandas-on-jupyter-ipython-notebook-fails"">Import pandas on jupyter ipython notebook fails</a>, but I believe the author is using anaconda (which I am not) and the Type Error is different.</p></li>
<li><p>I found a post relevant to my TypeError, but that's for a code he's writing: <a href=""https://stackoverflow.com/questions/43854086/getting-a-typeerror-expected-string-or-bytes-like-object"">Getting a TypeError: expected string or bytes-like object</a>.</p></li>
</ul>

<p>I have a hard time interpreting the Traceback, but here it is below:</p>

<pre><code>$ pip install --upgrade pip
Exception:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/usr/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""/usr/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""/usr/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""/usr/lib/python3.6/site-packages/pip/req/req_set.py"", line 487, in _prepare_file
    req_to_install, finder)
  File ""/usr/lib/python3.6/site-packages/pip/req/req_set.py"", line 428, in _check_skip_installed
    req_to_install, upgrade_allowed)
  File ""/usr/lib/python3.6/site-packages/pip/index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""/usr/lib/python3.6/site-packages/pip/index.py"", line 427, in find_all_candidates
    self._package_versions(page.links, search)
  File ""/usr/lib/python3.6/site-packages/pip/index.py"", line 595, in _package_versions
    v = self._link_package_versions(link, search)
  File ""/usr/lib/python3.6/site-packages/pip/index.py"", line 667, in _link_package_versions
    support_this_python = check_requires_python(link.requires_python)
  File ""/usr/lib/python3.6/site-packages/pip/utils/packaging.py"", line 34, in check_requires_python
    return python_version in requires_python_specifier
  File ""/usr/lib/python3.6/site-packages/packaging/specifiers.py"", line 698, in __contains__
    return self.contains(item)
  File ""/usr/lib/python3.6/site-packages/packaging/specifiers.py"", line 703, in contains
    item = parse(item)
  File ""/usr/lib/python3.6/site-packages/packaging/version.py"", line 31, in parse
    return Version(version)
  File ""/usr/lib/python3.6/site-packages/packaging/version.py"", line 200, in __init__
    match = self._regex.search(version)
TypeError: expected string or bytes-like object
</code></pre>

<p>Why is the version returned not a string or byte? Is there a bug in my python 3.6 package itself? I am thinking about opening up those files in the traceback to see if I can debug it, but I thought I should ask for help before attempting something that sounds a little ridiculous, imo.</p>

<p><strong>Other things I have tried</strong></p>

<ul>
<li><code>pip2 install --upgrade pip</code> - this returns a message saying pip in python2.7 is up-to-date.</li>
</ul>

<p>Traceback (pip2):</p>

<pre><code>$ pip2 install --upgrade pip
Requirement already up-to-date: pip in /usr/lib/python2.7/site-packages
</code></pre>

<ul>
<li><code>pip3 install --upgrade pip</code> - this returns the same TypeError above</li>
</ul>

<p>Traceback (pip3): <em>see the first Traceback</em></p>

<p><strong>pip version and python site output</strong></p>

<pre><code>$ pip2 --version
pip 9.0.1 from /usr/lib/python2.7/site-packages (python 2.7)

$ python -m site
sys.path = [
    '/home/jennings/Documents/hpv/hpv_missed_clinics',
    '/usr/lib/python2.7/site-packages/logilab_common-1.3.0-py2.7.egg',
    '/usr/lib/python2.7/site-packages/six-1.9.0-py2.7.egg',
    '/usr/lib/python27.zip',
    '/usr/lib/python2.7',
    '/usr/lib/python2.7/plat-cygwin',
    '/usr/lib/python2.7/lib-tk',
    '/usr/lib/python2.7/lib-old',
    '/usr/lib/python2.7/lib-dynload',
    '/usr/lib/python2.7/site-packages',
    '/usr/lib/python2.7/site-packages/gtk-2.0',
    '/usr/lib/python2.7/site-packages/wx-3.0-gtk3',
]
USER_BASE: '/home/jennings/.local' (exists)
USER_SITE: '/home/jennings/.local/lib/python2.7/site-packages' (doesn't exist)
ENABLE_USER_SITE: True

$ pip --version
pip 9.0.1 from /usr/lib/python3.6/site-packages (python 3.6)

$ python3 -m site
sys.path = [
    '/home/jennings/Documents/hpv/hpv_missed_clinics',
    '/usr/lib/python36.zip',
    '/usr/lib/python3.6',
    '/usr/lib/python3.6/lib-dynload',
    '/usr/lib/python3.6/site-packages',
    '/home/jennings/Documents/kaggle/SQL_scavenger/src/bq-helper',
    '/usr/lib/python3.6/site-packages/linkgrammar',
]
USER_BASE: '/home/jennings/.local' (exists)
USER_SITE: '/home/jennings/.local/lib/python3.6/site-packages' (doesn't exist)
ENABLE_USER_SITE: True
</code></pre>
",9335288,448,01-03-2018 23:38,03-07-2018 22:30,124,448,18,1,6,,"{'badge_counts': {'bronze': 18, 'silver': 6, 'gold': 1}, 'account_id': 4531985, 'is_employee': False, 'last_modified_date': 1657704600, 'last_access_date': 1711049770, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 448, 'creation_date': 1518121046, 'user_type': 'registered', 'user_id': 9335288, 'link': 'https://stackoverflow.com/users/9335288/jennings', 'profile_image': 'https://www.gravatar.com/avatar/cb0dcbc684dd5932829210df1a39695a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jennings'}","Background I was trying to install Jupyter Lab using , and it threw the same error. I did not figure it out. Found a workaround using -- it worked, but clearly was a bandaid fix. tried running in the python3 notebook (inside jupyter lab); module was not found. I am using Cygwin as my linux emulator on a PC. I prefer python3.x over python 2.7. My attempts to resolve the issue Similar reported JupyterLab post - I found this post here: Import pandas on jupyter ipython notebook fails, but I believe the author is using anaconda (which I am not) and the Type Error is different. I found a post relevant to my TypeError, but that's for a code he's writing: Getting a TypeError: expected string or bytes-like object. I have a hard time interpreting the Traceback, but here it is below: Why is the version returned not a string or byte? Is there a bug in my python 3.6 package itself? I am thinking about opening up those files in the traceback to see if I can debug it, but I thought I should ask for help before attempting something that sounds a little ridiculous, imo. Other things I have tried - this returns a message saying pip in python2.7 is up-to-date. Traceback (pip2): - this returns the same TypeError above Traceback (pip3): see the first Traceback pip version and python site output","pip3 install jupyterlab pip2 install jupyterlab import pandas as pd $ pip install --upgrade pip
Exception:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/usr/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""/usr/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""/usr/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""/usr/lib/python3.6/site-packages/pip/req/req_set.py"", line 487, in _prepare_file
    req_to_install, finder)
  File ""/usr/lib/python3.6/site-packages/pip/req/req_set.py"", line 428, in _check_skip_installed
    req_to_install, upgrade_allowed)
  File ""/usr/lib/python3.6/site-packages/pip/index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""/usr/lib/python3.6/site-packages/pip/index.py"", line 427, in find_all_candidates
    self._package_versions(page.links, search)
  File ""/usr/lib/python3.6/site-packages/pip/index.py"", line 595, in _package_versions
    v = self._link_package_versions(link, search)
  File ""/usr/lib/python3.6/site-packages/pip/index.py"", line 667, in _link_package_versions
    support_this_python = check_requires_python(link.requires_python)
  File ""/usr/lib/python3.6/site-packages/pip/utils/packaging.py"", line 34, in check_requires_python
    return python_version in requires_python_specifier
  File ""/usr/lib/python3.6/site-packages/packaging/specifiers.py"", line 698, in __contains__
    return self.contains(item)
  File ""/usr/lib/python3.6/site-packages/packaging/specifiers.py"", line 703, in contains
    item = parse(item)
  File ""/usr/lib/python3.6/site-packages/packaging/version.py"", line 31, in parse
    return Version(version)
  File ""/usr/lib/python3.6/site-packages/packaging/version.py"", line 200, in __init__
    match = self._regex.search(version)
TypeError: expected string or bytes-like object
 pip2 install --upgrade pip $ pip2 install --upgrade pip
Requirement already up-to-date: pip in /usr/lib/python2.7/site-packages
 pip3 install --upgrade pip $ pip2 --version
pip 9.0.1 from /usr/lib/python2.7/site-packages (python 2.7)

$ python -m site
sys.path = [
    '/home/jennings/Documents/hpv/hpv_missed_clinics',
    '/usr/lib/python2.7/site-packages/logilab_common-1.3.0-py2.7.egg',
    '/usr/lib/python2.7/site-packages/six-1.9.0-py2.7.egg',
    '/usr/lib/python27.zip',
    '/usr/lib/python2.7',
    '/usr/lib/python2.7/plat-cygwin',
    '/usr/lib/python2.7/lib-tk',
    '/usr/lib/python2.7/lib-old',
    '/usr/lib/python2.7/lib-dynload',
    '/usr/lib/python2.7/site-packages',
    '/usr/lib/python2.7/site-packages/gtk-2.0',
    '/usr/lib/python2.7/site-packages/wx-3.0-gtk3',
]
USER_BASE: '/home/jennings/.local' (exists)
USER_SITE: '/home/jennings/.local/lib/python2.7/site-packages' (doesn't exist)
ENABLE_USER_SITE: True

$ pip --version
pip 9.0.1 from /usr/lib/python3.6/site-packages (python 3.6)

$ python3 -m site
sys.path = [
    '/home/jennings/Documents/hpv/hpv_missed_clinics',
    '/usr/lib/python36.zip',
    '/usr/lib/python3.6',
    '/usr/lib/python3.6/lib-dynload',
    '/usr/lib/python3.6/site-packages',
    '/home/jennings/Documents/kaggle/SQL_scavenger/src/bq-helper',
    '/usr/lib/python3.6/site-packages/linkgrammar',
]
USER_BASE: '/home/jennings/.local' (exists)
USER_SITE: '/home/jennings/.local/lib/python3.6/site-packages' (doesn't exist)
ENABLE_USER_SITE: True
",66,116,0,2,
975,48717794,48717931,63889,matplotlib embed figures in auto generated html,2,<python><html><matplotlib>,34,"<p>I want to embed a figure generated by python matplotlib into a html file with other content. Is that possible?</p>

<p>What I have thought is saving figures as png file and then use <code>&lt;img&gt;</code> tag to refer to it.</p>

<p>Some code I was trying to use are like:</p>

<pre><code>import matplotlib.pyplot as plt
fig = plt.figure()
#plot sth
plt.savefig('test.png')

html = 'Some html head' + '&lt;img src=\'test.png\'&gt;' + 'Some more html'

with open('test.html','w') as f:
    f.write(html)
</code></pre>

<p>However, this will generate two files instead of one and I don't have a server to host the png file. Is that possible to embed the figure in the html? How do I do it in python.</p>

<p>Thank you.</p>
",9341366,343,10-02-2018 06:06,10-02-2018 06:27,0,343,6,1,3,,"{'badge_counts': {'bronze': 6, 'silver': 3, 'gold': 1}, 'account_id': 12917742, 'is_employee': False, 'last_modified_date': 1573678403, 'last_access_date': 1536866185, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 343, 'creation_date': 1518240086, 'user_type': 'registered', 'user_id': 9341366, 'link': 'https://stackoverflow.com/users/9341366/user9341366', 'profile_image': 'https://www.gravatar.com/avatar/f89521a3d3d2d9283eee8db724930178?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'user9341366'}","I want to embed a figure generated by python matplotlib into a html file with other content. Is that possible? What I have thought is saving figures as png file and then use tag to refer to it. Some code I was trying to use are like: However, this will generate two files instead of one and I don't have a server to host the png file. Is that possible to embed the figure in the html? How do I do it in python. Thank you.","&lt;img&gt; import matplotlib.pyplot as plt
fig = plt.figure()
#plot sth
plt.savefig('test.png')

html = 'Some html head' + '&lt;img src=\'test.png\'&gt;' + 'Some more html'

with open('test.html','w') as f:
    f.write(html)
",7,20,0,0,
976,50348975,50349027,153171,How to change python version in command prompt if I have 2 python version installed,7,<python>,22,"<p>I have installed <code>Anaconda 2 &amp; 3</code> in my system. <code>Anaconda 2</code> contains <code>python 2.7</code> &amp; <code>Anaconda 3</code> contains <code>python 3.6</code>.</p>

<p>I need to run my python code using command prompt &amp; I need to use python 3.6
While I'm running <code>python --version</code>, I'm getting <code>python 2.7.14</code>. How do I change it to python 3.6?</p>
",9343748,365,15-05-2018 11:23,15-05-2018 11:26,0,365,10,1,5,,"{'badge_counts': {'bronze': 10, 'silver': 5, 'gold': 1}, 'account_id': 12921420, 'is_employee': False, 'last_modified_date': 1681912135, 'last_access_date': 1526414940, 'reputation_change_year': 10, 'reputation_change_quarter': 10, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 365, 'creation_date': 1518294427, 'user_type': 'registered', 'user_id': 9343748, 'location': 'USA', 'link': 'https://stackoverflow.com/users/9343748/emily-johnson', 'profile_image': 'https://www.gravatar.com/avatar/6f80670a61f7533987edc28ce040b52a?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Emily Johnson'}","I have installed in my system. contains &amp; contains . I need to run my python code using command prompt &amp; I need to use python 3.6 While I'm running , I'm getting . How do I change it to python 3.6?",Anaconda 2 &amp; 3 Anaconda 2 python 2.7 Anaconda 3 python 3.6 python --version python 2.7.14,-7,4,0,0,
977,49901806,49905997,23121,Warning: Please use alternatives such as official/mnist/dataset.py from tensorflow/models,2,<python><python-3.x><tensorflow>,21,"<p>I'm doing a simple tutorial using Tensorflow, I have just installed so it should be updated, first I load the mnist data using the following code:</p>

<pre><code>import numpy as np
import os
from tensorflow.examples.tutorials.mnist import input_data
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
train_data = mnist.train.images  # Returns np.array
train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
eval_data = mnist.test.images  # Returns np.array
eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)
</code></pre>

<p>But when I run it I get the following warning:</p>

<pre><code>WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
WARNING:tensorflow:From C:/Users/user/PycharmProjects/TensorFlowRNN/sample.py:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-labels-idx1-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
</code></pre>

<p>I have used the line <code>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'</code> which should avoid getting warnings and tried other alternatives to obtain mnist, however always appear the same warnings, can someone help me figure out is this happening?</p>

<p>PD: I am using Python 3.6 in windows 10, in case it helps.</p>
",9349674,505,18-04-2018 14:04,18-04-2018 17:48,0,505,14,1,4,,"{'badge_counts': {'bronze': 14, 'silver': 4, 'gold': 1}, 'account_id': 12930506, 'is_employee': False, 'last_modified_date': 1573678401, 'last_access_date': 1638439281, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 505, 'creation_date': 1518435527, 'user_type': 'registered', 'user_id': 9349674, 'website_url': '', 'link': 'https://stackoverflow.com/users/9349674/jorge-rodriguez-molinuevo', 'profile_image': 'https://www.gravatar.com/avatar/3f176de9731f45e27f79e7c081f5a1e0?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Jorge Rodriguez Molinuevo'}","I'm doing a simple tutorial using Tensorflow, I have just installed so it should be updated, first I load the mnist data using the following code: But when I run it I get the following warning: I have used the line which should avoid getting warnings and tried other alternatives to obtain mnist, however always appear the same warnings, can someone help me figure out is this happening? PD: I am using Python 3.6 in windows 10, in case it helps.","import numpy as np
import os
from tensorflow.examples.tutorials.mnist import input_data
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
train_data = mnist.train.images  # Returns np.array
train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
eval_data = mnist.test.images  # Returns np.array
eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)
 WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
WARNING:tensorflow:From C:/Users/user/PycharmProjects/TensorFlowRNN/sample.py:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-labels-idx1-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'",32,46,0,0,
978,49177491,49177744,4455,Where is the API documentation for boto3 resources?,2,<python><amazon-web-services><boto3>,11,"<p>I have learned that <code>boto3</code> offers two levels of abstraction: a low-level API called <code>client</code> that is a thin wrapper around the AWS HTTP API, and a high-level client called <code>resource</code> that offers real Python objects. My question is, where is the API documentation for the <code>resource</code> API?</p>

<p>I found this:</p>

<p><a href=""https://boto3.readthedocs.io/en/stable/reference/services/ec2.html#client"" rel=""noreferrer"">https://boto3.readthedocs.io/en/stable/reference/services/ec2.html#client</a></p>

<p>But that describes the <code>client</code> API, and there's not a 1-to-1 mapping to the <code>resource</code> API. For example, enumerating instances is called <code>describe_instances()</code> on the <code>client</code> object, and it is called <code>instances.all()</code> on the <code>resource</code> object.</p>

<p>Next I found this:</p>

<p><a href=""http://boto3.readthedocs.io/en/stable/reference/core/resources.html?highlight=resource"" rel=""noreferrer"">http://boto3.readthedocs.io/en/stable/reference/core/resources.html?highlight=resource</a></p>

<p>This describes a set of base classes and factory methods, but it doesn't describe the API for a specific service like EC2.</p>

<p>At runtime, I printed out an object of interest and found that it is a <code>boto3.resources.factory.ec2.ServiceResource</code>, but searching the <code>boto3</code> documentation doesn't show me any human-readable documentation for this resource.</p>

<p>Is there an API document that explains what all of the different Python classes are, and what properties/methods they have? I can print this out at runtime, e.g. <code>print(dir(ec2))</code> but this is a pretty tedious way to discover the API.</p>
",122763,26296,08-03-2018 16:05,08-03-2018 16:18,0,26316,72,12,66,88,"{'badge_counts': {'bronze': 72, 'silver': 66, 'gold': 12}, 'account_id': 42163, 'is_employee': False, 'last_modified_date': 1708566000, 'last_access_date': 1711136790, 'reputation_change_year': 121, 'reputation_change_quarter': 121, 'reputation_change_month': 30, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 26316, 'creation_date': 1244994928, 'user_type': 'registered', 'user_id': 122763, 'accept_rate': 88, 'location': 'McLean, VA, United States', 'website_url': 'https://markhaa.se', 'link': 'https://stackoverflow.com/users/122763/mark-e-haase', 'profile_image': 'https://i.stack.imgur.com/7YWpy.jpg?s=256&g=1', 'display_name': 'Mark E. Haase'}","I have learned that offers two levels of abstraction: a low-level API called that is a thin wrapper around the AWS HTTP API, and a high-level client called that offers real Python objects. My question is, where is the API documentation for the API? I found this: https://boto3.readthedocs.io/en/stable/reference/services/ec2.html#client But that describes the API, and there's not a 1-to-1 mapping to the API. For example, enumerating instances is called on the object, and it is called on the object. Next I found this: http://boto3.readthedocs.io/en/stable/reference/core/resources.html?highlight=resource This describes a set of base classes and factory methods, but it doesn't describe the API for a specific service like EC2. At runtime, I printed out an object of interest and found that it is a , but searching the documentation doesn't show me any human-readable documentation for this resource. Is there an API document that explains what all of the different Python classes are, and what properties/methods they have? I can print this out at runtime, e.g. but this is a pretty tedious way to discover the API.",boto3 client resource resource client resource describe_instances() client instances.all() resource boto3.resources.factory.ec2.ServiceResource boto3 print(dir(ec2)),-13,17,0,2,
979,48925086,48925457,4502,Choosing subset of farthest points in given set of points,4,<python><algorithm><computational-geometry><dimensionality-reduction><multi-dimensional-scaling>,16,"<p>Imagine you are given set S of n points in 3 dimensions. Distance between any 2 points is simple Euclidean distance. You want to chose subset Q of k points from this set such that they are farthest from each other. In other words there is no other subset Q’ of k points exists such that min of all pair wise distances in Q is less than that in Q’.</p>

<p>If n is approximately 16 million and k is about 300, how do we efficiently do this?</p>

<p>My guess is that this NP-hard so may be we just want to focus on approximation. One idea I can think of is using Multidimensional scaling to sort these points in a line and then use version of binary search to get points that are furthest apart on this line. </p>
",207661,65578,22-02-2018 10:29,22-02-2018 10:47,0,65698,191,18,244,64,"{'badge_counts': {'bronze': 191, 'silver': 244, 'gold': 18}, 'account_id': 71838, 'is_employee': False, 'last_modified_date': 1698617400, 'last_access_date': 1710302962, 'reputation_change_year': 954, 'reputation_change_quarter': 954, 'reputation_change_month': 190, 'reputation_change_week': 100, 'reputation_change_day': 0, 'reputation': 65698, 'creation_date': 1257847064, 'user_type': 'registered', 'user_id': 207661, 'accept_rate': 64, 'location': 'Seattle, WA', 'website_url': 'http://www.ShitalShah.com', 'link': 'https://stackoverflow.com/users/207661/shital-shah', 'profile_image': 'https://i.stack.imgur.com/CFPkp.jpg?s=256&g=1', 'display_name': 'Shital Shah'}","Imagine you are given set S of n points in 3 dimensions. Distance between any 2 points is simple Euclidean distance. You want to chose subset Q of k points from this set such that they are farthest from each other. In other words there is no other subset Q’ of k points exists such that min of all pair wise distances in Q is less than that in Q’. If n is approximately 16 million and k is about 300, how do we efficiently do this? My guess is that this NP-hard so may be we just want to focus on approximation. One idea I can think of is using Multidimensional scaling to sort these points in a line and then use version of binary search to get points that are furthest apart on this line.",,0,5,0,0,
980,48282841,48300537,3447,"In trio, how can I have a background task that lives as long as my object does?",1,<python><async-await><python-trio>,38,"<p>I'm writing a class that will spawn tasks during its lifetime. Since I'm using <a href=""https://trio.readthedocs.io/"" rel=""noreferrer"">Trio</a>, I can't spawn tasks without a nursery. My first thought was to have a <code>self._nursery</code> in my class that I can spawn tasks into. But it seems that nursery objects can only be used in a context manager, so they are always closed in the same scope where they were created. I don't want to pass in a nursery from outside because it's an implementation detail, but I do want my objects to be able to spawn tasks that last as long as the object does (e.g. a heartbeat task).</p>

<p>How can I write such a class, which has long-lived background tasks, using Trio?</p>
",296473,1670,16-01-2018 13:52,17-01-2018 11:53,1,1670,41,0,20,80,"{'badge_counts': {'bronze': 41, 'silver': 20, 'gold': 0}, 'account_id': 112390, 'is_employee': False, 'last_modified_date': 1607615284, 'last_access_date': 1711167564, 'reputation_change_year': 29, 'reputation_change_quarter': 29, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1670, 'creation_date': 1268913994, 'user_type': 'registered', 'user_id': 296473, 'accept_rate': 80, 'website_url': '', 'link': 'https://stackoverflow.com/users/296473/lilydjwg', 'profile_image': 'https://www.gravatar.com/avatar/3e3c1cb94b9ad3551cd9d43f2e52e105?s=256&d=identicon&r=PG', 'display_name': 'lilydjwg'}","I'm writing a class that will spawn tasks during its lifetime. Since I'm using Trio, I can't spawn tasks without a nursery. My first thought was to have a in my class that I can spawn tasks into. But it seems that nursery objects can only be used in a context manager, so they are always closed in the same scope where they were created. I don't want to pass in a nursery from outside because it's an implementation detail, but I do want my objects to be able to spawn tasks that last as long as the object does (e.g. a heartbeat task). How can I write such a class, which has long-lived background tasks, using Trio?",self._nursery,-1,3,0,1,
981,48919003,48921208,24195,Pandas split on regex,3,<python><regex><pandas>,12,"<p>I have pandas df with a column containing comma-delimited characteristics like so:</p>

<pre><code>Shot - Wounded/Injured, Shot - Dead (murder, accidental, suicide), Suicide - Attempt, Murder/Suicide, Attempted Murder/Suicide (one variable unsuccessful), Institution/Group/Business, Mass Murder (4+ deceased victims excluding the subject/suspect/perpetrator , one location), Mass Shooting (4+ victims injured or killed excluding the subject/suspect
</code></pre>

<p>I would like to split this column into multiple dummy-variable columns, but cannot figure out how to start this process.  I am trying to split on columns like so:</p>

<pre><code>df['incident_characteristics'].str.split(',', expand=True)
</code></pre>

<p>This doesn't work, however, because there are commas in the middle of descriptions.  Instead, I need to split based on a regex match of a comma followed by a space and a capital letter.  Can str.split take regex?  If so, how is this done?</p>

<p>I think this Regex will do what I need:</p>

<pre><code>,\s[A-Z]
</code></pre>
",372526,11337,22-02-2018 03:07,22-02-2018 06:47,0,11347,167,30,98,93,"{'badge_counts': {'bronze': 167, 'silver': 98, 'gold': 30}, 'account_id': 155700, 'is_employee': False, 'last_modified_date': 1708738800, 'last_access_date': 1711051512, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 11347, 'creation_date': 1277000907, 'user_type': 'registered', 'user_id': 372526, 'accept_rate': 93, 'link': 'https://stackoverflow.com/users/372526/parseltongue', 'profile_image': 'https://www.gravatar.com/avatar/4d17467765f5923e9451ad0cbbd89848?s=256&d=identicon&r=PG&f=y&so-version=2', 'display_name': 'Parseltongue'}","I have pandas df with a column containing comma-delimited characteristics like so: I would like to split this column into multiple dummy-variable columns, but cannot figure out how to start this process. I am trying to split on columns like so: This doesn't work, however, because there are commas in the middle of descriptions. Instead, I need to split based on a regex match of a comma followed by a space and a capital letter. Can str.split take regex? If so, how is this done? I think this Regex will do what I need:","Shot - Wounded/Injured, Shot - Dead (murder, accidental, suicide), Suicide - Attempt, Murder/Suicide, Attempted Murder/Suicide (one variable unsuccessful), Institution/Group/Business, Mass Murder (4+ deceased victims excluding the subject/suspect/perpetrator , one location), Mass Shooting (4+ victims injured or killed excluding the subject/suspect
 df['incident_characteristics'].str.split(',', expand=True)
 ,\s[A-Z]
",0,16,0,0,
982,49047402,50826412,15037,Python glob include hidden files and folders,5,<python><glob><hidden-files>,19,"<p>I try to loop over all files matching a certain extension, including those inside hidden folders. So far I haven't found a way to do this with <code>iglob</code>.
This works for all folder except those starting with a dot:</p>
<pre><code>import glob
for filename in glob.iglob('/path/**/*.ext', recursive=True):
    print(filename)
</code></pre>
<p>I have tried to add the dot as an optional character to no avail. I'd really like to use <code>glob</code> instead of residing to <code>os.walk</code>.</p>
<p>How to include all files/folders, even those starting with <code>.</code>, with <code>glob</code>?</p>
",384645,1263,01-03-2018 10:17,12-06-2018 21:58,103,1263,25,2,15,70,"{'badge_counts': {'bronze': 25, 'silver': 15, 'gold': 2}, 'account_id': 162884, 'is_employee': False, 'last_modified_date': 1607615239, 'last_access_date': 1711114332, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1263, 'creation_date': 1278427177, 'user_type': 'registered', 'user_id': 384645, 'accept_rate': 70, 'link': 'https://stackoverflow.com/users/384645/seedoubleyou', 'profile_image': 'https://www.gravatar.com/avatar/197de7c3ab383d1eb4f40971a07b6222?s=256&d=identicon&r=PG', 'display_name': 'SeeDoubleYou'}","I try to loop over all files matching a certain extension, including those inside hidden folders. So far I haven't found a way to do this with . This works for all folder except those starting with a dot: I have tried to add the dot as an optional character to no avail. I'd really like to use instead of residing to . How to include all files/folders, even those starting with , with ?","iglob import glob
for filename in glob.iglob('/path/**/*.ext', recursive=True):
    print(filename)
 glob os.walk . glob",-3,8,0,0,
983,48255849,48255910,22179,how to get the max heap in python,2,<python><heap><max-heap>,13,"<p>I use heapq module in python, I find I only can the min heap, even if I use reverse=True </p>

<p>I still get the min top heap</p>

<pre><code>from heapq import *

h=[]
merge(h,key=lambda e:e[0],reverse=True)
heappush(h, (200, 1))
heappush(h, (300,2))
heappush(h, (400,3))
print(heappop(h))
</code></pre>

<p>I still get the result:</p>

<pre><code>(200, 1)
</code></pre>

<p>I want to get result:</p>

<pre><code>(400,3)
</code></pre>

<p>how to do it?</p>

<p>which is the smallest element. I want pop the largest emelment?</p>

<p>ps: this is part of question find the max and then divide into several elements and then put it back to the heap.</p>
",504909,9299,15-01-2018 01:20,15-01-2018 01:30,0,9299,110,13,68,72,"{'badge_counts': {'bronze': 110, 'silver': 68, 'gold': 13}, 'collectives': [{'collective': {'tags': ['azure-spring-cloud', 'azure-compute-emulator', 'azure-synapse', 'azure-webapps', 'azure-hub', 'azureml-python-sdk', 'azure-servicebus-topics', 'azure-service-runtime', 'azure-advisor', 'azure-debugger', 'azure-custom-providers', 'azure-availability-set', 'azure-app-service-plans', 'azure-rest-api', 'azure-security', 'azure-feature-manager', 'azure-load-balancer', 'azure-stream-analytics', 'azure-communication-services', 'azure-sdk-ruby', 'azure-agent', 'azure-sql-reporting', 'azure-application-insights', 'azure-service-fabric-mesh', 'azure-files', 'azure-traffic-manager', 'azure-ad-role', 'azure-function-async', 'azure-container-registry', 'azure-quantum', 'azure-triggers', 'azure-node-sdk', 'azure-servicebus-queues', 'azure-sql-server', 'azure-deployment-slots', 'azure-oauth', 'azure-adf', 'azure-secrets', 'azure-management-groups', 'azure-ai', 'azure-qna-maker', 'azure-ad-v2', 'azure-automation', 'azure-sas', 'spring-cloud-azure', 'azure-iot-edge', 'azure-data-catalog', 'azure-api-apps', 'azure-python-sdk', 'azure-data-lake-gen2', 'azure-private-dns', 'azure-git-deployment', 'azure-aks', 'azure-storage-account', 'azure-billing-api', 'adal', 'azure-table-storage', 'azure-private-link', 'azure-dns', 'azure-iot-suite', 'azure-servicebusrelay', 'azure', 'azure-acr', 'azure-sql-managed-instance', 'azure-functions-core-tools', 'azure-elasticpool', 'azure-app-api', 'azure-media-services', 'azure-iot-hub-device-management', 'azure-xplat-cli', 'azure-synapse-link', 'azure-blockchain-workbench', 'azure-web-app-for-containers', 'azure-cosmosdb-changefeed', 'azure-data-explorer', 'azure-blob-trigger', 'azure-storage', 'azure-digital-twins', 'azure-service-principal', 'azure-cosmosdb-gremlinapi', 'azure-store', 'azure-database-postgresql', 'azure-resource-graph', 'azure-packaging', 'azure-http-trigger', 'rebus-azureservicebus', 'azure-purview', 'azure-adal-deprecation', 'azure-security-center', 'fhir-server-for-azure', 'azure-elastic-scale', 'azure-monitor-workbooks', 'azure-resource-manager', 'azure-java-sdk', 'azure-sphere', 'azure-app-registration', 'azure-web-roles', 'azure-managed-app', 'azure-sdk-php', 'azure-timeseries-insights', 'azure-caching', 'azure-notificationhub', 'azure-blockchain-service', 'azure-data-share', 'azure-ad-powershell-v2', 'azure-zulu', 'azure-cosmosdb-cassandra-api', 'azure-file-copy', 'azure-defender', 'azure-durable-functions', 'azure-iot-central', 'azure-postgresql', 'azure.data.tables', 'azure-configuration', 'adal.js', 'azure-mcd', 'azure-cloud-shell', 'azure-sql-edge', 'azure-cosmosdb-mongoapi', 'azure-iot-hub-device-update', 'azure-management', 'azure-form-recognizer', 'azure-functions-runtime', 'azure-data-sync', 'azure-managed-database', 'azure-log-analytics-workspace', 'azure-dev-spaces', 'azure-batch', 'azure-container-instances', 'azure-elastic-sharding', 'azure-clouddrive', 'azure-management-portal', 'azure-application-settings', 'azure-emulator', 'defaultazurecredential', 'azurekinect', 'azure-authentication', 'microsoft-entra-id', 'azure-sdk-js', 'azure-vm', 'azure-image-builder', 'azure-ad-b2c-custom-policy', 'azure-sql', 'azure-metrics-advisor', 'azure-cognitive-search', 'azure-site-recovery', 'azure-runbook', 'azure-dashboard', 'azure-functions-proxies', 'azure-rtos', 'azure-webjobssdk', 'azure-analysis-services', 'azure-static-web-app', 'azure-managed-disk', 'azure-text-translation', 'azure-vpn', 'azure-keyvault', 'azure-api-management', 'azure-bastion', 'azure-load-testing', 'azure-data-factory', 'azure-deployment', 'terraform-provider-azure', 'azure-servicebus-subscriptions', 'azure-china', 'azure-sdk-python', 'azure-information-protection', 'azure-devtest-labs', 'azure-webhooks', 'azure-cdn', 'azure-ml-component', 'azure-web-app-service', 'azure-relay', 'azure-iot-dps', 'azure-acs', 'azure-static-website-routing', 'azure-function-queue', 'azure-database-mysql', 'azure-function-app', 'azure-vm-scale-set', 'azure-sdk-for-ruby', 'azure-signalr', 'azure-dsvm', 'azure-web-app-firewall', 'azure-linux', 'azure-spatial-anchors', 'azure-sdk-for-java', 'azure-log-analytics', 'azure-appfabric', 'azure-auto-ml', 'azure-promptflow', 'azure-logic-app-standard', 'azure-storage-queues', 'azure-ad-msal', 'azure-mysql-database', 'azure-identity', 'azure-functions', 'azure-remote-rendering', 'azure-ase', 'azure-waf', 'azure-managed-grafana', 'azureshell', 'azure-function-app-proxy', 'azure-mobile-engagement', 'azure-spring-boot', 'azure-ad-verifiable-credentials', 'azure-sdk', 'azure-calculator', 'azure-android-sdk', 'sql-server-azure', 'azure-logic-apps', 'azure-front-door', 'azure-static-web-app-routing', 'microsoft-entra-private-access', 'azure-vm-templates', 'azure-storage-explorer', 'azureadgraph-deprecation', 'azure-stack', 'azure-backup-vault', 'azure-web-pubsub', 'azure-powershell', 'azure-lab-services', 'azure-ad-b2c', 'azure-autoscaling-block', 'azure-notebooks', 'sitecore-azure', 'azure-hybrid-connections', 'azure-ad-graph-api', 'azure-cosmosdb-emulator', 'azure-cli', 'azure-ml-pipelines', 'azure-function-http', 'azure-rm', 'azure-maps', 'azure-application-roles', 'azure-app-configuration', 'azure-policy', 'azure-rm-template', 'azure-hdinsight', 'azure-virtual-machine', 'azure-databoxfamily', 'azure-custom-domain', 'azure-mapping-data-flow', 'azure-application-proxy', 'azure-worker-roles', 'azure-ai-translator', 'azure-sentinel', 'azure-redis-cache', 'azure-integration-account', 'azure-batch-account', 'azure-container-apps', 'azure-tablequery', 'azure-functions-docker', 'azure-container-service', 'azure-webjobs-triggered', 'azure-private-dns-zone', 'azure-oms', 'azure-in-role-cache', 'azure-storage-emulator', 'sql-azure-alerts', 'azure-connect', 'azure-compliance-policy', 'azure-databricks', 'azure-sdk-.net', 'azuremlsdk', 'azure-application-insights-profiler', 'azure-regions', 'azure-mobile-services', 'azure-application-gateway', 'azure-webjobs', 'azure-iot-hub', 'azure-speech', 'azure-ilb', 'azure-cosmosdb-sqlapi', 'azure-app-service-envrmnt', 'azure-update-management-center', 'azure-performancecounters', 'azure-ad-b2b', 'azure-java-tools', 'azure-analytics', 'azure-language-understanding', 'azure-affinity-group', 'azure-sql-database', 'azure-nsg', 'azure-pack', 'azure-resource-group', 'azure-bicep', 'azure-sdk-for-go', 'azure-anomaly-detector', 'azure-scheduler', 'azure-diagnostics', 'azure-data-lake', 'azure-application-registration', 'azure-webjobs-continuous', 'azure-search-.net-sdk', 'spark-bash-azure-databricks', 'azure-monitoring', 'azure-alerts', 'kitchen-azurerm', 'azure-monitor', 'azure-static-website-hosting', 'azure-public-ip', 'azure-storage-files', 'azure-blob-storage', 'azure-cosmosdb', 'azure-ddos', 'azure-pipelines-release-pipeline', 'azure-cosmosdb-tables', 'azure-service-fabric', 'azure-queues', 'azure-cosmosdb-mongovcore', 'azureservicebus', 'azure-billing', 'azure-free-services', 'azure-cli2', 'azure-fluent-api', 'azure-sdk-go', 'azure-marketplace', 'azure-disk', 'azure-workflow-automation', 'azure-video-indexer', 'azure-availability-zones', 'azure-appservice', 'azure-bot-service', 'azure-managed-identity', 'azure-subscription', 'azure-cognitive-services', 'azure-cloud-services', 'azure-data-studio', 'azure-eventgrid', 'azure-role-environment', 'django-pyodbc-azure', 'azurerm-app-service', 'azure-service-hooks', 'azure-anomaly-detection', 'azure-object-anchors', 'azure-management-api', 'azure-blueprints', 'azure-service-plan', 'azureclicredential', 'sql-azure-federations', 'azure-ad-domain-services', 'azure-industrial-iot', 'microsoft-custom-vision', 'microsoft-entra-external-id', 'kql', 'azure-media-player', 'azure-arc', 'azure-cost-calculation', 'azure-rbac', 'passport-azure-ad', 'azure-gov', 'microsoft-entra-internet-access', 'azure-iot-sdk', 'azure-migrate', 'azure-active-directory', 'azure-machine-learning-service', 'azureportal', 'azure-eventhub', 'pulumi-azure', 'azure-virtual-network'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers to engage, share, and learn about Microsoft Azure’s open-source frameworks, languages, and platform. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/azure', 'name': 'Microsoft Azure', 'slug': 'azure'}, 'role': 'member'}, {'collective': {'tags': ['google-cloud-save', 'firebase-admob', 'google-cloud-identity-aware-proxy', 'google-cloud-node', 'google-dataflow', 'jib', 'google-cloud-search', 'google-cloud-spanner', 'google-fusion-tables', 'google-cloud-data-fusion', 'google-cloud-interconnect', 'google-cloud-dataproc-metastore', 'google-cloud-repository', 'google-cloud-error-reporting', 'google-cloud-api-gateway', 'google-cloud-spanner-emulator', 'google-cloud-filestore', 'google-cloud-sql', 'google-cloud-endpoints-v2', 'firebase-dynamic-links', 'firebase-job-dispatcher', 'google-cloud-datalab', 'google-container-registry', 'gcloud', 'google-cloud-dataproc', 'google-app-engine-go', 'google-cloud-vpn', 'google-cloud-storage', 'google-cloud-workstations', 'google-cloud-tpu', 'google-app-engine-launch', 'google-cloud-instances', 'google-cloud-ai-platform-pipelines', 'google-cloud-iot', 'firebase-machine-learning', 'cordova-plugin-firebasex', 'google-cloud-resource-manager', 'dialogflow-es-fulfillment', 'firebase-polymer', 'google-cloud-logging', 'google-cloud-stackdriver', 'firebase-app-check', 'google-cloud-scheduler', 'firebase-console', 'firebase-performance', 'firebase-cli', 'firebase-queue', 'google-cloud-registry', 'google-cloud-composer', 'firebase-extensions', 'google-cloud-visualstudio', 'google-cloud-healthcare', 'google-cloud-webrisk', 'google-cloud-source-repos', 'google-bigquery', 'google-cloud-vision', 'google-cloud-metrics', 'google-cloud-shell-editor', 'google-cloud-network-load-balancer', 'google-cloud-deploy', 'google-cloud-instance-template', 'rest-firebase', 'google-cloud-messaging', 'firebase-predictions', 'google-cloud-url-maps', 'google-cloud-platform', 'google-cloud-translate', 'google-app-engine-php', 'google-cloud-ai', 'google-cloud-cdn', 'google-cloud-vertex-ai', 'google-cloud-marketplace', 'google-cloud-memorystore', 'firebase-in-app-messaging', 'google-cloud-networking', 'google-cloud-tasks', 'firebase-database', 'firebase-cloud-messaging', 'firebase-assistant', 'google-migrate-for-compute-engine', 'google-cloud-http-load-balancer', 'dialogflow-cx', 'firebase-mlkit', 'google-cloud-language', 'firebase-app-distribution', 'google-kubernetes-engine', 'react-redux-firebase', 'firebase-authentication', 'google-cloud-storage-r', 'google-cloud-automl', 'google-cloud-run', 'firebase-test-lab', 'google-cloud-billing', 'google-cloud-profiler', 'stackdriver', 'google-cloud-print', 'google-app-engine-python', 'google-compute-engine', 'firebase-remote-config', 'google-cloud-monitoring', 'apigee-baas', 'dialogflow-es', 'google-cloud-print-privet', 'maven-jib', 'google-cloud-pubsublite', 'firebase-analytics', 'google-cloud-shell', 'firebase-app-indexing', 'firebase-util', 'google-cloud-kms', 'firebase-ab-testing', 'google-cloud-internal-load-balancer', 'google-cloud-ml-engine', 'google-cloud-code', 'google-cloud-launcher', 'firebaseui', 'firebase-crash-reporting', 'firebasesimplelogin', 'looker', 'google-cloud-test-lab', 'firebase-invites', 'google-cloud-endpoints', 'vertex-ai-search', 'google-cloud-dlp', 'firebase-notifications', 'google-cloud-dns', 'google-anthos', 'google-cloud-cpp', 'google-cloud-talent-solution', 'firebase-admin', 'google-cloud-proxy', 'google-cloud-asset-inventory', 'google-cloud-intellij', 'firebase-storage', 'google-app-engine-golang', 'google-cloud-nl', 'looker-studio', 'google-cloud-build', 'google-cloud-trace', 'google-cloud-pubsub', 'google-app-engine-deploy', 'google-app-engine', 'google-translate', 'google-cloud-recommendation', 'google-container-os', 'google-cloud-bigtable', 'google-cloud-dataflow', 'nativescript-firebase', 'google-app-engine-patch', 'firebase', 'apigee', 'google-cloud-firestore', 'recaptcha-enterprise', 'google-cloud-dataprep', 'google-container-builder', 'firebase-tools', 'react-native-firebase', 'google-cloud-console', 'google-prediction', 'google-cloud-powershell', 'google-cloud-debugger', 'google-cloud-python', 'google-cloud-php-client', 'google-cloud-ops-agent', 'google-cloud-identity', 'firebase-realtime-database', 'bigtable', 'google-cloud-load-balancer', 'google-cloud-tools', 'redux-saga-firebase', 'google-cloud-datastore', 'google-cloud-data-transfer', 'google-cloud-armor', 'firebase-security', 'google-cloud-ml', 'google-cloud-robotics', 'google-cloud-speech', 'google-cloud-iam', 'google-cloud-sdk', 'google-cloud-functions', 'google-cloud-automl-nl', 'cloud-document-ai', 'google-container-optimized-os', 'firebase-hosting', 'google-analytics-firebase', 'google-cloud-router', 'google-cloud-transcoder'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective for developers who utilize Google Cloud’s infrastructure and platform capabilities. This collective is organized and managed by the Stack Overflow community.', 'link': '/collectives/google-cloud', 'name': 'Google Cloud', 'slug': 'google-cloud'}, 'role': 'member'}, {'collective': {'tags': ['amazon-location-service', 'amazon-elastic-transcoder', 'aws-sam-cli', 'amazon-cloudfront', 'aws-fargate', 'aws-lambda', 'aws-sdk-go-v2', 'amazon-kms', 'aws-mediapackage', 'aws-transfer-family', 'aws-sdk-rust', 'amazon-keyspaces', 'amazon-rds', 'amazon-elasticsearch', 'aws-iot-greengrass', 'aws-copilot-cli', 'aws-lake-formation', 'aws-fis', 'aws-documentdb', 'aws-resource-group', 'aws-codepipeline', 'aws-nlb', 'aws-snowball', 'amazon-sagemaker-compilers', 'aws-vpn', 'amazon-imagebuilder', 'aws-organizations', 'aws-elb', 'amazon-quicksight', 'aws-parameter-store', 'elastic-ip', 'amazon-route53', 'amazon-gamelift', 'aws-storage-gateway', 'amazon-ecr', 'aws-sdk-js-v3', 'aws-certificate-manager', 'alexa-sdk-python', 'amazon-translate', 'aws-iot-core', 'aws-opsworks', 'alexa-flash-briefing-skill', 'aws-service-catalog', 'amazon-vpc', 'aws-cli', 'amazon-simpledb', 'aws-security-group', 'amazon-cloudsearch', 'aws-appstream', 'amazon-eks', 'aws-iot-analytics', 'aws-lambda-powertools', 'amazon-comprehend', 'amazon-managed-blockchain', 'aws-copilot', 'aws-event-bridge', 'aws-billing', 'alexa-skills-kit', 'aws-directory-services', 'alexa-account-linking', 'aws-codecatalyst', 'aws-sdk-java-2.0', 'amazon-lightsail', 'aws-cloudformation', 'amazon-forecast', 'amazon-cloudhsm', 'amazon-honeycode', 'aws-sdk-nodejs', 'aws-codestar', 'aws-mediatailor', 'aws-chatbot', 'aws-sdk-net', 'amazon-sagemaker', 'amazon-web-services', 'amazon-data-pipeline', 'amazon-efs', 'aws-codeartifact', 'amazon-elb', 'amazon-dynamodb', 'aws-mediaconnect', 'aws-iot-sitewise', 'aws-dms', 'aws-lambda-edge', 'aws-control-tower', 'amazon-connect', 'amazon-cloudwatch', 'alexa-smart-home-skill', 'amazon-inspector', 'amazon-textract', 'amazon-s3', 'amazon-app-runner', 'aws-appsync', 'aws-security-hub', 'aws-sso', 'amazon-macie', 'aws-serverless', 'amazon-waf', 'amazon-appflow', 'aws-sdk-android', 'amazon-ec2', 'amazon-iam', 'alexa-interaction-model', 'amazon-opensearch', 'amazon-ec2-spot-market', 'amazon-personalize', 'aws-iam-identity-center', 'amazon-sumerian', 'aws-sdk-java', 'amazon-cognito', 'aws-sdk', 'aws-cloudshell', 'amazon-ivs', 'aws-sam', 'aws-global-accelerator', 'aws-secrets-manager', 'aws-xray', 'aws-sdk-comprehend', 'aws-private-link', 'aws-config', 'aws-sdk-ios', 'aws-sdk-mock', 'amazon-polly', 'amazon-lex', 'amazon-ebs', 'aws-amplify', 'aws-glue', 'aws-sct', 'aws-direct-connect', 'amazon-aurora', 'aws-step-functions', 'amazon-ecs', 'amazon-workmail', 'aws-deeplens', 'amazon-mq', 'amazon-workdocs', 'aws-batch', 'aws-app-config', 'amazon-kinesis-firehose', 'aws-media-convert', 'aws-app-mesh', 'aws-api-gateway', 'amazon-emr', 'aws-msk', 'amazon-elastic-beanstalk', 'aws-device-farm', 'amazon-transcribe', 'amazon-kinesis-video-streams', 'aws-cloud9', 'alexa-presentation-language', 'aws-elemental', 'amazon-cloudwatchlogs', 'amazon-elasticache', 'aws-application-load-balancer', 'aws-media-live', 'alexa-smapi', 'amazon-timestream', 'amazon-dynamodb-dax', 'amazon-memory-db', 'aws-codeguru', 'amazon-neptune', 'aws-sdk-cpp', 'amazon-kendra', 'aws-datasync', 'amazon-bedrock', 'aws-auto-scaling', 'amazon-guardduty', 'amazon-glacier', 'aws-graviton', 'aws-ssm', 'alexa-sdk-nodejs', 'aws-cloudmap', 'amazon-fsx', 'amazon-redshift', 'aws-cdk', 'aws-sdk-ruby', 'amazon-ami', 'amazon-redshift-spectrum', 'aws-mobilehub', 'amazon-cloudtrail', 'aws-databrew', 'amazon-s3-select', 'amazon-ses', 'aws-data-exchange', 'aws-iot', 'aws-backup', 'aws-codecommit', 'aws-sdk-go', 'aws-reserved-instances', 'amazon-swf', 'amazon-rekognition', 'aws-pinpoint', 'amazon-athena', 'amazon-workspaces', 'aws-iot-events', 'aws-code-deploy', 'aws-elastictranscoder', 'amazon-kinesis-analytics', 'aws-acm', 'aws-mediastore', 'aws-sdk-js', 'amazon-sqs', 'amazon-qldb', 'aws-codebuild', 'amazon-sns', 'amazon-kinesis'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 237026, 'is_employee': False, 'last_modified_date': 1679352600, 'last_access_date': 1711066490, 'reputation_change_year': 70, 'reputation_change_quarter': 70, 'reputation_change_month': 20, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 9299, 'creation_date': 1289500508, 'user_type': 'registered', 'user_id': 504909, 'accept_rate': 72, 'location': 'Sydney, New South Wales, Australia', 'link': 'https://stackoverflow.com/users/504909/user504909', 'profile_image': 'https://www.gravatar.com/avatar/aa6c866af04400dc1281bb430792317c?s=256&d=identicon&r=PG', 'display_name': 'user504909'}","I use heapq module in python, I find I only can the min heap, even if I use reverse=True I still get the min top heap I still get the result: I want to get result: how to do it? which is the smallest element. I want pop the largest emelment? ps: this is part of question find the max and then divide into several elements and then put it back to the heap.","from heapq import *

h=[]
merge(h,key=lambda e:e[0],reverse=True)
heappush(h, (200, 1))
heappush(h, (300,2))
heappush(h, (400,3))
print(heappop(h))
 (200, 1)
 (400,3)
",7,29,0,0,
984,48170682,48170955,21022,Can structured logging be done with Pythons standard library?,3,<python><logging><python-logging><structured-logging>,30,"<p>I recently read about structured logging (<a href=""https://stackify.com/what-is-structured-logging-and-why-developers-need-it/"" rel=""noreferrer"">here</a>). The idea seems to be to log not by appending simple strings as a line to a logfile, but instead JSON objects. This makes it possible to analyze the logfile by automatic tools.</p>

<p>Can Pythons <code>logging</code> library do structured logging? If not, is there a ""mainstream"" solution for it (e.g. like numpy/scipy is the mainstream solution for scientific calculations)? I found <a href=""https://github.com/hynek/structlog/"" rel=""noreferrer""><code>structlog</code></a>, but I'm not sure how widespread it is.</p>
",562769,129983,09-01-2018 14:46,09-01-2018 14:59,0,130438,995,162,641,61,"{'badge_counts': {'bronze': 995, 'silver': 641, 'gold': 162}, 'account_id': 271958, 'is_employee': False, 'last_modified_date': 1710522300, 'last_access_date': 1711142867, 'reputation_change_year': 2083, 'reputation_change_quarter': 2083, 'reputation_change_month': 649, 'reputation_change_week': 305, 'reputation_change_day': 30, 'reputation': 130438, 'creation_date': 1294155598, 'user_type': 'registered', 'user_id': 562769, 'accept_rate': 61, 'location': 'M&#252;nchen, Deutschland', 'website_url': 'http://www.martin-thoma.de', 'link': 'https://stackoverflow.com/users/562769/martin-thoma', 'profile_image': 'https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG', 'display_name': 'Martin Thoma'}","I recently read about structured logging (here). The idea seems to be to log not by appending simple strings as a line to a logfile, but instead JSON objects. This makes it possible to analyze the logfile by automatic tools. Can Pythons library do structured logging? If not, is there a ""mainstream"" solution for it (e.g. like numpy/scipy is the mainstream solution for scientific calculations)? I found , but I'm not sure how widespread it is.",logging structlog,-2,3,0,2,
985,48827501,48827615,4587,Can I access a nested dict with a list of keys?,7,<python><python-3.x><dictionary>,12,"<p>I would like to access a dictionary programmatically. I know how to do this with a recursive function, but is there a simpler way?</p>

<pre><code>example = {'a': {'b': 'c'},
           '1': {'2': {'3': {'4': '5'}}}}

keys = ('a', 'b')
example[keys] = 'new'
# Now it should be
#     example = {'a': {'b': 'new'},
#                '1': {'2': {'3': {'4': '5'}}}}


keys = ('1', '2', '3', '4')
example[keys] = 'foo'
# Now it should be
#     example = {'a': {'b': 'new'},
#                '1': {'2': {'3': {'4': 'foo'}}}}


keys = ('1', '2')
example[keys] = 'bar'
# Now it should be
#     example = {'a': {'b': 'new'},
#                '1': {'2': 'bar'}}
</code></pre>
",562769,129983,16-02-2018 13:08,16-02-2018 13:15,0,130438,995,162,641,61,"{'badge_counts': {'bronze': 995, 'silver': 641, 'gold': 162}, 'account_id': 271958, 'is_employee': False, 'last_modified_date': 1710522300, 'last_access_date': 1711142867, 'reputation_change_year': 2083, 'reputation_change_quarter': 2083, 'reputation_change_month': 649, 'reputation_change_week': 305, 'reputation_change_day': 30, 'reputation': 130438, 'creation_date': 1294155598, 'user_type': 'registered', 'user_id': 562769, 'accept_rate': 61, 'location': 'M&#252;nchen, Deutschland', 'website_url': 'http://www.martin-thoma.de', 'link': 'https://stackoverflow.com/users/562769/martin-thoma', 'profile_image': 'https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG', 'display_name': 'Martin Thoma'}","I would like to access a dictionary programmatically. I know how to do this with a recursive function, but is there a simpler way?","example = {'a': {'b': 'c'},
           '1': {'2': {'3': {'4': '5'}}}}

keys = ('a', 'b')
example[keys] = 'new'
# Now it should be
#     example = {'a': {'b': 'new'},
#                '1': {'2': {'3': {'4': '5'}}}}


keys = ('1', '2', '3', '4')
example[keys] = 'foo'
# Now it should be
#     example = {'a': {'b': 'new'},
#                '1': {'2': {'3': {'4': 'foo'}}}}


keys = ('1', '2')
example[keys] = 'bar'
# Now it should be
#     example = {'a': {'b': 'new'},
#                '1': {'2': 'bar'}}
",21,25,0,0,
986,48642111,48643567,11902,Can I make Python output exceptions in one line / via logging?,3,<python><logging><traceback><python-logging>,16,"<p>I am using AWS and use AWS cloudwatch to view logs. While things should not break on AWS, they could. I just had such a case. Then I searched for <code>Traceback</code> and just got the lines</p>

<pre><code>Traceback (most recent call last):
</code></pre>

<p>without the actual traceback. I have a working structured logging setup (see <a href=""https://stackoverflow.com/a/48202500/562769"">other question</a>) and I would like to get tracebacks in a similar way.</p>

<p>So instead of:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/math/Desktop/test.py"", line 32, in &lt;module&gt;
    adf
NameError: name 'adf' is not defined
</code></pre>

<p>something like</p>

<pre><code>{""message"": ""Traceback (most recent call last):\n      File \""/home/math/Desktop/test.py\"", line 32, in &lt;module&gt;\n        adf\n    NameError: name 'adf' is not defined"", ""lineno"": 35, ""pathname"": ""/home/math/Desktop/test.py""}
</code></pre>

<p>or even better also with the string in a JSON format.</p>

<p>The only way to achieve this I can think of is a giant try-except block. Pokemon-style. Is there a better solution?</p>
",562769,129983,06-02-2018 11:33,06-02-2018 12:55,0,130438,995,162,641,61,"{'badge_counts': {'bronze': 995, 'silver': 641, 'gold': 162}, 'account_id': 271958, 'is_employee': False, 'last_modified_date': 1710522300, 'last_access_date': 1711142867, 'reputation_change_year': 2083, 'reputation_change_quarter': 2083, 'reputation_change_month': 649, 'reputation_change_week': 305, 'reputation_change_day': 30, 'reputation': 130438, 'creation_date': 1294155598, 'user_type': 'registered', 'user_id': 562769, 'accept_rate': 61, 'location': 'M&#252;nchen, Deutschland', 'website_url': 'http://www.martin-thoma.de', 'link': 'https://stackoverflow.com/users/562769/martin-thoma', 'profile_image': 'https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG', 'display_name': 'Martin Thoma'}","I am using AWS and use AWS cloudwatch to view logs. While things should not break on AWS, they could. I just had such a case. Then I searched for and just got the lines without the actual traceback. I have a working structured logging setup (see other question) and I would like to get tracebacks in a similar way. So instead of: something like or even better also with the string in a JSON format. The only way to achieve this I can think of is a giant try-except block. Pokemon-style. Is there a better solution?","Traceback Traceback (most recent call last):
 Traceback (most recent call last):
  File ""/home/math/Desktop/test.py"", line 32, in &lt;module&gt;
    adf
NameError: name 'adf' is not defined
 {""message"": ""Traceback (most recent call last):\n      File \""/home/math/Desktop/test.py\"", line 32, in &lt;module&gt;\n        adf\n    NameError: name 'adf' is not defined"", ""lineno"": 35, ""pathname"": ""/home/math/Desktop/test.py""}
",2,23,0,1,
987,49862318,49865393,26518,Run mypy on all Python files of a project,2,<python><mypy>,24,"<p>How can I run mypy on all .py files in a project? I have seen I can specify a module to run mypy on but not something to specify a file mask or something like this.</p>
",574633,6126,16-04-2018 16:43,16-04-2018 20:05,0,6146,103,12,57,76,"{'badge_counts': {'bronze': 103, 'silver': 57, 'gold': 12}, 'account_id': 279463, 'is_employee': False, 'last_modified_date': 1711158600, 'last_access_date': 1710506938, 'reputation_change_year': 50, 'reputation_change_quarter': 50, 'reputation_change_month': 20, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 6146, 'creation_date': 1294939646, 'user_type': 'registered', 'user_id': 574633, 'accept_rate': 76, 'link': 'https://stackoverflow.com/users/574633/notbad', 'profile_image': 'https://www.gravatar.com/avatar/998cbb418b181eeaccdd47b70f14ed15?s=256&d=identicon&r=PG', 'display_name': 'Notbad'}",How can I run mypy on all .py files in a project? I have seen I can specify a module to run mypy on but not something to specify a file mask or something like this.,,0,1,0,0,
988,49020495,49020847,328,Is it reliable and documented how PYTHONPATH populates the sys.path?,1,<python><environment-variables><python-import>,12,"<p>On my machine, the values from <code>PYTHONPATH</code> appear to get inserted in <code>sys.path</code>:</p>

<ul>
<li>beginning at index 1</li>
<li>order preserved</li>
<li>de-duplicated</li>
</ul>

<p>For example, with <code>PYTHONPATH=/spam:/eggs:/spam</code> and then checking in <code>python -m site</code>, I get a result like:</p>

<pre><code>sys.path = [
    something,
    '/spam',
    '/eggs',
    more,
    stuff,
    after
]
</code></pre>

<p>It seems to be the same behaviour on Python 2 and Python 3.  The question is, how much of this handling of <code>PYTHONPATH</code> is documented / reliable, and what if any might be different on other platforms?  Is this baked into the interpreter, or is handled by <code>site.py</code> and/or in danger of being ""tweaked"" by sysadmins?</p>

<p>I can't see it explained in the documentation <a href=""https://docs.python.org/3/using/cmdline.html#envvar-PYTHONPATH"" rel=""noreferrer"">here</a>, it just says <code>sys.path</code> is ""augmented"" (and, contrary to the documentation, non-existent directories do not appear to be silently ignored).</p>
",674039,348162,28-02-2018 01:13,28-02-2018 01:59,0,348698,767,105,631,94,"{'badge_counts': {'bronze': 767, 'silver': 631, 'gold': 105}, 'account_id': 342731, 'is_employee': False, 'last_modified_date': 1710872103, 'last_access_date': 1711143082, 'reputation_change_year': 4383, 'reputation_change_quarter': 4383, 'reputation_change_month': 1127, 'reputation_change_week': 227, 'reputation_change_day': 10, 'reputation': 348698, 'creation_date': 1300923627, 'user_type': 'registered', 'user_id': 674039, 'accept_rate': 94, 'location': 'ℂ&#120153;&#120154;&#120148;&#120146;&#120152;&#120160;, &#120128;&#120131;', 'website_url': 'http://www.wimglenn.com', 'link': 'https://stackoverflow.com/users/674039/wim', 'profile_image': 'https://i.stack.imgur.com/leoFi.gif?s=256&g=1', 'display_name': 'wim'}","On my machine, the values from appear to get inserted in : beginning at index 1 order preserved de-duplicated For example, with and then checking in , I get a result like: It seems to be the same behaviour on Python 2 and Python 3. The question is, how much of this handling of is documented / reliable, and what if any might be different on other platforms? Is this baked into the interpreter, or is handled by and/or in danger of being ""tweaked"" by sysadmins? I can't see it explained in the documentation here, it just says is ""augmented"" (and, contrary to the documentation, non-existent directories do not appear to be silently ignored).","PYTHONPATH sys.path PYTHONPATH=/spam:/eggs:/spam python -m site sys.path = [
    something,
    '/spam',
    '/eggs',
    more,
    stuff,
    after
]
 PYTHONPATH site.py sys.path",0,23,0,1,
989,48124206,48124233,187625,Iterate through a file lines in python,2,<python>,44,"<p>I have a file which have some names listed line by line.</p>

<pre><code>gparasha-macOS:python_scripting gparasha$ cat topology_list.txt 
First-Topology
Third-topology
Second-Topology
</code></pre>

<p>Now I am trying to iterate through these contents, but I am unable to do so.</p>

<pre><code>file = open('topology_list.txt','r')
print file.readlines()
for i in file.readlines():
    print ""Entered For\n""
    print i

topology_list = file.readlines()
print topology_list
</code></pre>

<p>file.readlines() prints the lines of the files as a list. 
So I am getting this:</p>

<pre><code> ['First-Topology\n', 'Third-topology\n', 'Second-Topology\n']
</code></pre>

<p>However, When i iterate through this list, I am unable to do so.</p>

<p>Also, when I assign it to a variable 'topology_list' as in the penultimate line and print it. It gives me an empty list.</p>

<pre><code>[]
</code></pre>

<p>So I have two questions.</p>

<p>What is wrong with my approach?
How to accomplish this?</p>
",807362,1477,06-01-2018 04:16,06-01-2018 04:21,0,1477,24,2,20,33,"{'badge_counts': {'bronze': 24, 'silver': 20, 'gold': 2}, 'collectives': [{'collective': {'tags': ['amazon-location-service', 'amazon-elastic-transcoder', 'aws-sam-cli', 'amazon-cloudfront', 'aws-fargate', 'aws-lambda', 'aws-sdk-go-v2', 'amazon-kms', 'aws-mediapackage', 'aws-transfer-family', 'aws-sdk-rust', 'amazon-keyspaces', 'amazon-rds', 'amazon-elasticsearch', 'aws-iot-greengrass', 'aws-copilot-cli', 'aws-lake-formation', 'aws-fis', 'aws-documentdb', 'aws-resource-group', 'aws-codepipeline', 'aws-nlb', 'aws-snowball', 'amazon-sagemaker-compilers', 'aws-vpn', 'amazon-imagebuilder', 'aws-organizations', 'aws-elb', 'amazon-quicksight', 'aws-parameter-store', 'elastic-ip', 'amazon-route53', 'amazon-gamelift', 'aws-storage-gateway', 'amazon-ecr', 'aws-sdk-js-v3', 'aws-certificate-manager', 'alexa-sdk-python', 'amazon-translate', 'aws-iot-core', 'aws-opsworks', 'alexa-flash-briefing-skill', 'aws-service-catalog', 'amazon-vpc', 'aws-cli', 'amazon-simpledb', 'aws-security-group', 'amazon-cloudsearch', 'aws-appstream', 'amazon-eks', 'aws-iot-analytics', 'aws-lambda-powertools', 'amazon-comprehend', 'amazon-managed-blockchain', 'aws-copilot', 'aws-event-bridge', 'aws-billing', 'alexa-skills-kit', 'aws-directory-services', 'alexa-account-linking', 'aws-codecatalyst', 'aws-sdk-java-2.0', 'amazon-lightsail', 'aws-cloudformation', 'amazon-forecast', 'amazon-cloudhsm', 'amazon-honeycode', 'aws-sdk-nodejs', 'aws-codestar', 'aws-mediatailor', 'aws-chatbot', 'aws-sdk-net', 'amazon-sagemaker', 'amazon-web-services', 'amazon-data-pipeline', 'amazon-efs', 'aws-codeartifact', 'amazon-elb', 'amazon-dynamodb', 'aws-mediaconnect', 'aws-iot-sitewise', 'aws-dms', 'aws-lambda-edge', 'aws-control-tower', 'amazon-connect', 'amazon-cloudwatch', 'alexa-smart-home-skill', 'amazon-inspector', 'amazon-textract', 'amazon-s3', 'amazon-app-runner', 'aws-appsync', 'aws-security-hub', 'aws-sso', 'amazon-macie', 'aws-serverless', 'amazon-waf', 'amazon-appflow', 'aws-sdk-android', 'amazon-ec2', 'amazon-iam', 'alexa-interaction-model', 'amazon-opensearch', 'amazon-ec2-spot-market', 'amazon-personalize', 'aws-iam-identity-center', 'amazon-sumerian', 'aws-sdk-java', 'amazon-cognito', 'aws-sdk', 'aws-cloudshell', 'amazon-ivs', 'aws-sam', 'aws-global-accelerator', 'aws-secrets-manager', 'aws-xray', 'aws-sdk-comprehend', 'aws-private-link', 'aws-config', 'aws-sdk-ios', 'aws-sdk-mock', 'amazon-polly', 'amazon-lex', 'amazon-ebs', 'aws-amplify', 'aws-glue', 'aws-sct', 'aws-direct-connect', 'amazon-aurora', 'aws-step-functions', 'amazon-ecs', 'amazon-workmail', 'aws-deeplens', 'amazon-mq', 'amazon-workdocs', 'aws-batch', 'aws-app-config', 'amazon-kinesis-firehose', 'aws-media-convert', 'aws-app-mesh', 'aws-api-gateway', 'amazon-emr', 'aws-msk', 'amazon-elastic-beanstalk', 'aws-device-farm', 'amazon-transcribe', 'amazon-kinesis-video-streams', 'aws-cloud9', 'alexa-presentation-language', 'aws-elemental', 'amazon-cloudwatchlogs', 'amazon-elasticache', 'aws-application-load-balancer', 'aws-media-live', 'alexa-smapi', 'amazon-timestream', 'amazon-dynamodb-dax', 'amazon-memory-db', 'aws-codeguru', 'amazon-neptune', 'aws-sdk-cpp', 'amazon-kendra', 'aws-datasync', 'amazon-bedrock', 'aws-auto-scaling', 'amazon-guardduty', 'amazon-glacier', 'aws-graviton', 'aws-ssm', 'alexa-sdk-nodejs', 'aws-cloudmap', 'amazon-fsx', 'amazon-redshift', 'aws-cdk', 'aws-sdk-ruby', 'amazon-ami', 'amazon-redshift-spectrum', 'aws-mobilehub', 'amazon-cloudtrail', 'aws-databrew', 'amazon-s3-select', 'amazon-ses', 'aws-data-exchange', 'aws-iot', 'aws-backup', 'aws-codecommit', 'aws-sdk-go', 'aws-reserved-instances', 'amazon-swf', 'amazon-rekognition', 'aws-pinpoint', 'amazon-athena', 'amazon-workspaces', 'aws-iot-events', 'aws-code-deploy', 'aws-elastictranscoder', 'amazon-kinesis-analytics', 'aws-acm', 'aws-mediastore', 'aws-sdk-js', 'amazon-sqs', 'amazon-qldb', 'aws-codebuild', 'amazon-sns', 'amazon-kinesis'], 'external_links': [{'type': 'website', 'link': 'https://aws.amazon.com'}, {'type': 'support', 'link': 'mailto:awscollective@amazon.com'}, {'type': 'twitter', 'link': 'https://twitter.com/awsdevelopers'}, {'type': 'github', 'link': 'https://github.com/aws'}, {'type': 'facebook', 'link': 'https://facebook.com/amazonwebservices'}, {'type': 'instagram', 'link': 'https://instagram.com/amazonwebservices'}], 'description': 'Amazon Web Services (AWS) is the world’s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. The AWS Collective is a community-driven site with resources for  developers.', 'link': '/collectives/aws', 'name': 'AWS', 'slug': 'aws'}, 'role': 'member'}], 'account_id': 426257, 'is_employee': False, 'last_modified_date': 1691806800, 'last_access_date': 1704913817, 'reputation_change_year': 60, 'reputation_change_quarter': 60, 'reputation_change_month': 10, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1477, 'creation_date': 1308602210, 'user_type': 'registered', 'user_id': 807362, 'accept_rate': 33, 'website_url': '', 'link': 'https://stackoverflow.com/users/807362/gaurav-parashar', 'profile_image': 'https://www.gravatar.com/avatar/82b6e3ff39573f5e5e4241974f657d2e?s=256&d=identicon&r=PG', 'display_name': 'Gaurav Parashar'}","I have a file which have some names listed line by line. Now I am trying to iterate through these contents, but I am unable to do so. file.readlines() prints the lines of the files as a list. So I am getting this: However, When i iterate through this list, I am unable to do so. Also, when I assign it to a variable 'topology_list' as in the penultimate line and print it. It gives me an empty list. So I have two questions. What is wrong with my approach? How to accomplish this?","gparasha-macOS:python_scripting gparasha$ cat topology_list.txt 
First-Topology
Third-topology
Second-Topology
 file = open('topology_list.txt','r')
print file.readlines()
for i in file.readlines():
    print ""Entered For\n""
    print i

topology_list = file.readlines()
print topology_list
  ['First-Topology\n', 'Third-topology\n', 'Second-Topology\n']
 []
",10,37,0,0,
990,48729364,48729853,18931,Python - TypeError: Can't mix strings and bytes in path components,1,<python>,12,"<p>The following code: </p>

<pre><code>import os

directory_in_str = 'C:\\Work\\Test\\'
directory = os.fsencode(directory_in_str)

for file in os.listdir(directory):
    filename = os.fsdecode(file)
    if filename.lower().endswith("".xml""):
        with open(os.path.join(directory, filename), 'r') as handle:
            for line in handle:
                print(line)
    else:
        continue
</code></pre>

<p>is giving me this error:</p>

<pre><code>Traceback (most recent call last):
  File ""c:\Work\balance_search2.py"", line 9, in &lt;module&gt;
    with open(os.path.join(directory, filename), 'r') as handle:
  File ""C:\ProgramData\Anaconda3\lib\ntpath.py"", line 114, in join
    genericpath._check_arg_types('join', path, *paths)
  File ""C:\ProgramData\Anaconda3\lib\genericpath.py"", line 151, in _check_arg_types
    raise TypeError(""Can't mix strings and bytes in path components"") from None
TypeError: Can't mix strings and bytes in path components
</code></pre>

<p>Can anyone help me fix it please.</p>
",941397,7744,11-02-2018 07:35,11-02-2018 08:53,0,7794,145,19,87,86,"{'badge_counts': {'bronze': 145, 'silver': 87, 'gold': 19}, 'account_id': 906642, 'is_employee': False, 'last_modified_date': 1686964200, 'last_access_date': 1709907438, 'reputation_change_year': 100, 'reputation_change_quarter': 100, 'reputation_change_month': 70, 'reputation_change_week': 30, 'reputation_change_day': 0, 'reputation': 7794, 'creation_date': 1315862796, 'user_type': 'registered', 'user_id': 941397, 'accept_rate': 86, 'location': 'Cape Town, South Africa', 'website_url': '', 'link': 'https://stackoverflow.com/users/941397/superdooperhero', 'profile_image': 'https://www.gravatar.com/avatar/d85ff881af5ceab67fce4da7d62b5058?s=256&d=identicon&r=PG', 'display_name': 'Superdooperhero'}",The following code: is giving me this error: Can anyone help me fix it please.,"import os

directory_in_str = 'C:\\Work\\Test\\'
directory = os.fsencode(directory_in_str)

for file in os.listdir(directory):
    filename = os.fsdecode(file)
    if filename.lower().endswith("".xml""):
        with open(os.path.join(directory, filename), 'r') as handle:
            for line in handle:
                print(line)
    else:
        continue
 Traceback (most recent call last):
  File ""c:\Work\balance_search2.py"", line 9, in &lt;module&gt;
    with open(os.path.join(directory, filename), 'r') as handle:
  File ""C:\ProgramData\Anaconda3\lib\ntpath.py"", line 114, in join
    genericpath._check_arg_types('join', path, *paths)
  File ""C:\ProgramData\Anaconda3\lib\genericpath.py"", line 151, in _check_arg_types
    raise TypeError(""Can't mix strings and bytes in path components"") from None
TypeError: Can't mix strings and bytes in path components
",19,30,0,0,
991,48131812,48131825,26212,Get unique values of multiple columns as a new dataframe in pandas,1,<python><pandas><pandas-groupby>,20,"<p>Having pandas data frame <code>df</code> with at least columns C1,C2,C3 how would you get all the unique C1,C2,C3 values as a new DataFrame? </p>

<p>in other words, similiar to :</p>

<pre><code>SELECT C1,C2,C3
FROM T
GROUP BY C1,C2,C3
</code></pre>

<p>Tried that </p>

<pre><code>print df.groupby(by=['C1','C2','C3'])
</code></pre>

<p>but im getting </p>

<pre><code>&lt;pandas.core.groupby.DataFrameGroupBy object at 0x000000000769A9E8&gt;
</code></pre>
",891814,8492,06-01-2018 20:53,06-01-2018 20:55,0,8490,105,13,56,62,"{'badge_counts': {'bronze': 105, 'silver': 56, 'gold': 13}, 'account_id': 479245, 'is_employee': False, 'last_modified_date': 1693013700, 'last_access_date': 1710927645, 'reputation_change_year': 48, 'reputation_change_quarter': 48, 'reputation_change_month': 8, 'reputation_change_week': -2, 'reputation_change_day': 0, 'reputation': 8490, 'creation_date': 1313154223, 'user_type': 'registered', 'user_id': 891814, 'accept_rate': 62, 'location': 'Israel', 'website_url': '', 'link': 'https://stackoverflow.com/users/891814/ofek-ron', 'profile_image': 'https://www.gravatar.com/avatar/cf47dc9d0b1227557450c15c3826c67b?s=256&d=identicon&r=PG', 'display_name': 'Ofek Ron'}","Having pandas data frame with at least columns C1,C2,C3 how would you get all the unique C1,C2,C3 values as a new DataFrame? in other words, similiar to : Tried that but im getting","df SELECT C1,C2,C3
FROM T
GROUP BY C1,C2,C3
 print df.groupby(by=['C1','C2','C3'])
 &lt;pandas.core.groupby.DataFrameGroupBy object at 0x000000000769A9E8&gt;
",1,18,0,0,
992,50358327,54278757,23388,Using Pylint in Ipython (Jupyter-Notebook),5,<python><python-3.x><jupyter-notebook><pylint><flake8>,27,"<p>I want to run Pylint or any equivalent while using Jupyter-Notebook. Is there a way to install and run Pylint this way?</p>
",851043,1304,15-05-2018 20:00,20-01-2019 16:55,250,1304,45,5,26,86,"{'badge_counts': {'bronze': 45, 'silver': 26, 'gold': 5}, 'account_id': 453826, 'is_employee': False, 'last_modified_date': 1703341205, 'last_access_date': 1710699250, 'reputation_change_year': 20, 'reputation_change_quarter': 20, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 1304, 'creation_date': 1311035707, 'user_type': 'registered', 'user_id': 851043, 'accept_rate': 86, 'location': 'MA', 'website_url': '', 'link': 'https://stackoverflow.com/users/851043/mccurcio', 'profile_image': 'https://i.stack.imgur.com/WMA4l.jpg?s=256&g=1', 'display_name': 'mccurcio'}",I want to run Pylint or any equivalent while using Jupyter-Notebook. Is there a way to install and run Pylint this way?,,0,1,0,0,
993,48681816,50275763,13313,Upload to pypi from Gitlab Pipelines,7,<python><gitlab-ci><pypi><twine>,11,"<p>I'm trying to upload a package to pypi using a Gitlab CI job, but I cannot make it work :/ Anyone has a working example?</p>

<p>What I have tried so far in my <code>.gitlab-ci.yaml</code> (from my local machine all of them are working):</p>

<ol>
<li><p>Twine with a <code>.pypirc</code> file</p>

<pre><code>- echo ""[distutils]"" &gt;&gt; ~/.pypirc
- echo ""index-servers ="" &gt;&gt; ~/.pypirc
- echo ""    pypi"" &gt;&gt; ~/.pypirc
- echo """" &gt;&gt; ~/.pypirc
- echo ""[pypi]"" &gt;&gt; ~/.pypirc
- 'echo ""repository: https://upload.pypi.org/legacy/"" &gt;&gt; ~/.pypirc'
- 'echo ""username: ${PYPI_USER}"" &gt;&gt; ~/.pypirc'
- 'echo ""password: ${PYPI_PASSWORD}"" &gt;&gt; ~/.pypirc'
- python3 setup.py check sdist bdist  # This will fail if your creds are bad.
- cat ~/.pypirc
- twine upload dist/* --config-file ~/.pypirc
</code></pre></li>
<li><p>Same as before but with <code>$VARIABLE</code></p>

<pre><code>[...]
- 'echo ""username: $PYPI_USER"" &gt;&gt; ~/.pypirc'
- 'echo ""password: $PYPI_PASSWORD"" &gt;&gt; ~/.pypirc'
[...]
</code></pre></li>
<li><p>Two options before but using <code>python setup.py ... upload</code></p></li>
<li><code>twine upload dist/* -u $PYPI_USER -p $PYPI_PASSWORD</code></li>
<li><code>twine upload dist/*</code> wiht <code>TWINE_USERNAME</code> and <code>TWINE_PASSWORD</code> environment variables.</li>
</ol>

<p>... and always get a <code>403 Client Error: Invalid or non-existent authentication information</code>. I'm running out of options...</p>
",837816,716,08-02-2018 09:22,10-05-2018 14:52,91,716,19,1,9,62,"{'badge_counts': {'bronze': 19, 'silver': 9, 'gold': 1}, 'account_id': 445390, 'is_employee': False, 'last_modified_date': 1618055400, 'last_access_date': 1698321681, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 716, 'creation_date': 1310318693, 'user_type': 'registered', 'user_id': 837816, 'accept_rate': 62, 'link': 'https://stackoverflow.com/users/837816/jgsogo', 'profile_image': 'https://www.gravatar.com/avatar/74285139dbc24c216077d8c833cfd59c?s=256&d=identicon&r=PG', 'display_name': 'jgsogo'}","I'm trying to upload a package to pypi using a Gitlab CI job, but I cannot make it work :/ Anyone has a working example? What I have tried so far in my (from my local machine all of them are working): Twine with a file Same as before but with Two options before but using wiht and environment variables. ... and always get a . I'm running out of options...",".gitlab-ci.yaml .pypirc - echo ""[distutils]"" &gt;&gt; ~/.pypirc
- echo ""index-servers ="" &gt;&gt; ~/.pypirc
- echo ""    pypi"" &gt;&gt; ~/.pypirc
- echo """" &gt;&gt; ~/.pypirc
- echo ""[pypi]"" &gt;&gt; ~/.pypirc
- 'echo ""repository: https://upload.pypi.org/legacy/"" &gt;&gt; ~/.pypirc'
- 'echo ""username: ${PYPI_USER}"" &gt;&gt; ~/.pypirc'
- 'echo ""password: ${PYPI_PASSWORD}"" &gt;&gt; ~/.pypirc'
- python3 setup.py check sdist bdist  # This will fail if your creds are bad.
- cat ~/.pypirc
- twine upload dist/* --config-file ~/.pypirc
 $VARIABLE [...]
- 'echo ""username: $PYPI_USER"" &gt;&gt; ~/.pypirc'
- 'echo ""password: $PYPI_PASSWORD"" &gt;&gt; ~/.pypirc'
[...]
 python setup.py ... upload twine upload dist/* -u $PYPI_USER -p $PYPI_PASSWORD twine upload dist/* TWINE_USERNAME TWINE_PASSWORD 403 Client Error: Invalid or non-existent authentication information",4,32,0,0,
994,48880646,48882214,44394,How to use browser that is already open and logged in with login credentials,5,<python><python-3.x><selenium><selenium-webdriver>,26,"<p>Is there a way that for different runs of a python program that uses selenium I keep the browser that I have opened and logged in with my credentials, open and use in later runs? </p>

<p>I am debugging a code. On the browser each time I need to log in using my credentials. Currently, everytime I stop the code, the web-browser gets closed. Is there a way to keep a copy of browser that I have already open and logged in open and use it for my later debug so every time I don't need to enter my login credentials again?   </p>

<p>My code that opens the browser looks like this:</p>

<pre><code>driver = webdriver.Chrome(executable_path=""/the_path/chromedriver"", chrome_options=chrome_options) 
driver.get(url)
</code></pre>

<p><strong>EDIT:</strong></p>

<p>Actually, the way this website asks for authentication is as follows:
First, it asks for the username, then I need to press the continue button, then it asks for the password, after entering the password, it sends an SMS to my phone, I need to enter it before it goes to the intended page.</p>
",755000,7966,20-02-2018 08:22,20-02-2018 09:48,0,8048,124,20,78,81,"{'badge_counts': {'bronze': 124, 'silver': 78, 'gold': 20}, 'account_id': 393238, 'is_employee': False, 'last_modified_date': 1677291000, 'last_access_date': 1711159538, 'reputation_change_year': 230, 'reputation_change_quarter': 230, 'reputation_change_month': 100, 'reputation_change_week': 40, 'reputation_change_day': 0, 'reputation': 8048, 'creation_date': 1305511019, 'user_type': 'registered', 'user_id': 755000, 'accept_rate': 81, 'website_url': '', 'link': 'https://stackoverflow.com/users/755000/tj1', 'profile_image': 'https://i.stack.imgur.com/8H57C.jpg?s=256&g=1', 'display_name': 'TJ1'}","Is there a way that for different runs of a python program that uses selenium I keep the browser that I have opened and logged in with my credentials, open and use in later runs? I am debugging a code. On the browser each time I need to log in using my credentials. Currently, everytime I stop the code, the web-browser gets closed. Is there a way to keep a copy of browser that I have already open and logged in open and use it for my later debug so every time I don't need to enter my login credentials again? My code that opens the browser looks like this: EDIT: Actually, the way this website asks for authentication is as follows: First, it asks for the username, then I need to press the continue button, then it asks for the password, after entering the password, it sends an SMS to my phone, I need to enter it before it goes to the intended page.","driver = webdriver.Chrome(executable_path=""/the_path/chromedriver"", chrome_options=chrome_options) 
driver.get(url)
",1,14,0,0,
995,50495053,50500065,22796,"If I'm not specifying to use CPU/GPU, which one is my script using?",6,<python><pytorch>,14,"<p>In pytorch, if I'm not writing anything about using CPU/GPU, and my machine supports CUDA (<code>torch.cuda.is_available() == True</code>):</p>

<ol>
<li>What is my script using, CPU or GPU?</li>
<li>If CPU, what should I do to make it run on GPU? Do I need to rewrite everything?</li>
<li>If GPU, will this script crash if <code>torch.cuda.is_available() == False</code>?</li>
<li>Does this do anything about making the training faster?</li>
<li>I'm aware of <a href=""https://stackoverflow.com/questions/46704352/porting-pytorch-code-from-cpu-to-gpu?rq=1"">Porting PyTorch code from CPU to GPU</a> but this is old. Does this situation change in v0.4 or the upcoming v1.0?</li>
</ol>
",734263,844,23-05-2018 18:23,24-05-2018 02:30,1,844,27,5,14,63,"{'badge_counts': {'bronze': 27, 'silver': 14, 'gold': 5}, 'account_id': 380326, 'is_employee': False, 'last_modified_date': 1607615094, 'last_access_date': 1710894976, 'reputation_change_year': 0, 'reputation_change_quarter': 0, 'reputation_change_month': 0, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 844, 'creation_date': 1304330512, 'user_type': 'registered', 'user_id': 734263, 'accept_rate': 63, 'website_url': '', 'link': 'https://stackoverflow.com/users/734263/xxbidiao', 'profile_image': 'https://www.gravatar.com/avatar/d7c74225ad53248f4adaa3ca4803c5a1?s=256&d=identicon&r=PG', 'display_name': 'xxbidiao'}","In pytorch, if I'm not writing anything about using CPU/GPU, and my machine supports CUDA (): What is my script using, CPU or GPU? If CPU, what should I do to make it run on GPU? Do I need to rewrite everything? If GPU, will this script crash if ? Does this do anything about making the training faster? I'm aware of Porting PyTorch code from CPU to GPU but this is old. Does this situation change in v0.4 or the upcoming v1.0?",torch.cuda.is_available() == True torch.cuda.is_available() == False,-2,9,0,1,
996,50186125,50186126,76550,How do I parse a yaml string with python?,2,<python><yaml>,53,"<p>I see an API and many examples on how to parse a yaml file but what about a string?</p>
",718488,8939,05-05-2018 05:40,05-05-2018 05:40,0,8959,40,3,35,55,"{'badge_counts': {'bronze': 40, 'silver': 35, 'gold': 3}, 'account_id': 370506, 'is_employee': False, 'last_modified_date': 1613779801, 'last_access_date': 1710984406, 'reputation_change_year': 170, 'reputation_change_quarter': 170, 'reputation_change_month': 60, 'reputation_change_week': 10, 'reputation_change_day': 0, 'reputation': 8959, 'creation_date': 1303371219, 'user_type': 'registered', 'user_id': 718488, 'accept_rate': 55, 'location': 'United States', 'website_url': 'https://gae123.com', 'link': 'https://stackoverflow.com/users/718488/gae123', 'profile_image': 'https://i.stack.imgur.com/e7ZPl.jpg?s=256&g=1', 'display_name': 'gae123'}",I see an API and many examples on how to parse a yaml file but what about a string?,,0,1,0,0,
997,49112365,49112565,43891,Django redirecting http -> https,2,<python><django><http><redirect><https>,29,"<p>I am running:</p>

<pre><code>python manage.py runserver localhost:44100
</code></pre>

<p>And this is redirecting me to <code>https</code>:</p>

<pre><code>» http http://localhost:44100/
HTTP/1.0 301 Moved Permanently
Content-Type: text/html; charset=utf-8
Date: Mon, 05 Mar 2018 14:09:09 GMT
Location: https://localhost:44100/
Server: WSGIServer/0.1 Python/2.7.14
X-Frame-Options: SAMEORIGIN
</code></pre>

<p>Why / how is this happening? What setting does control whether <code>Django</code> accepts <code>http</code> / <code>https</code>?</p>
",647991,42451,05-03-2018 14:11,05-03-2018 14:22,0,42501,356,63,204,62,"{'badge_counts': {'bronze': 356, 'silver': 204, 'gold': 63}, 'account_id': 326297, 'is_employee': False, 'last_modified_date': 1703320800, 'last_access_date': 1711095658, 'reputation_change_year': 530, 'reputation_change_quarter': 530, 'reputation_change_month': 150, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 42501, 'creation_date': 1299494528, 'user_type': 'registered', 'user_id': 647991, 'accept_rate': 62, 'website_url': '', 'link': 'https://stackoverflow.com/users/647991/bluefast', 'profile_image': 'https://i.stack.imgur.com/yFZCC.jpg?s=256&g=1', 'display_name': 'blueFast'}",I am running: And this is redirecting me to : Why / how is this happening? What setting does control whether accepts / ?,"python manage.py runserver localhost:44100
 https » http http://localhost:44100/
HTTP/1.0 301 Moved Permanently
Content-Type: text/html; charset=utf-8
Date: Mon, 05 Mar 2018 14:09:09 GMT
Location: https://localhost:44100/
Server: WSGIServer/0.1 Python/2.7.14
X-Frame-Options: SAMEORIGIN
 Django http https",2,17,0,0,
998,48482256,48482831,21482,What is the pandas.Panel deprecation warning actually recommending?,1,<python><pandas><numpy><dataframe><python-xarray>,20,"<p>I have a package that uses pandas Panels to generate MultiIndex pandas DataFrames.  However, whenever I use pandas.Panel, I get the following DeprecationError:</p>

<blockquote>
  <p>DeprecationWarning: 
  Panel is deprecated and will be removed in a future version.
  The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method.
  Alternatively, you can use the xarray package <a href=""http://xarray.pydata.org/en/stable/"" rel=""noreferrer"">http://xarray.pydata.org/en/stable/</a>.
      Pandas provides a <code>.to_xarray()</code> method to help automate this conversion.</p>
</blockquote>

<p>However, I can't understand what the first recommendation here is actually recommending in order to create MultiIndex DataFrames.  If Panel is going to be removed, how am I going to be able to use Panel.to_frame?</p>

<hr>

<p>To clarify: I am not asking what deprecation is, or how to convert my Panels to DataFrames.  What I am asking is, if I am using pandas.Panel and then pandas.Panel.to_frame in a library to create MultiIndex DataFrames from 3D ndarrays, and Panels are going to be deprecated, then what is the best option for making those DataFrames without using the Panel API?</p>

<p>Eg, if I'm doing the following, with X as a ndarray with shape (N,J,K):</p>

<pre><code>p = pd.Panel(X, items=item_names, major_axis=names0, minor_axis=names1)
df = p.to_frame()
</code></pre>

<p>this is clearly no longer a viable future-proof option for DataFrame construction, though it was the recommended method in <a href=""https://stackoverflow.com/questions/43427189/3-dimensional-numpy-array-to-multiindex-pandas-dataframe"">this question</a>.</p>
",599265,9700,28-01-2018 01:21,28-01-2018 03:15,0,9699,51,3,33,80,"{'badge_counts': {'bronze': 51, 'silver': 33, 'gold': 3}, 'account_id': 295059, 'is_employee': False, 'last_modified_date': 1711159500, 'last_access_date': 1710864095, 'reputation_change_year': 18, 'reputation_change_quarter': 18, 'reputation_change_month': -1, 'reputation_change_week': 0, 'reputation_change_day': 0, 'reputation': 9699, 'creation_date': 1282696727, 'user_type': 'registered', 'user_id': 599265, 'accept_rate': 80, 'website_url': '', 'link': 'https://stackoverflow.com/users/599265/cge', 'profile_image': 'https://i.stack.imgur.com/NbetJ.jpg?s=256&g=1', 'display_name': 'cge'}","I have a package that uses pandas Panels to generate MultiIndex pandas DataFrames. However, whenever I use pandas.Panel, I get the following DeprecationError: DeprecationWarning: Panel is deprecated and will be removed in a future version. The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method. Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/. Pandas provides a method to help automate this conversion. However, I can't understand what the first recommendation here is actually recommending in order to create MultiIndex DataFrames. If Panel is going to be removed, how am I going to be able to use Panel.to_frame? To clarify: I am not asking what deprecation is, or how to convert my Panels to DataFrames. What I am asking is, if I am using pandas.Panel and then pandas.Panel.to_frame in a library to create MultiIndex DataFrames from 3D ndarrays, and Panels are going to be deprecated, then what is the best option for making those DataFrames without using the Panel API? Eg, if I'm doing the following, with X as a ndarray with shape (N,J,K): this is clearly no longer a viable future-proof option for DataFrame construction, though it was the recommended method in this question.",".to_xarray() p = pd.Panel(X, items=item_names, major_axis=names0, minor_axis=names1)
df = p.to_frame()
",0,23,0,2,
999,48694620,48694686,10919,How to set legend marker size and alpha?,4,<python><matplotlib><plot><legend><seaborn>,13,"<p>I have a <a href=""http://seaborn.pydata.org/"" rel=""noreferrer"">seaborn</a> scatter plot (<code>lmplot</code>) with over 10K points.  In order to perceive all the data, it works better when the plot size is larger (making the markers relatively small) and the alpha on the markers is low.  However, this makes the markers on the legend difficult to distinguish.  <strong>How does one set the marker size and marker alpha in Seaborn?</strong></p>

<p>I see that <code>g._legend</code> has a <code>markersize</code> attribute, but directly setting it doesn't do anything.</p>

<h1>Example</h1>

<pre><code>import numpy as np
import pandas as pd
import seaborn as sns

n_group = 4000

pos = np.concatenate((np.random.randn(n_group,2) + np.array([-1,-1]),
                      np.random.randn(n_group,2) + np.array([0.2, 1.5]),
                      np.random.randn(n_group,2) + np.array([0.6, -1.8])))
df = pd.DataFrame({""x"": pos[:,0], ""y"": pos[:, 1], 
                   ""label"": np.repeat(range(3), n_group)})

g = sns.lmplot(""x"", ""y"", df, hue = ""label"", fit_reg = False, 
               size = 8, scatter_kws = {""alpha"": 0.1})
g._legend.set_title(""Clusters"")
</code></pre>

<p><a href=""https://i.stack.imgur.com/Zahg6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Zahg6.png"" alt=""Scatter plot of three dense clusters of points, with different colors for each cluster. The cluster colors are easily distinguished in the plot, but the markers in the legend are barely visible.""></a></p>
",570918,71923,08-02-2018 20:54,08-02-2018 20:59,0,72206,265,15,195,94,"{'badge_counts': {'bronze': 265, 'silver': 195, 'gold': 15}, 'collectives': [{'collective': {'tags': ['purrr', 'stringr', 'readr', 'knitr', 'dtplyr', 'r-caret', 'shiny-server', 'ggplot2', 'shinyapps', 'r-package', 'rstudio', 'r', 'r-raster', 'quantmod', 'plyr', 'dplyr', 'shinydashboard', 'tibble', 'rlang', 'zoo', 'tidyr', 'data.table', 'tidyverse', 'lubridate', 'forcats', 'shiny', 'rvest'], 'external_links': [{'type': 'support', 'link': 'https://stackoverflow.com/contact?topic=15'}], 'description': 'A collective where data scientists and AI researchers gather to find, share, and learn about R and other subtags like knitr and dplyr.', 'link': '/collectives/r-language', 'name': 'R Language', 'slug': 'r-language'}, 'role': 'member'}], 'account_id': 277129, 'is_employee': False, 'last_modified_date': 1704911701, 'last_access_date': 1711150079, 'reputation_change_year': 2138, 'reputation_change_quarter': 2138, 'reputation_change_month': 523, 'reputation_change_week': 93, 'reputation_change_day': 0, 'reputation': 72206, 'creation_date': 1294730361, 'user_type': 'registered', 'user_id': 570918, 'accept_rate': 94, 'location': 'New York, NY', 'website_url': '', 'link': 'https://stackoverflow.com/users/570918/merv', 'profile_image': 'https://i.stack.imgur.com/t831Y.jpg?s=256&g=1', 'display_name': 'merv'}","I have a seaborn scatter plot () with over 10K points. In order to perceive all the data, it works better when the plot size is larger (making the markers relatively small) and the alpha on the markers is low. However, this makes the markers on the legend difficult to distinguish. How does one set the marker size and marker alpha in Seaborn? I see that has a attribute, but directly setting it doesn't do anything. Example","lmplot g._legend markersize import numpy as np
import pandas as pd
import seaborn as sns

n_group = 4000

pos = np.concatenate((np.random.randn(n_group,2) + np.array([-1,-1]),
                      np.random.randn(n_group,2) + np.array([0.2, 1.5]),
                      np.random.randn(n_group,2) + np.array([0.6, -1.8])))
df = pd.DataFrame({""x"": pos[:,0], ""y"": pos[:, 1], 
                   ""label"": np.repeat(range(3), n_group)})

g = sns.lmplot(""x"", ""y"", df, hue = ""label"", fit_reg = False, 
               size = 8, scatter_kws = {""alpha"": 0.1})
g._legend.set_title(""Clusters"")
",11,24,1,2,
